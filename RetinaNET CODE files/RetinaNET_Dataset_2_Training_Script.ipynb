{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YD73SWu0FhOc",
        "outputId": "37f90961-7cfa-4cf8-f6e8-79926ed2c452"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'RetinaNET'...\n",
            "remote: Enumerating objects: 39, done.\u001b[K\n",
            "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "remote: Total 39 (delta 8), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (39/39), 967.99 KiB | 20.59 MiB/s, done.\n",
            "Resolving deltas: 100% (8/8), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/huzi1998/RetinaNET.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install roboflow\n",
        "%cd /content/RetinaNET\n",
        "\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"HlkYE8VI3uI2kPFjyb9y\")\n",
        "project = rf.workspace(\"persondetection2\").project(\"person_detection-mj13v\")\n",
        "dataset = project.version(1).download(\"retinanet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VV4I1m89GNou",
        "outputId": "14b95a4c-83dd-4cdb-978f-22a23103907e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting roboflow\n",
            "  Downloading roboflow-1.1.7-py3-none-any.whl (58 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/58.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting certifi==2022.12.7 (from roboflow)\n",
            "  Downloading certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/155.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chardet==4.0.0 (from roboflow)\n",
            "  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cycler==0.10.0 (from roboflow)\n",
            "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
            "Collecting idna==2.10 (from roboflow)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.23.5)\n",
            "Collecting opencv-python-headless==4.8.0.74 (from roboflow)\n",
            "  Downloading opencv_python_headless-4.8.0.74-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (9.4.0)\n",
            "Collecting pyparsing==2.4.7 (from roboflow)\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\n",
            "Collecting python-dotenv (from roboflow)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.16.0)\n",
            "Collecting supervision (from roboflow)\n",
            "  Downloading supervision-0.14.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.0.5)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.66.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0.1)\n",
            "Collecting requests-toolbelt (from roboflow)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (1.1.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (4.43.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (23.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (3.2.0)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from supervision->roboflow) (1.11.3)\n",
            "Installing collected packages: python-dotenv, pyparsing, opencv-python-headless, idna, cycler, chardet, certifi, supervision, requests-toolbelt, roboflow\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.1.1\n",
            "    Uninstalling pyparsing-3.1.1:\n",
            "      Successfully uninstalled pyparsing-3.1.1\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.8.1.78\n",
            "    Uninstalling opencv-python-headless-4.8.1.78:\n",
            "      Successfully uninstalled opencv-python-headless-4.8.1.78\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.4\n",
            "    Uninstalling idna-3.4:\n",
            "      Successfully uninstalled idna-3.4\n",
            "  Attempting uninstall: cycler\n",
            "    Found existing installation: cycler 0.12.0\n",
            "    Uninstalling cycler-0.12.0:\n",
            "      Successfully uninstalled cycler-0.12.0\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2023.7.22\n",
            "    Uninstalling certifi-2023.7.22:\n",
            "      Successfully uninstalled certifi-2023.7.22\n",
            "Successfully installed certifi-2022.12.7 chardet-4.0.0 cycler-0.10.0 idna-2.10 opencv-python-headless-4.8.0.74 pyparsing-2.4.7 python-dotenv-1.0.0 requests-toolbelt-1.0.0 roboflow-1.1.7 supervision-0.14.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi",
                  "cycler",
                  "pyparsing"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/RetinaNET\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Exporting format retinanet in progress : 85.0%\n",
            "Version export complete for retinanet format\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in Person_Detection-1 to retinanet:: 100%|██████████| 127363/127363 [00:03<00:00, 34637.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to Person_Detection-1 in retinanet:: 100%|██████████| 1867/1867 [00:00<00:00, 4273.70it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/RetinaNET/extract_images_roboflow.py --folder_path /content/RetinaNET/Person_Detection-1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQQq5WUcGgrS",
        "outputId": "9d50b0e7-93c8-4357-ac64-10ecea4d6efd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images have been copied successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir 'results'"
      ],
      "metadata": {
        "id": "n0jq6tbtGxKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --depth 34 --epochs 150 --dataset csv --csv_train  /content/RetinaNET/Person_Detection-1/train/_annotations.csv --csv_classes /content/RetinaNET/class_list.csv --csv_val /content/RetinaNET/Person_Detection-1/valid/_annotations.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvOrftMvG0-M",
        "outputId": "3fc91ab7-f3f3-4ff8-f8bd-63a49416ec6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch: 142 | Iteration: 47 | Classification loss: 0.00009 | Regression loss: 0.02448 | Running loss: 0.02633\n",
            "Epoch: 142 | Iteration: 48 | Classification loss: 0.00024 | Regression loss: 0.02132 | Running loss: 0.02632\n",
            "Epoch: 142 | Iteration: 49 | Classification loss: 0.00008 | Regression loss: 0.01748 | Running loss: 0.02626\n",
            "Epoch: 142 | Iteration: 50 | Classification loss: 0.00028 | Regression loss: 0.01726 | Running loss: 0.02618\n",
            "Epoch: 142 | Iteration: 51 | Classification loss: 0.00051 | Regression loss: 0.03469 | Running loss: 0.02621\n",
            "Epoch: 142 | Iteration: 52 | Classification loss: 0.00020 | Regression loss: 0.01327 | Running loss: 0.02621\n",
            "Epoch: 142 | Iteration: 53 | Classification loss: 0.00024 | Regression loss: 0.02792 | Running loss: 0.02622\n",
            "Epoch: 142 | Iteration: 54 | Classification loss: 0.00117 | Regression loss: 0.04046 | Running loss: 0.02624\n",
            "Epoch: 142 | Iteration: 55 | Classification loss: 0.00057 | Regression loss: 0.02967 | Running loss: 0.02626\n",
            "Epoch: 142 | Iteration: 56 | Classification loss: 0.00010 | Regression loss: 0.01127 | Running loss: 0.02621\n",
            "Epoch: 142 | Iteration: 57 | Classification loss: 0.00077 | Regression loss: 0.03877 | Running loss: 0.02625\n",
            "Epoch: 142 | Iteration: 58 | Classification loss: 0.00020 | Regression loss: 0.03050 | Running loss: 0.02623\n",
            "Epoch: 142 | Iteration: 59 | Classification loss: 0.00019 | Regression loss: 0.02267 | Running loss: 0.02624\n",
            "Epoch: 142 | Iteration: 60 | Classification loss: 0.00026 | Regression loss: 0.01541 | Running loss: 0.02622\n",
            "Epoch: 142 | Iteration: 61 | Classification loss: 0.00110 | Regression loss: 0.04499 | Running loss: 0.02626\n",
            "Epoch: 142 | Iteration: 62 | Classification loss: 0.00075 | Regression loss: 0.03321 | Running loss: 0.02626\n",
            "Epoch: 142 | Iteration: 63 | Classification loss: 0.00041 | Regression loss: 0.04219 | Running loss: 0.02628\n",
            "Epoch: 142 | Iteration: 64 | Classification loss: 0.00018 | Regression loss: 0.03125 | Running loss: 0.02631\n",
            "Epoch: 142 | Iteration: 65 | Classification loss: 0.00030 | Regression loss: 0.02532 | Running loss: 0.02633\n",
            "Epoch: 142 | Iteration: 66 | Classification loss: 0.00039 | Regression loss: 0.02244 | Running loss: 0.02634\n",
            "Epoch: 142 | Iteration: 67 | Classification loss: 0.00067 | Regression loss: 0.03575 | Running loss: 0.02637\n",
            "Epoch: 142 | Iteration: 68 | Classification loss: 0.00019 | Regression loss: 0.01947 | Running loss: 0.02639\n",
            "Epoch: 142 | Iteration: 69 | Classification loss: 0.00034 | Regression loss: 0.02036 | Running loss: 0.02639\n",
            "Epoch: 142 | Iteration: 70 | Classification loss: 0.00009 | Regression loss: 0.02282 | Running loss: 0.02637\n",
            "Epoch: 142 | Iteration: 71 | Classification loss: 0.00015 | Regression loss: 0.01675 | Running loss: 0.02634\n",
            "Epoch: 142 | Iteration: 72 | Classification loss: 0.00020 | Regression loss: 0.02727 | Running loss: 0.02634\n",
            "Epoch: 142 | Iteration: 73 | Classification loss: 0.00035 | Regression loss: 0.02151 | Running loss: 0.02632\n",
            "Epoch: 142 | Iteration: 74 | Classification loss: 0.00017 | Regression loss: 0.01838 | Running loss: 0.02631\n",
            "Epoch: 142 | Iteration: 75 | Classification loss: 0.00034 | Regression loss: 0.02955 | Running loss: 0.02633\n",
            "Epoch: 142 | Iteration: 76 | Classification loss: 0.00046 | Regression loss: 0.03090 | Running loss: 0.02635\n",
            "Epoch: 142 | Iteration: 77 | Classification loss: 0.00081 | Regression loss: 0.04258 | Running loss: 0.02640\n",
            "Epoch: 142 | Iteration: 78 | Classification loss: 0.00030 | Regression loss: 0.01645 | Running loss: 0.02638\n",
            "Epoch: 142 | Iteration: 79 | Classification loss: 0.00032 | Regression loss: 0.02174 | Running loss: 0.02637\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7163095403698455\n",
            "Precision:  0.550561797752809\n",
            "Recall:  0.7945945945945946\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}]\n",
            "Epoch: 143 | Iteration: 0 | Classification loss: 0.00023 | Regression loss: 0.01292 | Running loss: 0.02632\n",
            "Epoch: 143 | Iteration: 1 | Classification loss: 0.00024 | Regression loss: 0.03777 | Running loss: 0.02635\n",
            "Epoch: 143 | Iteration: 2 | Classification loss: 0.00034 | Regression loss: 0.02116 | Running loss: 0.02630\n",
            "Epoch: 143 | Iteration: 3 | Classification loss: 0.00028 | Regression loss: 0.03987 | Running loss: 0.02635\n",
            "Epoch: 143 | Iteration: 4 | Classification loss: 0.00031 | Regression loss: 0.04186 | Running loss: 0.02638\n",
            "Epoch: 143 | Iteration: 5 | Classification loss: 0.00021 | Regression loss: 0.01811 | Running loss: 0.02633\n",
            "Epoch: 143 | Iteration: 6 | Classification loss: 0.00080 | Regression loss: 0.02474 | Running loss: 0.02633\n",
            "Epoch: 143 | Iteration: 7 | Classification loss: 0.00025 | Regression loss: 0.02646 | Running loss: 0.02635\n",
            "Epoch: 143 | Iteration: 8 | Classification loss: 0.00018 | Regression loss: 0.02243 | Running loss: 0.02635\n",
            "Epoch: 143 | Iteration: 9 | Classification loss: 0.00008 | Regression loss: 0.02352 | Running loss: 0.02639\n",
            "Epoch: 143 | Iteration: 10 | Classification loss: 0.00014 | Regression loss: 0.01128 | Running loss: 0.02631\n",
            "Epoch: 143 | Iteration: 11 | Classification loss: 0.00026 | Regression loss: 0.02481 | Running loss: 0.02631\n",
            "Epoch: 143 | Iteration: 12 | Classification loss: 0.00054 | Regression loss: 0.03518 | Running loss: 0.02635\n",
            "Epoch: 143 | Iteration: 13 | Classification loss: 0.00037 | Regression loss: 0.01639 | Running loss: 0.02632\n",
            "Epoch: 143 | Iteration: 14 | Classification loss: 0.00011 | Regression loss: 0.00719 | Running loss: 0.02629\n",
            "Epoch: 143 | Iteration: 15 | Classification loss: 0.00043 | Regression loss: 0.03738 | Running loss: 0.02632\n",
            "Epoch: 143 | Iteration: 16 | Classification loss: 0.00027 | Regression loss: 0.01892 | Running loss: 0.02630\n",
            "Epoch: 143 | Iteration: 17 | Classification loss: 0.00014 | Regression loss: 0.02255 | Running loss: 0.02629\n",
            "Epoch: 143 | Iteration: 18 | Classification loss: 0.00019 | Regression loss: 0.02498 | Running loss: 0.02631\n",
            "Epoch: 143 | Iteration: 19 | Classification loss: 0.00020 | Regression loss: 0.02236 | Running loss: 0.02631\n",
            "Epoch: 143 | Iteration: 20 | Classification loss: 0.00008 | Regression loss: 0.02057 | Running loss: 0.02631\n",
            "Epoch: 143 | Iteration: 21 | Classification loss: 0.00040 | Regression loss: 0.02771 | Running loss: 0.02631\n",
            "Epoch: 143 | Iteration: 22 | Classification loss: 0.00022 | Regression loss: 0.02216 | Running loss: 0.02631\n",
            "Epoch: 143 | Iteration: 23 | Classification loss: 0.00006 | Regression loss: 0.01190 | Running loss: 0.02629\n",
            "Epoch: 143 | Iteration: 24 | Classification loss: 0.00077 | Regression loss: 0.05565 | Running loss: 0.02634\n",
            "Epoch: 143 | Iteration: 25 | Classification loss: 0.00046 | Regression loss: 0.04659 | Running loss: 0.02639\n",
            "Epoch: 143 | Iteration: 26 | Classification loss: 0.00077 | Regression loss: 0.02486 | Running loss: 0.02641\n",
            "Epoch: 143 | Iteration: 27 | Classification loss: 0.00026 | Regression loss: 0.01767 | Running loss: 0.02640\n",
            "Epoch: 143 | Iteration: 28 | Classification loss: 0.00018 | Regression loss: 0.01776 | Running loss: 0.02639\n",
            "Epoch: 143 | Iteration: 29 | Classification loss: 0.00037 | Regression loss: 0.03638 | Running loss: 0.02638\n",
            "Epoch: 143 | Iteration: 30 | Classification loss: 0.00014 | Regression loss: 0.02846 | Running loss: 0.02635\n",
            "Epoch: 143 | Iteration: 31 | Classification loss: 0.00016 | Regression loss: 0.01670 | Running loss: 0.02635\n",
            "Epoch: 143 | Iteration: 32 | Classification loss: 0.00015 | Regression loss: 0.01436 | Running loss: 0.02631\n",
            "Epoch: 143 | Iteration: 33 | Classification loss: 0.00028 | Regression loss: 0.02762 | Running loss: 0.02633\n",
            "Epoch: 143 | Iteration: 34 | Classification loss: 0.00008 | Regression loss: 0.02739 | Running loss: 0.02631\n",
            "Epoch: 143 | Iteration: 35 | Classification loss: 0.00030 | Regression loss: 0.01517 | Running loss: 0.02628\n",
            "Epoch: 143 | Iteration: 36 | Classification loss: 0.00023 | Regression loss: 0.02071 | Running loss: 0.02629\n",
            "Epoch: 143 | Iteration: 37 | Classification loss: 0.00012 | Regression loss: 0.01387 | Running loss: 0.02628\n",
            "Epoch: 143 | Iteration: 38 | Classification loss: 0.00075 | Regression loss: 0.04223 | Running loss: 0.02631\n",
            "Epoch: 143 | Iteration: 39 | Classification loss: 0.00013 | Regression loss: 0.01976 | Running loss: 0.02632\n",
            "Epoch: 143 | Iteration: 40 | Classification loss: 0.00106 | Regression loss: 0.04500 | Running loss: 0.02637\n",
            "Epoch: 143 | Iteration: 41 | Classification loss: 0.00062 | Regression loss: 0.02896 | Running loss: 0.02635\n",
            "Epoch: 143 | Iteration: 42 | Classification loss: 0.00032 | Regression loss: 0.01395 | Running loss: 0.02633\n",
            "Epoch: 143 | Iteration: 43 | Classification loss: 0.00027 | Regression loss: 0.02485 | Running loss: 0.02628\n",
            "Epoch: 143 | Iteration: 44 | Classification loss: 0.00105 | Regression loss: 0.04005 | Running loss: 0.02632\n",
            "Epoch: 143 | Iteration: 45 | Classification loss: 0.00011 | Regression loss: 0.02306 | Running loss: 0.02631\n",
            "Epoch: 143 | Iteration: 46 | Classification loss: 0.00018 | Regression loss: 0.01791 | Running loss: 0.02626\n",
            "Epoch: 143 | Iteration: 47 | Classification loss: 0.00024 | Regression loss: 0.01631 | Running loss: 0.02627\n",
            "Epoch: 143 | Iteration: 48 | Classification loss: 0.00011 | Regression loss: 0.01902 | Running loss: 0.02625\n",
            "Epoch: 143 | Iteration: 49 | Classification loss: 0.00010 | Regression loss: 0.01599 | Running loss: 0.02620\n",
            "Epoch: 143 | Iteration: 50 | Classification loss: 0.00046 | Regression loss: 0.02868 | Running loss: 0.02622\n",
            "Epoch: 143 | Iteration: 51 | Classification loss: 0.00017 | Regression loss: 0.01971 | Running loss: 0.02623\n",
            "Epoch: 143 | Iteration: 52 | Classification loss: 0.00094 | Regression loss: 0.02361 | Running loss: 0.02624\n",
            "Epoch: 143 | Iteration: 53 | Classification loss: 0.00037 | Regression loss: 0.02023 | Running loss: 0.02623\n",
            "Epoch: 143 | Iteration: 54 | Classification loss: 0.00024 | Regression loss: 0.02253 | Running loss: 0.02625\n",
            "Epoch: 143 | Iteration: 55 | Classification loss: 0.00027 | Regression loss: 0.02384 | Running loss: 0.02625\n",
            "Epoch: 143 | Iteration: 56 | Classification loss: 0.00007 | Regression loss: 0.01807 | Running loss: 0.02620\n",
            "Epoch: 143 | Iteration: 57 | Classification loss: 0.00097 | Regression loss: 0.03678 | Running loss: 0.02624\n",
            "Epoch: 143 | Iteration: 58 | Classification loss: 0.00028 | Regression loss: 0.01522 | Running loss: 0.02621\n",
            "Epoch: 143 | Iteration: 59 | Classification loss: 0.00018 | Regression loss: 0.01916 | Running loss: 0.02614\n",
            "Epoch: 143 | Iteration: 60 | Classification loss: 0.00073 | Regression loss: 0.03696 | Running loss: 0.02616\n",
            "Epoch: 143 | Iteration: 61 | Classification loss: 0.00033 | Regression loss: 0.02452 | Running loss: 0.02615\n",
            "Epoch: 143 | Iteration: 62 | Classification loss: 0.00028 | Regression loss: 0.02814 | Running loss: 0.02617\n",
            "Epoch: 143 | Iteration: 63 | Classification loss: 0.00076 | Regression loss: 0.03941 | Running loss: 0.02618\n",
            "Epoch: 143 | Iteration: 64 | Classification loss: 0.00067 | Regression loss: 0.04946 | Running loss: 0.02625\n",
            "Epoch: 143 | Iteration: 65 | Classification loss: 0.00059 | Regression loss: 0.02978 | Running loss: 0.02625\n",
            "Epoch: 143 | Iteration: 66 | Classification loss: 0.00052 | Regression loss: 0.03509 | Running loss: 0.02624\n",
            "Epoch: 143 | Iteration: 67 | Classification loss: 0.00011 | Regression loss: 0.02631 | Running loss: 0.02627\n",
            "Epoch: 143 | Iteration: 68 | Classification loss: 0.00007 | Regression loss: 0.01027 | Running loss: 0.02623\n",
            "Epoch: 143 | Iteration: 69 | Classification loss: 0.00036 | Regression loss: 0.02132 | Running loss: 0.02626\n",
            "Epoch: 143 | Iteration: 70 | Classification loss: 0.00024 | Regression loss: 0.01050 | Running loss: 0.02626\n",
            "Epoch: 143 | Iteration: 71 | Classification loss: 0.00062 | Regression loss: 0.03602 | Running loss: 0.02628\n",
            "Epoch: 143 | Iteration: 72 | Classification loss: 0.00071 | Regression loss: 0.04187 | Running loss: 0.02632\n",
            "Epoch: 143 | Iteration: 73 | Classification loss: 0.00004 | Regression loss: 0.00538 | Running loss: 0.02624\n",
            "Epoch: 143 | Iteration: 74 | Classification loss: 0.00017 | Regression loss: 0.03373 | Running loss: 0.02625\n",
            "Epoch: 143 | Iteration: 75 | Classification loss: 0.00009 | Regression loss: 0.00921 | Running loss: 0.02620\n",
            "Epoch: 143 | Iteration: 76 | Classification loss: 0.00041 | Regression loss: 0.02415 | Running loss: 0.02618\n",
            "Epoch: 143 | Iteration: 77 | Classification loss: 0.00018 | Regression loss: 0.03233 | Running loss: 0.02617\n",
            "Epoch: 143 | Iteration: 78 | Classification loss: 0.00034 | Regression loss: 0.03101 | Running loss: 0.02617\n",
            "Epoch: 143 | Iteration: 79 | Classification loss: 0.00013 | Regression loss: 0.01637 | Running loss: 0.02618\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7203252039292192\n",
            "Precision:  0.556390977443609\n",
            "Recall:  0.8\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}]\n",
            "Epoch: 144 | Iteration: 0 | Classification loss: 0.00022 | Regression loss: 0.02218 | Running loss: 0.02617\n",
            "Epoch: 144 | Iteration: 1 | Classification loss: 0.00007 | Regression loss: 0.01501 | Running loss: 0.02613\n",
            "Epoch: 144 | Iteration: 2 | Classification loss: 0.00027 | Regression loss: 0.02754 | Running loss: 0.02616\n",
            "Epoch: 144 | Iteration: 3 | Classification loss: 0.00034 | Regression loss: 0.03620 | Running loss: 0.02619\n",
            "Epoch: 144 | Iteration: 4 | Classification loss: 0.00075 | Regression loss: 0.02322 | Running loss: 0.02616\n",
            "Epoch: 144 | Iteration: 5 | Classification loss: 0.00067 | Regression loss: 0.02513 | Running loss: 0.02612\n",
            "Epoch: 144 | Iteration: 6 | Classification loss: 0.00029 | Regression loss: 0.02815 | Running loss: 0.02614\n",
            "Epoch: 144 | Iteration: 7 | Classification loss: 0.00013 | Regression loss: 0.02207 | Running loss: 0.02614\n",
            "Epoch: 144 | Iteration: 8 | Classification loss: 0.00018 | Regression loss: 0.01713 | Running loss: 0.02612\n",
            "Epoch: 144 | Iteration: 9 | Classification loss: 0.00037 | Regression loss: 0.02272 | Running loss: 0.02611\n",
            "Epoch: 144 | Iteration: 10 | Classification loss: 0.00062 | Regression loss: 0.05343 | Running loss: 0.02619\n",
            "Epoch: 144 | Iteration: 11 | Classification loss: 0.00025 | Regression loss: 0.02206 | Running loss: 0.02619\n",
            "Epoch: 144 | Iteration: 12 | Classification loss: 0.00016 | Regression loss: 0.01104 | Running loss: 0.02612\n",
            "Epoch: 144 | Iteration: 13 | Classification loss: 0.00032 | Regression loss: 0.04313 | Running loss: 0.02619\n",
            "Epoch: 144 | Iteration: 14 | Classification loss: 0.00030 | Regression loss: 0.02655 | Running loss: 0.02620\n",
            "Epoch: 144 | Iteration: 15 | Classification loss: 0.00031 | Regression loss: 0.02086 | Running loss: 0.02620\n",
            "Epoch: 144 | Iteration: 16 | Classification loss: 0.00004 | Regression loss: 0.01149 | Running loss: 0.02619\n",
            "Epoch: 144 | Iteration: 17 | Classification loss: 0.00017 | Regression loss: 0.02855 | Running loss: 0.02619\n",
            "Epoch: 144 | Iteration: 18 | Classification loss: 0.00016 | Regression loss: 0.02489 | Running loss: 0.02614\n",
            "Epoch: 144 | Iteration: 19 | Classification loss: 0.00054 | Regression loss: 0.02929 | Running loss: 0.02615\n",
            "Epoch: 144 | Iteration: 20 | Classification loss: 0.00017 | Regression loss: 0.03146 | Running loss: 0.02617\n",
            "Epoch: 144 | Iteration: 21 | Classification loss: 0.00026 | Regression loss: 0.02332 | Running loss: 0.02620\n",
            "Epoch: 144 | Iteration: 22 | Classification loss: 0.00101 | Regression loss: 0.04228 | Running loss: 0.02622\n",
            "Epoch: 144 | Iteration: 23 | Classification loss: 0.00070 | Regression loss: 0.03280 | Running loss: 0.02622\n",
            "Epoch: 144 | Iteration: 24 | Classification loss: 0.00031 | Regression loss: 0.02293 | Running loss: 0.02618\n",
            "Epoch: 144 | Iteration: 25 | Classification loss: 0.00047 | Regression loss: 0.03474 | Running loss: 0.02621\n",
            "Epoch: 144 | Iteration: 26 | Classification loss: 0.00017 | Regression loss: 0.01636 | Running loss: 0.02621\n",
            "Epoch: 144 | Iteration: 27 | Classification loss: 0.00015 | Regression loss: 0.03138 | Running loss: 0.02625\n",
            "Epoch: 144 | Iteration: 28 | Classification loss: 0.00010 | Regression loss: 0.03033 | Running loss: 0.02625\n",
            "Epoch: 144 | Iteration: 29 | Classification loss: 0.00006 | Regression loss: 0.01919 | Running loss: 0.02626\n",
            "Epoch: 144 | Iteration: 30 | Classification loss: 0.00055 | Regression loss: 0.03809 | Running loss: 0.02628\n",
            "Epoch: 144 | Iteration: 31 | Classification loss: 0.00034 | Regression loss: 0.03630 | Running loss: 0.02630\n",
            "Epoch: 144 | Iteration: 32 | Classification loss: 0.00027 | Regression loss: 0.02350 | Running loss: 0.02626\n",
            "Epoch: 144 | Iteration: 33 | Classification loss: 0.00024 | Regression loss: 0.02032 | Running loss: 0.02626\n",
            "Epoch: 144 | Iteration: 34 | Classification loss: 0.00042 | Regression loss: 0.03711 | Running loss: 0.02629\n",
            "Epoch: 144 | Iteration: 35 | Classification loss: 0.00020 | Regression loss: 0.01817 | Running loss: 0.02624\n",
            "Epoch: 144 | Iteration: 36 | Classification loss: 0.00009 | Regression loss: 0.02282 | Running loss: 0.02622\n",
            "Epoch: 144 | Iteration: 37 | Classification loss: 0.00023 | Regression loss: 0.02544 | Running loss: 0.02618\n",
            "Epoch: 144 | Iteration: 38 | Classification loss: 0.00025 | Regression loss: 0.01525 | Running loss: 0.02617\n",
            "Epoch: 144 | Iteration: 39 | Classification loss: 0.00024 | Regression loss: 0.02401 | Running loss: 0.02618\n",
            "Epoch: 144 | Iteration: 40 | Classification loss: 0.00038 | Regression loss: 0.02948 | Running loss: 0.02618\n",
            "Epoch: 144 | Iteration: 41 | Classification loss: 0.00059 | Regression loss: 0.04533 | Running loss: 0.02623\n",
            "Epoch: 144 | Iteration: 42 | Classification loss: 0.00025 | Regression loss: 0.01761 | Running loss: 0.02624\n",
            "Epoch: 144 | Iteration: 43 | Classification loss: 0.00011 | Regression loss: 0.01864 | Running loss: 0.02625\n",
            "Epoch: 144 | Iteration: 44 | Classification loss: 0.00033 | Regression loss: 0.01988 | Running loss: 0.02623\n",
            "Epoch: 144 | Iteration: 45 | Classification loss: 0.00033 | Regression loss: 0.02869 | Running loss: 0.02622\n",
            "Epoch: 144 | Iteration: 46 | Classification loss: 0.00057 | Regression loss: 0.02813 | Running loss: 0.02626\n",
            "Epoch: 144 | Iteration: 47 | Classification loss: 0.00004 | Regression loss: 0.00522 | Running loss: 0.02620\n",
            "Epoch: 144 | Iteration: 48 | Classification loss: 0.00016 | Regression loss: 0.01976 | Running loss: 0.02620\n",
            "Epoch: 144 | Iteration: 49 | Classification loss: 0.00066 | Regression loss: 0.04504 | Running loss: 0.02623\n",
            "Epoch: 144 | Iteration: 50 | Classification loss: 0.00036 | Regression loss: 0.02098 | Running loss: 0.02622\n",
            "Epoch: 144 | Iteration: 51 | Classification loss: 0.00050 | Regression loss: 0.03630 | Running loss: 0.02625\n",
            "Epoch: 144 | Iteration: 52 | Classification loss: 0.00015 | Regression loss: 0.01613 | Running loss: 0.02623\n",
            "Epoch: 144 | Iteration: 53 | Classification loss: 0.00011 | Regression loss: 0.01851 | Running loss: 0.02623\n",
            "Epoch: 144 | Iteration: 54 | Classification loss: 0.00030 | Regression loss: 0.04461 | Running loss: 0.02624\n",
            "Epoch: 144 | Iteration: 55 | Classification loss: 0.00032 | Regression loss: 0.01380 | Running loss: 0.02623\n",
            "Epoch: 144 | Iteration: 56 | Classification loss: 0.00008 | Regression loss: 0.01798 | Running loss: 0.02617\n",
            "Epoch: 144 | Iteration: 57 | Classification loss: 0.00013 | Regression loss: 0.03253 | Running loss: 0.02617\n",
            "Epoch: 144 | Iteration: 58 | Classification loss: 0.00016 | Regression loss: 0.01255 | Running loss: 0.02613\n",
            "Epoch: 144 | Iteration: 59 | Classification loss: 0.00074 | Regression loss: 0.04496 | Running loss: 0.02613\n",
            "Epoch: 144 | Iteration: 60 | Classification loss: 0.00018 | Regression loss: 0.01593 | Running loss: 0.02612\n",
            "Epoch: 144 | Iteration: 61 | Classification loss: 0.00010 | Regression loss: 0.00650 | Running loss: 0.02611\n",
            "Epoch: 144 | Iteration: 62 | Classification loss: 0.00006 | Regression loss: 0.00784 | Running loss: 0.02609\n",
            "Epoch: 144 | Iteration: 63 | Classification loss: 0.00012 | Regression loss: 0.01339 | Running loss: 0.02608\n",
            "Epoch: 144 | Iteration: 64 | Classification loss: 0.00046 | Regression loss: 0.04472 | Running loss: 0.02614\n",
            "Epoch: 144 | Iteration: 65 | Classification loss: 0.00033 | Regression loss: 0.01583 | Running loss: 0.02613\n",
            "Epoch: 144 | Iteration: 66 | Classification loss: 0.00017 | Regression loss: 0.01569 | Running loss: 0.02611\n",
            "Epoch: 144 | Iteration: 67 | Classification loss: 0.00080 | Regression loss: 0.04228 | Running loss: 0.02616\n",
            "Epoch: 144 | Iteration: 68 | Classification loss: 0.00030 | Regression loss: 0.02394 | Running loss: 0.02616\n",
            "Epoch: 144 | Iteration: 69 | Classification loss: 0.00013 | Regression loss: 0.02793 | Running loss: 0.02616\n",
            "Epoch: 144 | Iteration: 70 | Classification loss: 0.00096 | Regression loss: 0.02524 | Running loss: 0.02616\n",
            "Epoch: 144 | Iteration: 71 | Classification loss: 0.00026 | Regression loss: 0.02071 | Running loss: 0.02617\n",
            "Epoch: 144 | Iteration: 72 | Classification loss: 0.00022 | Regression loss: 0.02170 | Running loss: 0.02615\n",
            "Epoch: 144 | Iteration: 73 | Classification loss: 0.00023 | Regression loss: 0.01057 | Running loss: 0.02610\n",
            "Epoch: 144 | Iteration: 74 | Classification loss: 0.00030 | Regression loss: 0.01660 | Running loss: 0.02606\n",
            "Epoch: 144 | Iteration: 75 | Classification loss: 0.00075 | Regression loss: 0.02395 | Running loss: 0.02607\n",
            "Epoch: 144 | Iteration: 76 | Classification loss: 0.00009 | Regression loss: 0.01003 | Running loss: 0.02604\n",
            "Epoch: 144 | Iteration: 77 | Classification loss: 0.00024 | Regression loss: 0.02231 | Running loss: 0.02604\n",
            "Epoch: 144 | Iteration: 78 | Classification loss: 0.00099 | Regression loss: 0.03629 | Running loss: 0.02606\n",
            "Epoch: 144 | Iteration: 79 | Classification loss: 0.00103 | Regression loss: 0.03945 | Running loss: 0.02609\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7425360533360579\n",
            "Precision:  0.5522388059701493\n",
            "Recall:  0.8\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}]\n",
            "Epoch: 145 | Iteration: 0 | Classification loss: 0.00025 | Regression loss: 0.01693 | Running loss: 0.02605\n",
            "Epoch: 145 | Iteration: 1 | Classification loss: 0.00065 | Regression loss: 0.04599 | Running loss: 0.02609\n",
            "Epoch: 145 | Iteration: 2 | Classification loss: 0.00063 | Regression loss: 0.05407 | Running loss: 0.02617\n",
            "Epoch: 145 | Iteration: 3 | Classification loss: 0.00017 | Regression loss: 0.02547 | Running loss: 0.02617\n",
            "Epoch: 145 | Iteration: 4 | Classification loss: 0.00014 | Regression loss: 0.02954 | Running loss: 0.02617\n",
            "Epoch: 145 | Iteration: 5 | Classification loss: 0.00026 | Regression loss: 0.01553 | Running loss: 0.02615\n",
            "Epoch: 145 | Iteration: 6 | Classification loss: 0.00018 | Regression loss: 0.01863 | Running loss: 0.02611\n",
            "Epoch: 145 | Iteration: 7 | Classification loss: 0.00013 | Regression loss: 0.01960 | Running loss: 0.02607\n",
            "Epoch: 145 | Iteration: 8 | Classification loss: 0.00009 | Regression loss: 0.00671 | Running loss: 0.02604\n",
            "Epoch: 145 | Iteration: 9 | Classification loss: 0.00060 | Regression loss: 0.02583 | Running loss: 0.02608\n",
            "Epoch: 145 | Iteration: 10 | Classification loss: 0.00035 | Regression loss: 0.01989 | Running loss: 0.02608\n",
            "Epoch: 145 | Iteration: 11 | Classification loss: 0.00014 | Regression loss: 0.01020 | Running loss: 0.02603\n",
            "Epoch: 145 | Iteration: 12 | Classification loss: 0.00017 | Regression loss: 0.01764 | Running loss: 0.02598\n",
            "Epoch: 145 | Iteration: 13 | Classification loss: 0.00017 | Regression loss: 0.01642 | Running loss: 0.02599\n",
            "Epoch: 145 | Iteration: 14 | Classification loss: 0.00029 | Regression loss: 0.02535 | Running loss: 0.02598\n",
            "Epoch: 145 | Iteration: 15 | Classification loss: 0.00010 | Regression loss: 0.02262 | Running loss: 0.02593\n",
            "Epoch: 145 | Iteration: 16 | Classification loss: 0.00061 | Regression loss: 0.04166 | Running loss: 0.02598\n",
            "Epoch: 145 | Iteration: 17 | Classification loss: 0.00046 | Regression loss: 0.02973 | Running loss: 0.02601\n",
            "Epoch: 145 | Iteration: 18 | Classification loss: 0.00064 | Regression loss: 0.04440 | Running loss: 0.02599\n",
            "Epoch: 145 | Iteration: 19 | Classification loss: 0.00021 | Regression loss: 0.01442 | Running loss: 0.02595\n",
            "Epoch: 145 | Iteration: 20 | Classification loss: 0.00029 | Regression loss: 0.01826 | Running loss: 0.02592\n",
            "Epoch: 145 | Iteration: 21 | Classification loss: 0.00050 | Regression loss: 0.02591 | Running loss: 0.02591\n",
            "Epoch: 145 | Iteration: 22 | Classification loss: 0.00074 | Regression loss: 0.03461 | Running loss: 0.02595\n",
            "Epoch: 145 | Iteration: 23 | Classification loss: 0.00013 | Regression loss: 0.02573 | Running loss: 0.02598\n",
            "Epoch: 145 | Iteration: 24 | Classification loss: 0.00021 | Regression loss: 0.01533 | Running loss: 0.02592\n",
            "Epoch: 145 | Iteration: 25 | Classification loss: 0.00011 | Regression loss: 0.02397 | Running loss: 0.02593\n",
            "Epoch: 145 | Iteration: 26 | Classification loss: 0.00054 | Regression loss: 0.03504 | Running loss: 0.02596\n",
            "Epoch: 145 | Iteration: 27 | Classification loss: 0.00021 | Regression loss: 0.02020 | Running loss: 0.02594\n",
            "Epoch: 145 | Iteration: 28 | Classification loss: 0.00096 | Regression loss: 0.04544 | Running loss: 0.02600\n",
            "Epoch: 145 | Iteration: 29 | Classification loss: 0.00012 | Regression loss: 0.01774 | Running loss: 0.02599\n",
            "Epoch: 145 | Iteration: 30 | Classification loss: 0.00007 | Regression loss: 0.00983 | Running loss: 0.02593\n",
            "Epoch: 145 | Iteration: 31 | Classification loss: 0.00009 | Regression loss: 0.02616 | Running loss: 0.02593\n",
            "Epoch: 145 | Iteration: 32 | Classification loss: 0.00025 | Regression loss: 0.02638 | Running loss: 0.02590\n",
            "Epoch: 145 | Iteration: 33 | Classification loss: 0.00015 | Regression loss: 0.01126 | Running loss: 0.02588\n",
            "Epoch: 145 | Iteration: 34 | Classification loss: 0.00022 | Regression loss: 0.02168 | Running loss: 0.02588\n",
            "Epoch: 145 | Iteration: 35 | Classification loss: 0.00031 | Regression loss: 0.04272 | Running loss: 0.02589\n",
            "Epoch: 145 | Iteration: 36 | Classification loss: 0.00010 | Regression loss: 0.02246 | Running loss: 0.02588\n",
            "Epoch: 145 | Iteration: 37 | Classification loss: 0.00014 | Regression loss: 0.02844 | Running loss: 0.02585\n",
            "Epoch: 145 | Iteration: 38 | Classification loss: 0.00034 | Regression loss: 0.02166 | Running loss: 0.02580\n",
            "Epoch: 145 | Iteration: 39 | Classification loss: 0.00008 | Regression loss: 0.01754 | Running loss: 0.02575\n",
            "Epoch: 145 | Iteration: 40 | Classification loss: 0.00023 | Regression loss: 0.01808 | Running loss: 0.02573\n",
            "Epoch: 145 | Iteration: 41 | Classification loss: 0.00056 | Regression loss: 0.03898 | Running loss: 0.02577\n",
            "Epoch: 145 | Iteration: 42 | Classification loss: 0.00010 | Regression loss: 0.01927 | Running loss: 0.02578\n",
            "Epoch: 145 | Iteration: 43 | Classification loss: 0.00083 | Regression loss: 0.02674 | Running loss: 0.02580\n",
            "Epoch: 145 | Iteration: 44 | Classification loss: 0.00020 | Regression loss: 0.01311 | Running loss: 0.02579\n",
            "Epoch: 145 | Iteration: 45 | Classification loss: 0.00047 | Regression loss: 0.02816 | Running loss: 0.02576\n",
            "Epoch: 145 | Iteration: 46 | Classification loss: 0.00026 | Regression loss: 0.02856 | Running loss: 0.02579\n",
            "Epoch: 145 | Iteration: 47 | Classification loss: 0.00027 | Regression loss: 0.02065 | Running loss: 0.02579\n",
            "Epoch: 145 | Iteration: 48 | Classification loss: 0.00072 | Regression loss: 0.04321 | Running loss: 0.02583\n",
            "Epoch: 145 | Iteration: 49 | Classification loss: 0.00041 | Regression loss: 0.03099 | Running loss: 0.02585\n",
            "Epoch: 145 | Iteration: 50 | Classification loss: 0.00022 | Regression loss: 0.03378 | Running loss: 0.02591\n",
            "Epoch: 145 | Iteration: 51 | Classification loss: 0.00017 | Regression loss: 0.01883 | Running loss: 0.02586\n",
            "Epoch: 145 | Iteration: 52 | Classification loss: 0.00015 | Regression loss: 0.01625 | Running loss: 0.02584\n",
            "Epoch: 145 | Iteration: 53 | Classification loss: 0.00039 | Regression loss: 0.02199 | Running loss: 0.02582\n",
            "Epoch: 145 | Iteration: 54 | Classification loss: 0.00033 | Regression loss: 0.02568 | Running loss: 0.02584\n",
            "Epoch: 145 | Iteration: 55 | Classification loss: 0.00011 | Regression loss: 0.02919 | Running loss: 0.02585\n",
            "Epoch: 145 | Iteration: 56 | Classification loss: 0.00007 | Regression loss: 0.00794 | Running loss: 0.02579\n",
            "Epoch: 145 | Iteration: 57 | Classification loss: 0.00020 | Regression loss: 0.02190 | Running loss: 0.02579\n",
            "Epoch: 145 | Iteration: 58 | Classification loss: 0.00019 | Regression loss: 0.01846 | Running loss: 0.02580\n",
            "Epoch: 145 | Iteration: 59 | Classification loss: 0.00003 | Regression loss: 0.00551 | Running loss: 0.02575\n",
            "Epoch: 145 | Iteration: 60 | Classification loss: 0.00040 | Regression loss: 0.03441 | Running loss: 0.02579\n",
            "Epoch: 145 | Iteration: 61 | Classification loss: 0.00020 | Regression loss: 0.01999 | Running loss: 0.02578\n",
            "Epoch: 145 | Iteration: 62 | Classification loss: 0.00011 | Regression loss: 0.01651 | Running loss: 0.02577\n",
            "Epoch: 145 | Iteration: 63 | Classification loss: 0.00039 | Regression loss: 0.03932 | Running loss: 0.02582\n",
            "Epoch: 145 | Iteration: 64 | Classification loss: 0.00098 | Regression loss: 0.03916 | Running loss: 0.02586\n",
            "Epoch: 145 | Iteration: 65 | Classification loss: 0.00035 | Regression loss: 0.02583 | Running loss: 0.02586\n",
            "Epoch: 145 | Iteration: 66 | Classification loss: 0.00026 | Regression loss: 0.03464 | Running loss: 0.02589\n",
            "Epoch: 145 | Iteration: 67 | Classification loss: 0.00016 | Regression loss: 0.02499 | Running loss: 0.02588\n",
            "Epoch: 145 | Iteration: 68 | Classification loss: 0.00018 | Regression loss: 0.03183 | Running loss: 0.02592\n",
            "Epoch: 145 | Iteration: 69 | Classification loss: 0.00101 | Regression loss: 0.03849 | Running loss: 0.02593\n",
            "Epoch: 145 | Iteration: 70 | Classification loss: 0.00016 | Regression loss: 0.02044 | Running loss: 0.02594\n",
            "Epoch: 145 | Iteration: 71 | Classification loss: 0.00115 | Regression loss: 0.02535 | Running loss: 0.02589\n",
            "Epoch: 145 | Iteration: 72 | Classification loss: 0.00033 | Regression loss: 0.04328 | Running loss: 0.02589\n",
            "Epoch: 145 | Iteration: 73 | Classification loss: 0.00072 | Regression loss: 0.02457 | Running loss: 0.02592\n",
            "Epoch: 145 | Iteration: 74 | Classification loss: 0.00043 | Regression loss: 0.03384 | Running loss: 0.02594\n",
            "Epoch: 145 | Iteration: 75 | Classification loss: 0.00024 | Regression loss: 0.01558 | Running loss: 0.02594\n",
            "Epoch: 145 | Iteration: 76 | Classification loss: 0.00005 | Regression loss: 0.01135 | Running loss: 0.02592\n",
            "Epoch: 145 | Iteration: 77 | Classification loss: 0.00064 | Regression loss: 0.04688 | Running loss: 0.02596\n",
            "Epoch: 145 | Iteration: 78 | Classification loss: 0.00025 | Regression loss: 0.02358 | Running loss: 0.02595\n",
            "Epoch: 145 | Iteration: 79 | Classification loss: 0.00037 | Regression loss: 0.02511 | Running loss: 0.02595\n",
            "Evaluating dataset\n",
            "\n",
            "mAP:\n",
            "person: 0.7248766344895328\n",
            "Precision:  0.5485074626865671\n",
            "Recall:  0.7945945945945946\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}]\n",
            "Epoch: 146 | Iteration: 0 | Classification loss: 0.00018 | Regression loss: 0.01261 | Running loss: 0.02590\n",
            "Epoch: 146 | Iteration: 1 | Classification loss: 0.00013 | Regression loss: 0.01590 | Running loss: 0.02589\n",
            "Epoch: 146 | Iteration: 2 | Classification loss: 0.00047 | Regression loss: 0.02837 | Running loss: 0.02592\n",
            "Epoch: 146 | Iteration: 3 | Classification loss: 0.00006 | Regression loss: 0.00944 | Running loss: 0.02585\n",
            "Epoch: 146 | Iteration: 4 | Classification loss: 0.00088 | Regression loss: 0.05457 | Running loss: 0.02594\n",
            "Epoch: 146 | Iteration: 5 | Classification loss: 0.00011 | Regression loss: 0.01633 | Running loss: 0.02593\n",
            "Epoch: 146 | Iteration: 6 | Classification loss: 0.00007 | Regression loss: 0.00870 | Running loss: 0.02591\n",
            "Epoch: 146 | Iteration: 7 | Classification loss: 0.00019 | Regression loss: 0.01935 | Running loss: 0.02583\n",
            "Epoch: 146 | Iteration: 8 | Classification loss: 0.00044 | Regression loss: 0.02991 | Running loss: 0.02587\n",
            "Epoch: 146 | Iteration: 9 | Classification loss: 0.00016 | Regression loss: 0.01430 | Running loss: 0.02584\n",
            "Epoch: 146 | Iteration: 10 | Classification loss: 0.00034 | Regression loss: 0.02349 | Running loss: 0.02584\n",
            "Epoch: 146 | Iteration: 11 | Classification loss: 0.00032 | Regression loss: 0.04031 | Running loss: 0.02585\n",
            "Epoch: 146 | Iteration: 12 | Classification loss: 0.00002 | Regression loss: 0.01149 | Running loss: 0.02582\n",
            "Epoch: 146 | Iteration: 13 | Classification loss: 0.00012 | Regression loss: 0.01030 | Running loss: 0.02582\n",
            "Epoch: 146 | Iteration: 14 | Classification loss: 0.00025 | Regression loss: 0.02217 | Running loss: 0.02581\n",
            "Epoch: 146 | Iteration: 15 | Classification loss: 0.00070 | Regression loss: 0.04071 | Running loss: 0.02581\n",
            "Epoch: 146 | Iteration: 16 | Classification loss: 0.00025 | Regression loss: 0.01406 | Running loss: 0.02576\n",
            "Epoch: 146 | Iteration: 17 | Classification loss: 0.00010 | Regression loss: 0.00627 | Running loss: 0.02572\n",
            "Epoch: 146 | Iteration: 18 | Classification loss: 0.00060 | Regression loss: 0.04804 | Running loss: 0.02578\n",
            "Epoch: 146 | Iteration: 19 | Classification loss: 0.00028 | Regression loss: 0.02032 | Running loss: 0.02576\n",
            "Epoch: 146 | Iteration: 20 | Classification loss: 0.00024 | Regression loss: 0.02031 | Running loss: 0.02577\n",
            "Epoch: 146 | Iteration: 21 | Classification loss: 0.00027 | Regression loss: 0.03451 | Running loss: 0.02580\n",
            "Epoch: 146 | Iteration: 22 | Classification loss: 0.00023 | Regression loss: 0.02129 | Running loss: 0.02575\n",
            "Epoch: 146 | Iteration: 23 | Classification loss: 0.00032 | Regression loss: 0.02385 | Running loss: 0.02575\n",
            "Epoch: 146 | Iteration: 24 | Classification loss: 0.00046 | Regression loss: 0.03803 | Running loss: 0.02582\n",
            "Epoch: 146 | Iteration: 25 | Classification loss: 0.00025 | Regression loss: 0.02774 | Running loss: 0.02579\n",
            "Epoch: 146 | Iteration: 26 | Classification loss: 0.00037 | Regression loss: 0.03609 | Running loss: 0.02582\n",
            "Epoch: 146 | Iteration: 27 | Classification loss: 0.00016 | Regression loss: 0.03054 | Running loss: 0.02583\n",
            "Epoch: 146 | Iteration: 28 | Classification loss: 0.00008 | Regression loss: 0.02513 | Running loss: 0.02584\n",
            "Epoch: 146 | Iteration: 29 | Classification loss: 0.00037 | Regression loss: 0.01567 | Running loss: 0.02583\n",
            "Epoch: 146 | Iteration: 30 | Classification loss: 0.00022 | Regression loss: 0.02468 | Running loss: 0.02576\n",
            "Epoch: 146 | Iteration: 31 | Classification loss: 0.00012 | Regression loss: 0.02379 | Running loss: 0.02577\n",
            "Epoch: 146 | Iteration: 32 | Classification loss: 0.00032 | Regression loss: 0.02805 | Running loss: 0.02580\n",
            "Epoch: 146 | Iteration: 33 | Classification loss: 0.00045 | Regression loss: 0.03374 | Running loss: 0.02584\n",
            "Epoch: 146 | Iteration: 34 | Classification loss: 0.00099 | Regression loss: 0.04145 | Running loss: 0.02588\n",
            "Epoch: 146 | Iteration: 35 | Classification loss: 0.00022 | Regression loss: 0.02209 | Running loss: 0.02589\n",
            "Epoch: 146 | Iteration: 36 | Classification loss: 0.00017 | Regression loss: 0.01972 | Running loss: 0.02584\n",
            "Epoch: 146 | Iteration: 37 | Classification loss: 0.00024 | Regression loss: 0.01765 | Running loss: 0.02582\n",
            "Epoch: 146 | Iteration: 38 | Classification loss: 0.00028 | Regression loss: 0.02756 | Running loss: 0.02582\n",
            "Epoch: 146 | Iteration: 39 | Classification loss: 0.00093 | Regression loss: 0.04426 | Running loss: 0.02585\n",
            "Epoch: 146 | Iteration: 40 | Classification loss: 0.00087 | Regression loss: 0.02299 | Running loss: 0.02585\n",
            "Epoch: 146 | Iteration: 41 | Classification loss: 0.00031 | Regression loss: 0.03357 | Running loss: 0.02588\n",
            "Epoch: 146 | Iteration: 42 | Classification loss: 0.00022 | Regression loss: 0.03256 | Running loss: 0.02592\n",
            "Epoch: 146 | Iteration: 43 | Classification loss: 0.00004 | Regression loss: 0.00540 | Running loss: 0.02590\n",
            "Epoch: 146 | Iteration: 44 | Classification loss: 0.00015 | Regression loss: 0.01756 | Running loss: 0.02589\n",
            "Epoch: 146 | Iteration: 45 | Classification loss: 0.00013 | Regression loss: 0.01178 | Running loss: 0.02588\n",
            "Epoch: 146 | Iteration: 46 | Classification loss: 0.00028 | Regression loss: 0.02786 | Running loss: 0.02590\n",
            "Epoch: 146 | Iteration: 47 | Classification loss: 0.00009 | Regression loss: 0.02302 | Running loss: 0.02590\n",
            "Epoch: 146 | Iteration: 48 | Classification loss: 0.00006 | Regression loss: 0.01742 | Running loss: 0.02587\n",
            "Epoch: 146 | Iteration: 49 | Classification loss: 0.00026 | Regression loss: 0.01766 | Running loss: 0.02584\n",
            "Epoch: 146 | Iteration: 50 | Classification loss: 0.00017 | Regression loss: 0.02842 | Running loss: 0.02584\n",
            "Epoch: 146 | Iteration: 51 | Classification loss: 0.00026 | Regression loss: 0.04167 | Running loss: 0.02587\n",
            "Epoch: 146 | Iteration: 52 | Classification loss: 0.00075 | Regression loss: 0.03939 | Running loss: 0.02589\n",
            "Epoch: 146 | Iteration: 53 | Classification loss: 0.00013 | Regression loss: 0.02441 | Running loss: 0.02587\n",
            "Epoch: 146 | Iteration: 54 | Classification loss: 0.00009 | Regression loss: 0.02939 | Running loss: 0.02586\n",
            "Epoch: 146 | Iteration: 55 | Classification loss: 0.00019 | Regression loss: 0.02548 | Running loss: 0.02584\n",
            "Epoch: 146 | Iteration: 56 | Classification loss: 0.00038 | Regression loss: 0.02059 | Running loss: 0.02583\n",
            "Epoch: 146 | Iteration: 57 | Classification loss: 0.00021 | Regression loss: 0.02215 | Running loss: 0.02581\n",
            "Epoch: 146 | Iteration: 58 | Classification loss: 0.00089 | Regression loss: 0.03500 | Running loss: 0.02583\n",
            "Epoch: 146 | Iteration: 59 | Classification loss: 0.00019 | Regression loss: 0.01681 | Running loss: 0.02581\n",
            "Epoch: 146 | Iteration: 60 | Classification loss: 0.00063 | Regression loss: 0.04634 | Running loss: 0.02582\n",
            "Epoch: 146 | Iteration: 61 | Classification loss: 0.00016 | Regression loss: 0.01617 | Running loss: 0.02580\n",
            "Epoch: 146 | Iteration: 62 | Classification loss: 0.00011 | Regression loss: 0.02387 | Running loss: 0.02579\n",
            "Epoch: 146 | Iteration: 63 | Classification loss: 0.00015 | Regression loss: 0.01730 | Running loss: 0.02574\n",
            "Epoch: 146 | Iteration: 64 | Classification loss: 0.00010 | Regression loss: 0.01861 | Running loss: 0.02573\n",
            "Epoch: 146 | Iteration: 65 | Classification loss: 0.00018 | Regression loss: 0.01973 | Running loss: 0.02572\n",
            "Epoch: 146 | Iteration: 66 | Classification loss: 0.00017 | Regression loss: 0.02435 | Running loss: 0.02571\n",
            "Epoch: 146 | Iteration: 67 | Classification loss: 0.00065 | Regression loss: 0.02572 | Running loss: 0.02575\n",
            "Epoch: 146 | Iteration: 68 | Classification loss: 0.00024 | Regression loss: 0.01938 | Running loss: 0.02573\n",
            "Epoch: 146 | Iteration: 69 | Classification loss: 0.00045 | Regression loss: 0.03419 | Running loss: 0.02573\n",
            "Epoch: 146 | Iteration: 70 | Classification loss: 0.00022 | Regression loss: 0.01462 | Running loss: 0.02569\n",
            "Epoch: 146 | Iteration: 71 | Classification loss: 0.00024 | Regression loss: 0.02663 | Running loss: 0.02565\n",
            "Epoch: 146 | Iteration: 72 | Classification loss: 0.00023 | Regression loss: 0.01815 | Running loss: 0.02564\n",
            "Epoch: 146 | Iteration: 73 | Classification loss: 0.00009 | Regression loss: 0.01567 | Running loss: 0.02562\n",
            "Epoch: 146 | Iteration: 74 | Classification loss: 0.00036 | Regression loss: 0.02105 | Running loss: 0.02563\n",
            "Epoch: 146 | Iteration: 75 | Classification loss: 0.00064 | Regression loss: 0.04703 | Running loss: 0.02566\n",
            "Epoch: 146 | Iteration: 76 | Classification loss: 0.00071 | Regression loss: 0.02354 | Running loss: 0.02569\n",
            "Epoch: 146 | Iteration: 77 | Classification loss: 0.00074 | Regression loss: 0.02511 | Running loss: 0.02566\n",
            "Epoch: 146 | Iteration: 78 | Classification loss: 0.00068 | Regression loss: 0.02971 | Running loss: 0.02564\n",
            "Epoch: 146 | Iteration: 79 | Classification loss: 0.00013 | Regression loss: 0.01864 | Running loss: 0.02564\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7287625569452764\n",
            "Precision:  0.550561797752809\n",
            "Recall:  0.7945945945945946\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}]\n",
            "Epoch: 147 | Iteration: 0 | Classification loss: 0.00024 | Regression loss: 0.01997 | Running loss: 0.02564\n",
            "Epoch: 147 | Iteration: 1 | Classification loss: 0.00030 | Regression loss: 0.03525 | Running loss: 0.02561\n",
            "Epoch: 147 | Iteration: 2 | Classification loss: 0.00004 | Regression loss: 0.00513 | Running loss: 0.02552\n",
            "Epoch: 147 | Iteration: 3 | Classification loss: 0.00068 | Regression loss: 0.02336 | Running loss: 0.02553\n",
            "Epoch: 147 | Iteration: 4 | Classification loss: 0.00016 | Regression loss: 0.02400 | Running loss: 0.02554\n",
            "Epoch: 147 | Iteration: 5 | Classification loss: 0.00042 | Regression loss: 0.03347 | Running loss: 0.02555\n",
            "Epoch: 147 | Iteration: 6 | Classification loss: 0.00063 | Regression loss: 0.03840 | Running loss: 0.02557\n",
            "Epoch: 147 | Iteration: 7 | Classification loss: 0.00019 | Regression loss: 0.02359 | Running loss: 0.02557\n",
            "Epoch: 147 | Iteration: 8 | Classification loss: 0.00020 | Regression loss: 0.01825 | Running loss: 0.02558\n",
            "Epoch: 147 | Iteration: 9 | Classification loss: 0.00021 | Regression loss: 0.03438 | Running loss: 0.02562\n",
            "Epoch: 147 | Iteration: 10 | Classification loss: 0.00028 | Regression loss: 0.02565 | Running loss: 0.02562\n",
            "Epoch: 147 | Iteration: 11 | Classification loss: 0.00023 | Regression loss: 0.02248 | Running loss: 0.02564\n",
            "Epoch: 147 | Iteration: 12 | Classification loss: 0.00021 | Regression loss: 0.02037 | Running loss: 0.02558\n",
            "Epoch: 147 | Iteration: 13 | Classification loss: 0.00016 | Regression loss: 0.01113 | Running loss: 0.02556\n",
            "Epoch: 147 | Iteration: 14 | Classification loss: 0.00011 | Regression loss: 0.01638 | Running loss: 0.02555\n",
            "Epoch: 147 | Iteration: 15 | Classification loss: 0.00007 | Regression loss: 0.02296 | Running loss: 0.02556\n",
            "Epoch: 147 | Iteration: 16 | Classification loss: 0.00029 | Regression loss: 0.02432 | Running loss: 0.02557\n",
            "Epoch: 147 | Iteration: 17 | Classification loss: 0.00006 | Regression loss: 0.01732 | Running loss: 0.02557\n",
            "Epoch: 147 | Iteration: 18 | Classification loss: 0.00035 | Regression loss: 0.02593 | Running loss: 0.02561\n",
            "Epoch: 147 | Iteration: 19 | Classification loss: 0.00008 | Regression loss: 0.01479 | Running loss: 0.02557\n",
            "Epoch: 147 | Iteration: 20 | Classification loss: 0.00011 | Regression loss: 0.02426 | Running loss: 0.02555\n",
            "Epoch: 147 | Iteration: 21 | Classification loss: 0.00010 | Regression loss: 0.02407 | Running loss: 0.02558\n",
            "Epoch: 147 | Iteration: 22 | Classification loss: 0.00035 | Regression loss: 0.02315 | Running loss: 0.02559\n",
            "Epoch: 147 | Iteration: 23 | Classification loss: 0.00060 | Regression loss: 0.03792 | Running loss: 0.02564\n",
            "Epoch: 147 | Iteration: 24 | Classification loss: 0.00025 | Regression loss: 0.01694 | Running loss: 0.02560\n",
            "Epoch: 147 | Iteration: 25 | Classification loss: 0.00006 | Regression loss: 0.00939 | Running loss: 0.02557\n",
            "Epoch: 147 | Iteration: 26 | Classification loss: 0.00016 | Regression loss: 0.02655 | Running loss: 0.02559\n",
            "Epoch: 147 | Iteration: 27 | Classification loss: 0.00082 | Regression loss: 0.05611 | Running loss: 0.02564\n",
            "Epoch: 147 | Iteration: 28 | Classification loss: 0.00105 | Regression loss: 0.03913 | Running loss: 0.02569\n",
            "Epoch: 147 | Iteration: 29 | Classification loss: 0.00069 | Regression loss: 0.02376 | Running loss: 0.02573\n",
            "Epoch: 147 | Iteration: 30 | Classification loss: 0.00011 | Regression loss: 0.01308 | Running loss: 0.02568\n",
            "Epoch: 147 | Iteration: 31 | Classification loss: 0.00009 | Regression loss: 0.00657 | Running loss: 0.02566\n",
            "Epoch: 147 | Iteration: 32 | Classification loss: 0.00030 | Regression loss: 0.02585 | Running loss: 0.02569\n",
            "Epoch: 147 | Iteration: 33 | Classification loss: 0.00026 | Regression loss: 0.02092 | Running loss: 0.02566\n",
            "Epoch: 147 | Iteration: 34 | Classification loss: 0.00004 | Regression loss: 0.01150 | Running loss: 0.02563\n",
            "Epoch: 147 | Iteration: 35 | Classification loss: 0.00026 | Regression loss: 0.01606 | Running loss: 0.02561\n",
            "Epoch: 147 | Iteration: 36 | Classification loss: 0.00024 | Regression loss: 0.01760 | Running loss: 0.02556\n",
            "Epoch: 147 | Iteration: 37 | Classification loss: 0.00020 | Regression loss: 0.03095 | Running loss: 0.02554\n",
            "Epoch: 147 | Iteration: 38 | Classification loss: 0.00015 | Regression loss: 0.01757 | Running loss: 0.02553\n",
            "Epoch: 147 | Iteration: 39 | Classification loss: 0.00021 | Regression loss: 0.01123 | Running loss: 0.02549\n",
            "Epoch: 147 | Iteration: 40 | Classification loss: 0.00023 | Regression loss: 0.03754 | Running loss: 0.02555\n",
            "Epoch: 147 | Iteration: 41 | Classification loss: 0.00037 | Regression loss: 0.03081 | Running loss: 0.02556\n",
            "Epoch: 147 | Iteration: 42 | Classification loss: 0.00034 | Regression loss: 0.01579 | Running loss: 0.02553\n",
            "Epoch: 147 | Iteration: 43 | Classification loss: 0.00043 | Regression loss: 0.03485 | Running loss: 0.02558\n",
            "Epoch: 147 | Iteration: 44 | Classification loss: 0.00021 | Regression loss: 0.02810 | Running loss: 0.02559\n",
            "Epoch: 147 | Iteration: 45 | Classification loss: 0.00011 | Regression loss: 0.01900 | Running loss: 0.02557\n",
            "Epoch: 147 | Iteration: 46 | Classification loss: 0.00029 | Regression loss: 0.01955 | Running loss: 0.02553\n",
            "Epoch: 147 | Iteration: 47 | Classification loss: 0.00029 | Regression loss: 0.04103 | Running loss: 0.02552\n",
            "Epoch: 147 | Iteration: 48 | Classification loss: 0.00090 | Regression loss: 0.04303 | Running loss: 0.02557\n",
            "Epoch: 147 | Iteration: 49 | Classification loss: 0.00016 | Regression loss: 0.01875 | Running loss: 0.02557\n",
            "Epoch: 147 | Iteration: 50 | Classification loss: 0.00031 | Regression loss: 0.02895 | Running loss: 0.02557\n",
            "Epoch: 147 | Iteration: 51 | Classification loss: 0.00012 | Regression loss: 0.01969 | Running loss: 0.02555\n",
            "Epoch: 147 | Iteration: 52 | Classification loss: 0.00019 | Regression loss: 0.01223 | Running loss: 0.02553\n",
            "Epoch: 147 | Iteration: 53 | Classification loss: 0.00016 | Regression loss: 0.02012 | Running loss: 0.02555\n",
            "Epoch: 147 | Iteration: 54 | Classification loss: 0.00012 | Regression loss: 0.01697 | Running loss: 0.02553\n",
            "Epoch: 147 | Iteration: 55 | Classification loss: 0.00112 | Regression loss: 0.02743 | Running loss: 0.02554\n",
            "Epoch: 147 | Iteration: 56 | Classification loss: 0.00061 | Regression loss: 0.04165 | Running loss: 0.02558\n",
            "Epoch: 147 | Iteration: 57 | Classification loss: 0.00009 | Regression loss: 0.02361 | Running loss: 0.02559\n",
            "Epoch: 147 | Iteration: 58 | Classification loss: 0.00071 | Regression loss: 0.03667 | Running loss: 0.02561\n",
            "Epoch: 147 | Iteration: 59 | Classification loss: 0.00042 | Regression loss: 0.02404 | Running loss: 0.02562\n",
            "Epoch: 147 | Iteration: 60 | Classification loss: 0.00016 | Regression loss: 0.01736 | Running loss: 0.02557\n",
            "Epoch: 147 | Iteration: 61 | Classification loss: 0.00047 | Regression loss: 0.04294 | Running loss: 0.02557\n",
            "Epoch: 147 | Iteration: 62 | Classification loss: 0.00013 | Regression loss: 0.02204 | Running loss: 0.02556\n",
            "Epoch: 147 | Iteration: 63 | Classification loss: 0.00034 | Regression loss: 0.02173 | Running loss: 0.02554\n",
            "Epoch: 147 | Iteration: 64 | Classification loss: 0.00022 | Regression loss: 0.01688 | Running loss: 0.02552\n",
            "Epoch: 147 | Iteration: 65 | Classification loss: 0.00030 | Regression loss: 0.03805 | Running loss: 0.02552\n",
            "Epoch: 147 | Iteration: 66 | Classification loss: 0.00091 | Regression loss: 0.03797 | Running loss: 0.02554\n",
            "Epoch: 147 | Iteration: 67 | Classification loss: 0.00059 | Regression loss: 0.04715 | Running loss: 0.02558\n",
            "Epoch: 147 | Iteration: 68 | Classification loss: 0.00023 | Regression loss: 0.02457 | Running loss: 0.02560\n",
            "Epoch: 147 | Iteration: 69 | Classification loss: 0.00025 | Regression loss: 0.02044 | Running loss: 0.02561\n",
            "Epoch: 147 | Iteration: 70 | Classification loss: 0.00019 | Regression loss: 0.02096 | Running loss: 0.02562\n",
            "Epoch: 147 | Iteration: 71 | Classification loss: 0.00067 | Regression loss: 0.04152 | Running loss: 0.02566\n",
            "Epoch: 147 | Iteration: 72 | Classification loss: 0.00026 | Regression loss: 0.02781 | Running loss: 0.02561\n",
            "Epoch: 147 | Iteration: 73 | Classification loss: 0.00026 | Regression loss: 0.01559 | Running loss: 0.02560\n",
            "Epoch: 147 | Iteration: 74 | Classification loss: 0.00043 | Regression loss: 0.02598 | Running loss: 0.02560\n",
            "Epoch: 147 | Iteration: 75 | Classification loss: 0.00019 | Regression loss: 0.01246 | Running loss: 0.02554\n",
            "Epoch: 147 | Iteration: 76 | Classification loss: 0.00042 | Regression loss: 0.03024 | Running loss: 0.02551\n",
            "Epoch: 147 | Iteration: 77 | Classification loss: 0.00007 | Regression loss: 0.00908 | Running loss: 0.02548\n",
            "Epoch: 147 | Iteration: 78 | Classification loss: 0.00059 | Regression loss: 0.03463 | Running loss: 0.02548\n",
            "Epoch: 147 | Iteration: 79 | Classification loss: 0.00019 | Regression loss: 0.01941 | Running loss: 0.02547\n",
            "Evaluating dataset\n",
            "\n",
            "mAP:\n",
            "person: 0.7203191939832536\n",
            "Precision:  0.5551330798479087\n",
            "Recall:  0.7891891891891892\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}]\n",
            "Epoch: 148 | Iteration: 0 | Classification loss: 0.00057 | Regression loss: 0.04696 | Running loss: 0.02551\n",
            "Epoch: 148 | Iteration: 1 | Classification loss: 0.00025 | Regression loss: 0.01519 | Running loss: 0.02547\n",
            "Epoch: 148 | Iteration: 2 | Classification loss: 0.00020 | Regression loss: 0.02693 | Running loss: 0.02549\n",
            "Epoch: 148 | Iteration: 3 | Classification loss: 0.00063 | Regression loss: 0.03491 | Running loss: 0.02549\n",
            "Epoch: 148 | Iteration: 4 | Classification loss: 0.00033 | Regression loss: 0.02261 | Running loss: 0.02549\n",
            "Epoch: 148 | Iteration: 5 | Classification loss: 0.00034 | Regression loss: 0.02073 | Running loss: 0.02550\n",
            "Epoch: 148 | Iteration: 6 | Classification loss: 0.00021 | Regression loss: 0.01918 | Running loss: 0.02549\n",
            "Epoch: 148 | Iteration: 7 | Classification loss: 0.00024 | Regression loss: 0.02360 | Running loss: 0.02544\n",
            "Epoch: 148 | Iteration: 8 | Classification loss: 0.00025 | Regression loss: 0.02009 | Running loss: 0.02545\n",
            "Epoch: 148 | Iteration: 9 | Classification loss: 0.00035 | Regression loss: 0.02082 | Running loss: 0.02541\n",
            "Epoch: 148 | Iteration: 10 | Classification loss: 0.00072 | Regression loss: 0.04175 | Running loss: 0.02540\n",
            "Epoch: 148 | Iteration: 11 | Classification loss: 0.00018 | Regression loss: 0.01253 | Running loss: 0.02537\n",
            "Epoch: 148 | Iteration: 12 | Classification loss: 0.00014 | Regression loss: 0.01102 | Running loss: 0.02535\n",
            "Epoch: 148 | Iteration: 13 | Classification loss: 0.00018 | Regression loss: 0.03188 | Running loss: 0.02538\n",
            "Epoch: 148 | Iteration: 14 | Classification loss: 0.00064 | Regression loss: 0.03234 | Running loss: 0.02537\n",
            "Epoch: 148 | Iteration: 15 | Classification loss: 0.00045 | Regression loss: 0.04641 | Running loss: 0.02542\n",
            "Epoch: 148 | Iteration: 16 | Classification loss: 0.00013 | Regression loss: 0.02581 | Running loss: 0.02543\n",
            "Epoch: 148 | Iteration: 17 | Classification loss: 0.00030 | Regression loss: 0.02386 | Running loss: 0.02546\n",
            "Epoch: 148 | Iteration: 18 | Classification loss: 0.00022 | Regression loss: 0.02046 | Running loss: 0.02548\n",
            "Epoch: 148 | Iteration: 19 | Classification loss: 0.00007 | Regression loss: 0.00871 | Running loss: 0.02547\n",
            "Epoch: 148 | Iteration: 20 | Classification loss: 0.00021 | Regression loss: 0.01987 | Running loss: 0.02545\n",
            "Epoch: 148 | Iteration: 21 | Classification loss: 0.00011 | Regression loss: 0.00987 | Running loss: 0.02539\n",
            "Epoch: 148 | Iteration: 22 | Classification loss: 0.00015 | Regression loss: 0.02420 | Running loss: 0.02538\n",
            "Epoch: 148 | Iteration: 23 | Classification loss: 0.00035 | Regression loss: 0.02970 | Running loss: 0.02538\n",
            "Epoch: 148 | Iteration: 24 | Classification loss: 0.00016 | Regression loss: 0.01473 | Running loss: 0.02537\n",
            "Epoch: 148 | Iteration: 25 | Classification loss: 0.00034 | Regression loss: 0.02758 | Running loss: 0.02537\n",
            "Epoch: 148 | Iteration: 26 | Classification loss: 0.00009 | Regression loss: 0.01533 | Running loss: 0.02533\n",
            "Epoch: 148 | Iteration: 27 | Classification loss: 0.00055 | Regression loss: 0.02427 | Running loss: 0.02532\n",
            "Epoch: 148 | Iteration: 28 | Classification loss: 0.00004 | Regression loss: 0.01133 | Running loss: 0.02531\n",
            "Epoch: 148 | Iteration: 29 | Classification loss: 0.00042 | Regression loss: 0.03361 | Running loss: 0.02530\n",
            "Epoch: 148 | Iteration: 30 | Classification loss: 0.00011 | Regression loss: 0.01356 | Running loss: 0.02530\n",
            "Epoch: 148 | Iteration: 31 | Classification loss: 0.00046 | Regression loss: 0.03396 | Running loss: 0.02533\n",
            "Epoch: 148 | Iteration: 32 | Classification loss: 0.00032 | Regression loss: 0.03377 | Running loss: 0.02534\n",
            "Epoch: 148 | Iteration: 33 | Classification loss: 0.00035 | Regression loss: 0.02263 | Running loss: 0.02534\n",
            "Epoch: 148 | Iteration: 34 | Classification loss: 0.00013 | Regression loss: 0.01717 | Running loss: 0.02534\n",
            "Epoch: 148 | Iteration: 35 | Classification loss: 0.00026 | Regression loss: 0.03788 | Running loss: 0.02534\n",
            "Epoch: 148 | Iteration: 36 | Classification loss: 0.00019 | Regression loss: 0.01867 | Running loss: 0.02533\n",
            "Epoch: 148 | Iteration: 37 | Classification loss: 0.00010 | Regression loss: 0.02998 | Running loss: 0.02534\n",
            "Epoch: 148 | Iteration: 38 | Classification loss: 0.00068 | Regression loss: 0.02293 | Running loss: 0.02530\n",
            "Epoch: 148 | Iteration: 39 | Classification loss: 0.00083 | Regression loss: 0.02230 | Running loss: 0.02532\n",
            "Epoch: 148 | Iteration: 40 | Classification loss: 0.00074 | Regression loss: 0.04114 | Running loss: 0.02537\n",
            "Epoch: 148 | Iteration: 41 | Classification loss: 0.00016 | Regression loss: 0.03404 | Running loss: 0.02538\n",
            "Epoch: 148 | Iteration: 42 | Classification loss: 0.00110 | Regression loss: 0.04639 | Running loss: 0.02544\n",
            "Epoch: 148 | Iteration: 43 | Classification loss: 0.00022 | Regression loss: 0.01658 | Running loss: 0.02536\n",
            "Epoch: 148 | Iteration: 44 | Classification loss: 0.00018 | Regression loss: 0.02573 | Running loss: 0.02538\n",
            "Epoch: 148 | Iteration: 45 | Classification loss: 0.00098 | Regression loss: 0.04039 | Running loss: 0.02545\n",
            "Epoch: 148 | Iteration: 46 | Classification loss: 0.00018 | Regression loss: 0.01970 | Running loss: 0.02546\n",
            "Epoch: 148 | Iteration: 47 | Classification loss: 0.00048 | Regression loss: 0.02684 | Running loss: 0.02550\n",
            "Epoch: 148 | Iteration: 48 | Classification loss: 0.00024 | Regression loss: 0.01511 | Running loss: 0.02548\n",
            "Epoch: 148 | Iteration: 49 | Classification loss: 0.00034 | Regression loss: 0.01977 | Running loss: 0.02549\n",
            "Epoch: 148 | Iteration: 50 | Classification loss: 0.00054 | Regression loss: 0.03058 | Running loss: 0.02548\n",
            "Epoch: 148 | Iteration: 51 | Classification loss: 0.00010 | Regression loss: 0.03145 | Running loss: 0.02547\n",
            "Epoch: 148 | Iteration: 52 | Classification loss: 0.00032 | Regression loss: 0.01607 | Running loss: 0.02545\n",
            "Epoch: 148 | Iteration: 53 | Classification loss: 0.00020 | Regression loss: 0.02085 | Running loss: 0.02547\n",
            "Epoch: 148 | Iteration: 54 | Classification loss: 0.00009 | Regression loss: 0.00620 | Running loss: 0.02541\n",
            "Epoch: 148 | Iteration: 55 | Classification loss: 0.00011 | Regression loss: 0.01977 | Running loss: 0.02540\n",
            "Epoch: 148 | Iteration: 56 | Classification loss: 0.00034 | Regression loss: 0.04098 | Running loss: 0.02545\n",
            "Epoch: 148 | Iteration: 57 | Classification loss: 0.00026 | Regression loss: 0.03366 | Running loss: 0.02547\n",
            "Epoch: 148 | Iteration: 58 | Classification loss: 0.00081 | Regression loss: 0.02822 | Running loss: 0.02550\n",
            "Epoch: 148 | Iteration: 59 | Classification loss: 0.00013 | Regression loss: 0.02932 | Running loss: 0.02551\n",
            "Epoch: 148 | Iteration: 60 | Classification loss: 0.00028 | Regression loss: 0.02474 | Running loss: 0.02552\n",
            "Epoch: 148 | Iteration: 61 | Classification loss: 0.00010 | Regression loss: 0.02613 | Running loss: 0.02548\n",
            "Epoch: 148 | Iteration: 62 | Classification loss: 0.00042 | Regression loss: 0.02601 | Running loss: 0.02548\n",
            "Epoch: 148 | Iteration: 63 | Classification loss: 0.00017 | Regression loss: 0.01689 | Running loss: 0.02547\n",
            "Epoch: 148 | Iteration: 64 | Classification loss: 0.00089 | Regression loss: 0.03742 | Running loss: 0.02545\n",
            "Epoch: 148 | Iteration: 65 | Classification loss: 0.00013 | Regression loss: 0.01482 | Running loss: 0.02545\n",
            "Epoch: 148 | Iteration: 66 | Classification loss: 0.00023 | Regression loss: 0.01591 | Running loss: 0.02545\n",
            "Epoch: 148 | Iteration: 67 | Classification loss: 0.00008 | Regression loss: 0.01994 | Running loss: 0.02544\n",
            "Epoch: 148 | Iteration: 68 | Classification loss: 0.00037 | Regression loss: 0.03907 | Running loss: 0.02547\n",
            "Epoch: 148 | Iteration: 69 | Classification loss: 0.00020 | Regression loss: 0.02745 | Running loss: 0.02549\n",
            "Epoch: 148 | Iteration: 70 | Classification loss: 0.00068 | Regression loss: 0.04299 | Running loss: 0.02554\n",
            "Epoch: 148 | Iteration: 71 | Classification loss: 0.00028 | Regression loss: 0.01716 | Running loss: 0.02551\n",
            "Epoch: 148 | Iteration: 72 | Classification loss: 0.00004 | Regression loss: 0.00501 | Running loss: 0.02549\n",
            "Epoch: 148 | Iteration: 73 | Classification loss: 0.00025 | Regression loss: 0.02162 | Running loss: 0.02548\n",
            "Epoch: 148 | Iteration: 74 | Classification loss: 0.00060 | Regression loss: 0.05429 | Running loss: 0.02551\n",
            "Epoch: 148 | Iteration: 75 | Classification loss: 0.00019 | Regression loss: 0.02093 | Running loss: 0.02549\n",
            "Epoch: 148 | Iteration: 76 | Classification loss: 0.00006 | Regression loss: 0.00933 | Running loss: 0.02548\n",
            "Epoch: 148 | Iteration: 77 | Classification loss: 0.00009 | Regression loss: 0.01843 | Running loss: 0.02544\n",
            "Epoch: 148 | Iteration: 78 | Classification loss: 0.00015 | Regression loss: 0.02484 | Running loss: 0.02543\n",
            "Epoch: 148 | Iteration: 79 | Classification loss: 0.00006 | Regression loss: 0.01753 | Running loss: 0.02542\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7267137859081885\n",
            "Precision:  0.5547169811320755\n",
            "Recall:  0.7945945945945946\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}]\n",
            "Epoch: 149 | Iteration: 0 | Classification loss: 0.00059 | Regression loss: 0.04466 | Running loss: 0.02548\n",
            "Epoch: 149 | Iteration: 1 | Classification loss: 0.00046 | Regression loss: 0.03386 | Running loss: 0.02546\n",
            "Epoch: 149 | Iteration: 2 | Classification loss: 0.00021 | Regression loss: 0.02022 | Running loss: 0.02543\n",
            "Epoch: 149 | Iteration: 3 | Classification loss: 0.00015 | Regression loss: 0.01906 | Running loss: 0.02538\n",
            "Epoch: 149 | Iteration: 4 | Classification loss: 0.00011 | Regression loss: 0.01813 | Running loss: 0.02536\n",
            "Epoch: 149 | Iteration: 5 | Classification loss: 0.00009 | Regression loss: 0.02292 | Running loss: 0.02535\n",
            "Epoch: 149 | Iteration: 6 | Classification loss: 0.00052 | Regression loss: 0.02511 | Running loss: 0.02536\n",
            "Epoch: 149 | Iteration: 7 | Classification loss: 0.00012 | Regression loss: 0.01343 | Running loss: 0.02531\n",
            "Epoch: 149 | Iteration: 8 | Classification loss: 0.00019 | Regression loss: 0.02481 | Running loss: 0.02532\n",
            "Epoch: 149 | Iteration: 9 | Classification loss: 0.00023 | Regression loss: 0.03021 | Running loss: 0.02534\n",
            "Epoch: 149 | Iteration: 10 | Classification loss: 0.00015 | Regression loss: 0.01873 | Running loss: 0.02533\n",
            "Epoch: 149 | Iteration: 11 | Classification loss: 0.00022 | Regression loss: 0.01904 | Running loss: 0.02534\n",
            "Epoch: 149 | Iteration: 12 | Classification loss: 0.00082 | Regression loss: 0.02413 | Running loss: 0.02533\n",
            "Epoch: 149 | Iteration: 13 | Classification loss: 0.00024 | Regression loss: 0.01669 | Running loss: 0.02532\n",
            "Epoch: 149 | Iteration: 14 | Classification loss: 0.00007 | Regression loss: 0.01462 | Running loss: 0.02531\n",
            "Epoch: 149 | Iteration: 15 | Classification loss: 0.00071 | Regression loss: 0.03655 | Running loss: 0.02533\n",
            "Epoch: 149 | Iteration: 16 | Classification loss: 0.00011 | Regression loss: 0.01061 | Running loss: 0.02529\n",
            "Epoch: 149 | Iteration: 17 | Classification loss: 0.00012 | Regression loss: 0.01667 | Running loss: 0.02523\n",
            "Epoch: 149 | Iteration: 18 | Classification loss: 0.00043 | Regression loss: 0.04654 | Running loss: 0.02530\n",
            "Epoch: 149 | Iteration: 19 | Classification loss: 0.00039 | Regression loss: 0.03300 | Running loss: 0.02532\n",
            "Epoch: 149 | Iteration: 20 | Classification loss: 0.00023 | Regression loss: 0.02804 | Running loss: 0.02535\n",
            "Epoch: 149 | Iteration: 21 | Classification loss: 0.00038 | Regression loss: 0.03490 | Running loss: 0.02534\n",
            "Epoch: 149 | Iteration: 22 | Classification loss: 0.00031 | Regression loss: 0.02079 | Running loss: 0.02534\n",
            "Epoch: 149 | Iteration: 23 | Classification loss: 0.00008 | Regression loss: 0.00658 | Running loss: 0.02527\n",
            "Epoch: 149 | Iteration: 24 | Classification loss: 0.00007 | Regression loss: 0.01875 | Running loss: 0.02523\n",
            "Epoch: 149 | Iteration: 25 | Classification loss: 0.00075 | Regression loss: 0.04974 | Running loss: 0.02529\n",
            "Epoch: 149 | Iteration: 26 | Classification loss: 0.00022 | Regression loss: 0.03408 | Running loss: 0.02531\n",
            "Epoch: 149 | Iteration: 27 | Classification loss: 0.00025 | Regression loss: 0.01668 | Running loss: 0.02529\n",
            "Epoch: 149 | Iteration: 28 | Classification loss: 0.00029 | Regression loss: 0.02603 | Running loss: 0.02530\n",
            "Epoch: 149 | Iteration: 29 | Classification loss: 0.00031 | Regression loss: 0.02672 | Running loss: 0.02530\n",
            "Epoch: 149 | Iteration: 30 | Classification loss: 0.00067 | Regression loss: 0.05287 | Running loss: 0.02539\n",
            "Epoch: 149 | Iteration: 31 | Classification loss: 0.00073 | Regression loss: 0.02515 | Running loss: 0.02539\n",
            "Epoch: 149 | Iteration: 32 | Classification loss: 0.00012 | Regression loss: 0.01409 | Running loss: 0.02535\n",
            "Epoch: 149 | Iteration: 33 | Classification loss: 0.00017 | Regression loss: 0.03288 | Running loss: 0.02538\n",
            "Epoch: 149 | Iteration: 34 | Classification loss: 0.00010 | Regression loss: 0.02460 | Running loss: 0.02541\n",
            "Epoch: 149 | Iteration: 35 | Classification loss: 0.00057 | Regression loss: 0.03789 | Running loss: 0.02542\n",
            "Epoch: 149 | Iteration: 36 | Classification loss: 0.00023 | Regression loss: 0.01846 | Running loss: 0.02542\n",
            "Epoch: 149 | Iteration: 37 | Classification loss: 0.00003 | Regression loss: 0.01112 | Running loss: 0.02539\n",
            "Epoch: 149 | Iteration: 38 | Classification loss: 0.00038 | Regression loss: 0.02041 | Running loss: 0.02538\n",
            "Epoch: 149 | Iteration: 39 | Classification loss: 0.00007 | Regression loss: 0.01712 | Running loss: 0.02537\n",
            "Epoch: 149 | Iteration: 40 | Classification loss: 0.00092 | Regression loss: 0.04265 | Running loss: 0.02542\n",
            "Epoch: 149 | Iteration: 41 | Classification loss: 0.00022 | Regression loss: 0.02783 | Running loss: 0.02542\n",
            "Epoch: 149 | Iteration: 42 | Classification loss: 0.00031 | Regression loss: 0.01966 | Running loss: 0.02541\n",
            "Epoch: 149 | Iteration: 43 | Classification loss: 0.00025 | Regression loss: 0.01570 | Running loss: 0.02542\n",
            "Epoch: 149 | Iteration: 44 | Classification loss: 0.00035 | Regression loss: 0.03918 | Running loss: 0.02539\n",
            "Epoch: 149 | Iteration: 45 | Classification loss: 0.00034 | Regression loss: 0.02139 | Running loss: 0.02534\n",
            "Epoch: 149 | Iteration: 46 | Classification loss: 0.00029 | Regression loss: 0.03969 | Running loss: 0.02537\n",
            "Epoch: 149 | Iteration: 47 | Classification loss: 0.00044 | Regression loss: 0.04082 | Running loss: 0.02541\n",
            "Epoch: 149 | Iteration: 48 | Classification loss: 0.00045 | Regression loss: 0.02644 | Running loss: 0.02543\n",
            "Epoch: 149 | Iteration: 49 | Classification loss: 0.00023 | Regression loss: 0.02231 | Running loss: 0.02540\n",
            "Epoch: 149 | Iteration: 50 | Classification loss: 0.00004 | Regression loss: 0.00504 | Running loss: 0.02535\n",
            "Epoch: 149 | Iteration: 51 | Classification loss: 0.00008 | Regression loss: 0.01077 | Running loss: 0.02534\n",
            "Epoch: 149 | Iteration: 52 | Classification loss: 0.00090 | Regression loss: 0.03535 | Running loss: 0.02539\n",
            "Epoch: 149 | Iteration: 53 | Classification loss: 0.00100 | Regression loss: 0.03742 | Running loss: 0.02541\n",
            "Epoch: 149 | Iteration: 54 | Classification loss: 0.00065 | Regression loss: 0.02577 | Running loss: 0.02541\n",
            "Epoch: 149 | Iteration: 55 | Classification loss: 0.00022 | Regression loss: 0.02132 | Running loss: 0.02542\n",
            "Epoch: 149 | Iteration: 56 | Classification loss: 0.00026 | Regression loss: 0.04293 | Running loss: 0.02546\n",
            "Epoch: 149 | Iteration: 57 | Classification loss: 0.00025 | Regression loss: 0.02390 | Running loss: 0.02548\n",
            "Epoch: 149 | Iteration: 58 | Classification loss: 0.00009 | Regression loss: 0.02790 | Running loss: 0.02545\n",
            "Epoch: 149 | Iteration: 59 | Classification loss: 0.00006 | Regression loss: 0.00849 | Running loss: 0.02543\n",
            "Epoch: 149 | Iteration: 60 | Classification loss: 0.00022 | Regression loss: 0.01586 | Running loss: 0.02537\n",
            "Epoch: 149 | Iteration: 61 | Classification loss: 0.00011 | Regression loss: 0.02311 | Running loss: 0.02536\n",
            "Epoch: 149 | Iteration: 62 | Classification loss: 0.00041 | Regression loss: 0.02997 | Running loss: 0.02539\n",
            "Epoch: 149 | Iteration: 63 | Classification loss: 0.00024 | Regression loss: 0.01398 | Running loss: 0.02537\n",
            "Epoch: 149 | Iteration: 64 | Classification loss: 0.00070 | Regression loss: 0.04186 | Running loss: 0.02537\n",
            "Epoch: 149 | Iteration: 65 | Classification loss: 0.00044 | Regression loss: 0.02591 | Running loss: 0.02538\n",
            "Epoch: 149 | Iteration: 66 | Classification loss: 0.00007 | Regression loss: 0.01729 | Running loss: 0.02538\n",
            "Epoch: 149 | Iteration: 67 | Classification loss: 0.00012 | Regression loss: 0.02826 | Running loss: 0.02540\n",
            "Epoch: 149 | Iteration: 68 | Classification loss: 0.00029 | Regression loss: 0.01336 | Running loss: 0.02539\n",
            "Epoch: 149 | Iteration: 69 | Classification loss: 0.00011 | Regression loss: 0.03003 | Running loss: 0.02542\n",
            "Epoch: 149 | Iteration: 70 | Classification loss: 0.00018 | Regression loss: 0.02078 | Running loss: 0.02540\n",
            "Epoch: 149 | Iteration: 71 | Classification loss: 0.00030 | Regression loss: 0.01898 | Running loss: 0.02540\n",
            "Epoch: 149 | Iteration: 72 | Classification loss: 0.00015 | Regression loss: 0.01645 | Running loss: 0.02538\n",
            "Epoch: 149 | Iteration: 73 | Classification loss: 0.00022 | Regression loss: 0.01687 | Running loss: 0.02538\n",
            "Epoch: 149 | Iteration: 74 | Classification loss: 0.00030 | Regression loss: 0.01565 | Running loss: 0.02536\n",
            "Epoch: 149 | Iteration: 75 | Classification loss: 0.00035 | Regression loss: 0.02703 | Running loss: 0.02537\n",
            "Epoch: 149 | Iteration: 76 | Classification loss: 0.00015 | Regression loss: 0.03308 | Running loss: 0.02540\n",
            "Epoch: 149 | Iteration: 77 | Classification loss: 0.00015 | Regression loss: 0.01216 | Running loss: 0.02535\n",
            "Epoch: 149 | Iteration: 78 | Classification loss: 0.00017 | Regression loss: 0.02223 | Running loss: 0.02536\n",
            "Epoch: 149 | Iteration: 79 | Classification loss: 0.00021 | Regression loss: 0.02115 | Running loss: 0.02537\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7196694265151026\n",
            "Precision:  0.5572519083969466\n",
            "Recall:  0.7891891891891892\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}]\n",
            "Epoch: 150 | Iteration: 0 | Classification loss: 0.00007 | Regression loss: 0.01430 | Running loss: 0.02532\n",
            "Epoch: 150 | Iteration: 1 | Classification loss: 0.00005 | Regression loss: 0.01863 | Running loss: 0.02531\n",
            "Epoch: 150 | Iteration: 2 | Classification loss: 0.00029 | Regression loss: 0.04129 | Running loss: 0.02533\n",
            "Epoch: 150 | Iteration: 3 | Classification loss: 0.00026 | Regression loss: 0.01879 | Running loss: 0.02529\n",
            "Epoch: 150 | Iteration: 4 | Classification loss: 0.00012 | Regression loss: 0.02752 | Running loss: 0.02525\n",
            "Epoch: 150 | Iteration: 5 | Classification loss: 0.00011 | Regression loss: 0.02354 | Running loss: 0.02523\n",
            "Epoch: 150 | Iteration: 6 | Classification loss: 0.00018 | Regression loss: 0.01660 | Running loss: 0.02519\n",
            "Epoch: 150 | Iteration: 7 | Classification loss: 0.00029 | Regression loss: 0.02453 | Running loss: 0.02519\n",
            "Epoch: 150 | Iteration: 8 | Classification loss: 0.00018 | Regression loss: 0.02986 | Running loss: 0.02523\n",
            "Epoch: 150 | Iteration: 9 | Classification loss: 0.00028 | Regression loss: 0.02237 | Running loss: 0.02523\n",
            "Epoch: 150 | Iteration: 10 | Classification loss: 0.00022 | Regression loss: 0.01099 | Running loss: 0.02523\n",
            "Epoch: 150 | Iteration: 11 | Classification loss: 0.00015 | Regression loss: 0.02459 | Running loss: 0.02521\n",
            "Epoch: 150 | Iteration: 12 | Classification loss: 0.00013 | Regression loss: 0.01472 | Running loss: 0.02515\n",
            "Epoch: 150 | Iteration: 13 | Classification loss: 0.00061 | Regression loss: 0.02476 | Running loss: 0.02519\n",
            "Epoch: 150 | Iteration: 14 | Classification loss: 0.00023 | Regression loss: 0.02436 | Running loss: 0.02518\n",
            "Epoch: 150 | Iteration: 15 | Classification loss: 0.00025 | Regression loss: 0.02577 | Running loss: 0.02521\n",
            "Epoch: 150 | Iteration: 16 | Classification loss: 0.00031 | Regression loss: 0.03063 | Running loss: 0.02522\n",
            "Epoch: 150 | Iteration: 17 | Classification loss: 0.00014 | Regression loss: 0.02999 | Running loss: 0.02522\n",
            "Epoch: 150 | Iteration: 18 | Classification loss: 0.00016 | Regression loss: 0.01367 | Running loss: 0.02518\n",
            "Epoch: 150 | Iteration: 19 | Classification loss: 0.00014 | Regression loss: 0.01101 | Running loss: 0.02517\n",
            "Epoch: 150 | Iteration: 20 | Classification loss: 0.00021 | Regression loss: 0.02021 | Running loss: 0.02517\n",
            "Epoch: 150 | Iteration: 21 | Classification loss: 0.00024 | Regression loss: 0.01530 | Running loss: 0.02517\n",
            "Epoch: 150 | Iteration: 22 | Classification loss: 0.00011 | Regression loss: 0.01309 | Running loss: 0.02514\n",
            "Epoch: 150 | Iteration: 23 | Classification loss: 0.00060 | Regression loss: 0.03444 | Running loss: 0.02514\n",
            "Epoch: 150 | Iteration: 24 | Classification loss: 0.00017 | Regression loss: 0.01852 | Running loss: 0.02513\n",
            "Epoch: 150 | Iteration: 25 | Classification loss: 0.00037 | Regression loss: 0.03530 | Running loss: 0.02515\n",
            "Epoch: 150 | Iteration: 26 | Classification loss: 0.00023 | Regression loss: 0.01796 | Running loss: 0.02512\n",
            "Epoch: 150 | Iteration: 27 | Classification loss: 0.00020 | Regression loss: 0.01983 | Running loss: 0.02512\n",
            "Epoch: 150 | Iteration: 28 | Classification loss: 0.00015 | Regression loss: 0.02348 | Running loss: 0.02513\n",
            "Epoch: 150 | Iteration: 29 | Classification loss: 0.00020 | Regression loss: 0.01867 | Running loss: 0.02512\n",
            "Epoch: 150 | Iteration: 30 | Classification loss: 0.00107 | Regression loss: 0.04490 | Running loss: 0.02511\n",
            "Epoch: 150 | Iteration: 31 | Classification loss: 0.00004 | Regression loss: 0.01101 | Running loss: 0.02509\n",
            "Epoch: 150 | Iteration: 32 | Classification loss: 0.00079 | Regression loss: 0.02397 | Running loss: 0.02511\n",
            "Epoch: 150 | Iteration: 33 | Classification loss: 0.00034 | Regression loss: 0.02171 | Running loss: 0.02507\n",
            "Epoch: 150 | Iteration: 34 | Classification loss: 0.00024 | Regression loss: 0.03899 | Running loss: 0.02510\n",
            "Epoch: 150 | Iteration: 35 | Classification loss: 0.00025 | Regression loss: 0.01944 | Running loss: 0.02509\n",
            "Epoch: 150 | Iteration: 36 | Classification loss: 0.00008 | Regression loss: 0.01679 | Running loss: 0.02510\n",
            "Epoch: 150 | Iteration: 37 | Classification loss: 0.00042 | Regression loss: 0.02899 | Running loss: 0.02510\n",
            "Epoch: 150 | Iteration: 38 | Classification loss: 0.00019 | Regression loss: 0.02071 | Running loss: 0.02510\n",
            "Epoch: 150 | Iteration: 39 | Classification loss: 0.00011 | Regression loss: 0.02279 | Running loss: 0.02508\n",
            "Epoch: 150 | Iteration: 40 | Classification loss: 0.00025 | Regression loss: 0.01613 | Running loss: 0.02505\n",
            "Epoch: 150 | Iteration: 41 | Classification loss: 0.00019 | Regression loss: 0.01733 | Running loss: 0.02504\n",
            "Epoch: 150 | Iteration: 42 | Classification loss: 0.00038 | Regression loss: 0.02419 | Running loss: 0.02500\n",
            "Epoch: 150 | Iteration: 43 | Classification loss: 0.00011 | Regression loss: 0.03099 | Running loss: 0.02500\n",
            "Epoch: 150 | Iteration: 44 | Classification loss: 0.00013 | Regression loss: 0.01868 | Running loss: 0.02499\n",
            "Epoch: 150 | Iteration: 45 | Classification loss: 0.00062 | Regression loss: 0.03576 | Running loss: 0.02499\n",
            "Epoch: 150 | Iteration: 46 | Classification loss: 0.00043 | Regression loss: 0.03439 | Running loss: 0.02503\n",
            "Epoch: 150 | Iteration: 47 | Classification loss: 0.00004 | Regression loss: 0.00467 | Running loss: 0.02497\n",
            "Epoch: 150 | Iteration: 48 | Classification loss: 0.00046 | Regression loss: 0.02739 | Running loss: 0.02497\n",
            "Epoch: 150 | Iteration: 49 | Classification loss: 0.00091 | Regression loss: 0.04048 | Running loss: 0.02501\n",
            "Epoch: 150 | Iteration: 50 | Classification loss: 0.00019 | Regression loss: 0.01240 | Running loss: 0.02496\n",
            "Epoch: 150 | Iteration: 51 | Classification loss: 0.00018 | Regression loss: 0.01186 | Running loss: 0.02491\n",
            "Epoch: 150 | Iteration: 52 | Classification loss: 0.00068 | Regression loss: 0.04251 | Running loss: 0.02495\n",
            "Epoch: 150 | Iteration: 53 | Classification loss: 0.00066 | Regression loss: 0.05384 | Running loss: 0.02502\n",
            "Epoch: 150 | Iteration: 54 | Classification loss: 0.00036 | Regression loss: 0.03833 | Running loss: 0.02502\n",
            "Epoch: 150 | Iteration: 55 | Classification loss: 0.00010 | Regression loss: 0.00639 | Running loss: 0.02500\n",
            "Epoch: 150 | Iteration: 56 | Classification loss: 0.00011 | Regression loss: 0.01689 | Running loss: 0.02498\n",
            "Epoch: 150 | Iteration: 57 | Classification loss: 0.00046 | Regression loss: 0.04070 | Running loss: 0.02502\n",
            "Epoch: 150 | Iteration: 58 | Classification loss: 0.00032 | Regression loss: 0.02116 | Running loss: 0.02503\n",
            "Epoch: 150 | Iteration: 59 | Classification loss: 0.00057 | Regression loss: 0.03368 | Running loss: 0.02505\n",
            "Epoch: 150 | Iteration: 60 | Classification loss: 0.00024 | Regression loss: 0.01504 | Running loss: 0.02502\n",
            "Epoch: 150 | Iteration: 61 | Classification loss: 0.00007 | Regression loss: 0.02371 | Running loss: 0.02497\n",
            "Epoch: 150 | Iteration: 62 | Classification loss: 0.00095 | Regression loss: 0.03954 | Running loss: 0.02502\n",
            "Epoch: 150 | Iteration: 63 | Classification loss: 0.00051 | Regression loss: 0.02391 | Running loss: 0.02503\n",
            "Epoch: 150 | Iteration: 64 | Classification loss: 0.00021 | Regression loss: 0.02018 | Running loss: 0.02503\n",
            "Epoch: 150 | Iteration: 65 | Classification loss: 0.00012 | Regression loss: 0.01567 | Running loss: 0.02500\n",
            "Epoch: 150 | Iteration: 66 | Classification loss: 0.00032 | Regression loss: 0.02729 | Running loss: 0.02500\n",
            "Epoch: 150 | Iteration: 67 | Classification loss: 0.00068 | Regression loss: 0.04520 | Running loss: 0.02508\n",
            "Epoch: 150 | Iteration: 68 | Classification loss: 0.00007 | Regression loss: 0.00854 | Running loss: 0.02506\n",
            "Epoch: 150 | Iteration: 69 | Classification loss: 0.00028 | Regression loss: 0.02089 | Running loss: 0.02501\n",
            "Epoch: 150 | Iteration: 70 | Classification loss: 0.00063 | Regression loss: 0.04144 | Running loss: 0.02505\n",
            "Epoch: 150 | Iteration: 71 | Classification loss: 0.00041 | Regression loss: 0.02922 | Running loss: 0.02504\n",
            "Epoch: 150 | Iteration: 72 | Classification loss: 0.00021 | Regression loss: 0.01805 | Running loss: 0.02504\n",
            "Epoch: 150 | Iteration: 73 | Classification loss: 0.00053 | Regression loss: 0.04926 | Running loss: 0.02511\n",
            "Epoch: 150 | Iteration: 74 | Classification loss: 0.00020 | Regression loss: 0.03063 | Running loss: 0.02508\n",
            "Epoch: 150 | Iteration: 75 | Classification loss: 0.00008 | Regression loss: 0.00853 | Running loss: 0.02507\n",
            "Epoch: 150 | Iteration: 76 | Classification loss: 0.00021 | Regression loss: 0.03703 | Running loss: 0.02510\n",
            "Epoch: 150 | Iteration: 77 | Classification loss: 0.00039 | Regression loss: 0.02256 | Running loss: 0.02509\n",
            "Epoch: 150 | Iteration: 78 | Classification loss: 0.00074 | Regression loss: 0.02504 | Running loss: 0.02511\n",
            "Epoch: 150 | Iteration: 79 | Classification loss: 0.00024 | Regression loss: 0.02147 | Running loss: 0.02506\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7248336954943913\n",
            "Precision:  0.5610687022900763\n",
            "Recall:  0.7945945945945946\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}]\n",
            "Epoch: 151 | Iteration: 0 | Classification loss: 0.00019 | Regression loss: 0.01704 | Running loss: 0.02507\n",
            "Epoch: 151 | Iteration: 1 | Classification loss: 0.00038 | Regression loss: 0.02706 | Running loss: 0.02511\n",
            "Epoch: 151 | Iteration: 2 | Classification loss: 0.00027 | Regression loss: 0.04032 | Running loss: 0.02517\n",
            "Epoch: 151 | Iteration: 3 | Classification loss: 0.00058 | Regression loss: 0.04363 | Running loss: 0.02523\n",
            "Epoch: 151 | Iteration: 4 | Classification loss: 0.00016 | Regression loss: 0.03069 | Running loss: 0.02521\n",
            "Epoch: 151 | Iteration: 5 | Classification loss: 0.00063 | Regression loss: 0.04100 | Running loss: 0.02526\n",
            "Epoch: 151 | Iteration: 6 | Classification loss: 0.00009 | Regression loss: 0.01318 | Running loss: 0.02525\n",
            "Epoch: 151 | Iteration: 7 | Classification loss: 0.00019 | Regression loss: 0.01806 | Running loss: 0.02520\n",
            "Epoch: 151 | Iteration: 8 | Classification loss: 0.00017 | Regression loss: 0.01729 | Running loss: 0.02519\n",
            "Epoch: 151 | Iteration: 9 | Classification loss: 0.00062 | Regression loss: 0.05288 | Running loss: 0.02524\n",
            "Epoch: 151 | Iteration: 10 | Classification loss: 0.00007 | Regression loss: 0.02173 | Running loss: 0.02523\n",
            "Epoch: 151 | Iteration: 11 | Classification loss: 0.00002 | Regression loss: 0.00499 | Running loss: 0.02520\n",
            "Epoch: 151 | Iteration: 12 | Classification loss: 0.00006 | Regression loss: 0.00822 | Running loss: 0.02517\n",
            "Epoch: 151 | Iteration: 13 | Classification loss: 0.00022 | Regression loss: 0.01369 | Running loss: 0.02518\n",
            "Epoch: 151 | Iteration: 14 | Classification loss: 0.00006 | Regression loss: 0.02170 | Running loss: 0.02519\n",
            "Epoch: 151 | Iteration: 15 | Classification loss: 0.00014 | Regression loss: 0.01597 | Running loss: 0.02517\n",
            "Epoch: 151 | Iteration: 16 | Classification loss: 0.00024 | Regression loss: 0.01636 | Running loss: 0.02518\n",
            "Epoch: 151 | Iteration: 17 | Classification loss: 0.00061 | Regression loss: 0.04238 | Running loss: 0.02522\n",
            "Epoch: 151 | Iteration: 18 | Classification loss: 0.00007 | Regression loss: 0.01462 | Running loss: 0.02518\n",
            "Epoch: 151 | Iteration: 19 | Classification loss: 0.00016 | Regression loss: 0.01617 | Running loss: 0.02513\n",
            "Epoch: 151 | Iteration: 20 | Classification loss: 0.00011 | Regression loss: 0.02531 | Running loss: 0.02515\n",
            "Epoch: 151 | Iteration: 21 | Classification loss: 0.00032 | Regression loss: 0.02118 | Running loss: 0.02510\n",
            "Epoch: 151 | Iteration: 22 | Classification loss: 0.00022 | Regression loss: 0.02001 | Running loss: 0.02503\n",
            "Epoch: 151 | Iteration: 23 | Classification loss: 0.00036 | Regression loss: 0.02312 | Running loss: 0.02502\n",
            "Epoch: 151 | Iteration: 24 | Classification loss: 0.00015 | Regression loss: 0.01460 | Running loss: 0.02499\n",
            "Epoch: 151 | Iteration: 25 | Classification loss: 0.00005 | Regression loss: 0.01097 | Running loss: 0.02498\n",
            "Epoch: 151 | Iteration: 26 | Classification loss: 0.00027 | Regression loss: 0.03398 | Running loss: 0.02501\n",
            "Epoch: 151 | Iteration: 27 | Classification loss: 0.00068 | Regression loss: 0.02599 | Running loss: 0.02503\n",
            "Epoch: 151 | Iteration: 28 | Classification loss: 0.00028 | Regression loss: 0.02520 | Running loss: 0.02507\n",
            "Epoch: 151 | Iteration: 29 | Classification loss: 0.00010 | Regression loss: 0.02845 | Running loss: 0.02507\n",
            "Epoch: 151 | Iteration: 30 | Classification loss: 0.00024 | Regression loss: 0.03792 | Running loss: 0.02511\n",
            "Epoch: 151 | Iteration: 31 | Classification loss: 0.00008 | Regression loss: 0.02286 | Running loss: 0.02513\n",
            "Epoch: 151 | Iteration: 32 | Classification loss: 0.00015 | Regression loss: 0.03281 | Running loss: 0.02516\n",
            "Epoch: 151 | Iteration: 33 | Classification loss: 0.00084 | Regression loss: 0.03540 | Running loss: 0.02520\n",
            "Epoch: 151 | Iteration: 34 | Classification loss: 0.00009 | Regression loss: 0.02603 | Running loss: 0.02520\n",
            "Epoch: 151 | Iteration: 35 | Classification loss: 0.00014 | Regression loss: 0.00971 | Running loss: 0.02518\n",
            "Epoch: 151 | Iteration: 36 | Classification loss: 0.00016 | Regression loss: 0.01909 | Running loss: 0.02513\n",
            "Epoch: 151 | Iteration: 37 | Classification loss: 0.00028 | Regression loss: 0.01456 | Running loss: 0.02510\n",
            "Epoch: 151 | Iteration: 38 | Classification loss: 0.00023 | Regression loss: 0.02401 | Running loss: 0.02506\n",
            "Epoch: 151 | Iteration: 39 | Classification loss: 0.00026 | Regression loss: 0.03304 | Running loss: 0.02509\n",
            "Epoch: 151 | Iteration: 40 | Classification loss: 0.00063 | Regression loss: 0.02486 | Running loss: 0.02511\n",
            "Epoch: 151 | Iteration: 41 | Classification loss: 0.00006 | Regression loss: 0.01673 | Running loss: 0.02509\n",
            "Epoch: 151 | Iteration: 42 | Classification loss: 0.00041 | Regression loss: 0.04502 | Running loss: 0.02511\n",
            "Epoch: 151 | Iteration: 43 | Classification loss: 0.00008 | Regression loss: 0.01862 | Running loss: 0.02510\n",
            "Epoch: 151 | Iteration: 44 | Classification loss: 0.00101 | Regression loss: 0.02665 | Running loss: 0.02512\n",
            "Epoch: 151 | Iteration: 45 | Classification loss: 0.00043 | Regression loss: 0.03397 | Running loss: 0.02514\n",
            "Epoch: 151 | Iteration: 46 | Classification loss: 0.00017 | Regression loss: 0.02337 | Running loss: 0.02512\n",
            "Epoch: 151 | Iteration: 47 | Classification loss: 0.00015 | Regression loss: 0.02501 | Running loss: 0.02513\n",
            "Epoch: 151 | Iteration: 48 | Classification loss: 0.00018 | Regression loss: 0.01934 | Running loss: 0.02507\n",
            "Epoch: 151 | Iteration: 49 | Classification loss: 0.00041 | Regression loss: 0.03436 | Running loss: 0.02511\n",
            "Epoch: 151 | Iteration: 50 | Classification loss: 0.00015 | Regression loss: 0.01991 | Running loss: 0.02513\n",
            "Epoch: 151 | Iteration: 51 | Classification loss: 0.00089 | Regression loss: 0.03714 | Running loss: 0.02515\n",
            "Epoch: 151 | Iteration: 52 | Classification loss: 0.00044 | Regression loss: 0.02611 | Running loss: 0.02515\n",
            "Epoch: 151 | Iteration: 53 | Classification loss: 0.00050 | Regression loss: 0.02842 | Running loss: 0.02518\n",
            "Epoch: 151 | Iteration: 54 | Classification loss: 0.00005 | Regression loss: 0.00842 | Running loss: 0.02516\n",
            "Epoch: 151 | Iteration: 55 | Classification loss: 0.00030 | Regression loss: 0.02060 | Running loss: 0.02511\n",
            "Epoch: 151 | Iteration: 56 | Classification loss: 0.00022 | Regression loss: 0.01801 | Running loss: 0.02510\n",
            "Epoch: 151 | Iteration: 57 | Classification loss: 0.00025 | Regression loss: 0.02709 | Running loss: 0.02510\n",
            "Epoch: 151 | Iteration: 58 | Classification loss: 0.00016 | Regression loss: 0.03067 | Running loss: 0.02512\n",
            "Epoch: 151 | Iteration: 59 | Classification loss: 0.00008 | Regression loss: 0.00629 | Running loss: 0.02510\n",
            "Epoch: 151 | Iteration: 60 | Classification loss: 0.00026 | Regression loss: 0.01563 | Running loss: 0.02509\n",
            "Epoch: 151 | Iteration: 61 | Classification loss: 0.00064 | Regression loss: 0.03563 | Running loss: 0.02509\n",
            "Epoch: 151 | Iteration: 62 | Classification loss: 0.00016 | Regression loss: 0.01211 | Running loss: 0.02507\n",
            "Epoch: 151 | Iteration: 63 | Classification loss: 0.00021 | Regression loss: 0.02662 | Running loss: 0.02507\n",
            "Epoch: 151 | Iteration: 64 | Classification loss: 0.00010 | Regression loss: 0.01079 | Running loss: 0.02507\n",
            "Epoch: 151 | Iteration: 65 | Classification loss: 0.00022 | Regression loss: 0.01003 | Running loss: 0.02503\n",
            "Epoch: 151 | Iteration: 66 | Classification loss: 0.00027 | Regression loss: 0.02658 | Running loss: 0.02502\n",
            "Epoch: 151 | Iteration: 67 | Classification loss: 0.00013 | Regression loss: 0.01751 | Running loss: 0.02502\n",
            "Epoch: 151 | Iteration: 68 | Classification loss: 0.00102 | Regression loss: 0.04342 | Running loss: 0.02502\n",
            "Epoch: 151 | Iteration: 69 | Classification loss: 0.00032 | Regression loss: 0.02691 | Running loss: 0.02501\n",
            "Epoch: 151 | Iteration: 70 | Classification loss: 0.00018 | Regression loss: 0.02039 | Running loss: 0.02498\n",
            "Epoch: 151 | Iteration: 71 | Classification loss: 0.00041 | Regression loss: 0.03307 | Running loss: 0.02501\n",
            "Epoch: 151 | Iteration: 72 | Classification loss: 0.00033 | Regression loss: 0.02042 | Running loss: 0.02502\n",
            "Epoch: 151 | Iteration: 73 | Classification loss: 0.00021 | Regression loss: 0.03346 | Running loss: 0.02504\n",
            "Epoch: 151 | Iteration: 74 | Classification loss: 0.00030 | Regression loss: 0.02083 | Running loss: 0.02503\n",
            "Epoch: 151 | Iteration: 75 | Classification loss: 0.00014 | Regression loss: 0.01407 | Running loss: 0.02500\n",
            "Epoch: 151 | Iteration: 76 | Classification loss: 0.00047 | Regression loss: 0.02346 | Running loss: 0.02504\n",
            "Epoch: 151 | Iteration: 77 | Classification loss: 0.00035 | Regression loss: 0.03521 | Running loss: 0.02506\n",
            "Epoch: 151 | Iteration: 78 | Classification loss: 0.00016 | Regression loss: 0.01623 | Running loss: 0.02506\n",
            "Epoch: 151 | Iteration: 79 | Classification loss: 0.00064 | Regression loss: 0.04061 | Running loss: 0.02513\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7191814857867086\n",
            "Precision:  0.553030303030303\n",
            "Recall:  0.7891891891891892\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}]\n",
            "Epoch: 152 | Iteration: 0 | Classification loss: 0.00025 | Regression loss: 0.03556 | Running loss: 0.02513\n",
            "Epoch: 152 | Iteration: 1 | Classification loss: 0.00064 | Regression loss: 0.04123 | Running loss: 0.02518\n",
            "Epoch: 152 | Iteration: 2 | Classification loss: 0.00013 | Regression loss: 0.02950 | Running loss: 0.02520\n",
            "Epoch: 152 | Iteration: 3 | Classification loss: 0.00013 | Regression loss: 0.01649 | Running loss: 0.02516\n",
            "Epoch: 152 | Iteration: 4 | Classification loss: 0.00010 | Regression loss: 0.00612 | Running loss: 0.02509\n",
            "Epoch: 152 | Iteration: 5 | Classification loss: 0.00027 | Regression loss: 0.02184 | Running loss: 0.02508\n",
            "Epoch: 152 | Iteration: 6 | Classification loss: 0.00020 | Regression loss: 0.01885 | Running loss: 0.02505\n",
            "Epoch: 152 | Iteration: 7 | Classification loss: 0.00013 | Regression loss: 0.02130 | Running loss: 0.02504\n",
            "Epoch: 152 | Iteration: 8 | Classification loss: 0.00019 | Regression loss: 0.02545 | Running loss: 0.02503\n",
            "Epoch: 152 | Iteration: 9 | Classification loss: 0.00025 | Regression loss: 0.02183 | Running loss: 0.02499\n",
            "Epoch: 152 | Iteration: 10 | Classification loss: 0.00019 | Regression loss: 0.01041 | Running loss: 0.02497\n",
            "Epoch: 152 | Iteration: 11 | Classification loss: 0.00057 | Regression loss: 0.02624 | Running loss: 0.02497\n",
            "Epoch: 152 | Iteration: 12 | Classification loss: 0.00013 | Regression loss: 0.01500 | Running loss: 0.02492\n",
            "Epoch: 152 | Iteration: 13 | Classification loss: 0.00036 | Regression loss: 0.01950 | Running loss: 0.02491\n",
            "Epoch: 152 | Iteration: 14 | Classification loss: 0.00056 | Regression loss: 0.04167 | Running loss: 0.02492\n",
            "Epoch: 152 | Iteration: 15 | Classification loss: 0.00018 | Regression loss: 0.02109 | Running loss: 0.02493\n",
            "Epoch: 152 | Iteration: 16 | Classification loss: 0.00019 | Regression loss: 0.01917 | Running loss: 0.02495\n",
            "Epoch: 152 | Iteration: 17 | Classification loss: 0.00030 | Regression loss: 0.02032 | Running loss: 0.02489\n",
            "Epoch: 152 | Iteration: 18 | Classification loss: 0.00015 | Regression loss: 0.02199 | Running loss: 0.02489\n",
            "Epoch: 152 | Iteration: 19 | Classification loss: 0.00089 | Regression loss: 0.03785 | Running loss: 0.02492\n",
            "Epoch: 152 | Iteration: 20 | Classification loss: 0.00007 | Regression loss: 0.00780 | Running loss: 0.02491\n",
            "Epoch: 152 | Iteration: 21 | Classification loss: 0.00008 | Regression loss: 0.01066 | Running loss: 0.02490\n",
            "Epoch: 152 | Iteration: 22 | Classification loss: 0.00015 | Regression loss: 0.01943 | Running loss: 0.02488\n",
            "Epoch: 152 | Iteration: 23 | Classification loss: 0.00016 | Regression loss: 0.01743 | Running loss: 0.02489\n",
            "Epoch: 152 | Iteration: 24 | Classification loss: 0.00039 | Regression loss: 0.03362 | Running loss: 0.02485\n",
            "Epoch: 152 | Iteration: 25 | Classification loss: 0.00021 | Regression loss: 0.02633 | Running loss: 0.02487\n",
            "Epoch: 152 | Iteration: 26 | Classification loss: 0.00027 | Regression loss: 0.01686 | Running loss: 0.02489\n",
            "Epoch: 152 | Iteration: 27 | Classification loss: 0.00055 | Regression loss: 0.03315 | Running loss: 0.02492\n",
            "Epoch: 152 | Iteration: 28 | Classification loss: 0.00020 | Regression loss: 0.01741 | Running loss: 0.02489\n",
            "Epoch: 152 | Iteration: 29 | Classification loss: 0.00024 | Regression loss: 0.02604 | Running loss: 0.02491\n",
            "Epoch: 152 | Iteration: 30 | Classification loss: 0.00056 | Regression loss: 0.02511 | Running loss: 0.02492\n",
            "Epoch: 152 | Iteration: 31 | Classification loss: 0.00008 | Regression loss: 0.02208 | Running loss: 0.02488\n",
            "Epoch: 152 | Iteration: 32 | Classification loss: 0.00014 | Regression loss: 0.02497 | Running loss: 0.02491\n",
            "Epoch: 152 | Iteration: 33 | Classification loss: 0.00066 | Regression loss: 0.03860 | Running loss: 0.02497\n",
            "Epoch: 152 | Iteration: 34 | Classification loss: 0.00024 | Regression loss: 0.02865 | Running loss: 0.02498\n",
            "Epoch: 152 | Iteration: 35 | Classification loss: 0.00072 | Regression loss: 0.04830 | Running loss: 0.02499\n",
            "Epoch: 152 | Iteration: 36 | Classification loss: 0.00069 | Regression loss: 0.02451 | Running loss: 0.02502\n",
            "Epoch: 152 | Iteration: 37 | Classification loss: 0.00003 | Regression loss: 0.01082 | Running loss: 0.02503\n",
            "Epoch: 152 | Iteration: 38 | Classification loss: 0.00014 | Regression loss: 0.00937 | Running loss: 0.02495\n",
            "Epoch: 152 | Iteration: 39 | Classification loss: 0.00019 | Regression loss: 0.01414 | Running loss: 0.02493\n",
            "Epoch: 152 | Iteration: 40 | Classification loss: 0.00018 | Regression loss: 0.02135 | Running loss: 0.02494\n",
            "Epoch: 152 | Iteration: 41 | Classification loss: 0.00055 | Regression loss: 0.04196 | Running loss: 0.02495\n",
            "Epoch: 152 | Iteration: 42 | Classification loss: 0.00010 | Regression loss: 0.01090 | Running loss: 0.02493\n",
            "Epoch: 152 | Iteration: 43 | Classification loss: 0.00032 | Regression loss: 0.02784 | Running loss: 0.02494\n",
            "Epoch: 152 | Iteration: 44 | Classification loss: 0.00015 | Regression loss: 0.01366 | Running loss: 0.02489\n",
            "Epoch: 152 | Iteration: 45 | Classification loss: 0.00022 | Regression loss: 0.01485 | Running loss: 0.02486\n",
            "Epoch: 152 | Iteration: 46 | Classification loss: 0.00035 | Regression loss: 0.03675 | Running loss: 0.02487\n",
            "Epoch: 152 | Iteration: 47 | Classification loss: 0.00010 | Regression loss: 0.01643 | Running loss: 0.02484\n",
            "Epoch: 152 | Iteration: 48 | Classification loss: 0.00087 | Regression loss: 0.04192 | Running loss: 0.02487\n",
            "Epoch: 152 | Iteration: 49 | Classification loss: 0.00036 | Regression loss: 0.03267 | Running loss: 0.02491\n",
            "Epoch: 152 | Iteration: 50 | Classification loss: 0.00017 | Regression loss: 0.01951 | Running loss: 0.02490\n",
            "Epoch: 152 | Iteration: 51 | Classification loss: 0.00053 | Regression loss: 0.05271 | Running loss: 0.02495\n",
            "Epoch: 152 | Iteration: 52 | Classification loss: 0.00027 | Regression loss: 0.03940 | Running loss: 0.02498\n",
            "Epoch: 152 | Iteration: 53 | Classification loss: 0.00010 | Regression loss: 0.03201 | Running loss: 0.02497\n",
            "Epoch: 152 | Iteration: 54 | Classification loss: 0.00011 | Regression loss: 0.02505 | Running loss: 0.02494\n",
            "Epoch: 152 | Iteration: 55 | Classification loss: 0.00041 | Regression loss: 0.02665 | Running loss: 0.02495\n",
            "Epoch: 152 | Iteration: 56 | Classification loss: 0.00011 | Regression loss: 0.01685 | Running loss: 0.02494\n",
            "Epoch: 152 | Iteration: 57 | Classification loss: 0.00012 | Regression loss: 0.02216 | Running loss: 0.02495\n",
            "Epoch: 152 | Iteration: 58 | Classification loss: 0.00080 | Regression loss: 0.02173 | Running loss: 0.02494\n",
            "Epoch: 152 | Iteration: 59 | Classification loss: 0.00021 | Regression loss: 0.02785 | Running loss: 0.02491\n",
            "Epoch: 152 | Iteration: 60 | Classification loss: 0.00022 | Regression loss: 0.02331 | Running loss: 0.02490\n",
            "Epoch: 152 | Iteration: 61 | Classification loss: 0.00015 | Regression loss: 0.01486 | Running loss: 0.02487\n",
            "Epoch: 152 | Iteration: 62 | Classification loss: 0.00011 | Regression loss: 0.01574 | Running loss: 0.02483\n",
            "Epoch: 152 | Iteration: 63 | Classification loss: 0.00034 | Regression loss: 0.03708 | Running loss: 0.02490\n",
            "Epoch: 152 | Iteration: 64 | Classification loss: 0.00021 | Regression loss: 0.01991 | Running loss: 0.02490\n",
            "Epoch: 152 | Iteration: 65 | Classification loss: 0.00026 | Regression loss: 0.01332 | Running loss: 0.02491\n",
            "Epoch: 152 | Iteration: 66 | Classification loss: 0.00006 | Regression loss: 0.01433 | Running loss: 0.02488\n",
            "Epoch: 152 | Iteration: 67 | Classification loss: 0.00003 | Regression loss: 0.00496 | Running loss: 0.02484\n",
            "Epoch: 152 | Iteration: 68 | Classification loss: 0.00048 | Regression loss: 0.02342 | Running loss: 0.02485\n",
            "Epoch: 152 | Iteration: 69 | Classification loss: 0.00074 | Regression loss: 0.03453 | Running loss: 0.02489\n",
            "Epoch: 152 | Iteration: 70 | Classification loss: 0.00010 | Regression loss: 0.01739 | Running loss: 0.02487\n",
            "Epoch: 152 | Iteration: 71 | Classification loss: 0.00017 | Regression loss: 0.01714 | Running loss: 0.02482\n",
            "Epoch: 152 | Iteration: 72 | Classification loss: 0.00024 | Regression loss: 0.03817 | Running loss: 0.02481\n",
            "Epoch: 152 | Iteration: 73 | Classification loss: 0.00017 | Regression loss: 0.03121 | Running loss: 0.02483\n",
            "Epoch: 152 | Iteration: 74 | Classification loss: 0.00017 | Regression loss: 0.02702 | Running loss: 0.02482\n",
            "Epoch: 152 | Iteration: 75 | Classification loss: 0.00007 | Regression loss: 0.01668 | Running loss: 0.02481\n",
            "Epoch: 152 | Iteration: 76 | Classification loss: 0.00030 | Regression loss: 0.02021 | Running loss: 0.02480\n",
            "Epoch: 152 | Iteration: 77 | Classification loss: 0.00062 | Regression loss: 0.03362 | Running loss: 0.02483\n",
            "Epoch: 152 | Iteration: 78 | Classification loss: 0.00008 | Regression loss: 0.02219 | Running loss: 0.02480\n",
            "Epoch: 152 | Iteration: 79 | Classification loss: 0.00048 | Regression loss: 0.02791 | Running loss: 0.02482\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7262342939555708\n",
            "Precision:  0.5610687022900763\n",
            "Recall:  0.7945945945945946\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}]\n",
            "Epoch: 153 | Iteration: 0 | Classification loss: 0.00020 | Regression loss: 0.02830 | Running loss: 0.02479\n",
            "Epoch: 153 | Iteration: 1 | Classification loss: 0.00023 | Regression loss: 0.01686 | Running loss: 0.02479\n",
            "Epoch: 153 | Iteration: 2 | Classification loss: 0.00018 | Regression loss: 0.02646 | Running loss: 0.02479\n",
            "Epoch: 153 | Iteration: 3 | Classification loss: 0.00054 | Regression loss: 0.03442 | Running loss: 0.02483\n",
            "Epoch: 153 | Iteration: 4 | Classification loss: 0.00020 | Regression loss: 0.02985 | Running loss: 0.02485\n",
            "Epoch: 153 | Iteration: 5 | Classification loss: 0.00028 | Regression loss: 0.03784 | Running loss: 0.02489\n",
            "Epoch: 153 | Iteration: 6 | Classification loss: 0.00059 | Regression loss: 0.04110 | Running loss: 0.02492\n",
            "Epoch: 153 | Iteration: 7 | Classification loss: 0.00020 | Regression loss: 0.02512 | Running loss: 0.02492\n",
            "Epoch: 153 | Iteration: 8 | Classification loss: 0.00008 | Regression loss: 0.02986 | Running loss: 0.02494\n",
            "Epoch: 153 | Iteration: 9 | Classification loss: 0.00017 | Regression loss: 0.02918 | Running loss: 0.02493\n",
            "Epoch: 153 | Iteration: 10 | Classification loss: 0.00008 | Regression loss: 0.02133 | Running loss: 0.02494\n",
            "Epoch: 153 | Iteration: 11 | Classification loss: 0.00025 | Regression loss: 0.03997 | Running loss: 0.02497\n",
            "Epoch: 153 | Iteration: 12 | Classification loss: 0.00018 | Regression loss: 0.01997 | Running loss: 0.02497\n",
            "Epoch: 153 | Iteration: 13 | Classification loss: 0.00006 | Regression loss: 0.01815 | Running loss: 0.02498\n",
            "Epoch: 153 | Iteration: 14 | Classification loss: 0.00042 | Regression loss: 0.03360 | Running loss: 0.02500\n",
            "Epoch: 153 | Iteration: 15 | Classification loss: 0.00015 | Regression loss: 0.02559 | Running loss: 0.02496\n",
            "Epoch: 153 | Iteration: 16 | Classification loss: 0.00022 | Regression loss: 0.03217 | Running loss: 0.02498\n",
            "Epoch: 153 | Iteration: 17 | Classification loss: 0.00016 | Regression loss: 0.01512 | Running loss: 0.02495\n",
            "Epoch: 153 | Iteration: 18 | Classification loss: 0.00014 | Regression loss: 0.02928 | Running loss: 0.02495\n",
            "Epoch: 153 | Iteration: 19 | Classification loss: 0.00027 | Regression loss: 0.02007 | Running loss: 0.02496\n",
            "Epoch: 153 | Iteration: 20 | Classification loss: 0.00014 | Regression loss: 0.02258 | Running loss: 0.02496\n",
            "Epoch: 153 | Iteration: 21 | Classification loss: 0.00075 | Regression loss: 0.02146 | Running loss: 0.02493\n",
            "Epoch: 153 | Iteration: 22 | Classification loss: 0.00086 | Regression loss: 0.03671 | Running loss: 0.02500\n",
            "Epoch: 153 | Iteration: 23 | Classification loss: 0.00022 | Regression loss: 0.01959 | Running loss: 0.02499\n",
            "Epoch: 153 | Iteration: 24 | Classification loss: 0.00017 | Regression loss: 0.01957 | Running loss: 0.02498\n",
            "Epoch: 153 | Iteration: 25 | Classification loss: 0.00050 | Regression loss: 0.03346 | Running loss: 0.02498\n",
            "Epoch: 153 | Iteration: 26 | Classification loss: 0.00003 | Regression loss: 0.00468 | Running loss: 0.02491\n",
            "Epoch: 153 | Iteration: 27 | Classification loss: 0.00024 | Regression loss: 0.02136 | Running loss: 0.02491\n",
            "Epoch: 153 | Iteration: 28 | Classification loss: 0.00041 | Regression loss: 0.03003 | Running loss: 0.02493\n",
            "Epoch: 153 | Iteration: 29 | Classification loss: 0.00039 | Regression loss: 0.02600 | Running loss: 0.02492\n",
            "Epoch: 153 | Iteration: 30 | Classification loss: 0.00056 | Regression loss: 0.03802 | Running loss: 0.02494\n",
            "Epoch: 153 | Iteration: 31 | Classification loss: 0.00031 | Regression loss: 0.03587 | Running loss: 0.02497\n",
            "Epoch: 153 | Iteration: 32 | Classification loss: 0.00010 | Regression loss: 0.02466 | Running loss: 0.02498\n",
            "Epoch: 153 | Iteration: 33 | Classification loss: 0.00021 | Regression loss: 0.02840 | Running loss: 0.02501\n",
            "Epoch: 153 | Iteration: 34 | Classification loss: 0.00020 | Regression loss: 0.01999 | Running loss: 0.02502\n",
            "Epoch: 153 | Iteration: 35 | Classification loss: 0.00003 | Regression loss: 0.01073 | Running loss: 0.02499\n",
            "Epoch: 153 | Iteration: 36 | Classification loss: 0.00024 | Regression loss: 0.03325 | Running loss: 0.02501\n",
            "Epoch: 153 | Iteration: 37 | Classification loss: 0.00025 | Regression loss: 0.01350 | Running loss: 0.02501\n",
            "Epoch: 153 | Iteration: 38 | Classification loss: 0.00011 | Regression loss: 0.01610 | Running loss: 0.02499\n",
            "Epoch: 153 | Iteration: 39 | Classification loss: 0.00010 | Regression loss: 0.03168 | Running loss: 0.02502\n",
            "Epoch: 153 | Iteration: 40 | Classification loss: 0.00010 | Regression loss: 0.01026 | Running loss: 0.02499\n",
            "Epoch: 153 | Iteration: 41 | Classification loss: 0.00008 | Regression loss: 0.02227 | Running loss: 0.02499\n",
            "Epoch: 153 | Iteration: 42 | Classification loss: 0.00015 | Regression loss: 0.01670 | Running loss: 0.02497\n",
            "Epoch: 153 | Iteration: 43 | Classification loss: 0.00010 | Regression loss: 0.01726 | Running loss: 0.02493\n",
            "Epoch: 153 | Iteration: 44 | Classification loss: 0.00010 | Regression loss: 0.01287 | Running loss: 0.02492\n",
            "Epoch: 153 | Iteration: 45 | Classification loss: 0.00017 | Regression loss: 0.01305 | Running loss: 0.02493\n",
            "Epoch: 153 | Iteration: 46 | Classification loss: 0.00039 | Regression loss: 0.03031 | Running loss: 0.02494\n",
            "Epoch: 153 | Iteration: 47 | Classification loss: 0.00031 | Regression loss: 0.02069 | Running loss: 0.02487\n",
            "Epoch: 153 | Iteration: 48 | Classification loss: 0.00014 | Regression loss: 0.01694 | Running loss: 0.02482\n",
            "Epoch: 153 | Iteration: 49 | Classification loss: 0.00021 | Regression loss: 0.01477 | Running loss: 0.02480\n",
            "Epoch: 153 | Iteration: 50 | Classification loss: 0.00049 | Regression loss: 0.04653 | Running loss: 0.02487\n",
            "Epoch: 153 | Iteration: 51 | Classification loss: 0.00085 | Regression loss: 0.03705 | Running loss: 0.02493\n",
            "Epoch: 153 | Iteration: 52 | Classification loss: 0.00017 | Regression loss: 0.01668 | Running loss: 0.02491\n",
            "Epoch: 153 | Iteration: 53 | Classification loss: 0.00011 | Regression loss: 0.01508 | Running loss: 0.02490\n",
            "Epoch: 153 | Iteration: 54 | Classification loss: 0.00065 | Regression loss: 0.02174 | Running loss: 0.02492\n",
            "Epoch: 153 | Iteration: 55 | Classification loss: 0.00017 | Regression loss: 0.01906 | Running loss: 0.02493\n",
            "Epoch: 153 | Iteration: 56 | Classification loss: 0.00026 | Regression loss: 0.03077 | Running loss: 0.02496\n",
            "Epoch: 153 | Iteration: 57 | Classification loss: 0.00021 | Regression loss: 0.01446 | Running loss: 0.02492\n",
            "Epoch: 153 | Iteration: 58 | Classification loss: 0.00014 | Regression loss: 0.01588 | Running loss: 0.02492\n",
            "Epoch: 153 | Iteration: 59 | Classification loss: 0.00027 | Regression loss: 0.02430 | Running loss: 0.02495\n",
            "Epoch: 153 | Iteration: 60 | Classification loss: 0.00067 | Regression loss: 0.02756 | Running loss: 0.02493\n",
            "Epoch: 153 | Iteration: 61 | Classification loss: 0.00035 | Regression loss: 0.02007 | Running loss: 0.02490\n",
            "Epoch: 153 | Iteration: 62 | Classification loss: 0.00054 | Regression loss: 0.02718 | Running loss: 0.02493\n",
            "Epoch: 153 | Iteration: 63 | Classification loss: 0.00052 | Regression loss: 0.03832 | Running loss: 0.02493\n",
            "Epoch: 153 | Iteration: 64 | Classification loss: 0.00035 | Regression loss: 0.02257 | Running loss: 0.02492\n",
            "Epoch: 153 | Iteration: 65 | Classification loss: 0.00008 | Regression loss: 0.00582 | Running loss: 0.02490\n",
            "Epoch: 153 | Iteration: 66 | Classification loss: 0.00018 | Regression loss: 0.01388 | Running loss: 0.02489\n",
            "Epoch: 153 | Iteration: 67 | Classification loss: 0.00015 | Regression loss: 0.02024 | Running loss: 0.02484\n",
            "Epoch: 153 | Iteration: 68 | Classification loss: 0.00053 | Regression loss: 0.04485 | Running loss: 0.02485\n",
            "Epoch: 153 | Iteration: 69 | Classification loss: 0.00006 | Regression loss: 0.00810 | Running loss: 0.02483\n",
            "Epoch: 153 | Iteration: 70 | Classification loss: 0.00024 | Regression loss: 0.01726 | Running loss: 0.02480\n",
            "Epoch: 153 | Iteration: 71 | Classification loss: 0.00086 | Regression loss: 0.04273 | Running loss: 0.02485\n",
            "Epoch: 153 | Iteration: 72 | Classification loss: 0.00006 | Regression loss: 0.01704 | Running loss: 0.02486\n",
            "Epoch: 153 | Iteration: 73 | Classification loss: 0.00007 | Regression loss: 0.01402 | Running loss: 0.02485\n",
            "Epoch: 153 | Iteration: 74 | Classification loss: 0.00015 | Regression loss: 0.01539 | Running loss: 0.02484\n",
            "Epoch: 153 | Iteration: 75 | Classification loss: 0.00016 | Regression loss: 0.01180 | Running loss: 0.02481\n",
            "Epoch: 153 | Iteration: 76 | Classification loss: 0.00026 | Regression loss: 0.02362 | Running loss: 0.02477\n",
            "Epoch: 153 | Iteration: 77 | Classification loss: 0.00006 | Regression loss: 0.00919 | Running loss: 0.02474\n",
            "Epoch: 153 | Iteration: 78 | Classification loss: 0.00062 | Regression loss: 0.04555 | Running loss: 0.02476\n",
            "Epoch: 153 | Iteration: 79 | Classification loss: 0.00064 | Regression loss: 0.05238 | Running loss: 0.02482\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7257730609271538\n",
            "Precision:  0.5568181818181818\n",
            "Recall:  0.7945945945945946\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}]\n",
            "Epoch: 154 | Iteration: 0 | Classification loss: 0.00022 | Regression loss: 0.01953 | Running loss: 0.02482\n",
            "Epoch: 154 | Iteration: 1 | Classification loss: 0.00011 | Regression loss: 0.01532 | Running loss: 0.02477\n",
            "Epoch: 154 | Iteration: 2 | Classification loss: 0.00098 | Regression loss: 0.02586 | Running loss: 0.02478\n",
            "Epoch: 154 | Iteration: 3 | Classification loss: 0.00008 | Regression loss: 0.00953 | Running loss: 0.02475\n",
            "Epoch: 154 | Iteration: 4 | Classification loss: 0.00011 | Regression loss: 0.02117 | Running loss: 0.02476\n",
            "Epoch: 154 | Iteration: 5 | Classification loss: 0.00028 | Regression loss: 0.01409 | Running loss: 0.02471\n",
            "Epoch: 154 | Iteration: 6 | Classification loss: 0.00022 | Regression loss: 0.03561 | Running loss: 0.02471\n",
            "Epoch: 154 | Iteration: 7 | Classification loss: 0.00007 | Regression loss: 0.00843 | Running loss: 0.02463\n",
            "Epoch: 154 | Iteration: 8 | Classification loss: 0.00012 | Regression loss: 0.02495 | Running loss: 0.02463\n",
            "Epoch: 154 | Iteration: 9 | Classification loss: 0.00014 | Regression loss: 0.02241 | Running loss: 0.02463\n",
            "Epoch: 154 | Iteration: 10 | Classification loss: 0.00051 | Regression loss: 0.02441 | Running loss: 0.02464\n",
            "Epoch: 154 | Iteration: 11 | Classification loss: 0.00021 | Regression loss: 0.01892 | Running loss: 0.02459\n",
            "Epoch: 154 | Iteration: 12 | Classification loss: 0.00011 | Regression loss: 0.02494 | Running loss: 0.02459\n",
            "Epoch: 154 | Iteration: 13 | Classification loss: 0.00017 | Regression loss: 0.01693 | Running loss: 0.02459\n",
            "Epoch: 154 | Iteration: 14 | Classification loss: 0.00028 | Regression loss: 0.01864 | Running loss: 0.02458\n",
            "Epoch: 154 | Iteration: 15 | Classification loss: 0.00017 | Regression loss: 0.02113 | Running loss: 0.02459\n",
            "Epoch: 154 | Iteration: 16 | Classification loss: 0.00021 | Regression loss: 0.01567 | Running loss: 0.02456\n",
            "Epoch: 154 | Iteration: 17 | Classification loss: 0.00056 | Regression loss: 0.04550 | Running loss: 0.02464\n",
            "Epoch: 154 | Iteration: 18 | Classification loss: 0.00019 | Regression loss: 0.01425 | Running loss: 0.02460\n",
            "Epoch: 154 | Iteration: 19 | Classification loss: 0.00022 | Regression loss: 0.02759 | Running loss: 0.02461\n",
            "Epoch: 154 | Iteration: 20 | Classification loss: 0.00038 | Regression loss: 0.02602 | Running loss: 0.02457\n",
            "Epoch: 154 | Iteration: 21 | Classification loss: 0.00023 | Regression loss: 0.02412 | Running loss: 0.02459\n",
            "Epoch: 154 | Iteration: 22 | Classification loss: 0.00062 | Regression loss: 0.02175 | Running loss: 0.02458\n",
            "Epoch: 154 | Iteration: 23 | Classification loss: 0.00050 | Regression loss: 0.05273 | Running loss: 0.02461\n",
            "Epoch: 154 | Iteration: 24 | Classification loss: 0.00010 | Regression loss: 0.01225 | Running loss: 0.02459\n",
            "Epoch: 154 | Iteration: 25 | Classification loss: 0.00047 | Regression loss: 0.03307 | Running loss: 0.02462\n",
            "Epoch: 154 | Iteration: 26 | Classification loss: 0.00019 | Regression loss: 0.03315 | Running loss: 0.02464\n",
            "Epoch: 154 | Iteration: 27 | Classification loss: 0.00020 | Regression loss: 0.02153 | Running loss: 0.02464\n",
            "Epoch: 154 | Iteration: 28 | Classification loss: 0.00026 | Regression loss: 0.02856 | Running loss: 0.02466\n",
            "Epoch: 154 | Iteration: 29 | Classification loss: 0.00065 | Regression loss: 0.02726 | Running loss: 0.02467\n",
            "Epoch: 154 | Iteration: 30 | Classification loss: 0.00007 | Regression loss: 0.01485 | Running loss: 0.02462\n",
            "Epoch: 154 | Iteration: 31 | Classification loss: 0.00027 | Regression loss: 0.01990 | Running loss: 0.02463\n",
            "Epoch: 154 | Iteration: 32 | Classification loss: 0.00022 | Regression loss: 0.04124 | Running loss: 0.02469\n",
            "Epoch: 154 | Iteration: 33 | Classification loss: 0.00006 | Regression loss: 0.01651 | Running loss: 0.02466\n",
            "Epoch: 154 | Iteration: 34 | Classification loss: 0.00018 | Regression loss: 0.01988 | Running loss: 0.02463\n",
            "Epoch: 154 | Iteration: 35 | Classification loss: 0.00023 | Regression loss: 0.01328 | Running loss: 0.02457\n",
            "Epoch: 154 | Iteration: 36 | Classification loss: 0.00024 | Regression loss: 0.02012 | Running loss: 0.02456\n",
            "Epoch: 154 | Iteration: 37 | Classification loss: 0.00006 | Regression loss: 0.01632 | Running loss: 0.02454\n",
            "Epoch: 154 | Iteration: 38 | Classification loss: 0.00004 | Regression loss: 0.00480 | Running loss: 0.02451\n",
            "Epoch: 154 | Iteration: 39 | Classification loss: 0.00029 | Regression loss: 0.01519 | Running loss: 0.02452\n",
            "Epoch: 154 | Iteration: 40 | Classification loss: 0.00012 | Regression loss: 0.02338 | Running loss: 0.02453\n",
            "Epoch: 154 | Iteration: 41 | Classification loss: 0.00026 | Regression loss: 0.03272 | Running loss: 0.02458\n",
            "Epoch: 154 | Iteration: 42 | Classification loss: 0.00016 | Regression loss: 0.01194 | Running loss: 0.02455\n",
            "Epoch: 154 | Iteration: 43 | Classification loss: 0.00024 | Regression loss: 0.02543 | Running loss: 0.02454\n",
            "Epoch: 154 | Iteration: 44 | Classification loss: 0.00016 | Regression loss: 0.01581 | Running loss: 0.02454\n",
            "Epoch: 154 | Iteration: 45 | Classification loss: 0.00060 | Regression loss: 0.04044 | Running loss: 0.02457\n",
            "Epoch: 154 | Iteration: 46 | Classification loss: 0.00013 | Regression loss: 0.02335 | Running loss: 0.02459\n",
            "Epoch: 154 | Iteration: 47 | Classification loss: 0.00008 | Regression loss: 0.02333 | Running loss: 0.02458\n",
            "Epoch: 154 | Iteration: 48 | Classification loss: 0.00047 | Regression loss: 0.03056 | Running loss: 0.02462\n",
            "Epoch: 154 | Iteration: 49 | Classification loss: 0.00045 | Regression loss: 0.02866 | Running loss: 0.02461\n",
            "Epoch: 154 | Iteration: 50 | Classification loss: 0.00027 | Regression loss: 0.03233 | Running loss: 0.02465\n",
            "Epoch: 154 | Iteration: 51 | Classification loss: 0.00032 | Regression loss: 0.02336 | Running loss: 0.02463\n",
            "Epoch: 154 | Iteration: 52 | Classification loss: 0.00034 | Regression loss: 0.03433 | Running loss: 0.02463\n",
            "Epoch: 154 | Iteration: 53 | Classification loss: 0.00052 | Regression loss: 0.04664 | Running loss: 0.02468\n",
            "Epoch: 154 | Iteration: 54 | Classification loss: 0.00083 | Regression loss: 0.03802 | Running loss: 0.02472\n",
            "Epoch: 154 | Iteration: 55 | Classification loss: 0.00021 | Regression loss: 0.01683 | Running loss: 0.02468\n",
            "Epoch: 154 | Iteration: 56 | Classification loss: 0.00013 | Regression loss: 0.01457 | Running loss: 0.02467\n",
            "Epoch: 154 | Iteration: 57 | Classification loss: 0.00010 | Regression loss: 0.01700 | Running loss: 0.02465\n",
            "Epoch: 154 | Iteration: 58 | Classification loss: 0.00058 | Regression loss: 0.04047 | Running loss: 0.02468\n",
            "Epoch: 154 | Iteration: 59 | Classification loss: 0.00073 | Regression loss: 0.03361 | Running loss: 0.02470\n",
            "Epoch: 154 | Iteration: 60 | Classification loss: 0.00024 | Regression loss: 0.02672 | Running loss: 0.02467\n",
            "Epoch: 154 | Iteration: 61 | Classification loss: 0.00019 | Regression loss: 0.01578 | Running loss: 0.02464\n",
            "Epoch: 154 | Iteration: 62 | Classification loss: 0.00033 | Regression loss: 0.02037 | Running loss: 0.02458\n",
            "Epoch: 154 | Iteration: 63 | Classification loss: 0.00006 | Regression loss: 0.02137 | Running loss: 0.02459\n",
            "Epoch: 154 | Iteration: 64 | Classification loss: 0.00018 | Regression loss: 0.01986 | Running loss: 0.02458\n",
            "Epoch: 154 | Iteration: 65 | Classification loss: 0.00039 | Regression loss: 0.03330 | Running loss: 0.02457\n",
            "Epoch: 154 | Iteration: 66 | Classification loss: 0.00022 | Regression loss: 0.01452 | Running loss: 0.02456\n",
            "Epoch: 154 | Iteration: 67 | Classification loss: 0.00008 | Regression loss: 0.03049 | Running loss: 0.02456\n",
            "Epoch: 154 | Iteration: 68 | Classification loss: 0.00013 | Regression loss: 0.02815 | Running loss: 0.02459\n",
            "Epoch: 154 | Iteration: 69 | Classification loss: 0.00018 | Regression loss: 0.01652 | Running loss: 0.02458\n",
            "Epoch: 154 | Iteration: 70 | Classification loss: 0.00021 | Regression loss: 0.02266 | Running loss: 0.02456\n",
            "Epoch: 154 | Iteration: 71 | Classification loss: 0.00004 | Regression loss: 0.01127 | Running loss: 0.02452\n",
            "Epoch: 154 | Iteration: 72 | Classification loss: 0.00014 | Regression loss: 0.01865 | Running loss: 0.02453\n",
            "Epoch: 154 | Iteration: 73 | Classification loss: 0.00040 | Regression loss: 0.03293 | Running loss: 0.02455\n",
            "Epoch: 154 | Iteration: 74 | Classification loss: 0.00012 | Regression loss: 0.01378 | Running loss: 0.02457\n",
            "Epoch: 154 | Iteration: 75 | Classification loss: 0.00096 | Regression loss: 0.04452 | Running loss: 0.02462\n",
            "Epoch: 154 | Iteration: 76 | Classification loss: 0.00014 | Regression loss: 0.01054 | Running loss: 0.02456\n",
            "Epoch: 154 | Iteration: 77 | Classification loss: 0.00058 | Regression loss: 0.04019 | Running loss: 0.02457\n",
            "Epoch: 154 | Iteration: 78 | Classification loss: 0.00007 | Regression loss: 0.00881 | Running loss: 0.02453\n",
            "Epoch: 154 | Iteration: 79 | Classification loss: 0.00019 | Regression loss: 0.03287 | Running loss: 0.02454\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7229906021615096\n",
            "Precision:  0.55893536121673\n",
            "Recall:  0.7945945945945946\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}, {0: (0.7229906021615096, 185.0)}]\n",
            "Epoch: 155 | Iteration: 0 | Classification loss: 0.00084 | Regression loss: 0.03789 | Running loss: 0.02457\n",
            "Epoch: 155 | Iteration: 1 | Classification loss: 0.00010 | Regression loss: 0.02927 | Running loss: 0.02457\n",
            "Epoch: 155 | Iteration: 2 | Classification loss: 0.00047 | Regression loss: 0.03273 | Running loss: 0.02459\n",
            "Epoch: 155 | Iteration: 3 | Classification loss: 0.00010 | Regression loss: 0.01085 | Running loss: 0.02457\n",
            "Epoch: 155 | Iteration: 4 | Classification loss: 0.00006 | Regression loss: 0.01391 | Running loss: 0.02453\n",
            "Epoch: 155 | Iteration: 5 | Classification loss: 0.00023 | Regression loss: 0.01528 | Running loss: 0.02453\n",
            "Epoch: 155 | Iteration: 6 | Classification loss: 0.00009 | Regression loss: 0.02685 | Running loss: 0.02455\n",
            "Epoch: 155 | Iteration: 7 | Classification loss: 0.00015 | Regression loss: 0.01940 | Running loss: 0.02455\n",
            "Epoch: 155 | Iteration: 8 | Classification loss: 0.00007 | Regression loss: 0.02263 | Running loss: 0.02451\n",
            "Epoch: 155 | Iteration: 9 | Classification loss: 0.00003 | Regression loss: 0.01066 | Running loss: 0.02448\n",
            "Epoch: 155 | Iteration: 10 | Classification loss: 0.00066 | Regression loss: 0.02553 | Running loss: 0.02444\n",
            "Epoch: 155 | Iteration: 11 | Classification loss: 0.00013 | Regression loss: 0.01439 | Running loss: 0.02444\n",
            "Epoch: 155 | Iteration: 12 | Classification loss: 0.00016 | Regression loss: 0.03121 | Running loss: 0.02449\n",
            "Epoch: 155 | Iteration: 13 | Classification loss: 0.00025 | Regression loss: 0.02596 | Running loss: 0.02450\n",
            "Epoch: 155 | Iteration: 14 | Classification loss: 0.00020 | Regression loss: 0.01441 | Running loss: 0.02442\n",
            "Epoch: 155 | Iteration: 15 | Classification loss: 0.00009 | Regression loss: 0.02991 | Running loss: 0.02444\n",
            "Epoch: 155 | Iteration: 16 | Classification loss: 0.00006 | Regression loss: 0.02085 | Running loss: 0.02446\n",
            "Epoch: 155 | Iteration: 17 | Classification loss: 0.00044 | Regression loss: 0.02290 | Running loss: 0.02447\n",
            "Epoch: 155 | Iteration: 18 | Classification loss: 0.00061 | Regression loss: 0.03878 | Running loss: 0.02450\n",
            "Epoch: 155 | Iteration: 19 | Classification loss: 0.00059 | Regression loss: 0.05286 | Running loss: 0.02457\n",
            "Epoch: 155 | Iteration: 20 | Classification loss: 0.00041 | Regression loss: 0.03394 | Running loss: 0.02455\n",
            "Epoch: 155 | Iteration: 21 | Classification loss: 0.00026 | Regression loss: 0.03474 | Running loss: 0.02455\n",
            "Epoch: 155 | Iteration: 22 | Classification loss: 0.00006 | Regression loss: 0.02147 | Running loss: 0.02455\n",
            "Epoch: 155 | Iteration: 23 | Classification loss: 0.00022 | Regression loss: 0.01704 | Running loss: 0.02455\n",
            "Epoch: 155 | Iteration: 24 | Classification loss: 0.00014 | Regression loss: 0.02192 | Running loss: 0.02456\n",
            "Epoch: 155 | Iteration: 25 | Classification loss: 0.00021 | Regression loss: 0.01879 | Running loss: 0.02455\n",
            "Epoch: 155 | Iteration: 26 | Classification loss: 0.00020 | Regression loss: 0.01370 | Running loss: 0.02452\n",
            "Epoch: 155 | Iteration: 27 | Classification loss: 0.00026 | Regression loss: 0.03662 | Running loss: 0.02457\n",
            "Epoch: 155 | Iteration: 28 | Classification loss: 0.00025 | Regression loss: 0.02743 | Running loss: 0.02458\n",
            "Epoch: 155 | Iteration: 29 | Classification loss: 0.00052 | Regression loss: 0.02452 | Running loss: 0.02457\n",
            "Epoch: 155 | Iteration: 30 | Classification loss: 0.00019 | Regression loss: 0.02586 | Running loss: 0.02458\n",
            "Epoch: 155 | Iteration: 31 | Classification loss: 0.00019 | Regression loss: 0.02018 | Running loss: 0.02458\n",
            "Epoch: 155 | Iteration: 32 | Classification loss: 0.00005 | Regression loss: 0.00768 | Running loss: 0.02455\n",
            "Epoch: 155 | Iteration: 33 | Classification loss: 0.00021 | Regression loss: 0.02478 | Running loss: 0.02456\n",
            "Epoch: 155 | Iteration: 34 | Classification loss: 0.00059 | Regression loss: 0.02426 | Running loss: 0.02458\n",
            "Epoch: 155 | Iteration: 35 | Classification loss: 0.00014 | Regression loss: 0.00940 | Running loss: 0.02453\n",
            "Epoch: 155 | Iteration: 36 | Classification loss: 0.00019 | Regression loss: 0.01738 | Running loss: 0.02454\n",
            "Epoch: 155 | Iteration: 37 | Classification loss: 0.00015 | Regression loss: 0.01567 | Running loss: 0.02454\n",
            "Epoch: 155 | Iteration: 38 | Classification loss: 0.00013 | Regression loss: 0.01751 | Running loss: 0.02448\n",
            "Epoch: 155 | Iteration: 39 | Classification loss: 0.00007 | Regression loss: 0.02823 | Running loss: 0.02447\n",
            "Epoch: 155 | Iteration: 40 | Classification loss: 0.00039 | Regression loss: 0.04499 | Running loss: 0.02451\n",
            "Epoch: 155 | Iteration: 41 | Classification loss: 0.00082 | Regression loss: 0.04214 | Running loss: 0.02452\n",
            "Epoch: 155 | Iteration: 42 | Classification loss: 0.00014 | Regression loss: 0.02527 | Running loss: 0.02453\n",
            "Epoch: 155 | Iteration: 43 | Classification loss: 0.00093 | Regression loss: 0.02561 | Running loss: 0.02457\n",
            "Epoch: 155 | Iteration: 44 | Classification loss: 0.00005 | Regression loss: 0.00833 | Running loss: 0.02455\n",
            "Epoch: 155 | Iteration: 45 | Classification loss: 0.00079 | Regression loss: 0.03677 | Running loss: 0.02452\n",
            "Epoch: 155 | Iteration: 46 | Classification loss: 0.00020 | Regression loss: 0.03574 | Running loss: 0.02453\n",
            "Epoch: 155 | Iteration: 47 | Classification loss: 0.00011 | Regression loss: 0.02445 | Running loss: 0.02454\n",
            "Epoch: 155 | Iteration: 48 | Classification loss: 0.00022 | Regression loss: 0.01916 | Running loss: 0.02453\n",
            "Epoch: 155 | Iteration: 49 | Classification loss: 0.00030 | Regression loss: 0.01928 | Running loss: 0.02451\n",
            "Epoch: 155 | Iteration: 50 | Classification loss: 0.00041 | Regression loss: 0.04238 | Running loss: 0.02449\n",
            "Epoch: 155 | Iteration: 51 | Classification loss: 0.00022 | Regression loss: 0.02229 | Running loss: 0.02448\n",
            "Epoch: 155 | Iteration: 52 | Classification loss: 0.00011 | Regression loss: 0.01233 | Running loss: 0.02448\n",
            "Epoch: 155 | Iteration: 53 | Classification loss: 0.00025 | Regression loss: 0.02091 | Running loss: 0.02446\n",
            "Epoch: 155 | Iteration: 54 | Classification loss: 0.00036 | Regression loss: 0.02662 | Running loss: 0.02446\n",
            "Epoch: 155 | Iteration: 55 | Classification loss: 0.00016 | Regression loss: 0.01939 | Running loss: 0.02442\n",
            "Epoch: 155 | Iteration: 56 | Classification loss: 0.00009 | Regression loss: 0.00537 | Running loss: 0.02440\n",
            "Epoch: 155 | Iteration: 57 | Classification loss: 0.00027 | Regression loss: 0.02130 | Running loss: 0.02442\n",
            "Epoch: 155 | Iteration: 58 | Classification loss: 0.00030 | Regression loss: 0.02002 | Running loss: 0.02442\n",
            "Epoch: 155 | Iteration: 59 | Classification loss: 0.00028 | Regression loss: 0.03944 | Running loss: 0.02446\n",
            "Epoch: 155 | Iteration: 60 | Classification loss: 0.00004 | Regression loss: 0.00446 | Running loss: 0.02438\n",
            "Epoch: 155 | Iteration: 61 | Classification loss: 0.00013 | Regression loss: 0.01354 | Running loss: 0.02436\n",
            "Epoch: 155 | Iteration: 62 | Classification loss: 0.00013 | Regression loss: 0.01686 | Running loss: 0.02435\n",
            "Epoch: 155 | Iteration: 63 | Classification loss: 0.00048 | Regression loss: 0.04029 | Running loss: 0.02440\n",
            "Epoch: 155 | Iteration: 64 | Classification loss: 0.00012 | Regression loss: 0.01539 | Running loss: 0.02435\n",
            "Epoch: 155 | Iteration: 65 | Classification loss: 0.00008 | Regression loss: 0.01728 | Running loss: 0.02434\n",
            "Epoch: 155 | Iteration: 66 | Classification loss: 0.00007 | Regression loss: 0.01662 | Running loss: 0.02430\n",
            "Epoch: 155 | Iteration: 67 | Classification loss: 0.00070 | Regression loss: 0.03574 | Running loss: 0.02429\n",
            "Epoch: 155 | Iteration: 68 | Classification loss: 0.00014 | Regression loss: 0.01833 | Running loss: 0.02427\n",
            "Epoch: 155 | Iteration: 69 | Classification loss: 0.00030 | Regression loss: 0.03753 | Running loss: 0.02430\n",
            "Epoch: 155 | Iteration: 70 | Classification loss: 0.00032 | Regression loss: 0.01462 | Running loss: 0.02432\n",
            "Epoch: 155 | Iteration: 71 | Classification loss: 0.00013 | Regression loss: 0.03014 | Running loss: 0.02436\n",
            "Epoch: 155 | Iteration: 72 | Classification loss: 0.00056 | Regression loss: 0.04451 | Running loss: 0.02438\n",
            "Epoch: 155 | Iteration: 73 | Classification loss: 0.00015 | Regression loss: 0.01318 | Running loss: 0.02433\n",
            "Epoch: 155 | Iteration: 74 | Classification loss: 0.00052 | Regression loss: 0.03392 | Running loss: 0.02434\n",
            "Epoch: 155 | Iteration: 75 | Classification loss: 0.00032 | Regression loss: 0.02231 | Running loss: 0.02434\n",
            "Epoch: 155 | Iteration: 76 | Classification loss: 0.00020 | Regression loss: 0.02068 | Running loss: 0.02430\n",
            "Epoch: 155 | Iteration: 77 | Classification loss: 0.00034 | Regression loss: 0.02985 | Running loss: 0.02431\n",
            "Epoch: 155 | Iteration: 78 | Classification loss: 0.00036 | Regression loss: 0.02931 | Running loss: 0.02432\n",
            "Epoch: 155 | Iteration: 79 | Classification loss: 0.00022 | Regression loss: 0.01296 | Running loss: 0.02432\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7259378939307871\n",
            "Precision:  0.5697674418604651\n",
            "Recall:  0.7945945945945946\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}, {0: (0.7229906021615096, 185.0)}, {0: (0.7259378939307871, 185.0)}]\n",
            "Epoch: 156 | Iteration: 0 | Classification loss: 0.00026 | Regression loss: 0.02053 | Running loss: 0.02433\n",
            "Epoch: 156 | Iteration: 1 | Classification loss: 0.00093 | Regression loss: 0.03698 | Running loss: 0.02436\n",
            "Epoch: 156 | Iteration: 2 | Classification loss: 0.00030 | Regression loss: 0.02169 | Running loss: 0.02435\n",
            "Epoch: 156 | Iteration: 3 | Classification loss: 0.00011 | Regression loss: 0.01552 | Running loss: 0.02435\n",
            "Epoch: 156 | Iteration: 4 | Classification loss: 0.00021 | Regression loss: 0.01870 | Running loss: 0.02430\n",
            "Epoch: 156 | Iteration: 5 | Classification loss: 0.00009 | Regression loss: 0.01572 | Running loss: 0.02428\n",
            "Epoch: 156 | Iteration: 6 | Classification loss: 0.00010 | Regression loss: 0.01600 | Running loss: 0.02428\n",
            "Epoch: 156 | Iteration: 7 | Classification loss: 0.00020 | Regression loss: 0.02503 | Running loss: 0.02427\n",
            "Epoch: 156 | Iteration: 8 | Classification loss: 0.00014 | Regression loss: 0.01854 | Running loss: 0.02428\n",
            "Epoch: 156 | Iteration: 9 | Classification loss: 0.00036 | Regression loss: 0.02922 | Running loss: 0.02428\n",
            "Epoch: 156 | Iteration: 10 | Classification loss: 0.00065 | Regression loss: 0.03812 | Running loss: 0.02432\n",
            "Epoch: 156 | Iteration: 11 | Classification loss: 0.00034 | Regression loss: 0.02032 | Running loss: 0.02432\n",
            "Epoch: 156 | Iteration: 12 | Classification loss: 0.00073 | Regression loss: 0.03409 | Running loss: 0.02436\n",
            "Epoch: 156 | Iteration: 13 | Classification loss: 0.00010 | Regression loss: 0.02325 | Running loss: 0.02437\n",
            "Epoch: 156 | Iteration: 14 | Classification loss: 0.00010 | Regression loss: 0.01568 | Running loss: 0.02437\n",
            "Epoch: 156 | Iteration: 15 | Classification loss: 0.00062 | Regression loss: 0.02253 | Running loss: 0.02436\n",
            "Epoch: 156 | Iteration: 16 | Classification loss: 0.00003 | Regression loss: 0.00446 | Running loss: 0.02430\n",
            "Epoch: 156 | Iteration: 17 | Classification loss: 0.00021 | Regression loss: 0.01493 | Running loss: 0.02431\n",
            "Epoch: 156 | Iteration: 18 | Classification loss: 0.00021 | Regression loss: 0.01549 | Running loss: 0.02429\n",
            "Epoch: 156 | Iteration: 19 | Classification loss: 0.00028 | Regression loss: 0.02940 | Running loss: 0.02431\n",
            "Epoch: 156 | Iteration: 20 | Classification loss: 0.00031 | Regression loss: 0.01955 | Running loss: 0.02432\n",
            "Epoch: 156 | Iteration: 21 | Classification loss: 0.00018 | Regression loss: 0.01947 | Running loss: 0.02432\n",
            "Epoch: 156 | Iteration: 22 | Classification loss: 0.00045 | Regression loss: 0.03907 | Running loss: 0.02432\n",
            "Epoch: 156 | Iteration: 23 | Classification loss: 0.00015 | Regression loss: 0.02901 | Running loss: 0.02434\n",
            "Epoch: 156 | Iteration: 24 | Classification loss: 0.00007 | Regression loss: 0.02185 | Running loss: 0.02433\n",
            "Epoch: 156 | Iteration: 25 | Classification loss: 0.00017 | Regression loss: 0.01878 | Running loss: 0.02432\n",
            "Epoch: 156 | Iteration: 26 | Classification loss: 0.00038 | Regression loss: 0.02452 | Running loss: 0.02434\n",
            "Epoch: 156 | Iteration: 27 | Classification loss: 0.00027 | Regression loss: 0.03701 | Running loss: 0.02436\n",
            "Epoch: 156 | Iteration: 28 | Classification loss: 0.00016 | Regression loss: 0.01125 | Running loss: 0.02432\n",
            "Epoch: 156 | Iteration: 29 | Classification loss: 0.00075 | Regression loss: 0.04175 | Running loss: 0.02436\n",
            "Epoch: 156 | Iteration: 30 | Classification loss: 0.00025 | Regression loss: 0.03142 | Running loss: 0.02440\n",
            "Epoch: 156 | Iteration: 31 | Classification loss: 0.00020 | Regression loss: 0.02596 | Running loss: 0.02441\n",
            "Epoch: 156 | Iteration: 32 | Classification loss: 0.00045 | Regression loss: 0.02774 | Running loss: 0.02443\n",
            "Epoch: 156 | Iteration: 33 | Classification loss: 0.00035 | Regression loss: 0.03240 | Running loss: 0.02445\n",
            "Epoch: 156 | Iteration: 34 | Classification loss: 0.00036 | Regression loss: 0.03894 | Running loss: 0.02448\n",
            "Epoch: 156 | Iteration: 35 | Classification loss: 0.00038 | Regression loss: 0.03284 | Running loss: 0.02449\n",
            "Epoch: 156 | Iteration: 36 | Classification loss: 0.00044 | Regression loss: 0.03440 | Running loss: 0.02450\n",
            "Epoch: 156 | Iteration: 37 | Classification loss: 0.00029 | Regression loss: 0.02219 | Running loss: 0.02448\n",
            "Epoch: 156 | Iteration: 38 | Classification loss: 0.00031 | Regression loss: 0.03427 | Running loss: 0.02453\n",
            "Epoch: 156 | Iteration: 39 | Classification loss: 0.00018 | Regression loss: 0.01947 | Running loss: 0.02454\n",
            "Epoch: 156 | Iteration: 40 | Classification loss: 0.00016 | Regression loss: 0.01860 | Running loss: 0.02454\n",
            "Epoch: 156 | Iteration: 41 | Classification loss: 0.00027 | Regression loss: 0.02369 | Running loss: 0.02456\n",
            "Epoch: 156 | Iteration: 42 | Classification loss: 0.00031 | Regression loss: 0.02571 | Running loss: 0.02458\n",
            "Epoch: 156 | Iteration: 43 | Classification loss: 0.00015 | Regression loss: 0.01678 | Running loss: 0.02455\n",
            "Epoch: 156 | Iteration: 44 | Classification loss: 0.00019 | Regression loss: 0.00972 | Running loss: 0.02453\n",
            "Epoch: 156 | Iteration: 45 | Classification loss: 0.00010 | Regression loss: 0.01062 | Running loss: 0.02448\n",
            "Epoch: 156 | Iteration: 46 | Classification loss: 0.00003 | Regression loss: 0.01060 | Running loss: 0.02446\n",
            "Epoch: 156 | Iteration: 47 | Classification loss: 0.00009 | Regression loss: 0.02451 | Running loss: 0.02447\n",
            "Epoch: 156 | Iteration: 48 | Classification loss: 0.00049 | Regression loss: 0.03484 | Running loss: 0.02450\n",
            "Epoch: 156 | Iteration: 49 | Classification loss: 0.00014 | Regression loss: 0.02312 | Running loss: 0.02450\n",
            "Epoch: 156 | Iteration: 50 | Classification loss: 0.00024 | Regression loss: 0.03886 | Running loss: 0.02449\n",
            "Epoch: 156 | Iteration: 51 | Classification loss: 0.00043 | Regression loss: 0.04650 | Running loss: 0.02456\n",
            "Epoch: 156 | Iteration: 52 | Classification loss: 0.00019 | Regression loss: 0.02379 | Running loss: 0.02456\n",
            "Epoch: 156 | Iteration: 53 | Classification loss: 0.00023 | Regression loss: 0.03452 | Running loss: 0.02459\n",
            "Epoch: 156 | Iteration: 54 | Classification loss: 0.00005 | Regression loss: 0.00827 | Running loss: 0.02452\n",
            "Epoch: 156 | Iteration: 55 | Classification loss: 0.00022 | Regression loss: 0.02674 | Running loss: 0.02454\n",
            "Epoch: 156 | Iteration: 56 | Classification loss: 0.00022 | Regression loss: 0.01405 | Running loss: 0.02453\n",
            "Epoch: 156 | Iteration: 57 | Classification loss: 0.00011 | Regression loss: 0.01639 | Running loss: 0.02451\n",
            "Epoch: 156 | Iteration: 58 | Classification loss: 0.00011 | Regression loss: 0.00924 | Running loss: 0.02449\n",
            "Epoch: 156 | Iteration: 59 | Classification loss: 0.00007 | Regression loss: 0.00888 | Running loss: 0.02446\n",
            "Epoch: 156 | Iteration: 60 | Classification loss: 0.00019 | Regression loss: 0.01863 | Running loss: 0.02446\n",
            "Epoch: 156 | Iteration: 61 | Classification loss: 0.00007 | Regression loss: 0.03198 | Running loss: 0.02449\n",
            "Epoch: 156 | Iteration: 62 | Classification loss: 0.00006 | Regression loss: 0.01618 | Running loss: 0.02447\n",
            "Epoch: 156 | Iteration: 63 | Classification loss: 0.00020 | Regression loss: 0.01711 | Running loss: 0.02445\n",
            "Epoch: 156 | Iteration: 64 | Classification loss: 0.00008 | Regression loss: 0.02932 | Running loss: 0.02447\n",
            "Epoch: 156 | Iteration: 65 | Classification loss: 0.00017 | Regression loss: 0.02585 | Running loss: 0.02445\n",
            "Epoch: 156 | Iteration: 66 | Classification loss: 0.00006 | Regression loss: 0.01406 | Running loss: 0.02441\n",
            "Epoch: 156 | Iteration: 67 | Classification loss: 0.00076 | Regression loss: 0.02342 | Running loss: 0.02444\n",
            "Epoch: 156 | Iteration: 68 | Classification loss: 0.00024 | Regression loss: 0.02228 | Running loss: 0.02443\n",
            "Epoch: 156 | Iteration: 69 | Classification loss: 0.00011 | Regression loss: 0.01887 | Running loss: 0.02439\n",
            "Epoch: 156 | Iteration: 70 | Classification loss: 0.00006 | Regression loss: 0.00973 | Running loss: 0.02438\n",
            "Epoch: 156 | Iteration: 71 | Classification loss: 0.00011 | Regression loss: 0.03248 | Running loss: 0.02442\n",
            "Epoch: 156 | Iteration: 72 | Classification loss: 0.00023 | Regression loss: 0.01464 | Running loss: 0.02437\n",
            "Epoch: 156 | Iteration: 73 | Classification loss: 0.00019 | Regression loss: 0.01848 | Running loss: 0.02430\n",
            "Epoch: 156 | Iteration: 74 | Classification loss: 0.00007 | Regression loss: 0.02213 | Running loss: 0.02426\n",
            "Epoch: 156 | Iteration: 75 | Classification loss: 0.00066 | Regression loss: 0.05254 | Running loss: 0.02436\n",
            "Epoch: 156 | Iteration: 76 | Classification loss: 0.00064 | Regression loss: 0.02395 | Running loss: 0.02437\n",
            "Epoch: 156 | Iteration: 77 | Classification loss: 0.00054 | Regression loss: 0.04265 | Running loss: 0.02438\n",
            "Epoch: 156 | Iteration: 78 | Classification loss: 0.00014 | Regression loss: 0.02761 | Running loss: 0.02439\n",
            "Epoch: 156 | Iteration: 79 | Classification loss: 0.00014 | Regression loss: 0.01655 | Running loss: 0.02435\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7306172160652471\n",
            "Precision:  0.5692307692307692\n",
            "Recall:  0.8\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}, {0: (0.7229906021615096, 185.0)}, {0: (0.7259378939307871, 185.0)}, {0: (0.7306172160652471, 185.0)}]\n",
            "Epoch: 157 | Iteration: 0 | Classification loss: 0.00010 | Regression loss: 0.02631 | Running loss: 0.02438\n",
            "Epoch: 157 | Iteration: 1 | Classification loss: 0.00052 | Regression loss: 0.02488 | Running loss: 0.02438\n",
            "Epoch: 157 | Iteration: 2 | Classification loss: 0.00069 | Regression loss: 0.02261 | Running loss: 0.02434\n",
            "Epoch: 157 | Iteration: 3 | Classification loss: 0.00010 | Regression loss: 0.01314 | Running loss: 0.02432\n",
            "Epoch: 157 | Iteration: 4 | Classification loss: 0.00018 | Regression loss: 0.01009 | Running loss: 0.02430\n",
            "Epoch: 157 | Iteration: 5 | Classification loss: 0.00007 | Regression loss: 0.02208 | Running loss: 0.02431\n",
            "Epoch: 157 | Iteration: 6 | Classification loss: 0.00015 | Regression loss: 0.02357 | Running loss: 0.02431\n",
            "Epoch: 157 | Iteration: 7 | Classification loss: 0.00007 | Regression loss: 0.00945 | Running loss: 0.02423\n",
            "Epoch: 157 | Iteration: 8 | Classification loss: 0.00017 | Regression loss: 0.01937 | Running loss: 0.02426\n",
            "Epoch: 157 | Iteration: 9 | Classification loss: 0.00004 | Regression loss: 0.01066 | Running loss: 0.02424\n",
            "Epoch: 157 | Iteration: 10 | Classification loss: 0.00005 | Regression loss: 0.00835 | Running loss: 0.02417\n",
            "Epoch: 157 | Iteration: 11 | Classification loss: 0.00018 | Regression loss: 0.01445 | Running loss: 0.02414\n",
            "Epoch: 157 | Iteration: 12 | Classification loss: 0.00010 | Regression loss: 0.02073 | Running loss: 0.02414\n",
            "Epoch: 157 | Iteration: 13 | Classification loss: 0.00015 | Regression loss: 0.01175 | Running loss: 0.02407\n",
            "Epoch: 157 | Iteration: 14 | Classification loss: 0.00051 | Regression loss: 0.03135 | Running loss: 0.02407\n",
            "Epoch: 157 | Iteration: 15 | Classification loss: 0.00006 | Regression loss: 0.00905 | Running loss: 0.02407\n",
            "Epoch: 157 | Iteration: 16 | Classification loss: 0.00061 | Regression loss: 0.02746 | Running loss: 0.02405\n",
            "Epoch: 157 | Iteration: 17 | Classification loss: 0.00022 | Regression loss: 0.02439 | Running loss: 0.02406\n",
            "Epoch: 157 | Iteration: 18 | Classification loss: 0.00018 | Regression loss: 0.02424 | Running loss: 0.02405\n",
            "Epoch: 157 | Iteration: 19 | Classification loss: 0.00012 | Regression loss: 0.02811 | Running loss: 0.02407\n",
            "Epoch: 157 | Iteration: 20 | Classification loss: 0.00012 | Regression loss: 0.03100 | Running loss: 0.02409\n",
            "Epoch: 157 | Iteration: 21 | Classification loss: 0.00013 | Regression loss: 0.02926 | Running loss: 0.02410\n",
            "Epoch: 157 | Iteration: 22 | Classification loss: 0.00006 | Regression loss: 0.01624 | Running loss: 0.02405\n",
            "Epoch: 157 | Iteration: 23 | Classification loss: 0.00051 | Regression loss: 0.04405 | Running loss: 0.02405\n",
            "Epoch: 157 | Iteration: 24 | Classification loss: 0.00013 | Regression loss: 0.01805 | Running loss: 0.02402\n",
            "Epoch: 157 | Iteration: 25 | Classification loss: 0.00050 | Regression loss: 0.05200 | Running loss: 0.02405\n",
            "Epoch: 157 | Iteration: 26 | Classification loss: 0.00004 | Regression loss: 0.00445 | Running loss: 0.02403\n",
            "Epoch: 157 | Iteration: 27 | Classification loss: 0.00058 | Regression loss: 0.04151 | Running loss: 0.02408\n",
            "Epoch: 157 | Iteration: 28 | Classification loss: 0.00023 | Regression loss: 0.02153 | Running loss: 0.02408\n",
            "Epoch: 157 | Iteration: 29 | Classification loss: 0.00022 | Regression loss: 0.02529 | Running loss: 0.02403\n",
            "Epoch: 157 | Iteration: 30 | Classification loss: 0.00025 | Regression loss: 0.02069 | Running loss: 0.02403\n",
            "Epoch: 157 | Iteration: 31 | Classification loss: 0.00030 | Regression loss: 0.01936 | Running loss: 0.02406\n",
            "Epoch: 157 | Iteration: 32 | Classification loss: 0.00018 | Regression loss: 0.01982 | Running loss: 0.02408\n",
            "Epoch: 157 | Iteration: 33 | Classification loss: 0.00010 | Regression loss: 0.01540 | Running loss: 0.02408\n",
            "Epoch: 157 | Iteration: 34 | Classification loss: 0.00018 | Regression loss: 0.01596 | Running loss: 0.02407\n",
            "Epoch: 157 | Iteration: 35 | Classification loss: 0.00014 | Regression loss: 0.01036 | Running loss: 0.02406\n",
            "Epoch: 157 | Iteration: 36 | Classification loss: 0.00009 | Regression loss: 0.01828 | Running loss: 0.02406\n",
            "Epoch: 157 | Iteration: 37 | Classification loss: 0.00039 | Regression loss: 0.04394 | Running loss: 0.02407\n",
            "Epoch: 157 | Iteration: 38 | Classification loss: 0.00012 | Regression loss: 0.02398 | Running loss: 0.02409\n",
            "Epoch: 157 | Iteration: 39 | Classification loss: 0.00027 | Regression loss: 0.02619 | Running loss: 0.02411\n",
            "Epoch: 157 | Iteration: 40 | Classification loss: 0.00026 | Regression loss: 0.01421 | Running loss: 0.02408\n",
            "Epoch: 157 | Iteration: 41 | Classification loss: 0.00010 | Regression loss: 0.02320 | Running loss: 0.02409\n",
            "Epoch: 157 | Iteration: 42 | Classification loss: 0.00020 | Regression loss: 0.01406 | Running loss: 0.02408\n",
            "Epoch: 157 | Iteration: 43 | Classification loss: 0.00026 | Regression loss: 0.02390 | Running loss: 0.02408\n",
            "Epoch: 157 | Iteration: 44 | Classification loss: 0.00013 | Regression loss: 0.01567 | Running loss: 0.02408\n",
            "Epoch: 157 | Iteration: 45 | Classification loss: 0.00018 | Regression loss: 0.01826 | Running loss: 0.02409\n",
            "Epoch: 157 | Iteration: 46 | Classification loss: 0.00070 | Regression loss: 0.03342 | Running loss: 0.02409\n",
            "Epoch: 157 | Iteration: 47 | Classification loss: 0.00029 | Regression loss: 0.03719 | Running loss: 0.02412\n",
            "Epoch: 157 | Iteration: 48 | Classification loss: 0.00036 | Regression loss: 0.03231 | Running loss: 0.02413\n",
            "Epoch: 157 | Iteration: 49 | Classification loss: 0.00043 | Regression loss: 0.02246 | Running loss: 0.02412\n",
            "Epoch: 157 | Iteration: 50 | Classification loss: 0.00014 | Regression loss: 0.01936 | Running loss: 0.02408\n",
            "Epoch: 157 | Iteration: 51 | Classification loss: 0.00017 | Regression loss: 0.01188 | Running loss: 0.02406\n",
            "Epoch: 157 | Iteration: 52 | Classification loss: 0.00016 | Regression loss: 0.02559 | Running loss: 0.02404\n",
            "Epoch: 157 | Iteration: 53 | Classification loss: 0.00007 | Regression loss: 0.02172 | Running loss: 0.02402\n",
            "Epoch: 157 | Iteration: 54 | Classification loss: 0.00026 | Regression loss: 0.01864 | Running loss: 0.02400\n",
            "Epoch: 157 | Iteration: 55 | Classification loss: 0.00036 | Regression loss: 0.02729 | Running loss: 0.02404\n",
            "Epoch: 157 | Iteration: 56 | Classification loss: 0.00018 | Regression loss: 0.01965 | Running loss: 0.02404\n",
            "Epoch: 157 | Iteration: 57 | Classification loss: 0.00024 | Regression loss: 0.02660 | Running loss: 0.02406\n",
            "Epoch: 157 | Iteration: 58 | Classification loss: 0.00087 | Regression loss: 0.03841 | Running loss: 0.02409\n",
            "Epoch: 157 | Iteration: 59 | Classification loss: 0.00058 | Regression loss: 0.02196 | Running loss: 0.02407\n",
            "Epoch: 157 | Iteration: 60 | Classification loss: 0.00009 | Regression loss: 0.02446 | Running loss: 0.02407\n",
            "Epoch: 157 | Iteration: 61 | Classification loss: 0.00041 | Regression loss: 0.02552 | Running loss: 0.02409\n",
            "Epoch: 157 | Iteration: 62 | Classification loss: 0.00023 | Regression loss: 0.01985 | Running loss: 0.02404\n",
            "Epoch: 157 | Iteration: 63 | Classification loss: 0.00038 | Regression loss: 0.03171 | Running loss: 0.02406\n",
            "Epoch: 157 | Iteration: 64 | Classification loss: 0.00035 | Regression loss: 0.03705 | Running loss: 0.02408\n",
            "Epoch: 157 | Iteration: 65 | Classification loss: 0.00054 | Regression loss: 0.03828 | Running loss: 0.02409\n",
            "Epoch: 157 | Iteration: 66 | Classification loss: 0.00024 | Regression loss: 0.03993 | Running loss: 0.02412\n",
            "Epoch: 157 | Iteration: 67 | Classification loss: 0.00020 | Regression loss: 0.03570 | Running loss: 0.02415\n",
            "Epoch: 157 | Iteration: 68 | Classification loss: 0.00009 | Regression loss: 0.01565 | Running loss: 0.02414\n",
            "Epoch: 157 | Iteration: 69 | Classification loss: 0.00012 | Regression loss: 0.01584 | Running loss: 0.02410\n",
            "Epoch: 157 | Iteration: 70 | Classification loss: 0.00019 | Regression loss: 0.01419 | Running loss: 0.02409\n",
            "Epoch: 157 | Iteration: 71 | Classification loss: 0.00008 | Regression loss: 0.01449 | Running loss: 0.02404\n",
            "Epoch: 157 | Iteration: 72 | Classification loss: 0.00005 | Regression loss: 0.01627 | Running loss: 0.02402\n",
            "Epoch: 157 | Iteration: 73 | Classification loss: 0.00074 | Regression loss: 0.03932 | Running loss: 0.02404\n",
            "Epoch: 157 | Iteration: 74 | Classification loss: 0.00051 | Regression loss: 0.03467 | Running loss: 0.02410\n",
            "Epoch: 157 | Iteration: 75 | Classification loss: 0.00026 | Regression loss: 0.03894 | Running loss: 0.02413\n",
            "Epoch: 157 | Iteration: 76 | Classification loss: 0.00051 | Regression loss: 0.04120 | Running loss: 0.02418\n",
            "Epoch: 157 | Iteration: 77 | Classification loss: 0.00016 | Regression loss: 0.01489 | Running loss: 0.02416\n",
            "Epoch: 157 | Iteration: 78 | Classification loss: 0.00016 | Regression loss: 0.01850 | Running loss: 0.02413\n",
            "Epoch: 157 | Iteration: 79 | Classification loss: 0.00025 | Regression loss: 0.02293 | Running loss: 0.02417\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7210590702449308\n",
            "Precision:  0.5488721804511278\n",
            "Recall:  0.7891891891891892\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}, {0: (0.7229906021615096, 185.0)}, {0: (0.7259378939307871, 185.0)}, {0: (0.7306172160652471, 185.0)}, {0: (0.7210590702449308, 185.0)}]\n",
            "Epoch: 158 | Iteration: 0 | Classification loss: 0.00038 | Regression loss: 0.04360 | Running loss: 0.02422\n",
            "Epoch: 158 | Iteration: 1 | Classification loss: 0.00015 | Regression loss: 0.03139 | Running loss: 0.02421\n",
            "Epoch: 158 | Iteration: 2 | Classification loss: 0.00010 | Regression loss: 0.01233 | Running loss: 0.02421\n",
            "Epoch: 158 | Iteration: 3 | Classification loss: 0.00022 | Regression loss: 0.02724 | Running loss: 0.02421\n",
            "Epoch: 158 | Iteration: 4 | Classification loss: 0.00022 | Regression loss: 0.02681 | Running loss: 0.02425\n",
            "Epoch: 158 | Iteration: 5 | Classification loss: 0.00059 | Regression loss: 0.03813 | Running loss: 0.02430\n",
            "Epoch: 158 | Iteration: 6 | Classification loss: 0.00014 | Regression loss: 0.02243 | Running loss: 0.02430\n",
            "Epoch: 158 | Iteration: 7 | Classification loss: 0.00022 | Regression loss: 0.02299 | Running loss: 0.02431\n",
            "Epoch: 158 | Iteration: 8 | Classification loss: 0.00009 | Regression loss: 0.03134 | Running loss: 0.02428\n",
            "Epoch: 158 | Iteration: 9 | Classification loss: 0.00019 | Regression loss: 0.01369 | Running loss: 0.02425\n",
            "Epoch: 158 | Iteration: 10 | Classification loss: 0.00016 | Regression loss: 0.01723 | Running loss: 0.02425\n",
            "Epoch: 158 | Iteration: 11 | Classification loss: 0.00026 | Regression loss: 0.01485 | Running loss: 0.02421\n",
            "Epoch: 158 | Iteration: 12 | Classification loss: 0.00006 | Regression loss: 0.02141 | Running loss: 0.02421\n",
            "Epoch: 158 | Iteration: 13 | Classification loss: 0.00004 | Regression loss: 0.00805 | Running loss: 0.02416\n",
            "Epoch: 158 | Iteration: 14 | Classification loss: 0.00029 | Regression loss: 0.03480 | Running loss: 0.02419\n",
            "Epoch: 158 | Iteration: 15 | Classification loss: 0.00003 | Regression loss: 0.01023 | Running loss: 0.02418\n",
            "Epoch: 158 | Iteration: 16 | Classification loss: 0.00076 | Regression loss: 0.04253 | Running loss: 0.02422\n",
            "Epoch: 158 | Iteration: 17 | Classification loss: 0.00090 | Regression loss: 0.02522 | Running loss: 0.02420\n",
            "Epoch: 158 | Iteration: 18 | Classification loss: 0.00013 | Regression loss: 0.02504 | Running loss: 0.02422\n",
            "Epoch: 158 | Iteration: 19 | Classification loss: 0.00012 | Regression loss: 0.01704 | Running loss: 0.02417\n",
            "Epoch: 158 | Iteration: 20 | Classification loss: 0.00006 | Regression loss: 0.00831 | Running loss: 0.02412\n",
            "Epoch: 158 | Iteration: 21 | Classification loss: 0.00062 | Regression loss: 0.02296 | Running loss: 0.02408\n",
            "Epoch: 158 | Iteration: 22 | Classification loss: 0.00023 | Regression loss: 0.03959 | Running loss: 0.02410\n",
            "Epoch: 158 | Iteration: 23 | Classification loss: 0.00079 | Regression loss: 0.03697 | Running loss: 0.02414\n",
            "Epoch: 158 | Iteration: 24 | Classification loss: 0.00012 | Regression loss: 0.01548 | Running loss: 0.02416\n",
            "Epoch: 158 | Iteration: 25 | Classification loss: 0.00016 | Regression loss: 0.01168 | Running loss: 0.02414\n",
            "Epoch: 158 | Iteration: 26 | Classification loss: 0.00012 | Regression loss: 0.00907 | Running loss: 0.02412\n",
            "Epoch: 158 | Iteration: 27 | Classification loss: 0.00032 | Regression loss: 0.02264 | Running loss: 0.02412\n",
            "Epoch: 158 | Iteration: 28 | Classification loss: 0.00024 | Regression loss: 0.01677 | Running loss: 0.02411\n",
            "Epoch: 158 | Iteration: 29 | Classification loss: 0.00068 | Regression loss: 0.05370 | Running loss: 0.02417\n",
            "Epoch: 158 | Iteration: 30 | Classification loss: 0.00027 | Regression loss: 0.01906 | Running loss: 0.02419\n",
            "Epoch: 158 | Iteration: 31 | Classification loss: 0.00008 | Regression loss: 0.00852 | Running loss: 0.02415\n",
            "Epoch: 158 | Iteration: 32 | Classification loss: 0.00020 | Regression loss: 0.02474 | Running loss: 0.02417\n",
            "Epoch: 158 | Iteration: 33 | Classification loss: 0.00018 | Regression loss: 0.01568 | Running loss: 0.02416\n",
            "Epoch: 158 | Iteration: 34 | Classification loss: 0.00010 | Regression loss: 0.01468 | Running loss: 0.02411\n",
            "Epoch: 158 | Iteration: 35 | Classification loss: 0.00053 | Regression loss: 0.02428 | Running loss: 0.02411\n",
            "Epoch: 158 | Iteration: 36 | Classification loss: 0.00003 | Regression loss: 0.00464 | Running loss: 0.02409\n",
            "Epoch: 158 | Iteration: 37 | Classification loss: 0.00016 | Regression loss: 0.02038 | Running loss: 0.02409\n",
            "Epoch: 158 | Iteration: 38 | Classification loss: 0.00004 | Regression loss: 0.01617 | Running loss: 0.02407\n",
            "Epoch: 158 | Iteration: 39 | Classification loss: 0.00014 | Regression loss: 0.01620 | Running loss: 0.02403\n",
            "Epoch: 158 | Iteration: 40 | Classification loss: 0.00087 | Regression loss: 0.03957 | Running loss: 0.02409\n",
            "Epoch: 158 | Iteration: 41 | Classification loss: 0.00020 | Regression loss: 0.01385 | Running loss: 0.02410\n",
            "Epoch: 158 | Iteration: 42 | Classification loss: 0.00007 | Regression loss: 0.01369 | Running loss: 0.02409\n",
            "Epoch: 158 | Iteration: 43 | Classification loss: 0.00021 | Regression loss: 0.02645 | Running loss: 0.02411\n",
            "Epoch: 158 | Iteration: 44 | Classification loss: 0.00059 | Regression loss: 0.03261 | Running loss: 0.02411\n",
            "Epoch: 158 | Iteration: 45 | Classification loss: 0.00040 | Regression loss: 0.03224 | Running loss: 0.02412\n",
            "Epoch: 158 | Iteration: 46 | Classification loss: 0.00010 | Regression loss: 0.01841 | Running loss: 0.02412\n",
            "Epoch: 158 | Iteration: 47 | Classification loss: 0.00012 | Regression loss: 0.01447 | Running loss: 0.02408\n",
            "Epoch: 158 | Iteration: 48 | Classification loss: 0.00040 | Regression loss: 0.03376 | Running loss: 0.02412\n",
            "Epoch: 158 | Iteration: 49 | Classification loss: 0.00018 | Regression loss: 0.01913 | Running loss: 0.02410\n",
            "Epoch: 158 | Iteration: 50 | Classification loss: 0.00013 | Regression loss: 0.02303 | Running loss: 0.02410\n",
            "Epoch: 158 | Iteration: 51 | Classification loss: 0.00006 | Regression loss: 0.01555 | Running loss: 0.02408\n",
            "Epoch: 158 | Iteration: 52 | Classification loss: 0.00035 | Regression loss: 0.02598 | Running loss: 0.02409\n",
            "Epoch: 158 | Iteration: 53 | Classification loss: 0.00020 | Regression loss: 0.04033 | Running loss: 0.02409\n",
            "Epoch: 158 | Iteration: 54 | Classification loss: 0.00011 | Regression loss: 0.02542 | Running loss: 0.02408\n",
            "Epoch: 158 | Iteration: 55 | Classification loss: 0.00046 | Regression loss: 0.04628 | Running loss: 0.02408\n",
            "Epoch: 158 | Iteration: 56 | Classification loss: 0.00035 | Regression loss: 0.02597 | Running loss: 0.02408\n",
            "Epoch: 158 | Iteration: 57 | Classification loss: 0.00042 | Regression loss: 0.02844 | Running loss: 0.02412\n",
            "Epoch: 158 | Iteration: 58 | Classification loss: 0.00023 | Regression loss: 0.01953 | Running loss: 0.02414\n",
            "Epoch: 158 | Iteration: 59 | Classification loss: 0.00009 | Regression loss: 0.01075 | Running loss: 0.02413\n",
            "Epoch: 158 | Iteration: 60 | Classification loss: 0.00030 | Regression loss: 0.01865 | Running loss: 0.02412\n",
            "Epoch: 158 | Iteration: 61 | Classification loss: 0.00028 | Regression loss: 0.02633 | Running loss: 0.02409\n",
            "Epoch: 158 | Iteration: 62 | Classification loss: 0.00022 | Regression loss: 0.01065 | Running loss: 0.02409\n",
            "Epoch: 158 | Iteration: 63 | Classification loss: 0.00015 | Regression loss: 0.01876 | Running loss: 0.02407\n",
            "Epoch: 158 | Iteration: 64 | Classification loss: 0.00024 | Regression loss: 0.02284 | Running loss: 0.02409\n",
            "Epoch: 158 | Iteration: 65 | Classification loss: 0.00016 | Regression loss: 0.02713 | Running loss: 0.02412\n",
            "Epoch: 158 | Iteration: 66 | Classification loss: 0.00011 | Regression loss: 0.01353 | Running loss: 0.02407\n",
            "Epoch: 158 | Iteration: 67 | Classification loss: 0.00028 | Regression loss: 0.02052 | Running loss: 0.02408\n",
            "Epoch: 158 | Iteration: 68 | Classification loss: 0.00033 | Regression loss: 0.03135 | Running loss: 0.02406\n",
            "Epoch: 158 | Iteration: 69 | Classification loss: 0.00060 | Regression loss: 0.03957 | Running loss: 0.02407\n",
            "Epoch: 158 | Iteration: 70 | Classification loss: 0.00009 | Regression loss: 0.02358 | Running loss: 0.02408\n",
            "Epoch: 158 | Iteration: 71 | Classification loss: 0.00036 | Regression loss: 0.03395 | Running loss: 0.02404\n",
            "Epoch: 158 | Iteration: 72 | Classification loss: 0.00018 | Regression loss: 0.03070 | Running loss: 0.02402\n",
            "Epoch: 158 | Iteration: 73 | Classification loss: 0.00015 | Regression loss: 0.02974 | Running loss: 0.02402\n",
            "Epoch: 158 | Iteration: 74 | Classification loss: 0.00020 | Regression loss: 0.01913 | Running loss: 0.02401\n",
            "Epoch: 158 | Iteration: 75 | Classification loss: 0.00020 | Regression loss: 0.01627 | Running loss: 0.02398\n",
            "Epoch: 158 | Iteration: 76 | Classification loss: 0.00015 | Regression loss: 0.01899 | Running loss: 0.02399\n",
            "Epoch: 158 | Iteration: 77 | Classification loss: 0.00007 | Regression loss: 0.02176 | Running loss: 0.02399\n",
            "Epoch: 158 | Iteration: 78 | Classification loss: 0.00054 | Regression loss: 0.03786 | Running loss: 0.02402\n",
            "Epoch: 158 | Iteration: 79 | Classification loss: 0.00018 | Regression loss: 0.03221 | Running loss: 0.02403\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7214498931770392\n",
            "Precision:  0.5658914728682171\n",
            "Recall:  0.7891891891891892\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}, {0: (0.7229906021615096, 185.0)}, {0: (0.7259378939307871, 185.0)}, {0: (0.7306172160652471, 185.0)}, {0: (0.7210590702449308, 185.0)}, {0: (0.7214498931770392, 185.0)}]\n",
            "Epoch: 159 | Iteration: 0 | Classification loss: 0.00074 | Regression loss: 0.03627 | Running loss: 0.02406\n",
            "Epoch: 159 | Iteration: 1 | Classification loss: 0.00021 | Regression loss: 0.02138 | Running loss: 0.02407\n",
            "Epoch: 159 | Iteration: 2 | Classification loss: 0.00014 | Regression loss: 0.02236 | Running loss: 0.02408\n",
            "Epoch: 159 | Iteration: 3 | Classification loss: 0.00015 | Regression loss: 0.01861 | Running loss: 0.02404\n",
            "Epoch: 159 | Iteration: 4 | Classification loss: 0.00007 | Regression loss: 0.01753 | Running loss: 0.02404\n",
            "Epoch: 159 | Iteration: 5 | Classification loss: 0.00035 | Regression loss: 0.03331 | Running loss: 0.02408\n",
            "Epoch: 159 | Iteration: 6 | Classification loss: 0.00012 | Regression loss: 0.01153 | Running loss: 0.02407\n",
            "Epoch: 159 | Iteration: 7 | Classification loss: 0.00009 | Regression loss: 0.01070 | Running loss: 0.02409\n",
            "Epoch: 159 | Iteration: 8 | Classification loss: 0.00029 | Regression loss: 0.01917 | Running loss: 0.02408\n",
            "Epoch: 159 | Iteration: 9 | Classification loss: 0.00028 | Regression loss: 0.02006 | Running loss: 0.02405\n",
            "Epoch: 159 | Iteration: 10 | Classification loss: 0.00060 | Regression loss: 0.02430 | Running loss: 0.02406\n",
            "Epoch: 159 | Iteration: 11 | Classification loss: 0.00008 | Regression loss: 0.02246 | Running loss: 0.02407\n",
            "Epoch: 159 | Iteration: 12 | Classification loss: 0.00018 | Regression loss: 0.01871 | Running loss: 0.02403\n",
            "Epoch: 159 | Iteration: 13 | Classification loss: 0.00020 | Regression loss: 0.03203 | Running loss: 0.02404\n",
            "Epoch: 159 | Iteration: 14 | Classification loss: 0.00007 | Regression loss: 0.02702 | Running loss: 0.02404\n",
            "Epoch: 159 | Iteration: 15 | Classification loss: 0.00031 | Regression loss: 0.02600 | Running loss: 0.02405\n",
            "Epoch: 159 | Iteration: 16 | Classification loss: 0.00011 | Regression loss: 0.01647 | Running loss: 0.02405\n",
            "Epoch: 159 | Iteration: 17 | Classification loss: 0.00008 | Regression loss: 0.01699 | Running loss: 0.02401\n",
            "Epoch: 159 | Iteration: 18 | Classification loss: 0.00020 | Regression loss: 0.01439 | Running loss: 0.02400\n",
            "Epoch: 159 | Iteration: 19 | Classification loss: 0.00013 | Regression loss: 0.01519 | Running loss: 0.02397\n",
            "Epoch: 159 | Iteration: 20 | Classification loss: 0.00005 | Regression loss: 0.00779 | Running loss: 0.02393\n",
            "Epoch: 159 | Iteration: 21 | Classification loss: 0.00013 | Regression loss: 0.02846 | Running loss: 0.02395\n",
            "Epoch: 159 | Iteration: 22 | Classification loss: 0.00027 | Regression loss: 0.04027 | Running loss: 0.02398\n",
            "Epoch: 159 | Iteration: 23 | Classification loss: 0.00038 | Regression loss: 0.02724 | Running loss: 0.02397\n",
            "Epoch: 159 | Iteration: 24 | Classification loss: 0.00049 | Regression loss: 0.02316 | Running loss: 0.02395\n",
            "Epoch: 159 | Iteration: 25 | Classification loss: 0.00032 | Regression loss: 0.01959 | Running loss: 0.02392\n",
            "Epoch: 159 | Iteration: 26 | Classification loss: 0.00054 | Regression loss: 0.04022 | Running loss: 0.02391\n",
            "Epoch: 159 | Iteration: 27 | Classification loss: 0.00002 | Regression loss: 0.01047 | Running loss: 0.02388\n",
            "Epoch: 159 | Iteration: 28 | Classification loss: 0.00019 | Regression loss: 0.02230 | Running loss: 0.02387\n",
            "Epoch: 159 | Iteration: 29 | Classification loss: 0.00039 | Regression loss: 0.04126 | Running loss: 0.02389\n",
            "Epoch: 159 | Iteration: 30 | Classification loss: 0.00044 | Regression loss: 0.02353 | Running loss: 0.02390\n",
            "Epoch: 159 | Iteration: 31 | Classification loss: 0.00018 | Regression loss: 0.01469 | Running loss: 0.02385\n",
            "Epoch: 159 | Iteration: 32 | Classification loss: 0.00031 | Regression loss: 0.02514 | Running loss: 0.02386\n",
            "Epoch: 159 | Iteration: 33 | Classification loss: 0.00019 | Regression loss: 0.01584 | Running loss: 0.02385\n",
            "Epoch: 159 | Iteration: 34 | Classification loss: 0.00019 | Regression loss: 0.02128 | Running loss: 0.02383\n",
            "Epoch: 159 | Iteration: 35 | Classification loss: 0.00009 | Regression loss: 0.01550 | Running loss: 0.02381\n",
            "Epoch: 159 | Iteration: 36 | Classification loss: 0.00008 | Regression loss: 0.00579 | Running loss: 0.02376\n",
            "Epoch: 159 | Iteration: 37 | Classification loss: 0.00016 | Regression loss: 0.01553 | Running loss: 0.02376\n",
            "Epoch: 159 | Iteration: 38 | Classification loss: 0.00090 | Regression loss: 0.03872 | Running loss: 0.02378\n",
            "Epoch: 159 | Iteration: 39 | Classification loss: 0.00007 | Regression loss: 0.00917 | Running loss: 0.02376\n",
            "Epoch: 159 | Iteration: 40 | Classification loss: 0.00018 | Regression loss: 0.01797 | Running loss: 0.02375\n",
            "Epoch: 159 | Iteration: 41 | Classification loss: 0.00035 | Regression loss: 0.02222 | Running loss: 0.02375\n",
            "Epoch: 159 | Iteration: 42 | Classification loss: 0.00011 | Regression loss: 0.02395 | Running loss: 0.02372\n",
            "Epoch: 159 | Iteration: 43 | Classification loss: 0.00043 | Regression loss: 0.03239 | Running loss: 0.02375\n",
            "Epoch: 159 | Iteration: 44 | Classification loss: 0.00051 | Regression loss: 0.03842 | Running loss: 0.02378\n",
            "Epoch: 159 | Iteration: 45 | Classification loss: 0.00022 | Regression loss: 0.01248 | Running loss: 0.02374\n",
            "Epoch: 159 | Iteration: 46 | Classification loss: 0.00026 | Regression loss: 0.02583 | Running loss: 0.02378\n",
            "Epoch: 159 | Iteration: 47 | Classification loss: 0.00017 | Regression loss: 0.01907 | Running loss: 0.02378\n",
            "Epoch: 159 | Iteration: 48 | Classification loss: 0.00010 | Regression loss: 0.01524 | Running loss: 0.02375\n",
            "Epoch: 159 | Iteration: 49 | Classification loss: 0.00013 | Regression loss: 0.02738 | Running loss: 0.02375\n",
            "Epoch: 159 | Iteration: 50 | Classification loss: 0.00023 | Regression loss: 0.03387 | Running loss: 0.02374\n",
            "Epoch: 159 | Iteration: 51 | Classification loss: 0.00017 | Regression loss: 0.02425 | Running loss: 0.02372\n",
            "Epoch: 159 | Iteration: 52 | Classification loss: 0.00012 | Regression loss: 0.01764 | Running loss: 0.02371\n",
            "Epoch: 159 | Iteration: 53 | Classification loss: 0.00010 | Regression loss: 0.01444 | Running loss: 0.02368\n",
            "Epoch: 159 | Iteration: 54 | Classification loss: 0.00013 | Regression loss: 0.01600 | Running loss: 0.02367\n",
            "Epoch: 159 | Iteration: 55 | Classification loss: 0.00020 | Regression loss: 0.03645 | Running loss: 0.02372\n",
            "Epoch: 159 | Iteration: 56 | Classification loss: 0.00019 | Regression loss: 0.01492 | Running loss: 0.02368\n",
            "Epoch: 159 | Iteration: 57 | Classification loss: 0.00034 | Regression loss: 0.03584 | Running loss: 0.02373\n",
            "Epoch: 159 | Iteration: 58 | Classification loss: 0.00010 | Regression loss: 0.01294 | Running loss: 0.02372\n",
            "Epoch: 159 | Iteration: 59 | Classification loss: 0.00005 | Regression loss: 0.02123 | Running loss: 0.02370\n",
            "Epoch: 159 | Iteration: 60 | Classification loss: 0.00004 | Regression loss: 0.01626 | Running loss: 0.02371\n",
            "Epoch: 159 | Iteration: 61 | Classification loss: 0.00026 | Regression loss: 0.01999 | Running loss: 0.02371\n",
            "Epoch: 159 | Iteration: 62 | Classification loss: 0.00069 | Regression loss: 0.02083 | Running loss: 0.02372\n",
            "Epoch: 159 | Iteration: 63 | Classification loss: 0.00012 | Regression loss: 0.02167 | Running loss: 0.02373\n",
            "Epoch: 159 | Iteration: 64 | Classification loss: 0.00028 | Regression loss: 0.03764 | Running loss: 0.02378\n",
            "Epoch: 159 | Iteration: 65 | Classification loss: 0.00018 | Regression loss: 0.01324 | Running loss: 0.02378\n",
            "Epoch: 159 | Iteration: 66 | Classification loss: 0.00056 | Regression loss: 0.05107 | Running loss: 0.02382\n",
            "Epoch: 159 | Iteration: 67 | Classification loss: 0.00069 | Regression loss: 0.04076 | Running loss: 0.02386\n",
            "Epoch: 159 | Iteration: 68 | Classification loss: 0.00019 | Regression loss: 0.02907 | Running loss: 0.02389\n",
            "Epoch: 159 | Iteration: 69 | Classification loss: 0.00009 | Regression loss: 0.02748 | Running loss: 0.02391\n",
            "Epoch: 159 | Iteration: 70 | Classification loss: 0.00024 | Regression loss: 0.01500 | Running loss: 0.02385\n",
            "Epoch: 159 | Iteration: 71 | Classification loss: 0.00016 | Regression loss: 0.02053 | Running loss: 0.02381\n",
            "Epoch: 159 | Iteration: 72 | Classification loss: 0.00041 | Regression loss: 0.04112 | Running loss: 0.02386\n",
            "Epoch: 159 | Iteration: 73 | Classification loss: 0.00039 | Regression loss: 0.03140 | Running loss: 0.02390\n",
            "Epoch: 159 | Iteration: 74 | Classification loss: 0.00019 | Regression loss: 0.02563 | Running loss: 0.02390\n",
            "Epoch: 159 | Iteration: 75 | Classification loss: 0.00008 | Regression loss: 0.02100 | Running loss: 0.02391\n",
            "Epoch: 159 | Iteration: 76 | Classification loss: 0.00003 | Regression loss: 0.00469 | Running loss: 0.02385\n",
            "Epoch: 159 | Iteration: 77 | Classification loss: 0.00049 | Regression loss: 0.03826 | Running loss: 0.02390\n",
            "Epoch: 159 | Iteration: 78 | Classification loss: 0.00023 | Regression loss: 0.02730 | Running loss: 0.02392\n",
            "Epoch: 159 | Iteration: 79 | Classification loss: 0.00018 | Regression loss: 0.01791 | Running loss: 0.02391\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7272071618498421\n",
            "Precision:  0.5632183908045977\n",
            "Recall:  0.7945945945945946\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}, {0: (0.7229906021615096, 185.0)}, {0: (0.7259378939307871, 185.0)}, {0: (0.7306172160652471, 185.0)}, {0: (0.7210590702449308, 185.0)}, {0: (0.7214498931770392, 185.0)}, {0: (0.7272071618498421, 185.0)}]\n",
            "Epoch: 160 | Iteration: 0 | Classification loss: 0.00023 | Regression loss: 0.02498 | Running loss: 0.02391\n",
            "Epoch: 160 | Iteration: 1 | Classification loss: 0.00018 | Regression loss: 0.01685 | Running loss: 0.02390\n",
            "Epoch: 160 | Iteration: 2 | Classification loss: 0.00010 | Regression loss: 0.02382 | Running loss: 0.02389\n",
            "Epoch: 160 | Iteration: 3 | Classification loss: 0.00007 | Regression loss: 0.02075 | Running loss: 0.02385\n",
            "Epoch: 160 | Iteration: 4 | Classification loss: 0.00033 | Regression loss: 0.04330 | Running loss: 0.02390\n",
            "Epoch: 160 | Iteration: 5 | Classification loss: 0.00051 | Regression loss: 0.03973 | Running loss: 0.02396\n",
            "Epoch: 160 | Iteration: 6 | Classification loss: 0.00007 | Regression loss: 0.00889 | Running loss: 0.02395\n",
            "Epoch: 160 | Iteration: 7 | Classification loss: 0.00035 | Regression loss: 0.02497 | Running loss: 0.02396\n",
            "Epoch: 160 | Iteration: 8 | Classification loss: 0.00024 | Regression loss: 0.02833 | Running loss: 0.02393\n",
            "Epoch: 160 | Iteration: 9 | Classification loss: 0.00056 | Regression loss: 0.03693 | Running loss: 0.02399\n",
            "Epoch: 160 | Iteration: 10 | Classification loss: 0.00083 | Regression loss: 0.03655 | Running loss: 0.02403\n",
            "Epoch: 160 | Iteration: 11 | Classification loss: 0.00029 | Regression loss: 0.01484 | Running loss: 0.02397\n",
            "Epoch: 160 | Iteration: 12 | Classification loss: 0.00034 | Regression loss: 0.04242 | Running loss: 0.02402\n",
            "Epoch: 160 | Iteration: 13 | Classification loss: 0.00018 | Regression loss: 0.01542 | Running loss: 0.02403\n",
            "Epoch: 160 | Iteration: 14 | Classification loss: 0.00011 | Regression loss: 0.01737 | Running loss: 0.02403\n",
            "Epoch: 160 | Iteration: 15 | Classification loss: 0.00013 | Regression loss: 0.03083 | Running loss: 0.02407\n",
            "Epoch: 160 | Iteration: 16 | Classification loss: 0.00020 | Regression loss: 0.01034 | Running loss: 0.02404\n",
            "Epoch: 160 | Iteration: 17 | Classification loss: 0.00009 | Regression loss: 0.00890 | Running loss: 0.02404\n",
            "Epoch: 160 | Iteration: 18 | Classification loss: 0.00015 | Regression loss: 0.01615 | Running loss: 0.02398\n",
            "Epoch: 160 | Iteration: 19 | Classification loss: 0.00009 | Regression loss: 0.01023 | Running loss: 0.02390\n",
            "Epoch: 160 | Iteration: 20 | Classification loss: 0.00008 | Regression loss: 0.01246 | Running loss: 0.02388\n",
            "Epoch: 160 | Iteration: 21 | Classification loss: 0.00027 | Regression loss: 0.02079 | Running loss: 0.02389\n",
            "Epoch: 160 | Iteration: 22 | Classification loss: 0.00004 | Regression loss: 0.01114 | Running loss: 0.02386\n",
            "Epoch: 160 | Iteration: 23 | Classification loss: 0.00007 | Regression loss: 0.02343 | Running loss: 0.02389\n",
            "Epoch: 160 | Iteration: 24 | Classification loss: 0.00049 | Regression loss: 0.03328 | Running loss: 0.02391\n",
            "Epoch: 160 | Iteration: 25 | Classification loss: 0.00018 | Regression loss: 0.02132 | Running loss: 0.02393\n",
            "Epoch: 160 | Iteration: 26 | Classification loss: 0.00005 | Regression loss: 0.00760 | Running loss: 0.02387\n",
            "Epoch: 160 | Iteration: 27 | Classification loss: 0.00032 | Regression loss: 0.02876 | Running loss: 0.02391\n",
            "Epoch: 160 | Iteration: 28 | Classification loss: 0.00012 | Regression loss: 0.01820 | Running loss: 0.02390\n",
            "Epoch: 160 | Iteration: 29 | Classification loss: 0.00005 | Regression loss: 0.00859 | Running loss: 0.02387\n",
            "Epoch: 160 | Iteration: 30 | Classification loss: 0.00068 | Regression loss: 0.04095 | Running loss: 0.02391\n",
            "Epoch: 160 | Iteration: 31 | Classification loss: 0.00004 | Regression loss: 0.01577 | Running loss: 0.02390\n",
            "Epoch: 160 | Iteration: 32 | Classification loss: 0.00017 | Regression loss: 0.03140 | Running loss: 0.02391\n",
            "Epoch: 160 | Iteration: 33 | Classification loss: 0.00059 | Regression loss: 0.02498 | Running loss: 0.02393\n",
            "Epoch: 160 | Iteration: 34 | Classification loss: 0.00018 | Regression loss: 0.01457 | Running loss: 0.02392\n",
            "Epoch: 160 | Iteration: 35 | Classification loss: 0.00013 | Regression loss: 0.03189 | Running loss: 0.02394\n",
            "Epoch: 160 | Iteration: 36 | Classification loss: 0.00011 | Regression loss: 0.01652 | Running loss: 0.02394\n",
            "Epoch: 160 | Iteration: 37 | Classification loss: 0.00084 | Regression loss: 0.02495 | Running loss: 0.02390\n",
            "Epoch: 160 | Iteration: 38 | Classification loss: 0.00017 | Regression loss: 0.01927 | Running loss: 0.02391\n",
            "Epoch: 160 | Iteration: 39 | Classification loss: 0.00063 | Regression loss: 0.03549 | Running loss: 0.02393\n",
            "Epoch: 160 | Iteration: 40 | Classification loss: 0.00048 | Regression loss: 0.05175 | Running loss: 0.02398\n",
            "Epoch: 160 | Iteration: 41 | Classification loss: 0.00079 | Regression loss: 0.03852 | Running loss: 0.02401\n",
            "Epoch: 160 | Iteration: 42 | Classification loss: 0.00021 | Regression loss: 0.01306 | Running loss: 0.02399\n",
            "Epoch: 160 | Iteration: 43 | Classification loss: 0.00027 | Regression loss: 0.01979 | Running loss: 0.02393\n",
            "Epoch: 160 | Iteration: 44 | Classification loss: 0.00006 | Regression loss: 0.02860 | Running loss: 0.02396\n",
            "Epoch: 160 | Iteration: 45 | Classification loss: 0.00014 | Regression loss: 0.01660 | Running loss: 0.02393\n",
            "Epoch: 160 | Iteration: 46 | Classification loss: 0.00020 | Regression loss: 0.02630 | Running loss: 0.02391\n",
            "Epoch: 160 | Iteration: 47 | Classification loss: 0.00043 | Regression loss: 0.02334 | Running loss: 0.02392\n",
            "Epoch: 160 | Iteration: 48 | Classification loss: 0.00021 | Regression loss: 0.04020 | Running loss: 0.02394\n",
            "Epoch: 160 | Iteration: 49 | Classification loss: 0.00016 | Regression loss: 0.02331 | Running loss: 0.02393\n",
            "Epoch: 160 | Iteration: 50 | Classification loss: 0.00013 | Regression loss: 0.01808 | Running loss: 0.02394\n",
            "Epoch: 160 | Iteration: 51 | Classification loss: 0.00008 | Regression loss: 0.01719 | Running loss: 0.02393\n",
            "Epoch: 160 | Iteration: 52 | Classification loss: 0.00034 | Regression loss: 0.02810 | Running loss: 0.02391\n",
            "Epoch: 160 | Iteration: 53 | Classification loss: 0.00035 | Regression loss: 0.03488 | Running loss: 0.02394\n",
            "Epoch: 160 | Iteration: 54 | Classification loss: 0.00011 | Regression loss: 0.02036 | Running loss: 0.02394\n",
            "Epoch: 160 | Iteration: 55 | Classification loss: 0.00021 | Regression loss: 0.02675 | Running loss: 0.02397\n",
            "Epoch: 160 | Iteration: 56 | Classification loss: 0.00018 | Regression loss: 0.01462 | Running loss: 0.02396\n",
            "Epoch: 160 | Iteration: 57 | Classification loss: 0.00032 | Regression loss: 0.03118 | Running loss: 0.02399\n",
            "Epoch: 160 | Iteration: 58 | Classification loss: 0.00006 | Regression loss: 0.02110 | Running loss: 0.02402\n",
            "Epoch: 160 | Iteration: 59 | Classification loss: 0.00013 | Regression loss: 0.02434 | Running loss: 0.02404\n",
            "Epoch: 160 | Iteration: 60 | Classification loss: 0.00019 | Regression loss: 0.01855 | Running loss: 0.02403\n",
            "Epoch: 160 | Iteration: 61 | Classification loss: 0.00017 | Regression loss: 0.02017 | Running loss: 0.02401\n",
            "Epoch: 160 | Iteration: 62 | Classification loss: 0.00025 | Regression loss: 0.02001 | Running loss: 0.02402\n",
            "Epoch: 160 | Iteration: 63 | Classification loss: 0.00013 | Regression loss: 0.01480 | Running loss: 0.02400\n",
            "Epoch: 160 | Iteration: 64 | Classification loss: 0.00029 | Regression loss: 0.03650 | Running loss: 0.02404\n",
            "Epoch: 160 | Iteration: 65 | Classification loss: 0.00024 | Regression loss: 0.02098 | Running loss: 0.02400\n",
            "Epoch: 160 | Iteration: 66 | Classification loss: 0.00008 | Regression loss: 0.01643 | Running loss: 0.02399\n",
            "Epoch: 160 | Iteration: 67 | Classification loss: 0.00020 | Regression loss: 0.01567 | Running loss: 0.02397\n",
            "Epoch: 160 | Iteration: 68 | Classification loss: 0.00028 | Regression loss: 0.03571 | Running loss: 0.02398\n",
            "Epoch: 160 | Iteration: 69 | Classification loss: 0.00019 | Regression loss: 0.01591 | Running loss: 0.02396\n",
            "Epoch: 160 | Iteration: 70 | Classification loss: 0.00009 | Regression loss: 0.01303 | Running loss: 0.02392\n",
            "Epoch: 160 | Iteration: 71 | Classification loss: 0.00010 | Regression loss: 0.02498 | Running loss: 0.02392\n",
            "Epoch: 160 | Iteration: 72 | Classification loss: 0.00025 | Regression loss: 0.02598 | Running loss: 0.02390\n",
            "Epoch: 160 | Iteration: 73 | Classification loss: 0.00024 | Regression loss: 0.02355 | Running loss: 0.02386\n",
            "Epoch: 160 | Iteration: 74 | Classification loss: 0.00050 | Regression loss: 0.02406 | Running loss: 0.02383\n",
            "Epoch: 160 | Iteration: 75 | Classification loss: 0.00013 | Regression loss: 0.01779 | Running loss: 0.02383\n",
            "Epoch: 160 | Iteration: 76 | Classification loss: 0.00045 | Regression loss: 0.03138 | Running loss: 0.02386\n",
            "Epoch: 160 | Iteration: 77 | Classification loss: 0.00004 | Regression loss: 0.00477 | Running loss: 0.02384\n",
            "Epoch: 160 | Iteration: 78 | Classification loss: 0.00025 | Regression loss: 0.04213 | Running loss: 0.02384\n",
            "Epoch: 160 | Iteration: 79 | Classification loss: 0.00050 | Regression loss: 0.02203 | Running loss: 0.02382\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.725966639522496\n",
            "Precision:  0.55893536121673\n",
            "Recall:  0.7945945945945946\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}, {0: (0.7229906021615096, 185.0)}, {0: (0.7259378939307871, 185.0)}, {0: (0.7306172160652471, 185.0)}, {0: (0.7210590702449308, 185.0)}, {0: (0.7214498931770392, 185.0)}, {0: (0.7272071618498421, 185.0)}, {0: (0.725966639522496, 185.0)}]\n",
            "Epoch: 161 | Iteration: 0 | Classification loss: 0.00046 | Regression loss: 0.03205 | Running loss: 0.02383\n",
            "Epoch: 161 | Iteration: 1 | Classification loss: 0.00008 | Regression loss: 0.01024 | Running loss: 0.02382\n",
            "Epoch: 161 | Iteration: 2 | Classification loss: 0.00006 | Regression loss: 0.00740 | Running loss: 0.02379\n",
            "Epoch: 161 | Iteration: 3 | Classification loss: 0.00014 | Regression loss: 0.01866 | Running loss: 0.02379\n",
            "Epoch: 161 | Iteration: 4 | Classification loss: 0.00020 | Regression loss: 0.01630 | Running loss: 0.02378\n",
            "Epoch: 161 | Iteration: 5 | Classification loss: 0.00023 | Regression loss: 0.01571 | Running loss: 0.02374\n",
            "Epoch: 161 | Iteration: 6 | Classification loss: 0.00037 | Regression loss: 0.03334 | Running loss: 0.02378\n",
            "Epoch: 161 | Iteration: 7 | Classification loss: 0.00050 | Regression loss: 0.04588 | Running loss: 0.02381\n",
            "Epoch: 161 | Iteration: 8 | Classification loss: 0.00094 | Regression loss: 0.04370 | Running loss: 0.02385\n",
            "Epoch: 161 | Iteration: 9 | Classification loss: 0.00003 | Regression loss: 0.00469 | Running loss: 0.02382\n",
            "Epoch: 161 | Iteration: 10 | Classification loss: 0.00013 | Regression loss: 0.01628 | Running loss: 0.02381\n",
            "Epoch: 161 | Iteration: 11 | Classification loss: 0.00015 | Regression loss: 0.03033 | Running loss: 0.02385\n",
            "Epoch: 161 | Iteration: 12 | Classification loss: 0.00007 | Regression loss: 0.01479 | Running loss: 0.02384\n",
            "Epoch: 161 | Iteration: 13 | Classification loss: 0.00024 | Regression loss: 0.02678 | Running loss: 0.02383\n",
            "Epoch: 161 | Iteration: 14 | Classification loss: 0.00024 | Regression loss: 0.02392 | Running loss: 0.02385\n",
            "Epoch: 161 | Iteration: 15 | Classification loss: 0.00014 | Regression loss: 0.02830 | Running loss: 0.02381\n",
            "Epoch: 161 | Iteration: 16 | Classification loss: 0.00028 | Regression loss: 0.02745 | Running loss: 0.02385\n",
            "Epoch: 161 | Iteration: 17 | Classification loss: 0.00020 | Regression loss: 0.01226 | Running loss: 0.02379\n",
            "Epoch: 161 | Iteration: 18 | Classification loss: 0.00010 | Regression loss: 0.02344 | Running loss: 0.02382\n",
            "Epoch: 161 | Iteration: 19 | Classification loss: 0.00026 | Regression loss: 0.01878 | Running loss: 0.02379\n",
            "Epoch: 161 | Iteration: 20 | Classification loss: 0.00022 | Regression loss: 0.01968 | Running loss: 0.02376\n",
            "Epoch: 161 | Iteration: 21 | Classification loss: 0.00080 | Regression loss: 0.03757 | Running loss: 0.02377\n",
            "Epoch: 161 | Iteration: 22 | Classification loss: 0.00018 | Regression loss: 0.02228 | Running loss: 0.02375\n",
            "Epoch: 161 | Iteration: 23 | Classification loss: 0.00016 | Regression loss: 0.01333 | Running loss: 0.02376\n",
            "Epoch: 161 | Iteration: 24 | Classification loss: 0.00041 | Regression loss: 0.02360 | Running loss: 0.02378\n",
            "Epoch: 161 | Iteration: 25 | Classification loss: 0.00007 | Regression loss: 0.00861 | Running loss: 0.02376\n",
            "Epoch: 161 | Iteration: 26 | Classification loss: 0.00014 | Regression loss: 0.02347 | Running loss: 0.02376\n",
            "Epoch: 161 | Iteration: 27 | Classification loss: 0.00059 | Regression loss: 0.02245 | Running loss: 0.02376\n",
            "Epoch: 161 | Iteration: 28 | Classification loss: 0.00051 | Regression loss: 0.03585 | Running loss: 0.02379\n",
            "Epoch: 161 | Iteration: 29 | Classification loss: 0.00016 | Regression loss: 0.02081 | Running loss: 0.02381\n",
            "Epoch: 161 | Iteration: 30 | Classification loss: 0.00010 | Regression loss: 0.01493 | Running loss: 0.02379\n",
            "Epoch: 161 | Iteration: 31 | Classification loss: 0.00019 | Regression loss: 0.03799 | Running loss: 0.02384\n",
            "Epoch: 161 | Iteration: 32 | Classification loss: 0.00012 | Regression loss: 0.01782 | Running loss: 0.02381\n",
            "Epoch: 161 | Iteration: 33 | Classification loss: 0.00015 | Regression loss: 0.03146 | Running loss: 0.02382\n",
            "Epoch: 161 | Iteration: 34 | Classification loss: 0.00014 | Regression loss: 0.02168 | Running loss: 0.02383\n",
            "Epoch: 161 | Iteration: 35 | Classification loss: 0.00018 | Regression loss: 0.02021 | Running loss: 0.02382\n",
            "Epoch: 161 | Iteration: 36 | Classification loss: 0.00010 | Regression loss: 0.01498 | Running loss: 0.02380\n",
            "Epoch: 161 | Iteration: 37 | Classification loss: 0.00066 | Regression loss: 0.02046 | Running loss: 0.02380\n",
            "Epoch: 161 | Iteration: 38 | Classification loss: 0.00008 | Regression loss: 0.02375 | Running loss: 0.02377\n",
            "Epoch: 161 | Iteration: 39 | Classification loss: 0.00016 | Regression loss: 0.01581 | Running loss: 0.02369\n",
            "Epoch: 161 | Iteration: 40 | Classification loss: 0.00020 | Regression loss: 0.01461 | Running loss: 0.02365\n",
            "Epoch: 161 | Iteration: 41 | Classification loss: 0.00006 | Regression loss: 0.01598 | Running loss: 0.02362\n",
            "Epoch: 161 | Iteration: 42 | Classification loss: 0.00017 | Regression loss: 0.01393 | Running loss: 0.02360\n",
            "Epoch: 161 | Iteration: 43 | Classification loss: 0.00033 | Regression loss: 0.03297 | Running loss: 0.02363\n",
            "Epoch: 161 | Iteration: 44 | Classification loss: 0.00006 | Regression loss: 0.02330 | Running loss: 0.02364\n",
            "Epoch: 161 | Iteration: 45 | Classification loss: 0.00009 | Regression loss: 0.01529 | Running loss: 0.02363\n",
            "Epoch: 161 | Iteration: 46 | Classification loss: 0.00047 | Regression loss: 0.04352 | Running loss: 0.02369\n",
            "Epoch: 161 | Iteration: 47 | Classification loss: 0.00024 | Regression loss: 0.02077 | Running loss: 0.02366\n",
            "Epoch: 161 | Iteration: 48 | Classification loss: 0.00007 | Regression loss: 0.01595 | Running loss: 0.02363\n",
            "Epoch: 161 | Iteration: 49 | Classification loss: 0.00018 | Regression loss: 0.01874 | Running loss: 0.02362\n",
            "Epoch: 161 | Iteration: 50 | Classification loss: 0.00008 | Regression loss: 0.02290 | Running loss: 0.02362\n",
            "Epoch: 161 | Iteration: 51 | Classification loss: 0.00055 | Regression loss: 0.02559 | Running loss: 0.02363\n",
            "Epoch: 161 | Iteration: 52 | Classification loss: 0.00015 | Regression loss: 0.01834 | Running loss: 0.02365\n",
            "Epoch: 161 | Iteration: 53 | Classification loss: 0.00027 | Regression loss: 0.03407 | Running loss: 0.02367\n",
            "Epoch: 161 | Iteration: 54 | Classification loss: 0.00018 | Regression loss: 0.01051 | Running loss: 0.02364\n",
            "Epoch: 161 | Iteration: 55 | Classification loss: 0.00005 | Regression loss: 0.00911 | Running loss: 0.02364\n",
            "Epoch: 161 | Iteration: 56 | Classification loss: 0.00053 | Regression loss: 0.04107 | Running loss: 0.02369\n",
            "Epoch: 161 | Iteration: 57 | Classification loss: 0.00015 | Regression loss: 0.01584 | Running loss: 0.02369\n",
            "Epoch: 161 | Iteration: 58 | Classification loss: 0.00024 | Regression loss: 0.02663 | Running loss: 0.02371\n",
            "Epoch: 161 | Iteration: 59 | Classification loss: 0.00050 | Regression loss: 0.03988 | Running loss: 0.02373\n",
            "Epoch: 161 | Iteration: 60 | Classification loss: 0.00013 | Regression loss: 0.01538 | Running loss: 0.02367\n",
            "Epoch: 161 | Iteration: 61 | Classification loss: 0.00016 | Regression loss: 0.01850 | Running loss: 0.02362\n",
            "Epoch: 161 | Iteration: 62 | Classification loss: 0.00012 | Regression loss: 0.01602 | Running loss: 0.02360\n",
            "Epoch: 161 | Iteration: 63 | Classification loss: 0.00036 | Regression loss: 0.02673 | Running loss: 0.02360\n",
            "Epoch: 161 | Iteration: 64 | Classification loss: 0.00019 | Regression loss: 0.02296 | Running loss: 0.02363\n",
            "Epoch: 161 | Iteration: 65 | Classification loss: 0.00008 | Regression loss: 0.02408 | Running loss: 0.02361\n",
            "Epoch: 161 | Iteration: 66 | Classification loss: 0.00029 | Regression loss: 0.03509 | Running loss: 0.02361\n",
            "Epoch: 161 | Iteration: 67 | Classification loss: 0.00042 | Regression loss: 0.05064 | Running loss: 0.02366\n",
            "Epoch: 161 | Iteration: 68 | Classification loss: 0.00066 | Regression loss: 0.03294 | Running loss: 0.02369\n",
            "Epoch: 161 | Iteration: 69 | Classification loss: 0.00006 | Regression loss: 0.02089 | Running loss: 0.02369\n",
            "Epoch: 161 | Iteration: 70 | Classification loss: 0.00027 | Regression loss: 0.02298 | Running loss: 0.02365\n",
            "Epoch: 161 | Iteration: 71 | Classification loss: 0.00011 | Regression loss: 0.01101 | Running loss: 0.02363\n",
            "Epoch: 161 | Iteration: 72 | Classification loss: 0.00002 | Regression loss: 0.01008 | Running loss: 0.02362\n",
            "Epoch: 161 | Iteration: 73 | Classification loss: 0.00027 | Regression loss: 0.03635 | Running loss: 0.02365\n",
            "Epoch: 161 | Iteration: 74 | Classification loss: 0.00031 | Regression loss: 0.03338 | Running loss: 0.02367\n",
            "Epoch: 161 | Iteration: 75 | Classification loss: 0.00039 | Regression loss: 0.02670 | Running loss: 0.02368\n",
            "Epoch: 161 | Iteration: 76 | Classification loss: 0.00016 | Regression loss: 0.01877 | Running loss: 0.02371\n",
            "Epoch: 161 | Iteration: 77 | Classification loss: 0.00048 | Regression loss: 0.03099 | Running loss: 0.02373\n",
            "Epoch: 161 | Iteration: 78 | Classification loss: 0.00034 | Regression loss: 0.02455 | Running loss: 0.02374\n",
            "Epoch: 161 | Iteration: 79 | Classification loss: 0.00010 | Regression loss: 0.02627 | Running loss: 0.02371\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7208028358685962\n",
            "Precision:  0.5615384615384615\n",
            "Recall:  0.7891891891891892\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}, {0: (0.7229906021615096, 185.0)}, {0: (0.7259378939307871, 185.0)}, {0: (0.7306172160652471, 185.0)}, {0: (0.7210590702449308, 185.0)}, {0: (0.7214498931770392, 185.0)}, {0: (0.7272071618498421, 185.0)}, {0: (0.725966639522496, 185.0)}, {0: (0.7208028358685962, 185.0)}]\n",
            "Epoch: 162 | Iteration: 0 | Classification loss: 0.00055 | Regression loss: 0.03285 | Running loss: 0.02377\n",
            "Epoch: 162 | Iteration: 1 | Classification loss: 0.00026 | Regression loss: 0.02017 | Running loss: 0.02378\n",
            "Epoch: 162 | Iteration: 2 | Classification loss: 0.00019 | Regression loss: 0.03389 | Running loss: 0.02382\n",
            "Epoch: 162 | Iteration: 3 | Classification loss: 0.00008 | Regression loss: 0.02065 | Running loss: 0.02378\n",
            "Epoch: 162 | Iteration: 4 | Classification loss: 0.00034 | Regression loss: 0.02711 | Running loss: 0.02380\n",
            "Epoch: 162 | Iteration: 5 | Classification loss: 0.00060 | Regression loss: 0.02318 | Running loss: 0.02381\n",
            "Epoch: 162 | Iteration: 6 | Classification loss: 0.00013 | Regression loss: 0.01092 | Running loss: 0.02380\n",
            "Epoch: 162 | Iteration: 7 | Classification loss: 0.00015 | Regression loss: 0.01511 | Running loss: 0.02376\n",
            "Epoch: 162 | Iteration: 8 | Classification loss: 0.00004 | Regression loss: 0.01582 | Running loss: 0.02375\n",
            "Epoch: 162 | Iteration: 9 | Classification loss: 0.00008 | Regression loss: 0.01464 | Running loss: 0.02371\n",
            "Epoch: 162 | Iteration: 10 | Classification loss: 0.00013 | Regression loss: 0.01588 | Running loss: 0.02371\n",
            "Epoch: 162 | Iteration: 11 | Classification loss: 0.00025 | Regression loss: 0.01811 | Running loss: 0.02369\n",
            "Epoch: 162 | Iteration: 12 | Classification loss: 0.00038 | Regression loss: 0.02598 | Running loss: 0.02365\n",
            "Epoch: 162 | Iteration: 13 | Classification loss: 0.00019 | Regression loss: 0.01418 | Running loss: 0.02365\n",
            "Epoch: 162 | Iteration: 14 | Classification loss: 0.00004 | Regression loss: 0.00719 | Running loss: 0.02360\n",
            "Epoch: 162 | Iteration: 15 | Classification loss: 0.00028 | Regression loss: 0.02438 | Running loss: 0.02360\n",
            "Epoch: 162 | Iteration: 16 | Classification loss: 0.00013 | Regression loss: 0.01776 | Running loss: 0.02360\n",
            "Epoch: 162 | Iteration: 17 | Classification loss: 0.00063 | Regression loss: 0.02190 | Running loss: 0.02358\n",
            "Epoch: 162 | Iteration: 18 | Classification loss: 0.00010 | Regression loss: 0.02944 | Running loss: 0.02358\n",
            "Epoch: 162 | Iteration: 19 | Classification loss: 0.00051 | Regression loss: 0.03684 | Running loss: 0.02363\n",
            "Epoch: 162 | Iteration: 20 | Classification loss: 0.00020 | Regression loss: 0.01287 | Running loss: 0.02361\n",
            "Epoch: 162 | Iteration: 21 | Classification loss: 0.00079 | Regression loss: 0.03735 | Running loss: 0.02361\n",
            "Epoch: 162 | Iteration: 22 | Classification loss: 0.00007 | Regression loss: 0.01574 | Running loss: 0.02360\n",
            "Epoch: 162 | Iteration: 23 | Classification loss: 0.00019 | Regression loss: 0.01345 | Running loss: 0.02360\n",
            "Epoch: 162 | Iteration: 24 | Classification loss: 0.00020 | Regression loss: 0.01590 | Running loss: 0.02359\n",
            "Epoch: 162 | Iteration: 25 | Classification loss: 0.00017 | Regression loss: 0.02033 | Running loss: 0.02360\n",
            "Epoch: 162 | Iteration: 26 | Classification loss: 0.00023 | Regression loss: 0.03042 | Running loss: 0.02363\n",
            "Epoch: 162 | Iteration: 27 | Classification loss: 0.00018 | Regression loss: 0.01762 | Running loss: 0.02361\n",
            "Epoch: 162 | Iteration: 28 | Classification loss: 0.00046 | Regression loss: 0.02557 | Running loss: 0.02363\n",
            "Epoch: 162 | Iteration: 29 | Classification loss: 0.00048 | Regression loss: 0.04550 | Running loss: 0.02366\n",
            "Epoch: 162 | Iteration: 30 | Classification loss: 0.00022 | Regression loss: 0.02613 | Running loss: 0.02364\n",
            "Epoch: 162 | Iteration: 31 | Classification loss: 0.00008 | Regression loss: 0.01593 | Running loss: 0.02363\n",
            "Epoch: 162 | Iteration: 32 | Classification loss: 0.00021 | Regression loss: 0.03062 | Running loss: 0.02362\n",
            "Epoch: 162 | Iteration: 33 | Classification loss: 0.00044 | Regression loss: 0.03148 | Running loss: 0.02364\n",
            "Epoch: 162 | Iteration: 34 | Classification loss: 0.00027 | Regression loss: 0.01902 | Running loss: 0.02364\n",
            "Epoch: 162 | Iteration: 35 | Classification loss: 0.00041 | Regression loss: 0.04257 | Running loss: 0.02368\n",
            "Epoch: 162 | Iteration: 36 | Classification loss: 0.00007 | Regression loss: 0.03159 | Running loss: 0.02374\n",
            "Epoch: 162 | Iteration: 37 | Classification loss: 0.00037 | Regression loss: 0.02290 | Running loss: 0.02375\n",
            "Epoch: 162 | Iteration: 38 | Classification loss: 0.00019 | Regression loss: 0.01879 | Running loss: 0.02376\n",
            "Epoch: 162 | Iteration: 39 | Classification loss: 0.00007 | Regression loss: 0.02328 | Running loss: 0.02375\n",
            "Epoch: 162 | Iteration: 40 | Classification loss: 0.00020 | Regression loss: 0.02408 | Running loss: 0.02376\n",
            "Epoch: 162 | Iteration: 41 | Classification loss: 0.00028 | Regression loss: 0.02510 | Running loss: 0.02377\n",
            "Epoch: 162 | Iteration: 42 | Classification loss: 0.00013 | Regression loss: 0.02604 | Running loss: 0.02374\n",
            "Epoch: 162 | Iteration: 43 | Classification loss: 0.00017 | Regression loss: 0.01328 | Running loss: 0.02371\n",
            "Epoch: 162 | Iteration: 44 | Classification loss: 0.00013 | Regression loss: 0.02361 | Running loss: 0.02371\n",
            "Epoch: 162 | Iteration: 45 | Classification loss: 0.00066 | Regression loss: 0.05150 | Running loss: 0.02378\n",
            "Epoch: 162 | Iteration: 46 | Classification loss: 0.00014 | Regression loss: 0.01896 | Running loss: 0.02377\n",
            "Epoch: 162 | Iteration: 47 | Classification loss: 0.00012 | Regression loss: 0.01726 | Running loss: 0.02373\n",
            "Epoch: 162 | Iteration: 48 | Classification loss: 0.00027 | Regression loss: 0.01457 | Running loss: 0.02374\n",
            "Epoch: 162 | Iteration: 49 | Classification loss: 0.00025 | Regression loss: 0.02285 | Running loss: 0.02370\n",
            "Epoch: 162 | Iteration: 50 | Classification loss: 0.00027 | Regression loss: 0.02146 | Running loss: 0.02368\n",
            "Epoch: 162 | Iteration: 51 | Classification loss: 0.00007 | Regression loss: 0.00859 | Running loss: 0.02364\n",
            "Epoch: 162 | Iteration: 52 | Classification loss: 0.00018 | Regression loss: 0.01882 | Running loss: 0.02362\n",
            "Epoch: 162 | Iteration: 53 | Classification loss: 0.00064 | Regression loss: 0.04095 | Running loss: 0.02364\n",
            "Epoch: 162 | Iteration: 54 | Classification loss: 0.00026 | Regression loss: 0.02284 | Running loss: 0.02361\n",
            "Epoch: 162 | Iteration: 55 | Classification loss: 0.00056 | Regression loss: 0.02190 | Running loss: 0.02359\n",
            "Epoch: 162 | Iteration: 56 | Classification loss: 0.00027 | Regression loss: 0.03358 | Running loss: 0.02359\n",
            "Epoch: 162 | Iteration: 57 | Classification loss: 0.00032 | Regression loss: 0.03059 | Running loss: 0.02360\n",
            "Epoch: 162 | Iteration: 58 | Classification loss: 0.00008 | Regression loss: 0.01228 | Running loss: 0.02356\n",
            "Epoch: 162 | Iteration: 59 | Classification loss: 0.00004 | Regression loss: 0.01028 | Running loss: 0.02354\n",
            "Epoch: 162 | Iteration: 60 | Classification loss: 0.00082 | Regression loss: 0.03562 | Running loss: 0.02357\n",
            "Epoch: 162 | Iteration: 61 | Classification loss: 0.00032 | Regression loss: 0.03267 | Running loss: 0.02359\n",
            "Epoch: 162 | Iteration: 62 | Classification loss: 0.00008 | Regression loss: 0.02542 | Running loss: 0.02359\n",
            "Epoch: 162 | Iteration: 63 | Classification loss: 0.00020 | Regression loss: 0.03819 | Running loss: 0.02363\n",
            "Epoch: 162 | Iteration: 64 | Classification loss: 0.00018 | Regression loss: 0.02594 | Running loss: 0.02367\n",
            "Epoch: 162 | Iteration: 65 | Classification loss: 0.00052 | Regression loss: 0.03506 | Running loss: 0.02372\n",
            "Epoch: 162 | Iteration: 66 | Classification loss: 0.00006 | Regression loss: 0.00936 | Running loss: 0.02371\n",
            "Epoch: 162 | Iteration: 67 | Classification loss: 0.00012 | Regression loss: 0.01755 | Running loss: 0.02370\n",
            "Epoch: 162 | Iteration: 68 | Classification loss: 0.00014 | Regression loss: 0.02853 | Running loss: 0.02369\n",
            "Epoch: 162 | Iteration: 69 | Classification loss: 0.00009 | Regression loss: 0.01320 | Running loss: 0.02367\n",
            "Epoch: 162 | Iteration: 70 | Classification loss: 0.00006 | Regression loss: 0.02172 | Running loss: 0.02363\n",
            "Epoch: 162 | Iteration: 71 | Classification loss: 0.00003 | Regression loss: 0.00445 | Running loss: 0.02355\n",
            "Epoch: 162 | Iteration: 72 | Classification loss: 0.00054 | Regression loss: 0.03685 | Running loss: 0.02357\n",
            "Epoch: 162 | Iteration: 73 | Classification loss: 0.00012 | Regression loss: 0.02698 | Running loss: 0.02356\n",
            "Epoch: 162 | Iteration: 74 | Classification loss: 0.00033 | Regression loss: 0.04339 | Running loss: 0.02363\n",
            "Epoch: 162 | Iteration: 75 | Classification loss: 0.00014 | Regression loss: 0.01131 | Running loss: 0.02360\n",
            "Epoch: 162 | Iteration: 76 | Classification loss: 0.00013 | Regression loss: 0.01075 | Running loss: 0.02359\n",
            "Epoch: 162 | Iteration: 77 | Classification loss: 0.00015 | Regression loss: 0.01960 | Running loss: 0.02360\n",
            "Epoch: 162 | Iteration: 78 | Classification loss: 0.00006 | Regression loss: 0.02156 | Running loss: 0.02362\n",
            "Epoch: 162 | Iteration: 79 | Classification loss: 0.00013 | Regression loss: 0.01358 | Running loss: 0.02363\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7255636845600553\n",
            "Precision:  0.5675675675675675\n",
            "Recall:  0.7945945945945946\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}, {0: (0.7229906021615096, 185.0)}, {0: (0.7259378939307871, 185.0)}, {0: (0.7306172160652471, 185.0)}, {0: (0.7210590702449308, 185.0)}, {0: (0.7214498931770392, 185.0)}, {0: (0.7272071618498421, 185.0)}, {0: (0.725966639522496, 185.0)}, {0: (0.7208028358685962, 185.0)}, {0: (0.7255636845600553, 185.0)}]\n",
            "Epoch: 163 | Iteration: 0 | Classification loss: 0.00032 | Regression loss: 0.03004 | Running loss: 0.02366\n",
            "Epoch: 163 | Iteration: 1 | Classification loss: 0.00022 | Regression loss: 0.02511 | Running loss: 0.02364\n",
            "Epoch: 163 | Iteration: 2 | Classification loss: 0.00023 | Regression loss: 0.02312 | Running loss: 0.02366\n",
            "Epoch: 163 | Iteration: 3 | Classification loss: 0.00009 | Regression loss: 0.01537 | Running loss: 0.02365\n",
            "Epoch: 163 | Iteration: 4 | Classification loss: 0.00020 | Regression loss: 0.02146 | Running loss: 0.02364\n",
            "Epoch: 163 | Iteration: 5 | Classification loss: 0.00007 | Regression loss: 0.02466 | Running loss: 0.02364\n",
            "Epoch: 163 | Iteration: 6 | Classification loss: 0.00015 | Regression loss: 0.01804 | Running loss: 0.02364\n",
            "Epoch: 163 | Iteration: 7 | Classification loss: 0.00018 | Regression loss: 0.02019 | Running loss: 0.02364\n",
            "Epoch: 163 | Iteration: 8 | Classification loss: 0.00064 | Regression loss: 0.03313 | Running loss: 0.02366\n",
            "Epoch: 163 | Iteration: 9 | Classification loss: 0.00031 | Regression loss: 0.03328 | Running loss: 0.02369\n",
            "Epoch: 163 | Iteration: 10 | Classification loss: 0.00016 | Regression loss: 0.02348 | Running loss: 0.02371\n",
            "Epoch: 163 | Iteration: 11 | Classification loss: 0.00030 | Regression loss: 0.02070 | Running loss: 0.02369\n",
            "Epoch: 163 | Iteration: 12 | Classification loss: 0.00015 | Regression loss: 0.01902 | Running loss: 0.02370\n",
            "Epoch: 163 | Iteration: 13 | Classification loss: 0.00023 | Regression loss: 0.01247 | Running loss: 0.02369\n",
            "Epoch: 163 | Iteration: 14 | Classification loss: 0.00035 | Regression loss: 0.02470 | Running loss: 0.02369\n",
            "Epoch: 163 | Iteration: 15 | Classification loss: 0.00014 | Regression loss: 0.01902 | Running loss: 0.02363\n",
            "Epoch: 163 | Iteration: 16 | Classification loss: 0.00023 | Regression loss: 0.01804 | Running loss: 0.02361\n",
            "Epoch: 163 | Iteration: 17 | Classification loss: 0.00053 | Regression loss: 0.03046 | Running loss: 0.02359\n",
            "Epoch: 163 | Iteration: 18 | Classification loss: 0.00034 | Regression loss: 0.02820 | Running loss: 0.02359\n",
            "Epoch: 163 | Iteration: 19 | Classification loss: 0.00010 | Regression loss: 0.03000 | Running loss: 0.02362\n",
            "Epoch: 163 | Iteration: 20 | Classification loss: 0.00018 | Regression loss: 0.01912 | Running loss: 0.02360\n",
            "Epoch: 163 | Iteration: 21 | Classification loss: 0.00060 | Regression loss: 0.03943 | Running loss: 0.02363\n",
            "Epoch: 163 | Iteration: 22 | Classification loss: 0.00015 | Regression loss: 0.01726 | Running loss: 0.02362\n",
            "Epoch: 163 | Iteration: 23 | Classification loss: 0.00046 | Regression loss: 0.04270 | Running loss: 0.02368\n",
            "Epoch: 163 | Iteration: 24 | Classification loss: 0.00019 | Regression loss: 0.01853 | Running loss: 0.02370\n",
            "Epoch: 163 | Iteration: 25 | Classification loss: 0.00003 | Regression loss: 0.00436 | Running loss: 0.02366\n",
            "Epoch: 163 | Iteration: 26 | Classification loss: 0.00020 | Regression loss: 0.03867 | Running loss: 0.02369\n",
            "Epoch: 163 | Iteration: 27 | Classification loss: 0.00017 | Regression loss: 0.03016 | Running loss: 0.02373\n",
            "Epoch: 163 | Iteration: 28 | Classification loss: 0.00006 | Regression loss: 0.01539 | Running loss: 0.02373\n",
            "Epoch: 163 | Iteration: 29 | Classification loss: 0.00007 | Regression loss: 0.00566 | Running loss: 0.02372\n",
            "Epoch: 163 | Iteration: 30 | Classification loss: 0.00008 | Regression loss: 0.02251 | Running loss: 0.02374\n",
            "Epoch: 163 | Iteration: 31 | Classification loss: 0.00006 | Regression loss: 0.00896 | Running loss: 0.02373\n",
            "Epoch: 163 | Iteration: 32 | Classification loss: 0.00013 | Regression loss: 0.02335 | Running loss: 0.02374\n",
            "Epoch: 163 | Iteration: 33 | Classification loss: 0.00008 | Regression loss: 0.01179 | Running loss: 0.02374\n",
            "Epoch: 163 | Iteration: 34 | Classification loss: 0.00026 | Regression loss: 0.01886 | Running loss: 0.02371\n",
            "Epoch: 163 | Iteration: 35 | Classification loss: 0.00016 | Regression loss: 0.00990 | Running loss: 0.02371\n",
            "Epoch: 163 | Iteration: 36 | Classification loss: 0.00017 | Regression loss: 0.01969 | Running loss: 0.02370\n",
            "Epoch: 163 | Iteration: 37 | Classification loss: 0.00048 | Regression loss: 0.02166 | Running loss: 0.02369\n",
            "Epoch: 163 | Iteration: 38 | Classification loss: 0.00019 | Regression loss: 0.01498 | Running loss: 0.02367\n",
            "Epoch: 163 | Iteration: 39 | Classification loss: 0.00018 | Regression loss: 0.03042 | Running loss: 0.02368\n",
            "Epoch: 163 | Iteration: 40 | Classification loss: 0.00045 | Regression loss: 0.03753 | Running loss: 0.02369\n",
            "Epoch: 163 | Iteration: 41 | Classification loss: 0.00019 | Regression loss: 0.02571 | Running loss: 0.02369\n",
            "Epoch: 163 | Iteration: 42 | Classification loss: 0.00018 | Regression loss: 0.01369 | Running loss: 0.02368\n",
            "Epoch: 163 | Iteration: 43 | Classification loss: 0.00044 | Regression loss: 0.04924 | Running loss: 0.02369\n",
            "Epoch: 163 | Iteration: 44 | Classification loss: 0.00034 | Regression loss: 0.03083 | Running loss: 0.02372\n",
            "Epoch: 163 | Iteration: 45 | Classification loss: 0.00017 | Regression loss: 0.03831 | Running loss: 0.02369\n",
            "Epoch: 163 | Iteration: 46 | Classification loss: 0.00022 | Regression loss: 0.02387 | Running loss: 0.02373\n",
            "Epoch: 163 | Iteration: 47 | Classification loss: 0.00010 | Regression loss: 0.02271 | Running loss: 0.02369\n",
            "Epoch: 163 | Iteration: 48 | Classification loss: 0.00012 | Regression loss: 0.01587 | Running loss: 0.02368\n",
            "Epoch: 163 | Iteration: 49 | Classification loss: 0.00024 | Regression loss: 0.02699 | Running loss: 0.02368\n",
            "Epoch: 163 | Iteration: 50 | Classification loss: 0.00017 | Regression loss: 0.01613 | Running loss: 0.02367\n",
            "Epoch: 163 | Iteration: 51 | Classification loss: 0.00009 | Regression loss: 0.02216 | Running loss: 0.02368\n",
            "Epoch: 163 | Iteration: 52 | Classification loss: 0.00055 | Regression loss: 0.02685 | Running loss: 0.02369\n",
            "Epoch: 163 | Iteration: 53 | Classification loss: 0.00005 | Regression loss: 0.01352 | Running loss: 0.02369\n",
            "Epoch: 163 | Iteration: 54 | Classification loss: 0.00014 | Regression loss: 0.01866 | Running loss: 0.02369\n",
            "Epoch: 163 | Iteration: 55 | Classification loss: 0.00013 | Regression loss: 0.01372 | Running loss: 0.02370\n",
            "Epoch: 163 | Iteration: 56 | Classification loss: 0.00017 | Regression loss: 0.01475 | Running loss: 0.02369\n",
            "Epoch: 163 | Iteration: 57 | Classification loss: 0.00008 | Regression loss: 0.02208 | Running loss: 0.02365\n",
            "Epoch: 163 | Iteration: 58 | Classification loss: 0.00012 | Regression loss: 0.01011 | Running loss: 0.02362\n",
            "Epoch: 163 | Iteration: 59 | Classification loss: 0.00064 | Regression loss: 0.02085 | Running loss: 0.02361\n",
            "Epoch: 163 | Iteration: 60 | Classification loss: 0.00016 | Regression loss: 0.02558 | Running loss: 0.02363\n",
            "Epoch: 163 | Iteration: 61 | Classification loss: 0.00006 | Regression loss: 0.00812 | Running loss: 0.02360\n",
            "Epoch: 163 | Iteration: 62 | Classification loss: 0.00002 | Regression loss: 0.00959 | Running loss: 0.02360\n",
            "Epoch: 163 | Iteration: 63 | Classification loss: 0.00009 | Regression loss: 0.01634 | Running loss: 0.02358\n",
            "Epoch: 163 | Iteration: 64 | Classification loss: 0.00011 | Regression loss: 0.01392 | Running loss: 0.02358\n",
            "Epoch: 163 | Iteration: 65 | Classification loss: 0.00020 | Regression loss: 0.01366 | Running loss: 0.02357\n",
            "Epoch: 163 | Iteration: 66 | Classification loss: 0.00008 | Regression loss: 0.01655 | Running loss: 0.02353\n",
            "Epoch: 163 | Iteration: 67 | Classification loss: 0.00047 | Regression loss: 0.04353 | Running loss: 0.02354\n",
            "Epoch: 163 | Iteration: 68 | Classification loss: 0.00035 | Regression loss: 0.03197 | Running loss: 0.02354\n",
            "Epoch: 163 | Iteration: 69 | Classification loss: 0.00020 | Regression loss: 0.02545 | Running loss: 0.02355\n",
            "Epoch: 163 | Iteration: 70 | Classification loss: 0.00024 | Regression loss: 0.01936 | Running loss: 0.02355\n",
            "Epoch: 163 | Iteration: 71 | Classification loss: 0.00011 | Regression loss: 0.01498 | Running loss: 0.02356\n",
            "Epoch: 163 | Iteration: 72 | Classification loss: 0.00045 | Regression loss: 0.02363 | Running loss: 0.02355\n",
            "Epoch: 163 | Iteration: 73 | Classification loss: 0.00074 | Regression loss: 0.03651 | Running loss: 0.02358\n",
            "Epoch: 163 | Iteration: 74 | Classification loss: 0.00040 | Regression loss: 0.04490 | Running loss: 0.02364\n",
            "Epoch: 163 | Iteration: 75 | Classification loss: 0.00011 | Regression loss: 0.02637 | Running loss: 0.02363\n",
            "Epoch: 163 | Iteration: 76 | Classification loss: 0.00032 | Regression loss: 0.03478 | Running loss: 0.02366\n",
            "Epoch: 163 | Iteration: 77 | Classification loss: 0.00008 | Regression loss: 0.00894 | Running loss: 0.02363\n",
            "Epoch: 163 | Iteration: 78 | Classification loss: 0.00042 | Regression loss: 0.03733 | Running loss: 0.02363\n",
            "Epoch: 163 | Iteration: 79 | Classification loss: 0.00004 | Regression loss: 0.01715 | Running loss: 0.02362\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7261742190616329\n",
            "Precision:  0.5719844357976653\n",
            "Recall:  0.7945945945945946\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}, {0: (0.7229906021615096, 185.0)}, {0: (0.7259378939307871, 185.0)}, {0: (0.7306172160652471, 185.0)}, {0: (0.7210590702449308, 185.0)}, {0: (0.7214498931770392, 185.0)}, {0: (0.7272071618498421, 185.0)}, {0: (0.725966639522496, 185.0)}, {0: (0.7208028358685962, 185.0)}, {0: (0.7255636845600553, 185.0)}, {0: (0.7261742190616329, 185.0)}]\n",
            "Epoch: 164 | Iteration: 0 | Classification loss: 0.00037 | Regression loss: 0.03664 | Running loss: 0.02364\n",
            "Epoch: 164 | Iteration: 1 | Classification loss: 0.00021 | Regression loss: 0.02809 | Running loss: 0.02364\n",
            "Epoch: 164 | Iteration: 2 | Classification loss: 0.00021 | Regression loss: 0.01441 | Running loss: 0.02363\n",
            "Epoch: 164 | Iteration: 3 | Classification loss: 0.00005 | Regression loss: 0.01540 | Running loss: 0.02360\n",
            "Epoch: 164 | Iteration: 4 | Classification loss: 0.00006 | Regression loss: 0.00874 | Running loss: 0.02354\n",
            "Epoch: 164 | Iteration: 5 | Classification loss: 0.00037 | Regression loss: 0.02646 | Running loss: 0.02352\n",
            "Epoch: 164 | Iteration: 6 | Classification loss: 0.00013 | Regression loss: 0.01084 | Running loss: 0.02346\n",
            "Epoch: 164 | Iteration: 7 | Classification loss: 0.00011 | Regression loss: 0.02055 | Running loss: 0.02343\n",
            "Epoch: 164 | Iteration: 8 | Classification loss: 0.00019 | Regression loss: 0.02070 | Running loss: 0.02344\n",
            "Epoch: 164 | Iteration: 9 | Classification loss: 0.00012 | Regression loss: 0.01052 | Running loss: 0.02343\n",
            "Epoch: 164 | Iteration: 10 | Classification loss: 0.00012 | Regression loss: 0.01719 | Running loss: 0.02344\n",
            "Epoch: 164 | Iteration: 11 | Classification loss: 0.00017 | Regression loss: 0.01667 | Running loss: 0.02344\n",
            "Epoch: 164 | Iteration: 12 | Classification loss: 0.00026 | Regression loss: 0.02448 | Running loss: 0.02346\n",
            "Epoch: 164 | Iteration: 13 | Classification loss: 0.00010 | Regression loss: 0.01492 | Running loss: 0.02341\n",
            "Epoch: 164 | Iteration: 14 | Classification loss: 0.00016 | Regression loss: 0.01344 | Running loss: 0.02336\n",
            "Epoch: 164 | Iteration: 15 | Classification loss: 0.00034 | Regression loss: 0.03165 | Running loss: 0.02335\n",
            "Epoch: 164 | Iteration: 16 | Classification loss: 0.00020 | Regression loss: 0.01374 | Running loss: 0.02329\n",
            "Epoch: 164 | Iteration: 17 | Classification loss: 0.00053 | Regression loss: 0.02132 | Running loss: 0.02331\n",
            "Epoch: 164 | Iteration: 18 | Classification loss: 0.00019 | Regression loss: 0.02070 | Running loss: 0.02331\n",
            "Epoch: 164 | Iteration: 19 | Classification loss: 0.00016 | Regression loss: 0.01954 | Running loss: 0.02331\n",
            "Epoch: 164 | Iteration: 20 | Classification loss: 0.00025 | Regression loss: 0.01949 | Running loss: 0.02326\n",
            "Epoch: 164 | Iteration: 21 | Classification loss: 0.00008 | Regression loss: 0.02013 | Running loss: 0.02323\n",
            "Epoch: 164 | Iteration: 22 | Classification loss: 0.00022 | Regression loss: 0.02057 | Running loss: 0.02325\n",
            "Epoch: 164 | Iteration: 23 | Classification loss: 0.00017 | Regression loss: 0.01758 | Running loss: 0.02323\n",
            "Epoch: 164 | Iteration: 24 | Classification loss: 0.00027 | Regression loss: 0.03702 | Running loss: 0.02325\n",
            "Epoch: 164 | Iteration: 25 | Classification loss: 0.00006 | Regression loss: 0.01996 | Running loss: 0.02321\n",
            "Epoch: 164 | Iteration: 26 | Classification loss: 0.00027 | Regression loss: 0.04080 | Running loss: 0.02325\n",
            "Epoch: 164 | Iteration: 27 | Classification loss: 0.00008 | Regression loss: 0.00968 | Running loss: 0.02322\n",
            "Epoch: 164 | Iteration: 28 | Classification loss: 0.00005 | Regression loss: 0.00694 | Running loss: 0.02318\n",
            "Epoch: 164 | Iteration: 29 | Classification loss: 0.00010 | Regression loss: 0.01378 | Running loss: 0.02318\n",
            "Epoch: 164 | Iteration: 30 | Classification loss: 0.00047 | Regression loss: 0.03845 | Running loss: 0.02322\n",
            "Epoch: 164 | Iteration: 31 | Classification loss: 0.00003 | Regression loss: 0.00448 | Running loss: 0.02320\n",
            "Epoch: 164 | Iteration: 32 | Classification loss: 0.00006 | Regression loss: 0.00872 | Running loss: 0.02317\n",
            "Epoch: 164 | Iteration: 33 | Classification loss: 0.00052 | Regression loss: 0.02653 | Running loss: 0.02321\n",
            "Epoch: 164 | Iteration: 34 | Classification loss: 0.00012 | Regression loss: 0.02791 | Running loss: 0.02320\n",
            "Epoch: 164 | Iteration: 35 | Classification loss: 0.00029 | Regression loss: 0.02850 | Running loss: 0.02323\n",
            "Epoch: 164 | Iteration: 36 | Classification loss: 0.00007 | Regression loss: 0.02181 | Running loss: 0.02319\n",
            "Epoch: 164 | Iteration: 37 | Classification loss: 0.00028 | Regression loss: 0.02026 | Running loss: 0.02318\n",
            "Epoch: 164 | Iteration: 38 | Classification loss: 0.00023 | Regression loss: 0.02391 | Running loss: 0.02318\n",
            "Epoch: 164 | Iteration: 39 | Classification loss: 0.00016 | Regression loss: 0.01338 | Running loss: 0.02317\n",
            "Epoch: 164 | Iteration: 40 | Classification loss: 0.00031 | Regression loss: 0.03115 | Running loss: 0.02322\n",
            "Epoch: 164 | Iteration: 41 | Classification loss: 0.00064 | Regression loss: 0.03494 | Running loss: 0.02324\n",
            "Epoch: 164 | Iteration: 42 | Classification loss: 0.00006 | Regression loss: 0.02756 | Running loss: 0.02322\n",
            "Epoch: 164 | Iteration: 43 | Classification loss: 0.00042 | Regression loss: 0.05072 | Running loss: 0.02324\n",
            "Epoch: 164 | Iteration: 44 | Classification loss: 0.00049 | Regression loss: 0.03376 | Running loss: 0.02328\n",
            "Epoch: 164 | Iteration: 45 | Classification loss: 0.00023 | Regression loss: 0.03645 | Running loss: 0.02333\n",
            "Epoch: 164 | Iteration: 46 | Classification loss: 0.00013 | Regression loss: 0.01596 | Running loss: 0.02334\n",
            "Epoch: 164 | Iteration: 47 | Classification loss: 0.00016 | Regression loss: 0.01654 | Running loss: 0.02333\n",
            "Epoch: 164 | Iteration: 48 | Classification loss: 0.00035 | Regression loss: 0.04205 | Running loss: 0.02338\n",
            "Epoch: 164 | Iteration: 49 | Classification loss: 0.00059 | Regression loss: 0.02010 | Running loss: 0.02331\n",
            "Epoch: 164 | Iteration: 50 | Classification loss: 0.00046 | Regression loss: 0.04413 | Running loss: 0.02336\n",
            "Epoch: 164 | Iteration: 51 | Classification loss: 0.00022 | Regression loss: 0.01738 | Running loss: 0.02338\n",
            "Epoch: 164 | Iteration: 52 | Classification loss: 0.00066 | Regression loss: 0.03475 | Running loss: 0.02340\n",
            "Epoch: 164 | Iteration: 53 | Classification loss: 0.00016 | Regression loss: 0.01886 | Running loss: 0.02341\n",
            "Epoch: 164 | Iteration: 54 | Classification loss: 0.00003 | Regression loss: 0.01026 | Running loss: 0.02340\n",
            "Epoch: 164 | Iteration: 55 | Classification loss: 0.00011 | Regression loss: 0.01427 | Running loss: 0.02338\n",
            "Epoch: 164 | Iteration: 56 | Classification loss: 0.00015 | Regression loss: 0.03059 | Running loss: 0.02343\n",
            "Epoch: 164 | Iteration: 57 | Classification loss: 0.00011 | Regression loss: 0.01512 | Running loss: 0.02342\n",
            "Epoch: 164 | Iteration: 58 | Classification loss: 0.00061 | Regression loss: 0.03937 | Running loss: 0.02347\n",
            "Epoch: 164 | Iteration: 59 | Classification loss: 0.00016 | Regression loss: 0.02595 | Running loss: 0.02349\n",
            "Epoch: 164 | Iteration: 60 | Classification loss: 0.00012 | Regression loss: 0.02320 | Running loss: 0.02345\n",
            "Epoch: 164 | Iteration: 61 | Classification loss: 0.00027 | Regression loss: 0.02995 | Running loss: 0.02349\n",
            "Epoch: 164 | Iteration: 62 | Classification loss: 0.00020 | Regression loss: 0.03264 | Running loss: 0.02352\n",
            "Epoch: 164 | Iteration: 63 | Classification loss: 0.00058 | Regression loss: 0.03522 | Running loss: 0.02354\n",
            "Epoch: 164 | Iteration: 64 | Classification loss: 0.00022 | Regression loss: 0.01931 | Running loss: 0.02352\n",
            "Epoch: 164 | Iteration: 65 | Classification loss: 0.00007 | Regression loss: 0.01529 | Running loss: 0.02348\n",
            "Epoch: 164 | Iteration: 66 | Classification loss: 0.00016 | Regression loss: 0.01412 | Running loss: 0.02347\n",
            "Epoch: 164 | Iteration: 67 | Classification loss: 0.00008 | Regression loss: 0.01261 | Running loss: 0.02347\n",
            "Epoch: 164 | Iteration: 68 | Classification loss: 0.00016 | Regression loss: 0.01762 | Running loss: 0.02344\n",
            "Epoch: 164 | Iteration: 69 | Classification loss: 0.00047 | Regression loss: 0.02395 | Running loss: 0.02345\n",
            "Epoch: 164 | Iteration: 70 | Classification loss: 0.00045 | Regression loss: 0.04342 | Running loss: 0.02349\n",
            "Epoch: 164 | Iteration: 71 | Classification loss: 0.00012 | Regression loss: 0.01828 | Running loss: 0.02349\n",
            "Epoch: 164 | Iteration: 72 | Classification loss: 0.00008 | Regression loss: 0.01786 | Running loss: 0.02348\n",
            "Epoch: 164 | Iteration: 73 | Classification loss: 0.00017 | Regression loss: 0.02632 | Running loss: 0.02345\n",
            "Epoch: 164 | Iteration: 74 | Classification loss: 0.00009 | Regression loss: 0.01578 | Running loss: 0.02343\n",
            "Epoch: 164 | Iteration: 75 | Classification loss: 0.00015 | Regression loss: 0.02942 | Running loss: 0.02339\n",
            "Epoch: 164 | Iteration: 76 | Classification loss: 0.00012 | Regression loss: 0.02441 | Running loss: 0.02339\n",
            "Epoch: 164 | Iteration: 77 | Classification loss: 0.00009 | Regression loss: 0.02202 | Running loss: 0.02338\n",
            "Epoch: 164 | Iteration: 78 | Classification loss: 0.00016 | Regression loss: 0.01891 | Running loss: 0.02338\n",
            "Epoch: 164 | Iteration: 79 | Classification loss: 0.00006 | Regression loss: 0.02078 | Running loss: 0.02340\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7217332811149212\n",
            "Precision:  0.5703125\n",
            "Recall:  0.7891891891891892\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}, {0: (0.7229906021615096, 185.0)}, {0: (0.7259378939307871, 185.0)}, {0: (0.7306172160652471, 185.0)}, {0: (0.7210590702449308, 185.0)}, {0: (0.7214498931770392, 185.0)}, {0: (0.7272071618498421, 185.0)}, {0: (0.725966639522496, 185.0)}, {0: (0.7208028358685962, 185.0)}, {0: (0.7255636845600553, 185.0)}, {0: (0.7261742190616329, 185.0)}, {0: (0.7217332811149212, 185.0)}]\n",
            "Epoch: 165 | Iteration: 0 | Classification loss: 0.00012 | Regression loss: 0.02322 | Running loss: 0.02341\n",
            "Epoch: 165 | Iteration: 1 | Classification loss: 0.00012 | Regression loss: 0.01259 | Running loss: 0.02338\n",
            "Epoch: 165 | Iteration: 2 | Classification loss: 0.00042 | Regression loss: 0.02291 | Running loss: 0.02340\n",
            "Epoch: 165 | Iteration: 3 | Classification loss: 0.00012 | Regression loss: 0.01763 | Running loss: 0.02340\n",
            "Epoch: 165 | Iteration: 4 | Classification loss: 0.00013 | Regression loss: 0.02970 | Running loss: 0.02341\n",
            "Epoch: 165 | Iteration: 5 | Classification loss: 0.00041 | Regression loss: 0.02968 | Running loss: 0.02342\n",
            "Epoch: 165 | Iteration: 6 | Classification loss: 0.00060 | Regression loss: 0.03546 | Running loss: 0.02346\n",
            "Epoch: 165 | Iteration: 7 | Classification loss: 0.00008 | Regression loss: 0.00951 | Running loss: 0.02344\n",
            "Epoch: 165 | Iteration: 8 | Classification loss: 0.00018 | Regression loss: 0.01840 | Running loss: 0.02342\n",
            "Epoch: 165 | Iteration: 9 | Classification loss: 0.00008 | Regression loss: 0.02618 | Running loss: 0.02339\n",
            "Epoch: 165 | Iteration: 10 | Classification loss: 0.00056 | Regression loss: 0.04035 | Running loss: 0.02342\n",
            "Epoch: 165 | Iteration: 11 | Classification loss: 0.00051 | Regression loss: 0.02454 | Running loss: 0.02340\n",
            "Epoch: 165 | Iteration: 12 | Classification loss: 0.00008 | Regression loss: 0.01679 | Running loss: 0.02338\n",
            "Epoch: 165 | Iteration: 13 | Classification loss: 0.00006 | Regression loss: 0.02013 | Running loss: 0.02336\n",
            "Epoch: 165 | Iteration: 14 | Classification loss: 0.00061 | Regression loss: 0.05017 | Running loss: 0.02342\n",
            "Epoch: 165 | Iteration: 15 | Classification loss: 0.00020 | Regression loss: 0.03721 | Running loss: 0.02346\n",
            "Epoch: 165 | Iteration: 16 | Classification loss: 0.00017 | Regression loss: 0.01596 | Running loss: 0.02345\n",
            "Epoch: 165 | Iteration: 17 | Classification loss: 0.00010 | Regression loss: 0.01650 | Running loss: 0.02344\n",
            "Epoch: 165 | Iteration: 18 | Classification loss: 0.00022 | Regression loss: 0.03593 | Running loss: 0.02344\n",
            "Epoch: 165 | Iteration: 19 | Classification loss: 0.00017 | Regression loss: 0.01744 | Running loss: 0.02341\n",
            "Epoch: 165 | Iteration: 20 | Classification loss: 0.00019 | Regression loss: 0.02353 | Running loss: 0.02338\n",
            "Epoch: 165 | Iteration: 21 | Classification loss: 0.00009 | Regression loss: 0.01456 | Running loss: 0.02337\n",
            "Epoch: 165 | Iteration: 22 | Classification loss: 0.00004 | Regression loss: 0.01596 | Running loss: 0.02336\n",
            "Epoch: 165 | Iteration: 23 | Classification loss: 0.00003 | Regression loss: 0.00429 | Running loss: 0.02333\n",
            "Epoch: 165 | Iteration: 24 | Classification loss: 0.00024 | Regression loss: 0.01763 | Running loss: 0.02333\n",
            "Epoch: 165 | Iteration: 25 | Classification loss: 0.00021 | Regression loss: 0.02473 | Running loss: 0.02331\n",
            "Epoch: 165 | Iteration: 26 | Classification loss: 0.00027 | Regression loss: 0.03318 | Running loss: 0.02335\n",
            "Epoch: 165 | Iteration: 27 | Classification loss: 0.00022 | Regression loss: 0.02213 | Running loss: 0.02338\n",
            "Epoch: 165 | Iteration: 28 | Classification loss: 0.00017 | Regression loss: 0.02478 | Running loss: 0.02339\n",
            "Epoch: 165 | Iteration: 29 | Classification loss: 0.00041 | Regression loss: 0.04320 | Running loss: 0.02344\n",
            "Epoch: 165 | Iteration: 30 | Classification loss: 0.00030 | Regression loss: 0.03889 | Running loss: 0.02346\n",
            "Epoch: 165 | Iteration: 31 | Classification loss: 0.00009 | Regression loss: 0.01588 | Running loss: 0.02345\n",
            "Epoch: 165 | Iteration: 32 | Classification loss: 0.00017 | Regression loss: 0.02610 | Running loss: 0.02347\n",
            "Epoch: 165 | Iteration: 33 | Classification loss: 0.00016 | Regression loss: 0.01963 | Running loss: 0.02344\n",
            "Epoch: 165 | Iteration: 34 | Classification loss: 0.00029 | Regression loss: 0.02523 | Running loss: 0.02344\n",
            "Epoch: 165 | Iteration: 35 | Classification loss: 0.00013 | Regression loss: 0.01573 | Running loss: 0.02342\n",
            "Epoch: 165 | Iteration: 36 | Classification loss: 0.00013 | Regression loss: 0.02124 | Running loss: 0.02343\n",
            "Epoch: 165 | Iteration: 37 | Classification loss: 0.00017 | Regression loss: 0.03114 | Running loss: 0.02345\n",
            "Epoch: 165 | Iteration: 38 | Classification loss: 0.00009 | Regression loss: 0.02983 | Running loss: 0.02349\n",
            "Epoch: 165 | Iteration: 39 | Classification loss: 0.00008 | Regression loss: 0.01536 | Running loss: 0.02349\n",
            "Epoch: 165 | Iteration: 40 | Classification loss: 0.00013 | Regression loss: 0.01745 | Running loss: 0.02351\n",
            "Epoch: 165 | Iteration: 41 | Classification loss: 0.00016 | Regression loss: 0.02074 | Running loss: 0.02349\n",
            "Epoch: 165 | Iteration: 42 | Classification loss: 0.00015 | Regression loss: 0.01800 | Running loss: 0.02344\n",
            "Epoch: 165 | Iteration: 43 | Classification loss: 0.00047 | Regression loss: 0.02273 | Running loss: 0.02344\n",
            "Epoch: 165 | Iteration: 44 | Classification loss: 0.00033 | Regression loss: 0.03087 | Running loss: 0.02345\n",
            "Epoch: 165 | Iteration: 45 | Classification loss: 0.00021 | Regression loss: 0.02436 | Running loss: 0.02346\n",
            "Epoch: 165 | Iteration: 46 | Classification loss: 0.00016 | Regression loss: 0.01339 | Running loss: 0.02341\n",
            "Epoch: 165 | Iteration: 47 | Classification loss: 0.00006 | Regression loss: 0.00590 | Running loss: 0.02340\n",
            "Epoch: 165 | Iteration: 48 | Classification loss: 0.00016 | Regression loss: 0.02020 | Running loss: 0.02339\n",
            "Epoch: 165 | Iteration: 49 | Classification loss: 0.00053 | Regression loss: 0.03585 | Running loss: 0.02338\n",
            "Epoch: 165 | Iteration: 50 | Classification loss: 0.00084 | Regression loss: 0.03761 | Running loss: 0.02341\n",
            "Epoch: 165 | Iteration: 51 | Classification loss: 0.00034 | Regression loss: 0.03018 | Running loss: 0.02344\n",
            "Epoch: 165 | Iteration: 52 | Classification loss: 0.00051 | Regression loss: 0.04024 | Running loss: 0.02347\n",
            "Epoch: 165 | Iteration: 53 | Classification loss: 0.00009 | Regression loss: 0.00832 | Running loss: 0.02346\n",
            "Epoch: 165 | Iteration: 54 | Classification loss: 0.00016 | Regression loss: 0.01278 | Running loss: 0.02344\n",
            "Epoch: 165 | Iteration: 55 | Classification loss: 0.00006 | Regression loss: 0.00742 | Running loss: 0.02342\n",
            "Epoch: 165 | Iteration: 56 | Classification loss: 0.00018 | Regression loss: 0.01504 | Running loss: 0.02344\n",
            "Epoch: 165 | Iteration: 57 | Classification loss: 0.00080 | Regression loss: 0.02279 | Running loss: 0.02346\n",
            "Epoch: 165 | Iteration: 58 | Classification loss: 0.00008 | Regression loss: 0.03015 | Running loss: 0.02344\n",
            "Epoch: 165 | Iteration: 59 | Classification loss: 0.00012 | Regression loss: 0.01556 | Running loss: 0.02345\n",
            "Epoch: 165 | Iteration: 60 | Classification loss: 0.00010 | Regression loss: 0.03018 | Running loss: 0.02348\n",
            "Epoch: 165 | Iteration: 61 | Classification loss: 0.00014 | Regression loss: 0.01317 | Running loss: 0.02346\n",
            "Epoch: 165 | Iteration: 62 | Classification loss: 0.00006 | Regression loss: 0.01392 | Running loss: 0.02344\n",
            "Epoch: 165 | Iteration: 63 | Classification loss: 0.00029 | Regression loss: 0.03369 | Running loss: 0.02344\n",
            "Epoch: 165 | Iteration: 64 | Classification loss: 0.00018 | Regression loss: 0.02548 | Running loss: 0.02341\n",
            "Epoch: 165 | Iteration: 65 | Classification loss: 0.00034 | Regression loss: 0.02758 | Running loss: 0.02345\n",
            "Epoch: 165 | Iteration: 66 | Classification loss: 0.00097 | Regression loss: 0.04296 | Running loss: 0.02348\n",
            "Epoch: 165 | Iteration: 67 | Classification loss: 0.00015 | Regression loss: 0.01651 | Running loss: 0.02348\n",
            "Epoch: 165 | Iteration: 68 | Classification loss: 0.00010 | Regression loss: 0.02348 | Running loss: 0.02349\n",
            "Epoch: 165 | Iteration: 69 | Classification loss: 0.00019 | Regression loss: 0.01430 | Running loss: 0.02347\n",
            "Epoch: 165 | Iteration: 70 | Classification loss: 0.00003 | Regression loss: 0.01012 | Running loss: 0.02342\n",
            "Epoch: 165 | Iteration: 71 | Classification loss: 0.00016 | Regression loss: 0.01848 | Running loss: 0.02341\n",
            "Epoch: 165 | Iteration: 72 | Classification loss: 0.00012 | Regression loss: 0.01628 | Running loss: 0.02340\n",
            "Epoch: 165 | Iteration: 73 | Classification loss: 0.00021 | Regression loss: 0.01215 | Running loss: 0.02340\n",
            "Epoch: 165 | Iteration: 74 | Classification loss: 0.00006 | Regression loss: 0.00878 | Running loss: 0.02338\n",
            "Epoch: 165 | Iteration: 75 | Classification loss: 0.00044 | Regression loss: 0.03232 | Running loss: 0.02338\n",
            "Epoch: 165 | Iteration: 76 | Classification loss: 0.00023 | Regression loss: 0.01946 | Running loss: 0.02339\n",
            "Epoch: 165 | Iteration: 77 | Classification loss: 0.00027 | Regression loss: 0.01793 | Running loss: 0.02335\n",
            "Epoch: 165 | Iteration: 78 | Classification loss: 0.00007 | Regression loss: 0.02248 | Running loss: 0.02337\n",
            "Epoch: 165 | Iteration: 79 | Classification loss: 0.00011 | Regression loss: 0.02157 | Running loss: 0.02337\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7261653370413381\n",
            "Precision:  0.57421875\n",
            "Recall:  0.7945945945945946\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}, {0: (0.7229906021615096, 185.0)}, {0: (0.7259378939307871, 185.0)}, {0: (0.7306172160652471, 185.0)}, {0: (0.7210590702449308, 185.0)}, {0: (0.7214498931770392, 185.0)}, {0: (0.7272071618498421, 185.0)}, {0: (0.725966639522496, 185.0)}, {0: (0.7208028358685962, 185.0)}, {0: (0.7255636845600553, 185.0)}, {0: (0.7261742190616329, 185.0)}, {0: (0.7217332811149212, 185.0)}, {0: (0.7261653370413381, 185.0)}]\n",
            "Epoch: 166 | Iteration: 0 | Classification loss: 0.00030 | Regression loss: 0.04026 | Running loss: 0.02342\n",
            "Epoch: 166 | Iteration: 1 | Classification loss: 0.00042 | Regression loss: 0.03124 | Running loss: 0.02344\n",
            "Epoch: 166 | Iteration: 2 | Classification loss: 0.00010 | Regression loss: 0.01787 | Running loss: 0.02343\n",
            "Epoch: 166 | Iteration: 3 | Classification loss: 0.00014 | Regression loss: 0.01599 | Running loss: 0.02342\n",
            "Epoch: 166 | Iteration: 4 | Classification loss: 0.00016 | Regression loss: 0.01688 | Running loss: 0.02338\n",
            "Epoch: 166 | Iteration: 5 | Classification loss: 0.00004 | Regression loss: 0.01576 | Running loss: 0.02339\n",
            "Epoch: 166 | Iteration: 6 | Classification loss: 0.00006 | Regression loss: 0.01996 | Running loss: 0.02332\n",
            "Epoch: 166 | Iteration: 7 | Classification loss: 0.00023 | Regression loss: 0.02285 | Running loss: 0.02329\n",
            "Epoch: 166 | Iteration: 8 | Classification loss: 0.00055 | Regression loss: 0.04710 | Running loss: 0.02332\n",
            "Epoch: 166 | Iteration: 9 | Classification loss: 0.00041 | Regression loss: 0.04824 | Running loss: 0.02336\n",
            "Epoch: 166 | Iteration: 10 | Classification loss: 0.00008 | Regression loss: 0.01505 | Running loss: 0.02336\n",
            "Epoch: 166 | Iteration: 11 | Classification loss: 0.00015 | Regression loss: 0.01866 | Running loss: 0.02336\n",
            "Epoch: 166 | Iteration: 12 | Classification loss: 0.00022 | Regression loss: 0.01843 | Running loss: 0.02332\n",
            "Epoch: 166 | Iteration: 13 | Classification loss: 0.00009 | Regression loss: 0.01603 | Running loss: 0.02328\n",
            "Epoch: 166 | Iteration: 14 | Classification loss: 0.00003 | Regression loss: 0.00980 | Running loss: 0.02325\n",
            "Epoch: 166 | Iteration: 15 | Classification loss: 0.00031 | Regression loss: 0.02266 | Running loss: 0.02326\n",
            "Epoch: 166 | Iteration: 16 | Classification loss: 0.00030 | Regression loss: 0.02658 | Running loss: 0.02330\n",
            "Epoch: 166 | Iteration: 17 | Classification loss: 0.00004 | Regression loss: 0.00789 | Running loss: 0.02324\n",
            "Epoch: 166 | Iteration: 18 | Classification loss: 0.00005 | Regression loss: 0.00834 | Running loss: 0.02320\n",
            "Epoch: 166 | Iteration: 19 | Classification loss: 0.00012 | Regression loss: 0.01324 | Running loss: 0.02319\n",
            "Epoch: 166 | Iteration: 20 | Classification loss: 0.00017 | Regression loss: 0.03069 | Running loss: 0.02320\n",
            "Epoch: 166 | Iteration: 21 | Classification loss: 0.00015 | Regression loss: 0.01824 | Running loss: 0.02320\n",
            "Epoch: 166 | Iteration: 22 | Classification loss: 0.00009 | Regression loss: 0.01459 | Running loss: 0.02319\n",
            "Epoch: 166 | Iteration: 23 | Classification loss: 0.00015 | Regression loss: 0.01746 | Running loss: 0.02318\n",
            "Epoch: 166 | Iteration: 24 | Classification loss: 0.00012 | Regression loss: 0.01590 | Running loss: 0.02312\n",
            "Epoch: 166 | Iteration: 25 | Classification loss: 0.00013 | Regression loss: 0.01059 | Running loss: 0.02307\n",
            "Epoch: 166 | Iteration: 26 | Classification loss: 0.00017 | Regression loss: 0.02538 | Running loss: 0.02310\n",
            "Epoch: 166 | Iteration: 27 | Classification loss: 0.00032 | Regression loss: 0.03102 | Running loss: 0.02311\n",
            "Epoch: 166 | Iteration: 28 | Classification loss: 0.00007 | Regression loss: 0.02951 | Running loss: 0.02311\n",
            "Epoch: 166 | Iteration: 29 | Classification loss: 0.00021 | Regression loss: 0.03932 | Running loss: 0.02312\n",
            "Epoch: 166 | Iteration: 30 | Classification loss: 0.00020 | Regression loss: 0.01905 | Running loss: 0.02308\n",
            "Epoch: 166 | Iteration: 31 | Classification loss: 0.00030 | Regression loss: 0.02967 | Running loss: 0.02311\n",
            "Epoch: 166 | Iteration: 32 | Classification loss: 0.00074 | Regression loss: 0.02310 | Running loss: 0.02307\n",
            "Epoch: 166 | Iteration: 33 | Classification loss: 0.00046 | Regression loss: 0.03606 | Running loss: 0.02311\n",
            "Epoch: 166 | Iteration: 34 | Classification loss: 0.00018 | Regression loss: 0.02978 | Running loss: 0.02314\n",
            "Epoch: 166 | Iteration: 35 | Classification loss: 0.00058 | Regression loss: 0.03970 | Running loss: 0.02316\n",
            "Epoch: 166 | Iteration: 36 | Classification loss: 0.00047 | Regression loss: 0.04302 | Running loss: 0.02322\n",
            "Epoch: 166 | Iteration: 37 | Classification loss: 0.00009 | Regression loss: 0.02387 | Running loss: 0.02325\n",
            "Epoch: 166 | Iteration: 38 | Classification loss: 0.00026 | Regression loss: 0.02403 | Running loss: 0.02327\n",
            "Epoch: 166 | Iteration: 39 | Classification loss: 0.00016 | Regression loss: 0.01350 | Running loss: 0.02328\n",
            "Epoch: 166 | Iteration: 40 | Classification loss: 0.00011 | Regression loss: 0.01048 | Running loss: 0.02327\n",
            "Epoch: 166 | Iteration: 41 | Classification loss: 0.00013 | Regression loss: 0.02830 | Running loss: 0.02329\n",
            "Epoch: 166 | Iteration: 42 | Classification loss: 0.00013 | Regression loss: 0.02508 | Running loss: 0.02332\n",
            "Epoch: 166 | Iteration: 43 | Classification loss: 0.00017 | Regression loss: 0.01592 | Running loss: 0.02330\n",
            "Epoch: 166 | Iteration: 44 | Classification loss: 0.00013 | Regression loss: 0.02122 | Running loss: 0.02328\n",
            "Epoch: 166 | Iteration: 45 | Classification loss: 0.00018 | Regression loss: 0.01168 | Running loss: 0.02326\n",
            "Epoch: 166 | Iteration: 46 | Classification loss: 0.00024 | Regression loss: 0.01924 | Running loss: 0.02328\n",
            "Epoch: 166 | Iteration: 47 | Classification loss: 0.00007 | Regression loss: 0.00826 | Running loss: 0.02324\n",
            "Epoch: 166 | Iteration: 48 | Classification loss: 0.00039 | Regression loss: 0.02264 | Running loss: 0.02325\n",
            "Epoch: 166 | Iteration: 49 | Classification loss: 0.00027 | Regression loss: 0.03728 | Running loss: 0.02331\n",
            "Epoch: 166 | Iteration: 50 | Classification loss: 0.00016 | Regression loss: 0.02733 | Running loss: 0.02328\n",
            "Epoch: 166 | Iteration: 51 | Classification loss: 0.00022 | Regression loss: 0.02605 | Running loss: 0.02330\n",
            "Epoch: 166 | Iteration: 52 | Classification loss: 0.00042 | Regression loss: 0.02954 | Running loss: 0.02330\n",
            "Epoch: 166 | Iteration: 53 | Classification loss: 0.00016 | Regression loss: 0.01297 | Running loss: 0.02327\n",
            "Epoch: 166 | Iteration: 54 | Classification loss: 0.00014 | Regression loss: 0.01276 | Running loss: 0.02327\n",
            "Epoch: 166 | Iteration: 55 | Classification loss: 0.00010 | Regression loss: 0.03004 | Running loss: 0.02326\n",
            "Epoch: 166 | Iteration: 56 | Classification loss: 0.00031 | Regression loss: 0.02786 | Running loss: 0.02329\n",
            "Epoch: 166 | Iteration: 57 | Classification loss: 0.00004 | Regression loss: 0.01686 | Running loss: 0.02327\n",
            "Epoch: 166 | Iteration: 58 | Classification loss: 0.00074 | Regression loss: 0.03639 | Running loss: 0.02330\n",
            "Epoch: 166 | Iteration: 59 | Classification loss: 0.00045 | Regression loss: 0.03790 | Running loss: 0.02331\n",
            "Epoch: 166 | Iteration: 60 | Classification loss: 0.00006 | Regression loss: 0.01511 | Running loss: 0.02323\n",
            "Epoch: 166 | Iteration: 61 | Classification loss: 0.00053 | Regression loss: 0.02089 | Running loss: 0.02320\n",
            "Epoch: 166 | Iteration: 62 | Classification loss: 0.00010 | Regression loss: 0.01766 | Running loss: 0.02321\n",
            "Epoch: 166 | Iteration: 63 | Classification loss: 0.00008 | Regression loss: 0.02142 | Running loss: 0.02321\n",
            "Epoch: 166 | Iteration: 64 | Classification loss: 0.00003 | Regression loss: 0.00457 | Running loss: 0.02316\n",
            "Epoch: 166 | Iteration: 65 | Classification loss: 0.00025 | Regression loss: 0.01486 | Running loss: 0.02316\n",
            "Epoch: 166 | Iteration: 66 | Classification loss: 0.00057 | Regression loss: 0.03215 | Running loss: 0.02317\n",
            "Epoch: 166 | Iteration: 67 | Classification loss: 0.00024 | Regression loss: 0.03377 | Running loss: 0.02319\n",
            "Epoch: 166 | Iteration: 68 | Classification loss: 0.00015 | Regression loss: 0.01737 | Running loss: 0.02315\n",
            "Epoch: 166 | Iteration: 69 | Classification loss: 0.00005 | Regression loss: 0.02219 | Running loss: 0.02314\n",
            "Epoch: 166 | Iteration: 70 | Classification loss: 0.00012 | Regression loss: 0.01640 | Running loss: 0.02314\n",
            "Epoch: 166 | Iteration: 71 | Classification loss: 0.00018 | Regression loss: 0.01613 | Running loss: 0.02314\n",
            "Epoch: 166 | Iteration: 72 | Classification loss: 0.00024 | Regression loss: 0.02271 | Running loss: 0.02313\n",
            "Epoch: 166 | Iteration: 73 | Classification loss: 0.00013 | Regression loss: 0.01874 | Running loss: 0.02309\n",
            "Epoch: 166 | Iteration: 74 | Classification loss: 0.00012 | Regression loss: 0.01791 | Running loss: 0.02309\n",
            "Epoch: 166 | Iteration: 75 | Classification loss: 0.00048 | Regression loss: 0.02499 | Running loss: 0.02309\n",
            "Epoch: 166 | Iteration: 76 | Classification loss: 0.00008 | Regression loss: 0.01602 | Running loss: 0.02309\n",
            "Epoch: 166 | Iteration: 77 | Classification loss: 0.00018 | Regression loss: 0.02087 | Running loss: 0.02307\n",
            "Epoch: 166 | Iteration: 78 | Classification loss: 0.00007 | Regression loss: 0.02191 | Running loss: 0.02307\n",
            "Epoch: 166 | Iteration: 79 | Classification loss: 0.00019 | Regression loss: 0.02580 | Running loss: 0.02307\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7215292823555803\n",
            "Precision:  0.5770750988142292\n",
            "Recall:  0.7891891891891892\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}, {0: (0.7229906021615096, 185.0)}, {0: (0.7259378939307871, 185.0)}, {0: (0.7306172160652471, 185.0)}, {0: (0.7210590702449308, 185.0)}, {0: (0.7214498931770392, 185.0)}, {0: (0.7272071618498421, 185.0)}, {0: (0.725966639522496, 185.0)}, {0: (0.7208028358685962, 185.0)}, {0: (0.7255636845600553, 185.0)}, {0: (0.7261742190616329, 185.0)}, {0: (0.7217332811149212, 185.0)}, {0: (0.7261653370413381, 185.0)}, {0: (0.7215292823555803, 185.0)}]\n",
            "Epoch: 167 | Iteration: 0 | Classification loss: 0.00010 | Regression loss: 0.01465 | Running loss: 0.02306\n",
            "Epoch: 167 | Iteration: 1 | Classification loss: 0.00023 | Regression loss: 0.03848 | Running loss: 0.02310\n",
            "Epoch: 167 | Iteration: 2 | Classification loss: 0.00013 | Regression loss: 0.01090 | Running loss: 0.02308\n",
            "Epoch: 167 | Iteration: 3 | Classification loss: 0.00048 | Regression loss: 0.03261 | Running loss: 0.02312\n",
            "Epoch: 167 | Iteration: 4 | Classification loss: 0.00010 | Regression loss: 0.01672 | Running loss: 0.02308\n",
            "Epoch: 167 | Iteration: 5 | Classification loss: 0.00029 | Regression loss: 0.03011 | Running loss: 0.02310\n",
            "Epoch: 167 | Iteration: 6 | Classification loss: 0.00015 | Regression loss: 0.01971 | Running loss: 0.02310\n",
            "Epoch: 167 | Iteration: 7 | Classification loss: 0.00011 | Regression loss: 0.01265 | Running loss: 0.02310\n",
            "Epoch: 167 | Iteration: 8 | Classification loss: 0.00036 | Regression loss: 0.02692 | Running loss: 0.02308\n",
            "Epoch: 167 | Iteration: 9 | Classification loss: 0.00016 | Regression loss: 0.01572 | Running loss: 0.02308\n",
            "Epoch: 167 | Iteration: 10 | Classification loss: 0.00005 | Regression loss: 0.01303 | Running loss: 0.02308\n",
            "Epoch: 167 | Iteration: 11 | Classification loss: 0.00048 | Regression loss: 0.02109 | Running loss: 0.02307\n",
            "Epoch: 167 | Iteration: 12 | Classification loss: 0.00021 | Regression loss: 0.01234 | Running loss: 0.02305\n",
            "Epoch: 167 | Iteration: 13 | Classification loss: 0.00025 | Regression loss: 0.03213 | Running loss: 0.02306\n",
            "Epoch: 167 | Iteration: 14 | Classification loss: 0.00013 | Regression loss: 0.01556 | Running loss: 0.02305\n",
            "Epoch: 167 | Iteration: 15 | Classification loss: 0.00053 | Regression loss: 0.05019 | Running loss: 0.02311\n",
            "Epoch: 167 | Iteration: 16 | Classification loss: 0.00007 | Regression loss: 0.02219 | Running loss: 0.02309\n",
            "Epoch: 167 | Iteration: 17 | Classification loss: 0.00013 | Regression loss: 0.01071 | Running loss: 0.02310\n",
            "Epoch: 167 | Iteration: 18 | Classification loss: 0.00009 | Regression loss: 0.01780 | Running loss: 0.02305\n",
            "Epoch: 167 | Iteration: 19 | Classification loss: 0.00032 | Regression loss: 0.03588 | Running loss: 0.02308\n",
            "Epoch: 167 | Iteration: 20 | Classification loss: 0.00048 | Regression loss: 0.02267 | Running loss: 0.02306\n",
            "Epoch: 167 | Iteration: 21 | Classification loss: 0.00017 | Regression loss: 0.01828 | Running loss: 0.02308\n",
            "Epoch: 167 | Iteration: 22 | Classification loss: 0.00025 | Regression loss: 0.02067 | Running loss: 0.02311\n",
            "Epoch: 167 | Iteration: 23 | Classification loss: 0.00015 | Regression loss: 0.02074 | Running loss: 0.02311\n",
            "Epoch: 167 | Iteration: 24 | Classification loss: 0.00015 | Regression loss: 0.01264 | Running loss: 0.02310\n",
            "Epoch: 167 | Iteration: 25 | Classification loss: 0.00019 | Regression loss: 0.02317 | Running loss: 0.02312\n",
            "Epoch: 167 | Iteration: 26 | Classification loss: 0.00010 | Regression loss: 0.01342 | Running loss: 0.02308\n",
            "Epoch: 167 | Iteration: 27 | Classification loss: 0.00012 | Regression loss: 0.02281 | Running loss: 0.02303\n",
            "Epoch: 167 | Iteration: 28 | Classification loss: 0.00018 | Regression loss: 0.02048 | Running loss: 0.02298\n",
            "Epoch: 167 | Iteration: 29 | Classification loss: 0.00023 | Regression loss: 0.01798 | Running loss: 0.02301\n",
            "Epoch: 167 | Iteration: 30 | Classification loss: 0.00015 | Regression loss: 0.03057 | Running loss: 0.02304\n",
            "Epoch: 167 | Iteration: 31 | Classification loss: 0.00024 | Regression loss: 0.02551 | Running loss: 0.02303\n",
            "Epoch: 167 | Iteration: 32 | Classification loss: 0.00031 | Regression loss: 0.02411 | Running loss: 0.02305\n",
            "Epoch: 167 | Iteration: 33 | Classification loss: 0.00020 | Regression loss: 0.01401 | Running loss: 0.02302\n",
            "Epoch: 167 | Iteration: 34 | Classification loss: 0.00007 | Regression loss: 0.02198 | Running loss: 0.02302\n",
            "Epoch: 167 | Iteration: 35 | Classification loss: 0.00014 | Regression loss: 0.01791 | Running loss: 0.02300\n",
            "Epoch: 167 | Iteration: 36 | Classification loss: 0.00029 | Regression loss: 0.02326 | Running loss: 0.02299\n",
            "Epoch: 167 | Iteration: 37 | Classification loss: 0.00007 | Regression loss: 0.00565 | Running loss: 0.02298\n",
            "Epoch: 167 | Iteration: 38 | Classification loss: 0.00019 | Regression loss: 0.02477 | Running loss: 0.02298\n",
            "Epoch: 167 | Iteration: 39 | Classification loss: 0.00007 | Regression loss: 0.01539 | Running loss: 0.02297\n",
            "Epoch: 167 | Iteration: 40 | Classification loss: 0.00057 | Regression loss: 0.01967 | Running loss: 0.02297\n",
            "Epoch: 167 | Iteration: 41 | Classification loss: 0.00012 | Regression loss: 0.00998 | Running loss: 0.02292\n",
            "Epoch: 167 | Iteration: 42 | Classification loss: 0.00011 | Regression loss: 0.01548 | Running loss: 0.02290\n",
            "Epoch: 167 | Iteration: 43 | Classification loss: 0.00003 | Regression loss: 0.00413 | Running loss: 0.02288\n",
            "Epoch: 167 | Iteration: 44 | Classification loss: 0.00022 | Regression loss: 0.02604 | Running loss: 0.02289\n",
            "Epoch: 167 | Iteration: 45 | Classification loss: 0.00017 | Regression loss: 0.01805 | Running loss: 0.02291\n",
            "Epoch: 167 | Iteration: 46 | Classification loss: 0.00007 | Regression loss: 0.02213 | Running loss: 0.02290\n",
            "Epoch: 167 | Iteration: 47 | Classification loss: 0.00031 | Regression loss: 0.03159 | Running loss: 0.02292\n",
            "Epoch: 167 | Iteration: 48 | Classification loss: 0.00014 | Regression loss: 0.02172 | Running loss: 0.02289\n",
            "Epoch: 167 | Iteration: 49 | Classification loss: 0.00023 | Regression loss: 0.03641 | Running loss: 0.02292\n",
            "Epoch: 167 | Iteration: 50 | Classification loss: 0.00015 | Regression loss: 0.01695 | Running loss: 0.02293\n",
            "Epoch: 167 | Iteration: 51 | Classification loss: 0.00003 | Regression loss: 0.00985 | Running loss: 0.02287\n",
            "Epoch: 167 | Iteration: 52 | Classification loss: 0.00007 | Regression loss: 0.01204 | Running loss: 0.02286\n",
            "Epoch: 167 | Iteration: 53 | Classification loss: 0.00006 | Regression loss: 0.02010 | Running loss: 0.02284\n",
            "Epoch: 167 | Iteration: 54 | Classification loss: 0.00080 | Regression loss: 0.03720 | Running loss: 0.02287\n",
            "Epoch: 167 | Iteration: 55 | Classification loss: 0.00008 | Regression loss: 0.01620 | Running loss: 0.02286\n",
            "Epoch: 167 | Iteration: 56 | Classification loss: 0.00006 | Regression loss: 0.00693 | Running loss: 0.02284\n",
            "Epoch: 167 | Iteration: 57 | Classification loss: 0.00045 | Regression loss: 0.03621 | Running loss: 0.02288\n",
            "Epoch: 167 | Iteration: 58 | Classification loss: 0.00007 | Regression loss: 0.02048 | Running loss: 0.02287\n",
            "Epoch: 167 | Iteration: 59 | Classification loss: 0.00088 | Regression loss: 0.04211 | Running loss: 0.02292\n",
            "Epoch: 167 | Iteration: 60 | Classification loss: 0.00021 | Regression loss: 0.01915 | Running loss: 0.02293\n",
            "Epoch: 167 | Iteration: 61 | Classification loss: 0.00004 | Regression loss: 0.01522 | Running loss: 0.02293\n",
            "Epoch: 167 | Iteration: 62 | Classification loss: 0.00009 | Regression loss: 0.02555 | Running loss: 0.02295\n",
            "Epoch: 167 | Iteration: 63 | Classification loss: 0.00016 | Regression loss: 0.01397 | Running loss: 0.02292\n",
            "Epoch: 167 | Iteration: 64 | Classification loss: 0.00010 | Regression loss: 0.01473 | Running loss: 0.02290\n",
            "Epoch: 167 | Iteration: 65 | Classification loss: 0.00055 | Regression loss: 0.03197 | Running loss: 0.02293\n",
            "Epoch: 167 | Iteration: 66 | Classification loss: 0.00010 | Regression loss: 0.02035 | Running loss: 0.02289\n",
            "Epoch: 167 | Iteration: 67 | Classification loss: 0.00044 | Regression loss: 0.04336 | Running loss: 0.02293\n",
            "Epoch: 167 | Iteration: 68 | Classification loss: 0.00043 | Regression loss: 0.04256 | Running loss: 0.02299\n",
            "Epoch: 167 | Iteration: 69 | Classification loss: 0.00024 | Regression loss: 0.01880 | Running loss: 0.02299\n",
            "Epoch: 167 | Iteration: 70 | Classification loss: 0.00005 | Regression loss: 0.00868 | Running loss: 0.02296\n",
            "Epoch: 167 | Iteration: 71 | Classification loss: 0.00009 | Regression loss: 0.01978 | Running loss: 0.02294\n",
            "Epoch: 167 | Iteration: 72 | Classification loss: 0.00039 | Regression loss: 0.03049 | Running loss: 0.02297\n",
            "Epoch: 167 | Iteration: 73 | Classification loss: 0.00048 | Regression loss: 0.04047 | Running loss: 0.02298\n",
            "Epoch: 167 | Iteration: 74 | Classification loss: 0.00048 | Regression loss: 0.03598 | Running loss: 0.02303\n",
            "Epoch: 167 | Iteration: 75 | Classification loss: 0.00024 | Regression loss: 0.03172 | Running loss: 0.02308\n",
            "Epoch: 167 | Iteration: 76 | Classification loss: 0.00012 | Regression loss: 0.02926 | Running loss: 0.02306\n",
            "Epoch: 167 | Iteration: 77 | Classification loss: 0.00014 | Regression loss: 0.03123 | Running loss: 0.02309\n",
            "Epoch: 167 | Iteration: 78 | Classification loss: 0.00017 | Regression loss: 0.02274 | Running loss: 0.02308\n",
            "Epoch: 167 | Iteration: 79 | Classification loss: 0.00018 | Regression loss: 0.01615 | Running loss: 0.02303\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7277253912015531\n",
            "Precision:  0.5697674418604651\n",
            "Recall:  0.7945945945945946\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}, {0: (0.7229906021615096, 185.0)}, {0: (0.7259378939307871, 185.0)}, {0: (0.7306172160652471, 185.0)}, {0: (0.7210590702449308, 185.0)}, {0: (0.7214498931770392, 185.0)}, {0: (0.7272071618498421, 185.0)}, {0: (0.725966639522496, 185.0)}, {0: (0.7208028358685962, 185.0)}, {0: (0.7255636845600553, 185.0)}, {0: (0.7261742190616329, 185.0)}, {0: (0.7217332811149212, 185.0)}, {0: (0.7261653370413381, 185.0)}, {0: (0.7215292823555803, 185.0)}, {0: (0.7277253912015531, 185.0)}]\n",
            "Epoch: 168 | Iteration: 0 | Classification loss: 0.00020 | Regression loss: 0.02652 | Running loss: 0.02305\n",
            "Epoch: 168 | Iteration: 1 | Classification loss: 0.00005 | Regression loss: 0.01716 | Running loss: 0.02305\n",
            "Epoch: 168 | Iteration: 2 | Classification loss: 0.00017 | Regression loss: 0.01352 | Running loss: 0.02304\n",
            "Epoch: 168 | Iteration: 3 | Classification loss: 0.00049 | Regression loss: 0.01921 | Running loss: 0.02303\n",
            "Epoch: 168 | Iteration: 4 | Classification loss: 0.00033 | Regression loss: 0.02635 | Running loss: 0.02304\n",
            "Epoch: 168 | Iteration: 5 | Classification loss: 0.00049 | Regression loss: 0.03320 | Running loss: 0.02306\n",
            "Epoch: 168 | Iteration: 6 | Classification loss: 0.00057 | Regression loss: 0.03509 | Running loss: 0.02306\n",
            "Epoch: 168 | Iteration: 7 | Classification loss: 0.00029 | Regression loss: 0.04095 | Running loss: 0.02304\n",
            "Epoch: 168 | Iteration: 8 | Classification loss: 0.00004 | Regression loss: 0.01517 | Running loss: 0.02300\n",
            "Epoch: 168 | Iteration: 9 | Classification loss: 0.00009 | Regression loss: 0.02123 | Running loss: 0.02300\n",
            "Epoch: 168 | Iteration: 10 | Classification loss: 0.00025 | Regression loss: 0.01838 | Running loss: 0.02299\n",
            "Epoch: 168 | Iteration: 11 | Classification loss: 0.00006 | Regression loss: 0.02622 | Running loss: 0.02302\n",
            "Epoch: 168 | Iteration: 12 | Classification loss: 0.00014 | Regression loss: 0.01598 | Running loss: 0.02303\n",
            "Epoch: 168 | Iteration: 13 | Classification loss: 0.00015 | Regression loss: 0.02454 | Running loss: 0.02301\n",
            "Epoch: 168 | Iteration: 14 | Classification loss: 0.00006 | Regression loss: 0.00611 | Running loss: 0.02296\n",
            "Epoch: 168 | Iteration: 15 | Classification loss: 0.00008 | Regression loss: 0.02641 | Running loss: 0.02295\n",
            "Epoch: 168 | Iteration: 16 | Classification loss: 0.00011 | Regression loss: 0.01334 | Running loss: 0.02294\n",
            "Epoch: 168 | Iteration: 17 | Classification loss: 0.00015 | Regression loss: 0.01286 | Running loss: 0.02291\n",
            "Epoch: 168 | Iteration: 18 | Classification loss: 0.00022 | Regression loss: 0.01854 | Running loss: 0.02289\n",
            "Epoch: 168 | Iteration: 19 | Classification loss: 0.00014 | Regression loss: 0.02504 | Running loss: 0.02289\n",
            "Epoch: 168 | Iteration: 20 | Classification loss: 0.00009 | Regression loss: 0.02204 | Running loss: 0.02287\n",
            "Epoch: 168 | Iteration: 21 | Classification loss: 0.00011 | Regression loss: 0.02053 | Running loss: 0.02287\n",
            "Epoch: 168 | Iteration: 22 | Classification loss: 0.00048 | Regression loss: 0.03662 | Running loss: 0.02288\n",
            "Epoch: 168 | Iteration: 23 | Classification loss: 0.00007 | Regression loss: 0.02562 | Running loss: 0.02289\n",
            "Epoch: 168 | Iteration: 24 | Classification loss: 0.00010 | Regression loss: 0.02163 | Running loss: 0.02287\n",
            "Epoch: 168 | Iteration: 25 | Classification loss: 0.00005 | Regression loss: 0.01364 | Running loss: 0.02285\n",
            "Epoch: 168 | Iteration: 26 | Classification loss: 0.00063 | Regression loss: 0.03386 | Running loss: 0.02290\n",
            "Epoch: 168 | Iteration: 27 | Classification loss: 0.00010 | Regression loss: 0.01409 | Running loss: 0.02290\n",
            "Epoch: 168 | Iteration: 28 | Classification loss: 0.00017 | Regression loss: 0.03766 | Running loss: 0.02294\n",
            "Epoch: 168 | Iteration: 29 | Classification loss: 0.00035 | Regression loss: 0.04100 | Running loss: 0.02300\n",
            "Epoch: 168 | Iteration: 30 | Classification loss: 0.00018 | Regression loss: 0.01482 | Running loss: 0.02299\n",
            "Epoch: 168 | Iteration: 31 | Classification loss: 0.00007 | Regression loss: 0.01701 | Running loss: 0.02299\n",
            "Epoch: 168 | Iteration: 32 | Classification loss: 0.00035 | Regression loss: 0.02225 | Running loss: 0.02298\n",
            "Epoch: 168 | Iteration: 33 | Classification loss: 0.00059 | Regression loss: 0.03965 | Running loss: 0.02304\n",
            "Epoch: 168 | Iteration: 34 | Classification loss: 0.00022 | Regression loss: 0.03861 | Running loss: 0.02310\n",
            "Epoch: 168 | Iteration: 35 | Classification loss: 0.00014 | Regression loss: 0.01904 | Running loss: 0.02309\n",
            "Epoch: 168 | Iteration: 36 | Classification loss: 0.00054 | Regression loss: 0.01916 | Running loss: 0.02309\n",
            "Epoch: 168 | Iteration: 37 | Classification loss: 0.00022 | Regression loss: 0.02019 | Running loss: 0.02309\n",
            "Epoch: 168 | Iteration: 38 | Classification loss: 0.00026 | Regression loss: 0.02268 | Running loss: 0.02307\n",
            "Epoch: 168 | Iteration: 39 | Classification loss: 0.00012 | Regression loss: 0.01201 | Running loss: 0.02302\n",
            "Epoch: 168 | Iteration: 40 | Classification loss: 0.00059 | Regression loss: 0.05144 | Running loss: 0.02310\n",
            "Epoch: 168 | Iteration: 41 | Classification loss: 0.00008 | Regression loss: 0.01507 | Running loss: 0.02306\n",
            "Epoch: 168 | Iteration: 42 | Classification loss: 0.00018 | Regression loss: 0.01231 | Running loss: 0.02305\n",
            "Epoch: 168 | Iteration: 43 | Classification loss: 0.00014 | Regression loss: 0.01739 | Running loss: 0.02306\n",
            "Epoch: 168 | Iteration: 44 | Classification loss: 0.00004 | Regression loss: 0.00643 | Running loss: 0.02304\n",
            "Epoch: 168 | Iteration: 45 | Classification loss: 0.00007 | Regression loss: 0.01003 | Running loss: 0.02302\n",
            "Epoch: 168 | Iteration: 46 | Classification loss: 0.00019 | Regression loss: 0.02482 | Running loss: 0.02301\n",
            "Epoch: 168 | Iteration: 47 | Classification loss: 0.00020 | Regression loss: 0.03043 | Running loss: 0.02303\n",
            "Epoch: 168 | Iteration: 48 | Classification loss: 0.00008 | Regression loss: 0.00806 | Running loss: 0.02299\n",
            "Epoch: 168 | Iteration: 49 | Classification loss: 0.00010 | Regression loss: 0.01564 | Running loss: 0.02293\n",
            "Epoch: 168 | Iteration: 50 | Classification loss: 0.00030 | Regression loss: 0.03735 | Running loss: 0.02296\n",
            "Epoch: 168 | Iteration: 51 | Classification loss: 0.00013 | Regression loss: 0.01864 | Running loss: 0.02296\n",
            "Epoch: 168 | Iteration: 52 | Classification loss: 0.00010 | Regression loss: 0.01453 | Running loss: 0.02293\n",
            "Epoch: 168 | Iteration: 53 | Classification loss: 0.00009 | Regression loss: 0.02336 | Running loss: 0.02291\n",
            "Epoch: 168 | Iteration: 54 | Classification loss: 0.00014 | Regression loss: 0.03064 | Running loss: 0.02294\n",
            "Epoch: 168 | Iteration: 55 | Classification loss: 0.00012 | Regression loss: 0.01834 | Running loss: 0.02289\n",
            "Epoch: 168 | Iteration: 56 | Classification loss: 0.00017 | Regression loss: 0.02064 | Running loss: 0.02287\n",
            "Epoch: 168 | Iteration: 57 | Classification loss: 0.00022 | Regression loss: 0.01017 | Running loss: 0.02284\n",
            "Epoch: 168 | Iteration: 58 | Classification loss: 0.00014 | Regression loss: 0.01674 | Running loss: 0.02284\n",
            "Epoch: 168 | Iteration: 59 | Classification loss: 0.00014 | Regression loss: 0.01905 | Running loss: 0.02283\n",
            "Epoch: 168 | Iteration: 60 | Classification loss: 0.00045 | Regression loss: 0.04205 | Running loss: 0.02286\n",
            "Epoch: 168 | Iteration: 61 | Classification loss: 0.00036 | Regression loss: 0.02410 | Running loss: 0.02286\n",
            "Epoch: 168 | Iteration: 62 | Classification loss: 0.00016 | Regression loss: 0.01757 | Running loss: 0.02284\n",
            "Epoch: 168 | Iteration: 63 | Classification loss: 0.00027 | Regression loss: 0.03313 | Running loss: 0.02288\n",
            "Epoch: 168 | Iteration: 64 | Classification loss: 0.00019 | Regression loss: 0.02449 | Running loss: 0.02289\n",
            "Epoch: 168 | Iteration: 65 | Classification loss: 0.00046 | Regression loss: 0.02326 | Running loss: 0.02283\n",
            "Epoch: 168 | Iteration: 66 | Classification loss: 0.00015 | Regression loss: 0.01898 | Running loss: 0.02283\n",
            "Epoch: 168 | Iteration: 67 | Classification loss: 0.00026 | Regression loss: 0.03637 | Running loss: 0.02287\n",
            "Epoch: 168 | Iteration: 68 | Classification loss: 0.00018 | Regression loss: 0.02319 | Running loss: 0.02289\n",
            "Epoch: 168 | Iteration: 69 | Classification loss: 0.00042 | Regression loss: 0.03169 | Running loss: 0.02290\n",
            "Epoch: 168 | Iteration: 70 | Classification loss: 0.00008 | Regression loss: 0.01595 | Running loss: 0.02289\n",
            "Epoch: 168 | Iteration: 71 | Classification loss: 0.00005 | Regression loss: 0.00931 | Running loss: 0.02289\n",
            "Epoch: 168 | Iteration: 72 | Classification loss: 0.00009 | Regression loss: 0.01454 | Running loss: 0.02288\n",
            "Epoch: 168 | Iteration: 73 | Classification loss: 0.00027 | Regression loss: 0.03017 | Running loss: 0.02286\n",
            "Epoch: 168 | Iteration: 74 | Classification loss: 0.00007 | Regression loss: 0.01932 | Running loss: 0.02286\n",
            "Epoch: 168 | Iteration: 75 | Classification loss: 0.00002 | Regression loss: 0.00420 | Running loss: 0.02282\n",
            "Epoch: 168 | Iteration: 76 | Classification loss: 0.00003 | Regression loss: 0.00934 | Running loss: 0.02277\n",
            "Epoch: 168 | Iteration: 77 | Classification loss: 0.00016 | Regression loss: 0.01317 | Running loss: 0.02273\n",
            "Epoch: 168 | Iteration: 78 | Classification loss: 0.00023 | Regression loss: 0.01962 | Running loss: 0.02275\n",
            "Epoch: 168 | Iteration: 79 | Classification loss: 0.00030 | Regression loss: 0.03061 | Running loss: 0.02279\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7219177066363771\n",
            "Precision:  0.5703125\n",
            "Recall:  0.7891891891891892\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}, {0: (0.7229906021615096, 185.0)}, {0: (0.7259378939307871, 185.0)}, {0: (0.7306172160652471, 185.0)}, {0: (0.7210590702449308, 185.0)}, {0: (0.7214498931770392, 185.0)}, {0: (0.7272071618498421, 185.0)}, {0: (0.725966639522496, 185.0)}, {0: (0.7208028358685962, 185.0)}, {0: (0.7255636845600553, 185.0)}, {0: (0.7261742190616329, 185.0)}, {0: (0.7217332811149212, 185.0)}, {0: (0.7261653370413381, 185.0)}, {0: (0.7215292823555803, 185.0)}, {0: (0.7277253912015531, 185.0)}, {0: (0.7219177066363771, 185.0)}]\n",
            "Epoch: 169 | Iteration: 0 | Classification loss: 0.00031 | Regression loss: 0.03116 | Running loss: 0.02278\n",
            "Epoch: 169 | Iteration: 1 | Classification loss: 0.00027 | Regression loss: 0.02256 | Running loss: 0.02276\n",
            "Epoch: 169 | Iteration: 2 | Classification loss: 0.00044 | Regression loss: 0.02122 | Running loss: 0.02275\n",
            "Epoch: 169 | Iteration: 3 | Classification loss: 0.00028 | Regression loss: 0.02151 | Running loss: 0.02272\n",
            "Epoch: 169 | Iteration: 4 | Classification loss: 0.00011 | Regression loss: 0.02147 | Running loss: 0.02271\n",
            "Epoch: 169 | Iteration: 5 | Classification loss: 0.00046 | Regression loss: 0.04015 | Running loss: 0.02272\n",
            "Epoch: 169 | Iteration: 6 | Classification loss: 0.00008 | Regression loss: 0.01173 | Running loss: 0.02273\n",
            "Epoch: 169 | Iteration: 7 | Classification loss: 0.00018 | Regression loss: 0.01905 | Running loss: 0.02273\n",
            "Epoch: 169 | Iteration: 8 | Classification loss: 0.00059 | Regression loss: 0.03429 | Running loss: 0.02274\n",
            "Epoch: 169 | Iteration: 9 | Classification loss: 0.00022 | Regression loss: 0.02482 | Running loss: 0.02276\n",
            "Epoch: 169 | Iteration: 10 | Classification loss: 0.00022 | Regression loss: 0.03769 | Running loss: 0.02280\n",
            "Epoch: 169 | Iteration: 11 | Classification loss: 0.00015 | Regression loss: 0.03061 | Running loss: 0.02285\n",
            "Epoch: 169 | Iteration: 12 | Classification loss: 0.00015 | Regression loss: 0.01277 | Running loss: 0.02280\n",
            "Epoch: 169 | Iteration: 13 | Classification loss: 0.00053 | Regression loss: 0.03911 | Running loss: 0.02283\n",
            "Epoch: 169 | Iteration: 14 | Classification loss: 0.00007 | Regression loss: 0.01569 | Running loss: 0.02277\n",
            "Epoch: 169 | Iteration: 15 | Classification loss: 0.00021 | Regression loss: 0.01734 | Running loss: 0.02278\n",
            "Epoch: 169 | Iteration: 16 | Classification loss: 0.00007 | Regression loss: 0.00528 | Running loss: 0.02277\n",
            "Epoch: 169 | Iteration: 17 | Classification loss: 0.00023 | Regression loss: 0.01789 | Running loss: 0.02277\n",
            "Epoch: 169 | Iteration: 18 | Classification loss: 0.00013 | Regression loss: 0.02746 | Running loss: 0.02278\n",
            "Epoch: 169 | Iteration: 19 | Classification loss: 0.00052 | Regression loss: 0.01900 | Running loss: 0.02279\n",
            "Epoch: 169 | Iteration: 20 | Classification loss: 0.00025 | Regression loss: 0.02110 | Running loss: 0.02277\n",
            "Epoch: 169 | Iteration: 21 | Classification loss: 0.00018 | Regression loss: 0.01767 | Running loss: 0.02276\n",
            "Epoch: 169 | Iteration: 22 | Classification loss: 0.00014 | Regression loss: 0.01537 | Running loss: 0.02274\n",
            "Epoch: 169 | Iteration: 23 | Classification loss: 0.00007 | Regression loss: 0.01407 | Running loss: 0.02274\n",
            "Epoch: 169 | Iteration: 24 | Classification loss: 0.00014 | Regression loss: 0.02009 | Running loss: 0.02274\n",
            "Epoch: 169 | Iteration: 25 | Classification loss: 0.00038 | Regression loss: 0.02966 | Running loss: 0.02275\n",
            "Epoch: 169 | Iteration: 26 | Classification loss: 0.00043 | Regression loss: 0.02365 | Running loss: 0.02276\n",
            "Epoch: 169 | Iteration: 27 | Classification loss: 0.00018 | Regression loss: 0.02454 | Running loss: 0.02277\n",
            "Epoch: 169 | Iteration: 28 | Classification loss: 0.00030 | Regression loss: 0.02668 | Running loss: 0.02275\n",
            "Epoch: 169 | Iteration: 29 | Classification loss: 0.00012 | Regression loss: 0.01728 | Running loss: 0.02272\n",
            "Epoch: 169 | Iteration: 30 | Classification loss: 0.00066 | Regression loss: 0.03393 | Running loss: 0.02274\n",
            "Epoch: 169 | Iteration: 31 | Classification loss: 0.00008 | Regression loss: 0.02057 | Running loss: 0.02274\n",
            "Epoch: 169 | Iteration: 32 | Classification loss: 0.00005 | Regression loss: 0.00725 | Running loss: 0.02272\n",
            "Epoch: 169 | Iteration: 33 | Classification loss: 0.00029 | Regression loss: 0.02739 | Running loss: 0.02275\n",
            "Epoch: 169 | Iteration: 34 | Classification loss: 0.00020 | Regression loss: 0.01440 | Running loss: 0.02273\n",
            "Epoch: 169 | Iteration: 35 | Classification loss: 0.00005 | Regression loss: 0.01503 | Running loss: 0.02272\n",
            "Epoch: 169 | Iteration: 36 | Classification loss: 0.00009 | Regression loss: 0.02869 | Running loss: 0.02274\n",
            "Epoch: 169 | Iteration: 37 | Classification loss: 0.00004 | Regression loss: 0.01937 | Running loss: 0.02272\n",
            "Epoch: 169 | Iteration: 38 | Classification loss: 0.00043 | Regression loss: 0.02256 | Running loss: 0.02271\n",
            "Epoch: 169 | Iteration: 39 | Classification loss: 0.00029 | Regression loss: 0.03899 | Running loss: 0.02273\n",
            "Epoch: 169 | Iteration: 40 | Classification loss: 0.00008 | Regression loss: 0.01344 | Running loss: 0.02271\n",
            "Epoch: 169 | Iteration: 41 | Classification loss: 0.00011 | Regression loss: 0.01454 | Running loss: 0.02266\n",
            "Epoch: 169 | Iteration: 42 | Classification loss: 0.00007 | Regression loss: 0.01552 | Running loss: 0.02266\n",
            "Epoch: 169 | Iteration: 43 | Classification loss: 0.00011 | Regression loss: 0.01774 | Running loss: 0.02261\n",
            "Epoch: 169 | Iteration: 44 | Classification loss: 0.00003 | Regression loss: 0.00441 | Running loss: 0.02258\n",
            "Epoch: 169 | Iteration: 45 | Classification loss: 0.00039 | Regression loss: 0.03088 | Running loss: 0.02263\n",
            "Epoch: 169 | Iteration: 46 | Classification loss: 0.00012 | Regression loss: 0.01718 | Running loss: 0.02259\n",
            "Epoch: 169 | Iteration: 47 | Classification loss: 0.00008 | Regression loss: 0.00933 | Running loss: 0.02255\n",
            "Epoch: 169 | Iteration: 48 | Classification loss: 0.00007 | Regression loss: 0.02557 | Running loss: 0.02257\n",
            "Epoch: 169 | Iteration: 49 | Classification loss: 0.00006 | Regression loss: 0.02142 | Running loss: 0.02260\n",
            "Epoch: 169 | Iteration: 50 | Classification loss: 0.00013 | Regression loss: 0.01906 | Running loss: 0.02259\n",
            "Epoch: 169 | Iteration: 51 | Classification loss: 0.00038 | Regression loss: 0.04275 | Running loss: 0.02266\n",
            "Epoch: 169 | Iteration: 52 | Classification loss: 0.00002 | Regression loss: 0.00940 | Running loss: 0.02263\n",
            "Epoch: 169 | Iteration: 53 | Classification loss: 0.00016 | Regression loss: 0.01397 | Running loss: 0.02264\n",
            "Epoch: 169 | Iteration: 54 | Classification loss: 0.00024 | Regression loss: 0.03577 | Running loss: 0.02267\n",
            "Epoch: 169 | Iteration: 55 | Classification loss: 0.00083 | Regression loss: 0.04315 | Running loss: 0.02274\n",
            "Epoch: 169 | Iteration: 56 | Classification loss: 0.00010 | Regression loss: 0.01503 | Running loss: 0.02273\n",
            "Epoch: 169 | Iteration: 57 | Classification loss: 0.00006 | Regression loss: 0.00895 | Running loss: 0.02270\n",
            "Epoch: 169 | Iteration: 58 | Classification loss: 0.00021 | Regression loss: 0.01860 | Running loss: 0.02271\n",
            "Epoch: 169 | Iteration: 59 | Classification loss: 0.00005 | Regression loss: 0.02612 | Running loss: 0.02270\n",
            "Epoch: 169 | Iteration: 60 | Classification loss: 0.00011 | Regression loss: 0.01609 | Running loss: 0.02266\n",
            "Epoch: 169 | Iteration: 61 | Classification loss: 0.00018 | Regression loss: 0.03688 | Running loss: 0.02268\n",
            "Epoch: 169 | Iteration: 62 | Classification loss: 0.00046 | Regression loss: 0.04820 | Running loss: 0.02275\n",
            "Epoch: 169 | Iteration: 63 | Classification loss: 0.00008 | Regression loss: 0.00862 | Running loss: 0.02267\n",
            "Epoch: 169 | Iteration: 64 | Classification loss: 0.00018 | Regression loss: 0.02229 | Running loss: 0.02265\n",
            "Epoch: 169 | Iteration: 65 | Classification loss: 0.00018 | Regression loss: 0.01347 | Running loss: 0.02260\n",
            "Epoch: 169 | Iteration: 66 | Classification loss: 0.00015 | Regression loss: 0.02429 | Running loss: 0.02260\n",
            "Epoch: 169 | Iteration: 67 | Classification loss: 0.00018 | Regression loss: 0.01640 | Running loss: 0.02259\n",
            "Epoch: 169 | Iteration: 68 | Classification loss: 0.00009 | Regression loss: 0.01347 | Running loss: 0.02259\n",
            "Epoch: 169 | Iteration: 69 | Classification loss: 0.00045 | Regression loss: 0.03270 | Running loss: 0.02260\n",
            "Epoch: 169 | Iteration: 70 | Classification loss: 0.00017 | Regression loss: 0.01272 | Running loss: 0.02259\n",
            "Epoch: 169 | Iteration: 71 | Classification loss: 0.00011 | Regression loss: 0.02255 | Running loss: 0.02259\n",
            "Epoch: 169 | Iteration: 72 | Classification loss: 0.00026 | Regression loss: 0.03109 | Running loss: 0.02260\n",
            "Epoch: 169 | Iteration: 73 | Classification loss: 0.00009 | Regression loss: 0.01959 | Running loss: 0.02261\n",
            "Epoch: 169 | Iteration: 74 | Classification loss: 0.00035 | Regression loss: 0.03358 | Running loss: 0.02264\n",
            "Epoch: 169 | Iteration: 75 | Classification loss: 0.00017 | Regression loss: 0.02604 | Running loss: 0.02267\n",
            "Epoch: 169 | Iteration: 76 | Classification loss: 0.00010 | Regression loss: 0.02705 | Running loss: 0.02269\n",
            "Epoch: 169 | Iteration: 77 | Classification loss: 0.00020 | Regression loss: 0.01938 | Running loss: 0.02269\n",
            "Epoch: 169 | Iteration: 78 | Classification loss: 0.00024 | Regression loss: 0.01381 | Running loss: 0.02269\n",
            "Epoch: 169 | Iteration: 79 | Classification loss: 0.00015 | Regression loss: 0.01191 | Running loss: 0.02267\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7274783215794732\n",
            "Precision:  0.5697674418604651\n",
            "Recall:  0.7945945945945946\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}, {0: (0.7229906021615096, 185.0)}, {0: (0.7259378939307871, 185.0)}, {0: (0.7306172160652471, 185.0)}, {0: (0.7210590702449308, 185.0)}, {0: (0.7214498931770392, 185.0)}, {0: (0.7272071618498421, 185.0)}, {0: (0.725966639522496, 185.0)}, {0: (0.7208028358685962, 185.0)}, {0: (0.7255636845600553, 185.0)}, {0: (0.7261742190616329, 185.0)}, {0: (0.7217332811149212, 185.0)}, {0: (0.7261653370413381, 185.0)}, {0: (0.7215292823555803, 185.0)}, {0: (0.7277253912015531, 185.0)}, {0: (0.7219177066363771, 185.0)}, {0: (0.7274783215794732, 185.0)}]\n",
            "Epoch: 170 | Iteration: 0 | Classification loss: 0.00038 | Regression loss: 0.04798 | Running loss: 0.02272\n",
            "Epoch: 170 | Iteration: 1 | Classification loss: 0.00014 | Regression loss: 0.02409 | Running loss: 0.02275\n",
            "Epoch: 170 | Iteration: 2 | Classification loss: 0.00015 | Regression loss: 0.01322 | Running loss: 0.02276\n",
            "Epoch: 170 | Iteration: 3 | Classification loss: 0.00012 | Regression loss: 0.02986 | Running loss: 0.02279\n",
            "Epoch: 170 | Iteration: 4 | Classification loss: 0.00016 | Regression loss: 0.02050 | Running loss: 0.02280\n",
            "Epoch: 170 | Iteration: 5 | Classification loss: 0.00006 | Regression loss: 0.01464 | Running loss: 0.02280\n",
            "Epoch: 170 | Iteration: 6 | Classification loss: 0.00015 | Regression loss: 0.01571 | Running loss: 0.02280\n",
            "Epoch: 170 | Iteration: 7 | Classification loss: 0.00016 | Regression loss: 0.01564 | Running loss: 0.02274\n",
            "Epoch: 170 | Iteration: 8 | Classification loss: 0.00018 | Regression loss: 0.01898 | Running loss: 0.02272\n",
            "Epoch: 170 | Iteration: 9 | Classification loss: 0.00006 | Regression loss: 0.03088 | Running loss: 0.02273\n",
            "Epoch: 170 | Iteration: 10 | Classification loss: 0.00006 | Regression loss: 0.02365 | Running loss: 0.02274\n",
            "Epoch: 170 | Iteration: 11 | Classification loss: 0.00021 | Regression loss: 0.01948 | Running loss: 0.02275\n",
            "Epoch: 170 | Iteration: 12 | Classification loss: 0.00004 | Regression loss: 0.00774 | Running loss: 0.02271\n",
            "Epoch: 170 | Iteration: 13 | Classification loss: 0.00050 | Regression loss: 0.03255 | Running loss: 0.02270\n",
            "Epoch: 170 | Iteration: 14 | Classification loss: 0.00011 | Regression loss: 0.01487 | Running loss: 0.02264\n",
            "Epoch: 170 | Iteration: 15 | Classification loss: 0.00040 | Regression loss: 0.03719 | Running loss: 0.02267\n",
            "Epoch: 170 | Iteration: 16 | Classification loss: 0.00028 | Regression loss: 0.03214 | Running loss: 0.02266\n",
            "Epoch: 170 | Iteration: 17 | Classification loss: 0.00039 | Regression loss: 0.03757 | Running loss: 0.02272\n",
            "Epoch: 170 | Iteration: 18 | Classification loss: 0.00019 | Regression loss: 0.03549 | Running loss: 0.02271\n",
            "Epoch: 170 | Iteration: 19 | Classification loss: 0.00043 | Regression loss: 0.02263 | Running loss: 0.02273\n",
            "Epoch: 170 | Iteration: 20 | Classification loss: 0.00007 | Regression loss: 0.00939 | Running loss: 0.02267\n",
            "Epoch: 170 | Iteration: 21 | Classification loss: 0.00025 | Regression loss: 0.02304 | Running loss: 0.02266\n",
            "Epoch: 170 | Iteration: 22 | Classification loss: 0.00023 | Regression loss: 0.01932 | Running loss: 0.02267\n",
            "Epoch: 170 | Iteration: 23 | Classification loss: 0.00012 | Regression loss: 0.01758 | Running loss: 0.02268\n",
            "Epoch: 170 | Iteration: 24 | Classification loss: 0.00018 | Regression loss: 0.02698 | Running loss: 0.02271\n",
            "Epoch: 170 | Iteration: 25 | Classification loss: 0.00006 | Regression loss: 0.00890 | Running loss: 0.02268\n",
            "Epoch: 170 | Iteration: 26 | Classification loss: 0.00041 | Regression loss: 0.02251 | Running loss: 0.02270\n",
            "Epoch: 170 | Iteration: 27 | Classification loss: 0.00039 | Regression loss: 0.04334 | Running loss: 0.02275\n",
            "Epoch: 170 | Iteration: 28 | Classification loss: 0.00071 | Regression loss: 0.02254 | Running loss: 0.02275\n",
            "Epoch: 170 | Iteration: 29 | Classification loss: 0.00009 | Regression loss: 0.01298 | Running loss: 0.02276\n",
            "Epoch: 170 | Iteration: 30 | Classification loss: 0.00034 | Regression loss: 0.02613 | Running loss: 0.02277\n",
            "Epoch: 170 | Iteration: 31 | Classification loss: 0.00023 | Regression loss: 0.02199 | Running loss: 0.02278\n",
            "Epoch: 170 | Iteration: 32 | Classification loss: 0.00018 | Regression loss: 0.02132 | Running loss: 0.02278\n",
            "Epoch: 170 | Iteration: 33 | Classification loss: 0.00008 | Regression loss: 0.01443 | Running loss: 0.02278\n",
            "Epoch: 170 | Iteration: 34 | Classification loss: 0.00010 | Regression loss: 0.01627 | Running loss: 0.02278\n",
            "Epoch: 170 | Iteration: 35 | Classification loss: 0.00005 | Regression loss: 0.01735 | Running loss: 0.02275\n",
            "Epoch: 170 | Iteration: 36 | Classification loss: 0.00013 | Regression loss: 0.01770 | Running loss: 0.02276\n",
            "Epoch: 170 | Iteration: 37 | Classification loss: 0.00010 | Regression loss: 0.01295 | Running loss: 0.02274\n",
            "Epoch: 170 | Iteration: 38 | Classification loss: 0.00075 | Regression loss: 0.03729 | Running loss: 0.02278\n",
            "Epoch: 170 | Iteration: 39 | Classification loss: 0.00003 | Regression loss: 0.00996 | Running loss: 0.02276\n",
            "Epoch: 170 | Iteration: 40 | Classification loss: 0.00029 | Regression loss: 0.02232 | Running loss: 0.02276\n",
            "Epoch: 170 | Iteration: 41 | Classification loss: 0.00057 | Regression loss: 0.03791 | Running loss: 0.02280\n",
            "Epoch: 170 | Iteration: 42 | Classification loss: 0.00018 | Regression loss: 0.02638 | Running loss: 0.02281\n",
            "Epoch: 170 | Iteration: 43 | Classification loss: 0.00015 | Regression loss: 0.01287 | Running loss: 0.02280\n",
            "Epoch: 170 | Iteration: 44 | Classification loss: 0.00024 | Regression loss: 0.03649 | Running loss: 0.02280\n",
            "Epoch: 170 | Iteration: 45 | Classification loss: 0.00006 | Regression loss: 0.02032 | Running loss: 0.02280\n",
            "Epoch: 170 | Iteration: 46 | Classification loss: 0.00031 | Regression loss: 0.04091 | Running loss: 0.02280\n",
            "Epoch: 170 | Iteration: 47 | Classification loss: 0.00020 | Regression loss: 0.03868 | Running loss: 0.02286\n",
            "Epoch: 170 | Iteration: 48 | Classification loss: 0.00010 | Regression loss: 0.01437 | Running loss: 0.02288\n",
            "Epoch: 170 | Iteration: 49 | Classification loss: 0.00018 | Regression loss: 0.02854 | Running loss: 0.02291\n",
            "Epoch: 170 | Iteration: 50 | Classification loss: 0.00008 | Regression loss: 0.01714 | Running loss: 0.02286\n",
            "Epoch: 170 | Iteration: 51 | Classification loss: 0.00002 | Regression loss: 0.00419 | Running loss: 0.02286\n",
            "Epoch: 170 | Iteration: 52 | Classification loss: 0.00013 | Regression loss: 0.01070 | Running loss: 0.02287\n",
            "Epoch: 170 | Iteration: 53 | Classification loss: 0.00013 | Regression loss: 0.01787 | Running loss: 0.02285\n",
            "Epoch: 170 | Iteration: 54 | Classification loss: 0.00012 | Regression loss: 0.02935 | Running loss: 0.02285\n",
            "Epoch: 170 | Iteration: 55 | Classification loss: 0.00042 | Regression loss: 0.04118 | Running loss: 0.02288\n",
            "Epoch: 170 | Iteration: 56 | Classification loss: 0.00010 | Regression loss: 0.01661 | Running loss: 0.02287\n",
            "Epoch: 170 | Iteration: 57 | Classification loss: 0.00016 | Regression loss: 0.01354 | Running loss: 0.02285\n",
            "Epoch: 170 | Iteration: 58 | Classification loss: 0.00012 | Regression loss: 0.02648 | Running loss: 0.02286\n",
            "Epoch: 170 | Iteration: 59 | Classification loss: 0.00016 | Regression loss: 0.02617 | Running loss: 0.02288\n",
            "Epoch: 170 | Iteration: 60 | Classification loss: 0.00016 | Regression loss: 0.01363 | Running loss: 0.02285\n",
            "Epoch: 170 | Iteration: 61 | Classification loss: 0.00038 | Regression loss: 0.03028 | Running loss: 0.02284\n",
            "Epoch: 170 | Iteration: 62 | Classification loss: 0.00003 | Regression loss: 0.01521 | Running loss: 0.02281\n",
            "Epoch: 170 | Iteration: 63 | Classification loss: 0.00013 | Regression loss: 0.01956 | Running loss: 0.02275\n",
            "Epoch: 170 | Iteration: 64 | Classification loss: 0.00019 | Regression loss: 0.02128 | Running loss: 0.02272\n",
            "Epoch: 170 | Iteration: 65 | Classification loss: 0.00013 | Regression loss: 0.02225 | Running loss: 0.02270\n",
            "Epoch: 170 | Iteration: 66 | Classification loss: 0.00010 | Regression loss: 0.00863 | Running loss: 0.02268\n",
            "Epoch: 170 | Iteration: 67 | Classification loss: 0.00014 | Regression loss: 0.02509 | Running loss: 0.02270\n",
            "Epoch: 170 | Iteration: 68 | Classification loss: 0.00009 | Regression loss: 0.02092 | Running loss: 0.02266\n",
            "Epoch: 170 | Iteration: 69 | Classification loss: 0.00014 | Regression loss: 0.02104 | Running loss: 0.02266\n",
            "Epoch: 170 | Iteration: 70 | Classification loss: 0.00012 | Regression loss: 0.01889 | Running loss: 0.02261\n",
            "Epoch: 170 | Iteration: 71 | Classification loss: 0.00042 | Regression loss: 0.03449 | Running loss: 0.02264\n",
            "Epoch: 170 | Iteration: 72 | Classification loss: 0.00005 | Regression loss: 0.00675 | Running loss: 0.02258\n",
            "Epoch: 170 | Iteration: 73 | Classification loss: 0.00021 | Regression loss: 0.02940 | Running loss: 0.02260\n",
            "Epoch: 170 | Iteration: 74 | Classification loss: 0.00011 | Regression loss: 0.01181 | Running loss: 0.02261\n",
            "Epoch: 170 | Iteration: 75 | Classification loss: 0.00070 | Regression loss: 0.03562 | Running loss: 0.02265\n",
            "Epoch: 170 | Iteration: 76 | Classification loss: 0.00005 | Regression loss: 0.02811 | Running loss: 0.02265\n",
            "Epoch: 170 | Iteration: 77 | Classification loss: 0.00011 | Regression loss: 0.01659 | Running loss: 0.02265\n",
            "Epoch: 170 | Iteration: 78 | Classification loss: 0.00007 | Regression loss: 0.01465 | Running loss: 0.02260\n",
            "Epoch: 170 | Iteration: 79 | Classification loss: 0.00015 | Regression loss: 0.01491 | Running loss: 0.02258\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7218983427777539\n",
            "Precision:  0.5770750988142292\n",
            "Recall:  0.7891891891891892\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}, {0: (0.7229906021615096, 185.0)}, {0: (0.7259378939307871, 185.0)}, {0: (0.7306172160652471, 185.0)}, {0: (0.7210590702449308, 185.0)}, {0: (0.7214498931770392, 185.0)}, {0: (0.7272071618498421, 185.0)}, {0: (0.725966639522496, 185.0)}, {0: (0.7208028358685962, 185.0)}, {0: (0.7255636845600553, 185.0)}, {0: (0.7261742190616329, 185.0)}, {0: (0.7217332811149212, 185.0)}, {0: (0.7261653370413381, 185.0)}, {0: (0.7215292823555803, 185.0)}, {0: (0.7277253912015531, 185.0)}, {0: (0.7219177066363771, 185.0)}, {0: (0.7274783215794732, 185.0)}, {0: (0.7218983427777539, 185.0)}]\n",
            "Epoch: 171 | Iteration: 0 | Classification loss: 0.00012 | Regression loss: 0.02204 | Running loss: 0.02257\n",
            "Epoch: 171 | Iteration: 1 | Classification loss: 0.00027 | Regression loss: 0.02266 | Running loss: 0.02256\n",
            "Epoch: 171 | Iteration: 2 | Classification loss: 0.00007 | Regression loss: 0.00803 | Running loss: 0.02251\n",
            "Epoch: 171 | Iteration: 3 | Classification loss: 0.00014 | Regression loss: 0.02313 | Running loss: 0.02248\n",
            "Epoch: 171 | Iteration: 4 | Classification loss: 0.00018 | Regression loss: 0.01837 | Running loss: 0.02248\n",
            "Epoch: 171 | Iteration: 5 | Classification loss: 0.00009 | Regression loss: 0.01299 | Running loss: 0.02248\n",
            "Epoch: 171 | Iteration: 6 | Classification loss: 0.00003 | Regression loss: 0.00941 | Running loss: 0.02247\n",
            "Epoch: 171 | Iteration: 7 | Classification loss: 0.00038 | Regression loss: 0.04940 | Running loss: 0.02254\n",
            "Epoch: 171 | Iteration: 8 | Classification loss: 0.00008 | Regression loss: 0.02222 | Running loss: 0.02255\n",
            "Epoch: 171 | Iteration: 9 | Classification loss: 0.00021 | Regression loss: 0.01800 | Running loss: 0.02254\n",
            "Epoch: 171 | Iteration: 10 | Classification loss: 0.00073 | Regression loss: 0.03646 | Running loss: 0.02253\n",
            "Epoch: 171 | Iteration: 11 | Classification loss: 0.00011 | Regression loss: 0.02322 | Running loss: 0.02254\n",
            "Epoch: 171 | Iteration: 12 | Classification loss: 0.00038 | Regression loss: 0.02949 | Running loss: 0.02256\n",
            "Epoch: 171 | Iteration: 13 | Classification loss: 0.00028 | Regression loss: 0.03243 | Running loss: 0.02257\n",
            "Epoch: 171 | Iteration: 14 | Classification loss: 0.00049 | Regression loss: 0.03824 | Running loss: 0.02262\n",
            "Epoch: 171 | Iteration: 15 | Classification loss: 0.00014 | Regression loss: 0.01851 | Running loss: 0.02260\n",
            "Epoch: 171 | Iteration: 16 | Classification loss: 0.00003 | Regression loss: 0.00428 | Running loss: 0.02256\n",
            "Epoch: 171 | Iteration: 17 | Classification loss: 0.00008 | Regression loss: 0.01142 | Running loss: 0.02253\n",
            "Epoch: 171 | Iteration: 18 | Classification loss: 0.00007 | Regression loss: 0.01588 | Running loss: 0.02253\n",
            "Epoch: 171 | Iteration: 19 | Classification loss: 0.00015 | Regression loss: 0.02104 | Running loss: 0.02253\n",
            "Epoch: 171 | Iteration: 20 | Classification loss: 0.00014 | Regression loss: 0.01271 | Running loss: 0.02251\n",
            "Epoch: 171 | Iteration: 21 | Classification loss: 0.00016 | Regression loss: 0.01242 | Running loss: 0.02251\n",
            "Epoch: 171 | Iteration: 22 | Classification loss: 0.00009 | Regression loss: 0.01498 | Running loss: 0.02249\n",
            "Epoch: 171 | Iteration: 23 | Classification loss: 0.00009 | Regression loss: 0.02421 | Running loss: 0.02250\n",
            "Epoch: 171 | Iteration: 24 | Classification loss: 0.00016 | Regression loss: 0.01569 | Running loss: 0.02248\n",
            "Epoch: 171 | Iteration: 25 | Classification loss: 0.00011 | Regression loss: 0.02625 | Running loss: 0.02247\n",
            "Epoch: 171 | Iteration: 26 | Classification loss: 0.00011 | Regression loss: 0.01831 | Running loss: 0.02243\n",
            "Epoch: 171 | Iteration: 27 | Classification loss: 0.00053 | Regression loss: 0.03200 | Running loss: 0.02248\n",
            "Epoch: 171 | Iteration: 28 | Classification loss: 0.00033 | Regression loss: 0.02337 | Running loss: 0.02249\n",
            "Epoch: 171 | Iteration: 29 | Classification loss: 0.00044 | Regression loss: 0.04065 | Running loss: 0.02252\n",
            "Epoch: 171 | Iteration: 30 | Classification loss: 0.00013 | Regression loss: 0.01777 | Running loss: 0.02247\n",
            "Epoch: 171 | Iteration: 31 | Classification loss: 0.00030 | Regression loss: 0.04173 | Running loss: 0.02251\n",
            "Epoch: 171 | Iteration: 32 | Classification loss: 0.00008 | Regression loss: 0.00922 | Running loss: 0.02249\n",
            "Epoch: 171 | Iteration: 33 | Classification loss: 0.00015 | Regression loss: 0.03292 | Running loss: 0.02252\n",
            "Epoch: 171 | Iteration: 34 | Classification loss: 0.00011 | Regression loss: 0.01520 | Running loss: 0.02245\n",
            "Epoch: 171 | Iteration: 35 | Classification loss: 0.00014 | Regression loss: 0.01771 | Running loss: 0.02241\n",
            "Epoch: 171 | Iteration: 36 | Classification loss: 0.00016 | Regression loss: 0.01690 | Running loss: 0.02241\n",
            "Epoch: 171 | Iteration: 37 | Classification loss: 0.00040 | Regression loss: 0.03437 | Running loss: 0.02245\n",
            "Epoch: 171 | Iteration: 38 | Classification loss: 0.00016 | Regression loss: 0.03537 | Running loss: 0.02244\n",
            "Epoch: 171 | Iteration: 39 | Classification loss: 0.00005 | Regression loss: 0.02214 | Running loss: 0.02245\n",
            "Epoch: 171 | Iteration: 40 | Classification loss: 0.00009 | Regression loss: 0.01384 | Running loss: 0.02243\n",
            "Epoch: 171 | Iteration: 41 | Classification loss: 0.00016 | Regression loss: 0.01937 | Running loss: 0.02244\n",
            "Epoch: 171 | Iteration: 42 | Classification loss: 0.00018 | Regression loss: 0.02675 | Running loss: 0.02247\n",
            "Epoch: 171 | Iteration: 43 | Classification loss: 0.00012 | Regression loss: 0.01850 | Running loss: 0.02249\n",
            "Epoch: 171 | Iteration: 44 | Classification loss: 0.00011 | Regression loss: 0.01479 | Running loss: 0.02249\n",
            "Epoch: 171 | Iteration: 45 | Classification loss: 0.00019 | Regression loss: 0.01991 | Running loss: 0.02248\n",
            "Epoch: 171 | Iteration: 46 | Classification loss: 0.00004 | Regression loss: 0.00689 | Running loss: 0.02243\n",
            "Epoch: 171 | Iteration: 47 | Classification loss: 0.00032 | Regression loss: 0.03037 | Running loss: 0.02244\n",
            "Epoch: 171 | Iteration: 48 | Classification loss: 0.00031 | Regression loss: 0.02710 | Running loss: 0.02245\n",
            "Epoch: 171 | Iteration: 49 | Classification loss: 0.00004 | Regression loss: 0.01536 | Running loss: 0.02239\n",
            "Epoch: 171 | Iteration: 50 | Classification loss: 0.00018 | Regression loss: 0.01198 | Running loss: 0.02234\n",
            "Epoch: 171 | Iteration: 51 | Classification loss: 0.00014 | Regression loss: 0.01582 | Running loss: 0.02234\n",
            "Epoch: 171 | Iteration: 52 | Classification loss: 0.00048 | Regression loss: 0.01957 | Running loss: 0.02232\n",
            "Epoch: 171 | Iteration: 53 | Classification loss: 0.00006 | Regression loss: 0.00841 | Running loss: 0.02230\n",
            "Epoch: 171 | Iteration: 54 | Classification loss: 0.00007 | Regression loss: 0.01602 | Running loss: 0.02228\n",
            "Epoch: 171 | Iteration: 55 | Classification loss: 0.00005 | Regression loss: 0.02007 | Running loss: 0.02229\n",
            "Epoch: 171 | Iteration: 56 | Classification loss: 0.00006 | Regression loss: 0.00775 | Running loss: 0.02226\n",
            "Epoch: 171 | Iteration: 57 | Classification loss: 0.00068 | Regression loss: 0.02229 | Running loss: 0.02225\n",
            "Epoch: 171 | Iteration: 58 | Classification loss: 0.00014 | Regression loss: 0.03103 | Running loss: 0.02225\n",
            "Epoch: 171 | Iteration: 59 | Classification loss: 0.00042 | Regression loss: 0.04123 | Running loss: 0.02230\n",
            "Epoch: 171 | Iteration: 60 | Classification loss: 0.00022 | Regression loss: 0.03844 | Running loss: 0.02234\n",
            "Epoch: 171 | Iteration: 61 | Classification loss: 0.00015 | Regression loss: 0.01715 | Running loss: 0.02234\n",
            "Epoch: 171 | Iteration: 62 | Classification loss: 0.00041 | Regression loss: 0.03458 | Running loss: 0.02237\n",
            "Epoch: 171 | Iteration: 63 | Classification loss: 0.00006 | Regression loss: 0.01459 | Running loss: 0.02235\n",
            "Epoch: 171 | Iteration: 64 | Classification loss: 0.00006 | Regression loss: 0.02067 | Running loss: 0.02233\n",
            "Epoch: 171 | Iteration: 65 | Classification loss: 0.00019 | Regression loss: 0.02525 | Running loss: 0.02233\n",
            "Epoch: 171 | Iteration: 66 | Classification loss: 0.00023 | Regression loss: 0.03395 | Running loss: 0.02238\n",
            "Epoch: 171 | Iteration: 67 | Classification loss: 0.00014 | Regression loss: 0.01248 | Running loss: 0.02239\n",
            "Epoch: 171 | Iteration: 68 | Classification loss: 0.00036 | Regression loss: 0.02955 | Running loss: 0.02241\n",
            "Epoch: 171 | Iteration: 69 | Classification loss: 0.00020 | Regression loss: 0.02622 | Running loss: 0.02239\n",
            "Epoch: 171 | Iteration: 70 | Classification loss: 0.00056 | Regression loss: 0.02239 | Running loss: 0.02236\n",
            "Epoch: 171 | Iteration: 71 | Classification loss: 0.00012 | Regression loss: 0.02766 | Running loss: 0.02235\n",
            "Epoch: 171 | Iteration: 72 | Classification loss: 0.00015 | Regression loss: 0.01345 | Running loss: 0.02230\n",
            "Epoch: 171 | Iteration: 73 | Classification loss: 0.00023 | Regression loss: 0.01930 | Running loss: 0.02232\n",
            "Epoch: 171 | Iteration: 74 | Classification loss: 0.00021 | Regression loss: 0.01393 | Running loss: 0.02232\n",
            "Epoch: 171 | Iteration: 75 | Classification loss: 0.00030 | Regression loss: 0.02163 | Running loss: 0.02235\n",
            "Epoch: 171 | Iteration: 76 | Classification loss: 0.00005 | Regression loss: 0.02084 | Running loss: 0.02236\n",
            "Epoch: 171 | Iteration: 77 | Classification loss: 0.00086 | Regression loss: 0.04044 | Running loss: 0.02240\n",
            "Epoch: 171 | Iteration: 78 | Classification loss: 0.00021 | Regression loss: 0.02883 | Running loss: 0.02240\n",
            "Epoch: 171 | Iteration: 79 | Classification loss: 0.00035 | Regression loss: 0.02210 | Running loss: 0.02241\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7270037695200344\n",
            "Precision:  0.57421875\n",
            "Recall:  0.7945945945945946\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}, {0: (0.7229906021615096, 185.0)}, {0: (0.7259378939307871, 185.0)}, {0: (0.7306172160652471, 185.0)}, {0: (0.7210590702449308, 185.0)}, {0: (0.7214498931770392, 185.0)}, {0: (0.7272071618498421, 185.0)}, {0: (0.725966639522496, 185.0)}, {0: (0.7208028358685962, 185.0)}, {0: (0.7255636845600553, 185.0)}, {0: (0.7261742190616329, 185.0)}, {0: (0.7217332811149212, 185.0)}, {0: (0.7261653370413381, 185.0)}, {0: (0.7215292823555803, 185.0)}, {0: (0.7277253912015531, 185.0)}, {0: (0.7219177066363771, 185.0)}, {0: (0.7274783215794732, 185.0)}, {0: (0.7218983427777539, 185.0)}, {0: (0.7270037695200344, 185.0)}]\n",
            "Epoch: 172 | Iteration: 0 | Classification loss: 0.00015 | Regression loss: 0.01274 | Running loss: 0.02237\n",
            "Epoch: 172 | Iteration: 1 | Classification loss: 0.00042 | Regression loss: 0.04285 | Running loss: 0.02243\n",
            "Epoch: 172 | Iteration: 2 | Classification loss: 0.00019 | Regression loss: 0.01706 | Running loss: 0.02244\n",
            "Epoch: 172 | Iteration: 3 | Classification loss: 0.00023 | Regression loss: 0.02039 | Running loss: 0.02241\n",
            "Epoch: 172 | Iteration: 4 | Classification loss: 0.00012 | Regression loss: 0.01046 | Running loss: 0.02238\n",
            "Epoch: 172 | Iteration: 5 | Classification loss: 0.00009 | Regression loss: 0.01412 | Running loss: 0.02236\n",
            "Epoch: 172 | Iteration: 6 | Classification loss: 0.00065 | Regression loss: 0.03438 | Running loss: 0.02234\n",
            "Epoch: 172 | Iteration: 7 | Classification loss: 0.00034 | Regression loss: 0.04440 | Running loss: 0.02239\n",
            "Epoch: 172 | Iteration: 8 | Classification loss: 0.00010 | Regression loss: 0.02114 | Running loss: 0.02239\n",
            "Epoch: 172 | Iteration: 9 | Classification loss: 0.00023 | Regression loss: 0.01329 | Running loss: 0.02239\n",
            "Epoch: 172 | Iteration: 10 | Classification loss: 0.00005 | Regression loss: 0.00835 | Running loss: 0.02238\n",
            "Epoch: 172 | Iteration: 11 | Classification loss: 0.00008 | Regression loss: 0.01394 | Running loss: 0.02238\n",
            "Epoch: 172 | Iteration: 12 | Classification loss: 0.00019 | Regression loss: 0.02544 | Running loss: 0.02239\n",
            "Epoch: 172 | Iteration: 13 | Classification loss: 0.00013 | Regression loss: 0.02431 | Running loss: 0.02242\n",
            "Epoch: 172 | Iteration: 14 | Classification loss: 0.00034 | Regression loss: 0.03081 | Running loss: 0.02246\n",
            "Epoch: 172 | Iteration: 15 | Classification loss: 0.00012 | Regression loss: 0.02169 | Running loss: 0.02244\n",
            "Epoch: 172 | Iteration: 16 | Classification loss: 0.00006 | Regression loss: 0.00521 | Running loss: 0.02241\n",
            "Epoch: 172 | Iteration: 17 | Classification loss: 0.00004 | Regression loss: 0.01519 | Running loss: 0.02241\n",
            "Epoch: 172 | Iteration: 18 | Classification loss: 0.00022 | Regression loss: 0.03695 | Running loss: 0.02244\n",
            "Epoch: 172 | Iteration: 19 | Classification loss: 0.00013 | Regression loss: 0.01711 | Running loss: 0.02243\n",
            "Epoch: 172 | Iteration: 20 | Classification loss: 0.00032 | Regression loss: 0.03881 | Running loss: 0.02242\n",
            "Epoch: 172 | Iteration: 21 | Classification loss: 0.00009 | Regression loss: 0.02143 | Running loss: 0.02240\n",
            "Epoch: 172 | Iteration: 22 | Classification loss: 0.00020 | Regression loss: 0.03937 | Running loss: 0.02245\n",
            "Epoch: 172 | Iteration: 23 | Classification loss: 0.00067 | Regression loss: 0.02363 | Running loss: 0.02246\n",
            "Epoch: 172 | Iteration: 24 | Classification loss: 0.00009 | Regression loss: 0.01978 | Running loss: 0.02247\n",
            "Epoch: 172 | Iteration: 25 | Classification loss: 0.00004 | Regression loss: 0.00755 | Running loss: 0.02245\n",
            "Epoch: 172 | Iteration: 26 | Classification loss: 0.00008 | Regression loss: 0.00932 | Running loss: 0.02243\n",
            "Epoch: 172 | Iteration: 27 | Classification loss: 0.00028 | Regression loss: 0.03297 | Running loss: 0.02245\n",
            "Epoch: 172 | Iteration: 28 | Classification loss: 0.00011 | Regression loss: 0.01711 | Running loss: 0.02239\n",
            "Epoch: 172 | Iteration: 29 | Classification loss: 0.00008 | Regression loss: 0.01450 | Running loss: 0.02232\n",
            "Epoch: 172 | Iteration: 30 | Classification loss: 0.00015 | Regression loss: 0.02923 | Running loss: 0.02235\n",
            "Epoch: 172 | Iteration: 31 | Classification loss: 0.00010 | Regression loss: 0.02657 | Running loss: 0.02237\n",
            "Epoch: 172 | Iteration: 32 | Classification loss: 0.00014 | Regression loss: 0.01350 | Running loss: 0.02236\n",
            "Epoch: 172 | Iteration: 33 | Classification loss: 0.00007 | Regression loss: 0.01228 | Running loss: 0.02235\n",
            "Epoch: 172 | Iteration: 34 | Classification loss: 0.00006 | Regression loss: 0.02051 | Running loss: 0.02237\n",
            "Epoch: 172 | Iteration: 35 | Classification loss: 0.00039 | Regression loss: 0.03930 | Running loss: 0.02240\n",
            "Epoch: 172 | Iteration: 36 | Classification loss: 0.00015 | Regression loss: 0.01687 | Running loss: 0.02238\n",
            "Epoch: 172 | Iteration: 37 | Classification loss: 0.00009 | Regression loss: 0.02621 | Running loss: 0.02242\n",
            "Epoch: 172 | Iteration: 38 | Classification loss: 0.00015 | Regression loss: 0.01402 | Running loss: 0.02243\n",
            "Epoch: 172 | Iteration: 39 | Classification loss: 0.00019 | Regression loss: 0.03584 | Running loss: 0.02248\n",
            "Epoch: 172 | Iteration: 40 | Classification loss: 0.00012 | Regression loss: 0.01885 | Running loss: 0.02245\n",
            "Epoch: 172 | Iteration: 41 | Classification loss: 0.00009 | Regression loss: 0.01337 | Running loss: 0.02244\n",
            "Epoch: 172 | Iteration: 42 | Classification loss: 0.00007 | Regression loss: 0.02640 | Running loss: 0.02247\n",
            "Epoch: 172 | Iteration: 43 | Classification loss: 0.00011 | Regression loss: 0.02907 | Running loss: 0.02249\n",
            "Epoch: 172 | Iteration: 44 | Classification loss: 0.00047 | Regression loss: 0.03209 | Running loss: 0.02252\n",
            "Epoch: 172 | Iteration: 45 | Classification loss: 0.00011 | Regression loss: 0.01005 | Running loss: 0.02252\n",
            "Epoch: 172 | Iteration: 46 | Classification loss: 0.00014 | Regression loss: 0.01775 | Running loss: 0.02251\n",
            "Epoch: 172 | Iteration: 47 | Classification loss: 0.00022 | Regression loss: 0.01796 | Running loss: 0.02248\n",
            "Epoch: 172 | Iteration: 48 | Classification loss: 0.00014 | Regression loss: 0.01773 | Running loss: 0.02246\n",
            "Epoch: 172 | Iteration: 49 | Classification loss: 0.00020 | Regression loss: 0.02165 | Running loss: 0.02242\n",
            "Epoch: 172 | Iteration: 50 | Classification loss: 0.00016 | Regression loss: 0.02589 | Running loss: 0.02244\n",
            "Epoch: 172 | Iteration: 51 | Classification loss: 0.00011 | Regression loss: 0.01841 | Running loss: 0.02241\n",
            "Epoch: 172 | Iteration: 52 | Classification loss: 0.00016 | Regression loss: 0.01486 | Running loss: 0.02240\n",
            "Epoch: 172 | Iteration: 53 | Classification loss: 0.00006 | Regression loss: 0.01669 | Running loss: 0.02236\n",
            "Epoch: 172 | Iteration: 54 | Classification loss: 0.00024 | Regression loss: 0.02964 | Running loss: 0.02236\n",
            "Epoch: 172 | Iteration: 55 | Classification loss: 0.00076 | Regression loss: 0.04086 | Running loss: 0.02236\n",
            "Epoch: 172 | Iteration: 56 | Classification loss: 0.00013 | Regression loss: 0.01624 | Running loss: 0.02230\n",
            "Epoch: 172 | Iteration: 57 | Classification loss: 0.00002 | Regression loss: 0.00454 | Running loss: 0.02227\n",
            "Epoch: 172 | Iteration: 58 | Classification loss: 0.00027 | Regression loss: 0.03094 | Running loss: 0.02228\n",
            "Epoch: 172 | Iteration: 59 | Classification loss: 0.00024 | Regression loss: 0.02091 | Running loss: 0.02229\n",
            "Epoch: 172 | Iteration: 60 | Classification loss: 0.00014 | Regression loss: 0.02024 | Running loss: 0.02231\n",
            "Epoch: 172 | Iteration: 61 | Classification loss: 0.00017 | Regression loss: 0.02289 | Running loss: 0.02230\n",
            "Epoch: 172 | Iteration: 62 | Classification loss: 0.00045 | Regression loss: 0.05025 | Running loss: 0.02235\n",
            "Epoch: 172 | Iteration: 63 | Classification loss: 0.00029 | Regression loss: 0.02571 | Running loss: 0.02237\n",
            "Epoch: 172 | Iteration: 64 | Classification loss: 0.00049 | Regression loss: 0.03757 | Running loss: 0.02241\n",
            "Epoch: 172 | Iteration: 65 | Classification loss: 0.00002 | Regression loss: 0.00951 | Running loss: 0.02240\n",
            "Epoch: 172 | Iteration: 66 | Classification loss: 0.00016 | Regression loss: 0.02988 | Running loss: 0.02242\n",
            "Epoch: 172 | Iteration: 67 | Classification loss: 0.00054 | Regression loss: 0.02336 | Running loss: 0.02245\n",
            "Epoch: 172 | Iteration: 68 | Classification loss: 0.00006 | Regression loss: 0.01518 | Running loss: 0.02244\n",
            "Epoch: 172 | Iteration: 69 | Classification loss: 0.00070 | Regression loss: 0.03666 | Running loss: 0.02244\n",
            "Epoch: 172 | Iteration: 70 | Classification loss: 0.00029 | Regression loss: 0.02414 | Running loss: 0.02243\n",
            "Epoch: 172 | Iteration: 71 | Classification loss: 0.00005 | Regression loss: 0.01402 | Running loss: 0.02241\n",
            "Epoch: 172 | Iteration: 72 | Classification loss: 0.00005 | Regression loss: 0.01953 | Running loss: 0.02239\n",
            "Epoch: 172 | Iteration: 73 | Classification loss: 0.00016 | Regression loss: 0.02479 | Running loss: 0.02241\n",
            "Epoch: 172 | Iteration: 74 | Classification loss: 0.00020 | Regression loss: 0.01706 | Running loss: 0.02242\n",
            "Epoch: 172 | Iteration: 75 | Classification loss: 0.00041 | Regression loss: 0.02258 | Running loss: 0.02241\n",
            "Epoch: 172 | Iteration: 76 | Classification loss: 0.00039 | Regression loss: 0.02312 | Running loss: 0.02240\n",
            "Epoch: 172 | Iteration: 77 | Classification loss: 0.00016 | Regression loss: 0.01833 | Running loss: 0.02240\n",
            "Epoch: 172 | Iteration: 78 | Classification loss: 0.00008 | Regression loss: 0.01277 | Running loss: 0.02235\n",
            "Epoch: 172 | Iteration: 79 | Classification loss: 0.00006 | Regression loss: 0.02226 | Running loss: 0.02232\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7272851879977948\n",
            "Precision:  0.5787401574803149\n",
            "Recall:  0.7945945945945946\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}, {0: (0.7229906021615096, 185.0)}, {0: (0.7259378939307871, 185.0)}, {0: (0.7306172160652471, 185.0)}, {0: (0.7210590702449308, 185.0)}, {0: (0.7214498931770392, 185.0)}, {0: (0.7272071618498421, 185.0)}, {0: (0.725966639522496, 185.0)}, {0: (0.7208028358685962, 185.0)}, {0: (0.7255636845600553, 185.0)}, {0: (0.7261742190616329, 185.0)}, {0: (0.7217332811149212, 185.0)}, {0: (0.7261653370413381, 185.0)}, {0: (0.7215292823555803, 185.0)}, {0: (0.7277253912015531, 185.0)}, {0: (0.7219177066363771, 185.0)}, {0: (0.7274783215794732, 185.0)}, {0: (0.7218983427777539, 185.0)}, {0: (0.7270037695200344, 185.0)}, {0: (0.7272851879977948, 185.0)}]\n",
            "Epoch: 173 | Iteration: 0 | Classification loss: 0.00029 | Regression loss: 0.02860 | Running loss: 0.02235\n",
            "Epoch: 173 | Iteration: 1 | Classification loss: 0.00017 | Regression loss: 0.01474 | Running loss: 0.02233\n",
            "Epoch: 173 | Iteration: 2 | Classification loss: 0.00006 | Regression loss: 0.00947 | Running loss: 0.02232\n",
            "Epoch: 173 | Iteration: 3 | Classification loss: 0.00006 | Regression loss: 0.01501 | Running loss: 0.02230\n",
            "Epoch: 173 | Iteration: 4 | Classification loss: 0.00015 | Regression loss: 0.02864 | Running loss: 0.02235\n",
            "Epoch: 173 | Iteration: 5 | Classification loss: 0.00003 | Regression loss: 0.01519 | Running loss: 0.02235\n",
            "Epoch: 173 | Iteration: 6 | Classification loss: 0.00031 | Regression loss: 0.03580 | Running loss: 0.02236\n",
            "Epoch: 173 | Iteration: 7 | Classification loss: 0.00019 | Regression loss: 0.01867 | Running loss: 0.02233\n",
            "Epoch: 173 | Iteration: 8 | Classification loss: 0.00012 | Regression loss: 0.02540 | Running loss: 0.02234\n",
            "Epoch: 173 | Iteration: 9 | Classification loss: 0.00035 | Regression loss: 0.02962 | Running loss: 0.02236\n",
            "Epoch: 173 | Iteration: 10 | Classification loss: 0.00010 | Regression loss: 0.01853 | Running loss: 0.02236\n",
            "Epoch: 173 | Iteration: 11 | Classification loss: 0.00020 | Regression loss: 0.02848 | Running loss: 0.02239\n",
            "Epoch: 173 | Iteration: 12 | Classification loss: 0.00062 | Regression loss: 0.03449 | Running loss: 0.02241\n",
            "Epoch: 173 | Iteration: 13 | Classification loss: 0.00032 | Regression loss: 0.04394 | Running loss: 0.02246\n",
            "Epoch: 173 | Iteration: 14 | Classification loss: 0.00062 | Regression loss: 0.02309 | Running loss: 0.02248\n",
            "Epoch: 173 | Iteration: 15 | Classification loss: 0.00005 | Regression loss: 0.00776 | Running loss: 0.02244\n",
            "Epoch: 173 | Iteration: 16 | Classification loss: 0.00013 | Regression loss: 0.01955 | Running loss: 0.02245\n",
            "Epoch: 173 | Iteration: 17 | Classification loss: 0.00012 | Regression loss: 0.01784 | Running loss: 0.02244\n",
            "Epoch: 173 | Iteration: 18 | Classification loss: 0.00047 | Regression loss: 0.03795 | Running loss: 0.02247\n",
            "Epoch: 173 | Iteration: 19 | Classification loss: 0.00005 | Regression loss: 0.00724 | Running loss: 0.02244\n",
            "Epoch: 173 | Iteration: 20 | Classification loss: 0.00018 | Regression loss: 0.01285 | Running loss: 0.02243\n",
            "Epoch: 173 | Iteration: 21 | Classification loss: 0.00002 | Regression loss: 0.00410 | Running loss: 0.02236\n",
            "Epoch: 173 | Iteration: 22 | Classification loss: 0.00059 | Regression loss: 0.03536 | Running loss: 0.02241\n",
            "Epoch: 173 | Iteration: 23 | Classification loss: 0.00021 | Regression loss: 0.03483 | Running loss: 0.02242\n",
            "Epoch: 173 | Iteration: 24 | Classification loss: 0.00026 | Regression loss: 0.03073 | Running loss: 0.02245\n",
            "Epoch: 173 | Iteration: 25 | Classification loss: 0.00015 | Regression loss: 0.01348 | Running loss: 0.02241\n",
            "Epoch: 173 | Iteration: 26 | Classification loss: 0.00010 | Regression loss: 0.01271 | Running loss: 0.02240\n",
            "Epoch: 173 | Iteration: 27 | Classification loss: 0.00007 | Regression loss: 0.01194 | Running loss: 0.02240\n",
            "Epoch: 173 | Iteration: 28 | Classification loss: 0.00038 | Regression loss: 0.02288 | Running loss: 0.02239\n",
            "Epoch: 173 | Iteration: 29 | Classification loss: 0.00005 | Regression loss: 0.00812 | Running loss: 0.02237\n",
            "Epoch: 173 | Iteration: 30 | Classification loss: 0.00012 | Regression loss: 0.01674 | Running loss: 0.02238\n",
            "Epoch: 173 | Iteration: 31 | Classification loss: 0.00006 | Regression loss: 0.01886 | Running loss: 0.02238\n",
            "Epoch: 173 | Iteration: 32 | Classification loss: 0.00021 | Regression loss: 0.01984 | Running loss: 0.02239\n",
            "Epoch: 173 | Iteration: 33 | Classification loss: 0.00013 | Regression loss: 0.02297 | Running loss: 0.02237\n",
            "Epoch: 173 | Iteration: 34 | Classification loss: 0.00022 | Regression loss: 0.01870 | Running loss: 0.02238\n",
            "Epoch: 173 | Iteration: 35 | Classification loss: 0.00011 | Regression loss: 0.01058 | Running loss: 0.02230\n",
            "Epoch: 173 | Iteration: 36 | Classification loss: 0.00015 | Regression loss: 0.01596 | Running loss: 0.02229\n",
            "Epoch: 173 | Iteration: 37 | Classification loss: 0.00008 | Regression loss: 0.00758 | Running loss: 0.02228\n",
            "Epoch: 173 | Iteration: 38 | Classification loss: 0.00015 | Regression loss: 0.02580 | Running loss: 0.02230\n",
            "Epoch: 173 | Iteration: 39 | Classification loss: 0.00004 | Regression loss: 0.01888 | Running loss: 0.02226\n",
            "Epoch: 173 | Iteration: 40 | Classification loss: 0.00010 | Regression loss: 0.01657 | Running loss: 0.02225\n",
            "Epoch: 173 | Iteration: 41 | Classification loss: 0.00009 | Regression loss: 0.02169 | Running loss: 0.02226\n",
            "Epoch: 173 | Iteration: 42 | Classification loss: 0.00021 | Regression loss: 0.02517 | Running loss: 0.02226\n",
            "Epoch: 173 | Iteration: 43 | Classification loss: 0.00012 | Regression loss: 0.01667 | Running loss: 0.02226\n",
            "Epoch: 173 | Iteration: 44 | Classification loss: 0.00030 | Regression loss: 0.02606 | Running loss: 0.02228\n",
            "Epoch: 173 | Iteration: 45 | Classification loss: 0.00036 | Regression loss: 0.03455 | Running loss: 0.02231\n",
            "Epoch: 173 | Iteration: 46 | Classification loss: 0.00014 | Regression loss: 0.01605 | Running loss: 0.02231\n",
            "Epoch: 173 | Iteration: 47 | Classification loss: 0.00044 | Regression loss: 0.04090 | Running loss: 0.02235\n",
            "Epoch: 173 | Iteration: 48 | Classification loss: 0.00011 | Regression loss: 0.01778 | Running loss: 0.02234\n",
            "Epoch: 173 | Iteration: 49 | Classification loss: 0.00010 | Regression loss: 0.02255 | Running loss: 0.02235\n",
            "Epoch: 173 | Iteration: 50 | Classification loss: 0.00011 | Regression loss: 0.01262 | Running loss: 0.02232\n",
            "Epoch: 173 | Iteration: 51 | Classification loss: 0.00008 | Regression loss: 0.02969 | Running loss: 0.02232\n",
            "Epoch: 173 | Iteration: 52 | Classification loss: 0.00007 | Regression loss: 0.01645 | Running loss: 0.02231\n",
            "Epoch: 173 | Iteration: 53 | Classification loss: 0.00021 | Regression loss: 0.03539 | Running loss: 0.02235\n",
            "Epoch: 173 | Iteration: 54 | Classification loss: 0.00010 | Regression loss: 0.01498 | Running loss: 0.02234\n",
            "Epoch: 173 | Iteration: 55 | Classification loss: 0.00070 | Regression loss: 0.04124 | Running loss: 0.02238\n",
            "Epoch: 173 | Iteration: 56 | Classification loss: 0.00006 | Regression loss: 0.02560 | Running loss: 0.02239\n",
            "Epoch: 173 | Iteration: 57 | Classification loss: 0.00041 | Regression loss: 0.03119 | Running loss: 0.02244\n",
            "Epoch: 173 | Iteration: 58 | Classification loss: 0.00054 | Regression loss: 0.05003 | Running loss: 0.02249\n",
            "Epoch: 173 | Iteration: 59 | Classification loss: 0.00021 | Regression loss: 0.02415 | Running loss: 0.02251\n",
            "Epoch: 173 | Iteration: 60 | Classification loss: 0.00017 | Regression loss: 0.02335 | Running loss: 0.02252\n",
            "Epoch: 173 | Iteration: 61 | Classification loss: 0.00011 | Regression loss: 0.02395 | Running loss: 0.02254\n",
            "Epoch: 173 | Iteration: 62 | Classification loss: 0.00015 | Regression loss: 0.02533 | Running loss: 0.02256\n",
            "Epoch: 173 | Iteration: 63 | Classification loss: 0.00015 | Regression loss: 0.01956 | Running loss: 0.02260\n",
            "Epoch: 173 | Iteration: 64 | Classification loss: 0.00009 | Regression loss: 0.01537 | Running loss: 0.02257\n",
            "Epoch: 173 | Iteration: 65 | Classification loss: 0.00009 | Regression loss: 0.02728 | Running loss: 0.02259\n",
            "Epoch: 173 | Iteration: 66 | Classification loss: 0.00006 | Regression loss: 0.01839 | Running loss: 0.02258\n",
            "Epoch: 173 | Iteration: 67 | Classification loss: 0.00008 | Regression loss: 0.01367 | Running loss: 0.02255\n",
            "Epoch: 173 | Iteration: 68 | Classification loss: 0.00020 | Regression loss: 0.03735 | Running loss: 0.02258\n",
            "Epoch: 173 | Iteration: 69 | Classification loss: 0.00020 | Regression loss: 0.01762 | Running loss: 0.02254\n",
            "Epoch: 173 | Iteration: 70 | Classification loss: 0.00006 | Regression loss: 0.01533 | Running loss: 0.02254\n",
            "Epoch: 173 | Iteration: 71 | Classification loss: 0.00026 | Regression loss: 0.03013 | Running loss: 0.02258\n",
            "Epoch: 173 | Iteration: 72 | Classification loss: 0.00018 | Regression loss: 0.01392 | Running loss: 0.02258\n",
            "Epoch: 173 | Iteration: 73 | Classification loss: 0.00002 | Regression loss: 0.00916 | Running loss: 0.02256\n",
            "Epoch: 173 | Iteration: 74 | Classification loss: 0.00009 | Regression loss: 0.01384 | Running loss: 0.02251\n",
            "Epoch: 173 | Iteration: 75 | Classification loss: 0.00031 | Regression loss: 0.02361 | Running loss: 0.02253\n",
            "Epoch: 173 | Iteration: 76 | Classification loss: 0.00042 | Regression loss: 0.02270 | Running loss: 0.02256\n",
            "Epoch: 173 | Iteration: 77 | Classification loss: 0.00015 | Regression loss: 0.00874 | Running loss: 0.02251\n",
            "Epoch: 173 | Iteration: 78 | Classification loss: 0.00006 | Regression loss: 0.02642 | Running loss: 0.02252\n",
            "Epoch: 173 | Iteration: 79 | Classification loss: 0.00045 | Regression loss: 0.02040 | Running loss: 0.02247\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7221052381441093\n",
            "Precision:  0.5816733067729084\n",
            "Recall:  0.7891891891891892\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}, {0: (0.7229906021615096, 185.0)}, {0: (0.7259378939307871, 185.0)}, {0: (0.7306172160652471, 185.0)}, {0: (0.7210590702449308, 185.0)}, {0: (0.7214498931770392, 185.0)}, {0: (0.7272071618498421, 185.0)}, {0: (0.725966639522496, 185.0)}, {0: (0.7208028358685962, 185.0)}, {0: (0.7255636845600553, 185.0)}, {0: (0.7261742190616329, 185.0)}, {0: (0.7217332811149212, 185.0)}, {0: (0.7261653370413381, 185.0)}, {0: (0.7215292823555803, 185.0)}, {0: (0.7277253912015531, 185.0)}, {0: (0.7219177066363771, 185.0)}, {0: (0.7274783215794732, 185.0)}, {0: (0.7218983427777539, 185.0)}, {0: (0.7270037695200344, 185.0)}, {0: (0.7272851879977948, 185.0)}, {0: (0.7221052381441093, 185.0)}]\n",
            "Epoch: 174 | Iteration: 0 | Classification loss: 0.00005 | Regression loss: 0.00634 | Running loss: 0.02245\n",
            "Epoch: 174 | Iteration: 1 | Classification loss: 0.00011 | Regression loss: 0.00957 | Running loss: 0.02244\n",
            "Epoch: 174 | Iteration: 2 | Classification loss: 0.00015 | Regression loss: 0.01722 | Running loss: 0.02242\n",
            "Epoch: 174 | Iteration: 3 | Classification loss: 0.00044 | Regression loss: 0.03139 | Running loss: 0.02245\n",
            "Epoch: 174 | Iteration: 4 | Classification loss: 0.00007 | Regression loss: 0.01474 | Running loss: 0.02245\n",
            "Epoch: 174 | Iteration: 5 | Classification loss: 0.00014 | Regression loss: 0.01621 | Running loss: 0.02242\n",
            "Epoch: 174 | Iteration: 6 | Classification loss: 0.00011 | Regression loss: 0.01825 | Running loss: 0.02242\n",
            "Epoch: 174 | Iteration: 7 | Classification loss: 0.00020 | Regression loss: 0.02392 | Running loss: 0.02238\n",
            "Epoch: 174 | Iteration: 8 | Classification loss: 0.00019 | Regression loss: 0.02189 | Running loss: 0.02234\n",
            "Epoch: 174 | Iteration: 9 | Classification loss: 0.00009 | Regression loss: 0.01483 | Running loss: 0.02233\n",
            "Epoch: 174 | Iteration: 10 | Classification loss: 0.00036 | Regression loss: 0.03545 | Running loss: 0.02238\n",
            "Epoch: 174 | Iteration: 11 | Classification loss: 0.00009 | Regression loss: 0.02105 | Running loss: 0.02239\n",
            "Epoch: 174 | Iteration: 12 | Classification loss: 0.00005 | Regression loss: 0.01387 | Running loss: 0.02235\n",
            "Epoch: 174 | Iteration: 13 | Classification loss: 0.00013 | Regression loss: 0.02165 | Running loss: 0.02231\n",
            "Epoch: 174 | Iteration: 14 | Classification loss: 0.00010 | Regression loss: 0.02707 | Running loss: 0.02229\n",
            "Epoch: 174 | Iteration: 15 | Classification loss: 0.00007 | Regression loss: 0.01719 | Running loss: 0.02227\n",
            "Epoch: 174 | Iteration: 16 | Classification loss: 0.00016 | Regression loss: 0.01359 | Running loss: 0.02223\n",
            "Epoch: 174 | Iteration: 17 | Classification loss: 0.00012 | Regression loss: 0.01312 | Running loss: 0.02220\n",
            "Epoch: 174 | Iteration: 18 | Classification loss: 0.00007 | Regression loss: 0.02181 | Running loss: 0.02220\n",
            "Epoch: 174 | Iteration: 19 | Classification loss: 0.00017 | Regression loss: 0.02447 | Running loss: 0.02221\n",
            "Epoch: 174 | Iteration: 20 | Classification loss: 0.00020 | Regression loss: 0.01881 | Running loss: 0.02220\n",
            "Epoch: 174 | Iteration: 21 | Classification loss: 0.00029 | Regression loss: 0.02593 | Running loss: 0.02221\n",
            "Epoch: 174 | Iteration: 22 | Classification loss: 0.00018 | Regression loss: 0.03384 | Running loss: 0.02226\n",
            "Epoch: 174 | Iteration: 23 | Classification loss: 0.00003 | Regression loss: 0.00750 | Running loss: 0.02223\n",
            "Epoch: 174 | Iteration: 24 | Classification loss: 0.00041 | Regression loss: 0.02152 | Running loss: 0.02222\n",
            "Epoch: 174 | Iteration: 25 | Classification loss: 0.00004 | Regression loss: 0.01962 | Running loss: 0.02219\n",
            "Epoch: 174 | Iteration: 26 | Classification loss: 0.00014 | Regression loss: 0.00964 | Running loss: 0.02214\n",
            "Epoch: 174 | Iteration: 27 | Classification loss: 0.00019 | Regression loss: 0.02147 | Running loss: 0.02210\n",
            "Epoch: 174 | Iteration: 28 | Classification loss: 0.00011 | Regression loss: 0.01596 | Running loss: 0.02210\n",
            "Epoch: 174 | Iteration: 29 | Classification loss: 0.00010 | Regression loss: 0.01207 | Running loss: 0.02209\n",
            "Epoch: 174 | Iteration: 30 | Classification loss: 0.00009 | Regression loss: 0.01287 | Running loss: 0.02207\n",
            "Epoch: 174 | Iteration: 31 | Classification loss: 0.00028 | Regression loss: 0.02239 | Running loss: 0.02207\n",
            "Epoch: 174 | Iteration: 32 | Classification loss: 0.00010 | Regression loss: 0.02245 | Running loss: 0.02208\n",
            "Epoch: 174 | Iteration: 33 | Classification loss: 0.00011 | Regression loss: 0.01030 | Running loss: 0.02205\n",
            "Epoch: 174 | Iteration: 34 | Classification loss: 0.00029 | Regression loss: 0.04336 | Running loss: 0.02213\n",
            "Epoch: 174 | Iteration: 35 | Classification loss: 0.00031 | Regression loss: 0.02505 | Running loss: 0.02212\n",
            "Epoch: 174 | Iteration: 36 | Classification loss: 0.00006 | Regression loss: 0.02595 | Running loss: 0.02215\n",
            "Epoch: 174 | Iteration: 37 | Classification loss: 0.00058 | Regression loss: 0.02103 | Running loss: 0.02217\n",
            "Epoch: 174 | Iteration: 38 | Classification loss: 0.00023 | Regression loss: 0.01934 | Running loss: 0.02217\n",
            "Epoch: 174 | Iteration: 39 | Classification loss: 0.00017 | Regression loss: 0.03200 | Running loss: 0.02218\n",
            "Epoch: 174 | Iteration: 40 | Classification loss: 0.00054 | Regression loss: 0.03658 | Running loss: 0.02221\n",
            "Epoch: 174 | Iteration: 41 | Classification loss: 0.00008 | Regression loss: 0.01433 | Running loss: 0.02220\n",
            "Epoch: 174 | Iteration: 42 | Classification loss: 0.00010 | Regression loss: 0.02649 | Running loss: 0.02218\n",
            "Epoch: 174 | Iteration: 43 | Classification loss: 0.00007 | Regression loss: 0.01849 | Running loss: 0.02216\n",
            "Epoch: 174 | Iteration: 44 | Classification loss: 0.00015 | Regression loss: 0.02106 | Running loss: 0.02216\n",
            "Epoch: 174 | Iteration: 45 | Classification loss: 0.00056 | Regression loss: 0.03318 | Running loss: 0.02220\n",
            "Epoch: 174 | Iteration: 46 | Classification loss: 0.00005 | Regression loss: 0.01322 | Running loss: 0.02216\n",
            "Epoch: 174 | Iteration: 47 | Classification loss: 0.00011 | Regression loss: 0.02086 | Running loss: 0.02217\n",
            "Epoch: 174 | Iteration: 48 | Classification loss: 0.00011 | Regression loss: 0.01842 | Running loss: 0.02214\n",
            "Epoch: 174 | Iteration: 49 | Classification loss: 0.00024 | Regression loss: 0.02973 | Running loss: 0.02211\n",
            "Epoch: 174 | Iteration: 50 | Classification loss: 0.00033 | Regression loss: 0.02165 | Running loss: 0.02213\n",
            "Epoch: 174 | Iteration: 51 | Classification loss: 0.00040 | Regression loss: 0.04140 | Running loss: 0.02218\n",
            "Epoch: 174 | Iteration: 52 | Classification loss: 0.00050 | Regression loss: 0.02142 | Running loss: 0.02218\n",
            "Epoch: 174 | Iteration: 53 | Classification loss: 0.00010 | Regression loss: 0.01383 | Running loss: 0.02212\n",
            "Epoch: 174 | Iteration: 54 | Classification loss: 0.00008 | Regression loss: 0.02190 | Running loss: 0.02209\n",
            "Epoch: 174 | Iteration: 55 | Classification loss: 0.00029 | Regression loss: 0.02977 | Running loss: 0.02211\n",
            "Epoch: 174 | Iteration: 56 | Classification loss: 0.00014 | Regression loss: 0.02021 | Running loss: 0.02211\n",
            "Epoch: 174 | Iteration: 57 | Classification loss: 0.00015 | Regression loss: 0.01304 | Running loss: 0.02210\n",
            "Epoch: 174 | Iteration: 58 | Classification loss: 0.00066 | Regression loss: 0.03633 | Running loss: 0.02213\n",
            "Epoch: 174 | Iteration: 59 | Classification loss: 0.00012 | Regression loss: 0.01672 | Running loss: 0.02214\n",
            "Epoch: 174 | Iteration: 60 | Classification loss: 0.00018 | Regression loss: 0.01758 | Running loss: 0.02207\n",
            "Epoch: 174 | Iteration: 61 | Classification loss: 0.00005 | Regression loss: 0.00578 | Running loss: 0.02205\n",
            "Epoch: 174 | Iteration: 62 | Classification loss: 0.00017 | Regression loss: 0.02538 | Running loss: 0.02207\n",
            "Epoch: 174 | Iteration: 63 | Classification loss: 0.00010 | Regression loss: 0.01628 | Running loss: 0.02207\n",
            "Epoch: 174 | Iteration: 64 | Classification loss: 0.00023 | Regression loss: 0.03167 | Running loss: 0.02212\n",
            "Epoch: 174 | Iteration: 65 | Classification loss: 0.00008 | Regression loss: 0.01655 | Running loss: 0.02214\n",
            "Epoch: 174 | Iteration: 66 | Classification loss: 0.00034 | Regression loss: 0.03613 | Running loss: 0.02216\n",
            "Epoch: 174 | Iteration: 67 | Classification loss: 0.00028 | Regression loss: 0.03110 | Running loss: 0.02216\n",
            "Epoch: 174 | Iteration: 68 | Classification loss: 0.00002 | Regression loss: 0.00904 | Running loss: 0.02216\n",
            "Epoch: 174 | Iteration: 69 | Classification loss: 0.00015 | Regression loss: 0.01179 | Running loss: 0.02215\n",
            "Epoch: 174 | Iteration: 70 | Classification loss: 0.00051 | Regression loss: 0.05116 | Running loss: 0.02218\n",
            "Epoch: 174 | Iteration: 71 | Classification loss: 0.00019 | Regression loss: 0.03606 | Running loss: 0.02222\n",
            "Epoch: 174 | Iteration: 72 | Classification loss: 0.00005 | Regression loss: 0.01515 | Running loss: 0.02222\n",
            "Epoch: 174 | Iteration: 73 | Classification loss: 0.00040 | Regression loss: 0.03578 | Running loss: 0.02224\n",
            "Epoch: 174 | Iteration: 74 | Classification loss: 0.00003 | Regression loss: 0.00387 | Running loss: 0.02219\n",
            "Epoch: 174 | Iteration: 75 | Classification loss: 0.00015 | Regression loss: 0.01312 | Running loss: 0.02218\n",
            "Epoch: 174 | Iteration: 76 | Classification loss: 0.00025 | Regression loss: 0.03973 | Running loss: 0.02222\n",
            "Epoch: 174 | Iteration: 77 | Classification loss: 0.00026 | Regression loss: 0.02139 | Running loss: 0.02224\n",
            "Epoch: 174 | Iteration: 78 | Classification loss: 0.00011 | Regression loss: 0.01042 | Running loss: 0.02223\n",
            "Epoch: 174 | Iteration: 79 | Classification loss: 0.00012 | Regression loss: 0.01841 | Running loss: 0.02223\n",
            "Evaluating dataset\n",
            "\n",
            "mAP:\n",
            "person: 0.7218191972667742\n",
            "Precision:  0.5770750988142292\n",
            "Recall:  0.7891891891891892\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}, {0: (0.7229906021615096, 185.0)}, {0: (0.7259378939307871, 185.0)}, {0: (0.7306172160652471, 185.0)}, {0: (0.7210590702449308, 185.0)}, {0: (0.7214498931770392, 185.0)}, {0: (0.7272071618498421, 185.0)}, {0: (0.725966639522496, 185.0)}, {0: (0.7208028358685962, 185.0)}, {0: (0.7255636845600553, 185.0)}, {0: (0.7261742190616329, 185.0)}, {0: (0.7217332811149212, 185.0)}, {0: (0.7261653370413381, 185.0)}, {0: (0.7215292823555803, 185.0)}, {0: (0.7277253912015531, 185.0)}, {0: (0.7219177066363771, 185.0)}, {0: (0.7274783215794732, 185.0)}, {0: (0.7218983427777539, 185.0)}, {0: (0.7270037695200344, 185.0)}, {0: (0.7272851879977948, 185.0)}, {0: (0.7221052381441093, 185.0)}, {0: (0.7218191972667742, 185.0)}]\n",
            "Epoch: 175 | Iteration: 0 | Classification loss: 0.00056 | Regression loss: 0.03448 | Running loss: 0.02221\n",
            "Epoch: 175 | Iteration: 1 | Classification loss: 0.00016 | Regression loss: 0.01763 | Running loss: 0.02220\n",
            "Epoch: 175 | Iteration: 2 | Classification loss: 0.00013 | Regression loss: 0.02007 | Running loss: 0.02220\n",
            "Epoch: 175 | Iteration: 3 | Classification loss: 0.00016 | Regression loss: 0.01605 | Running loss: 0.02217\n",
            "Epoch: 175 | Iteration: 4 | Classification loss: 0.00005 | Regression loss: 0.01484 | Running loss: 0.02215\n",
            "Epoch: 175 | Iteration: 5 | Classification loss: 0.00018 | Regression loss: 0.01813 | Running loss: 0.02214\n",
            "Epoch: 175 | Iteration: 6 | Classification loss: 0.00019 | Regression loss: 0.01705 | Running loss: 0.02213\n",
            "Epoch: 175 | Iteration: 7 | Classification loss: 0.00009 | Regression loss: 0.02119 | Running loss: 0.02210\n",
            "Epoch: 175 | Iteration: 8 | Classification loss: 0.00009 | Regression loss: 0.02093 | Running loss: 0.02210\n",
            "Epoch: 175 | Iteration: 9 | Classification loss: 0.00042 | Regression loss: 0.02138 | Running loss: 0.02208\n",
            "Epoch: 175 | Iteration: 10 | Classification loss: 0.00022 | Regression loss: 0.03176 | Running loss: 0.02211\n",
            "Epoch: 175 | Iteration: 11 | Classification loss: 0.00022 | Regression loss: 0.01381 | Running loss: 0.02212\n",
            "Epoch: 175 | Iteration: 12 | Classification loss: 0.00006 | Regression loss: 0.02691 | Running loss: 0.02214\n",
            "Epoch: 175 | Iteration: 13 | Classification loss: 0.00017 | Regression loss: 0.02286 | Running loss: 0.02213\n",
            "Epoch: 175 | Iteration: 14 | Classification loss: 0.00041 | Regression loss: 0.02923 | Running loss: 0.02215\n",
            "Epoch: 175 | Iteration: 15 | Classification loss: 0.00024 | Regression loss: 0.03111 | Running loss: 0.02220\n",
            "Epoch: 175 | Iteration: 16 | Classification loss: 0.00008 | Regression loss: 0.02613 | Running loss: 0.02224\n",
            "Epoch: 175 | Iteration: 17 | Classification loss: 0.00012 | Regression loss: 0.02982 | Running loss: 0.02227\n",
            "Epoch: 175 | Iteration: 18 | Classification loss: 0.00008 | Regression loss: 0.01404 | Running loss: 0.02226\n",
            "Epoch: 175 | Iteration: 19 | Classification loss: 0.00061 | Regression loss: 0.03653 | Running loss: 0.02227\n",
            "Epoch: 175 | Iteration: 20 | Classification loss: 0.00012 | Regression loss: 0.00988 | Running loss: 0.02223\n",
            "Epoch: 175 | Iteration: 21 | Classification loss: 0.00013 | Regression loss: 0.01418 | Running loss: 0.02221\n",
            "Epoch: 175 | Iteration: 22 | Classification loss: 0.00011 | Regression loss: 0.01044 | Running loss: 0.02219\n",
            "Epoch: 175 | Iteration: 23 | Classification loss: 0.00011 | Regression loss: 0.02161 | Running loss: 0.02219\n",
            "Epoch: 175 | Iteration: 24 | Classification loss: 0.00006 | Regression loss: 0.00806 | Running loss: 0.02216\n",
            "Epoch: 175 | Iteration: 25 | Classification loss: 0.00030 | Regression loss: 0.02519 | Running loss: 0.02213\n",
            "Epoch: 175 | Iteration: 26 | Classification loss: 0.00010 | Regression loss: 0.02519 | Running loss: 0.02216\n",
            "Epoch: 175 | Iteration: 27 | Classification loss: 0.00005 | Regression loss: 0.02245 | Running loss: 0.02217\n",
            "Epoch: 175 | Iteration: 28 | Classification loss: 0.00009 | Regression loss: 0.01262 | Running loss: 0.02212\n",
            "Epoch: 175 | Iteration: 29 | Classification loss: 0.00020 | Regression loss: 0.03761 | Running loss: 0.02215\n",
            "Epoch: 175 | Iteration: 30 | Classification loss: 0.00006 | Regression loss: 0.01826 | Running loss: 0.02211\n",
            "Epoch: 175 | Iteration: 31 | Classification loss: 0.00016 | Regression loss: 0.02366 | Running loss: 0.02209\n",
            "Epoch: 175 | Iteration: 32 | Classification loss: 0.00013 | Regression loss: 0.01325 | Running loss: 0.02210\n",
            "Epoch: 175 | Iteration: 33 | Classification loss: 0.00056 | Regression loss: 0.01869 | Running loss: 0.02205\n",
            "Epoch: 175 | Iteration: 34 | Classification loss: 0.00030 | Regression loss: 0.02088 | Running loss: 0.02207\n",
            "Epoch: 175 | Iteration: 35 | Classification loss: 0.00012 | Regression loss: 0.01778 | Running loss: 0.02207\n",
            "Epoch: 175 | Iteration: 36 | Classification loss: 0.00007 | Regression loss: 0.01681 | Running loss: 0.02209\n",
            "Epoch: 175 | Iteration: 37 | Classification loss: 0.00013 | Regression loss: 0.01299 | Running loss: 0.02208\n",
            "Epoch: 175 | Iteration: 38 | Classification loss: 0.00017 | Regression loss: 0.01487 | Running loss: 0.02205\n",
            "Epoch: 175 | Iteration: 39 | Classification loss: 0.00017 | Regression loss: 0.03450 | Running loss: 0.02208\n",
            "Epoch: 175 | Iteration: 40 | Classification loss: 0.00008 | Regression loss: 0.01448 | Running loss: 0.02207\n",
            "Epoch: 175 | Iteration: 41 | Classification loss: 0.00029 | Regression loss: 0.02521 | Running loss: 0.02209\n",
            "Epoch: 175 | Iteration: 42 | Classification loss: 0.00022 | Regression loss: 0.03165 | Running loss: 0.02212\n",
            "Epoch: 175 | Iteration: 43 | Classification loss: 0.00015 | Regression loss: 0.01649 | Running loss: 0.02212\n",
            "Epoch: 175 | Iteration: 44 | Classification loss: 0.00006 | Regression loss: 0.01288 | Running loss: 0.02211\n",
            "Epoch: 175 | Iteration: 45 | Classification loss: 0.00016 | Regression loss: 0.02398 | Running loss: 0.02210\n",
            "Epoch: 175 | Iteration: 46 | Classification loss: 0.00037 | Regression loss: 0.04187 | Running loss: 0.02213\n",
            "Epoch: 175 | Iteration: 47 | Classification loss: 0.00009 | Regression loss: 0.02181 | Running loss: 0.02213\n",
            "Epoch: 175 | Iteration: 48 | Classification loss: 0.00004 | Regression loss: 0.00734 | Running loss: 0.02209\n",
            "Epoch: 175 | Iteration: 49 | Classification loss: 0.00040 | Regression loss: 0.03437 | Running loss: 0.02212\n",
            "Epoch: 175 | Iteration: 50 | Classification loss: 0.00037 | Regression loss: 0.04144 | Running loss: 0.02214\n",
            "Epoch: 175 | Iteration: 51 | Classification loss: 0.00003 | Regression loss: 0.00906 | Running loss: 0.02212\n",
            "Epoch: 175 | Iteration: 52 | Classification loss: 0.00040 | Regression loss: 0.04136 | Running loss: 0.02218\n",
            "Epoch: 175 | Iteration: 53 | Classification loss: 0.00014 | Regression loss: 0.02418 | Running loss: 0.02218\n",
            "Epoch: 175 | Iteration: 54 | Classification loss: 0.00018 | Regression loss: 0.02009 | Running loss: 0.02219\n",
            "Epoch: 175 | Iteration: 55 | Classification loss: 0.00016 | Regression loss: 0.02047 | Running loss: 0.02220\n",
            "Epoch: 175 | Iteration: 56 | Classification loss: 0.00038 | Regression loss: 0.03663 | Running loss: 0.02222\n",
            "Epoch: 175 | Iteration: 57 | Classification loss: 0.00012 | Regression loss: 0.01733 | Running loss: 0.02221\n",
            "Epoch: 175 | Iteration: 58 | Classification loss: 0.00055 | Regression loss: 0.03644 | Running loss: 0.02224\n",
            "Epoch: 175 | Iteration: 59 | Classification loss: 0.00021 | Regression loss: 0.03227 | Running loss: 0.02223\n",
            "Epoch: 175 | Iteration: 60 | Classification loss: 0.00002 | Regression loss: 0.00433 | Running loss: 0.02221\n",
            "Epoch: 175 | Iteration: 61 | Classification loss: 0.00011 | Regression loss: 0.01666 | Running loss: 0.02221\n",
            "Epoch: 175 | Iteration: 62 | Classification loss: 0.00009 | Regression loss: 0.00964 | Running loss: 0.02220\n",
            "Epoch: 175 | Iteration: 63 | Classification loss: 0.00011 | Regression loss: 0.01634 | Running loss: 0.02220\n",
            "Epoch: 175 | Iteration: 64 | Classification loss: 0.00016 | Regression loss: 0.01928 | Running loss: 0.02223\n",
            "Epoch: 175 | Iteration: 65 | Classification loss: 0.00009 | Regression loss: 0.01618 | Running loss: 0.02220\n",
            "Epoch: 175 | Iteration: 66 | Classification loss: 0.00004 | Regression loss: 0.01512 | Running loss: 0.02219\n",
            "Epoch: 175 | Iteration: 67 | Classification loss: 0.00016 | Regression loss: 0.02209 | Running loss: 0.02222\n",
            "Epoch: 175 | Iteration: 68 | Classification loss: 0.00041 | Regression loss: 0.02189 | Running loss: 0.02221\n",
            "Epoch: 175 | Iteration: 69 | Classification loss: 0.00009 | Regression loss: 0.01234 | Running loss: 0.02220\n",
            "Epoch: 175 | Iteration: 70 | Classification loss: 0.00012 | Regression loss: 0.01870 | Running loss: 0.02219\n",
            "Epoch: 175 | Iteration: 71 | Classification loss: 0.00018 | Regression loss: 0.02455 | Running loss: 0.02216\n",
            "Epoch: 175 | Iteration: 72 | Classification loss: 0.00007 | Regression loss: 0.01135 | Running loss: 0.02216\n",
            "Epoch: 175 | Iteration: 73 | Classification loss: 0.00028 | Regression loss: 0.02917 | Running loss: 0.02219\n",
            "Epoch: 175 | Iteration: 74 | Classification loss: 0.00028 | Regression loss: 0.03014 | Running loss: 0.02218\n",
            "Epoch: 175 | Iteration: 75 | Classification loss: 0.00014 | Regression loss: 0.01431 | Running loss: 0.02212\n",
            "Epoch: 175 | Iteration: 76 | Classification loss: 0.00005 | Regression loss: 0.02008 | Running loss: 0.02213\n",
            "Epoch: 175 | Iteration: 77 | Classification loss: 0.00041 | Regression loss: 0.04898 | Running loss: 0.02221\n",
            "Epoch: 175 | Iteration: 78 | Classification loss: 0.00018 | Regression loss: 0.01716 | Running loss: 0.02221\n",
            "Epoch: 175 | Iteration: 79 | Classification loss: 0.00004 | Regression loss: 0.00773 | Running loss: 0.02217\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7259991707220859\n",
            "Precision:  0.5810276679841897\n",
            "Recall:  0.7945945945945946\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}, {0: (0.7229906021615096, 185.0)}, {0: (0.7259378939307871, 185.0)}, {0: (0.7306172160652471, 185.0)}, {0: (0.7210590702449308, 185.0)}, {0: (0.7214498931770392, 185.0)}, {0: (0.7272071618498421, 185.0)}, {0: (0.725966639522496, 185.0)}, {0: (0.7208028358685962, 185.0)}, {0: (0.7255636845600553, 185.0)}, {0: (0.7261742190616329, 185.0)}, {0: (0.7217332811149212, 185.0)}, {0: (0.7261653370413381, 185.0)}, {0: (0.7215292823555803, 185.0)}, {0: (0.7277253912015531, 185.0)}, {0: (0.7219177066363771, 185.0)}, {0: (0.7274783215794732, 185.0)}, {0: (0.7218983427777539, 185.0)}, {0: (0.7270037695200344, 185.0)}, {0: (0.7272851879977948, 185.0)}, {0: (0.7221052381441093, 185.0)}, {0: (0.7218191972667742, 185.0)}, {0: (0.7259991707220859, 185.0)}]\n",
            "Epoch: 176 | Iteration: 0 | Classification loss: 0.00005 | Regression loss: 0.02192 | Running loss: 0.02218\n",
            "Epoch: 176 | Iteration: 1 | Classification loss: 0.00023 | Regression loss: 0.02000 | Running loss: 0.02215\n",
            "Epoch: 176 | Iteration: 2 | Classification loss: 0.00008 | Regression loss: 0.01259 | Running loss: 0.02208\n",
            "Epoch: 176 | Iteration: 3 | Classification loss: 0.00008 | Regression loss: 0.02107 | Running loss: 0.02210\n",
            "Epoch: 176 | Iteration: 4 | Classification loss: 0.00038 | Regression loss: 0.03770 | Running loss: 0.02213\n",
            "Epoch: 176 | Iteration: 5 | Classification loss: 0.00052 | Regression loss: 0.03421 | Running loss: 0.02218\n",
            "Epoch: 176 | Iteration: 6 | Classification loss: 0.00044 | Regression loss: 0.02970 | Running loss: 0.02219\n",
            "Epoch: 176 | Iteration: 7 | Classification loss: 0.00016 | Regression loss: 0.02909 | Running loss: 0.02221\n",
            "Epoch: 176 | Iteration: 8 | Classification loss: 0.00042 | Regression loss: 0.03665 | Running loss: 0.02226\n",
            "Epoch: 176 | Iteration: 9 | Classification loss: 0.00011 | Regression loss: 0.01752 | Running loss: 0.02223\n",
            "Epoch: 176 | Iteration: 10 | Classification loss: 0.00013 | Regression loss: 0.01232 | Running loss: 0.02223\n",
            "Epoch: 176 | Iteration: 11 | Classification loss: 0.00003 | Regression loss: 0.01464 | Running loss: 0.02221\n",
            "Epoch: 176 | Iteration: 12 | Classification loss: 0.00020 | Regression loss: 0.01739 | Running loss: 0.02219\n",
            "Epoch: 176 | Iteration: 13 | Classification loss: 0.00019 | Regression loss: 0.02109 | Running loss: 0.02219\n",
            "Epoch: 176 | Iteration: 14 | Classification loss: 0.00007 | Regression loss: 0.02435 | Running loss: 0.02217\n",
            "Epoch: 176 | Iteration: 15 | Classification loss: 0.00010 | Regression loss: 0.01683 | Running loss: 0.02215\n",
            "Epoch: 176 | Iteration: 16 | Classification loss: 0.00014 | Regression loss: 0.03564 | Running loss: 0.02217\n",
            "Epoch: 176 | Iteration: 17 | Classification loss: 0.00006 | Regression loss: 0.01354 | Running loss: 0.02216\n",
            "Epoch: 176 | Iteration: 18 | Classification loss: 0.00018 | Regression loss: 0.02708 | Running loss: 0.02218\n",
            "Epoch: 176 | Iteration: 19 | Classification loss: 0.00020 | Regression loss: 0.01741 | Running loss: 0.02219\n",
            "Epoch: 176 | Iteration: 20 | Classification loss: 0.00012 | Regression loss: 0.01511 | Running loss: 0.02213\n",
            "Epoch: 176 | Iteration: 21 | Classification loss: 0.00013 | Regression loss: 0.02312 | Running loss: 0.02213\n",
            "Epoch: 176 | Iteration: 22 | Classification loss: 0.00007 | Regression loss: 0.00873 | Running loss: 0.02212\n",
            "Epoch: 176 | Iteration: 23 | Classification loss: 0.00010 | Regression loss: 0.01530 | Running loss: 0.02209\n",
            "Epoch: 176 | Iteration: 24 | Classification loss: 0.00051 | Regression loss: 0.01831 | Running loss: 0.02208\n",
            "Epoch: 176 | Iteration: 25 | Classification loss: 0.00009 | Regression loss: 0.01207 | Running loss: 0.02208\n",
            "Epoch: 176 | Iteration: 26 | Classification loss: 0.00010 | Regression loss: 0.01675 | Running loss: 0.02208\n",
            "Epoch: 176 | Iteration: 27 | Classification loss: 0.00009 | Regression loss: 0.02065 | Running loss: 0.02209\n",
            "Epoch: 176 | Iteration: 28 | Classification loss: 0.00022 | Regression loss: 0.01810 | Running loss: 0.02209\n",
            "Epoch: 176 | Iteration: 29 | Classification loss: 0.00012 | Regression loss: 0.02827 | Running loss: 0.02208\n",
            "Epoch: 176 | Iteration: 30 | Classification loss: 0.00008 | Regression loss: 0.02885 | Running loss: 0.02209\n",
            "Epoch: 176 | Iteration: 31 | Classification loss: 0.00005 | Regression loss: 0.01945 | Running loss: 0.02209\n",
            "Epoch: 176 | Iteration: 32 | Classification loss: 0.00013 | Regression loss: 0.01804 | Running loss: 0.02211\n",
            "Epoch: 176 | Iteration: 33 | Classification loss: 0.00013 | Regression loss: 0.01572 | Running loss: 0.02208\n",
            "Epoch: 176 | Iteration: 34 | Classification loss: 0.00036 | Regression loss: 0.04773 | Running loss: 0.02215\n",
            "Epoch: 176 | Iteration: 35 | Classification loss: 0.00014 | Regression loss: 0.02448 | Running loss: 0.02212\n",
            "Epoch: 176 | Iteration: 36 | Classification loss: 0.00009 | Regression loss: 0.00993 | Running loss: 0.02208\n",
            "Epoch: 176 | Iteration: 37 | Classification loss: 0.00011 | Regression loss: 0.01881 | Running loss: 0.02204\n",
            "Epoch: 176 | Iteration: 38 | Classification loss: 0.00014 | Regression loss: 0.01245 | Running loss: 0.02199\n",
            "Epoch: 176 | Iteration: 39 | Classification loss: 0.00063 | Regression loss: 0.03419 | Running loss: 0.02201\n",
            "Epoch: 176 | Iteration: 40 | Classification loss: 0.00010 | Regression loss: 0.01470 | Running loss: 0.02203\n",
            "Epoch: 176 | Iteration: 41 | Classification loss: 0.00005 | Regression loss: 0.01324 | Running loss: 0.02201\n",
            "Epoch: 176 | Iteration: 42 | Classification loss: 0.00039 | Regression loss: 0.03606 | Running loss: 0.02204\n",
            "Epoch: 176 | Iteration: 43 | Classification loss: 0.00006 | Regression loss: 0.00864 | Running loss: 0.02202\n",
            "Epoch: 176 | Iteration: 44 | Classification loss: 0.00041 | Regression loss: 0.01869 | Running loss: 0.02201\n",
            "Epoch: 176 | Iteration: 45 | Classification loss: 0.00014 | Regression loss: 0.01757 | Running loss: 0.02202\n",
            "Epoch: 176 | Iteration: 46 | Classification loss: 0.00024 | Regression loss: 0.02326 | Running loss: 0.02202\n",
            "Epoch: 176 | Iteration: 47 | Classification loss: 0.00006 | Regression loss: 0.01455 | Running loss: 0.02197\n",
            "Epoch: 176 | Iteration: 48 | Classification loss: 0.00019 | Regression loss: 0.01757 | Running loss: 0.02195\n",
            "Epoch: 176 | Iteration: 49 | Classification loss: 0.00009 | Regression loss: 0.02119 | Running loss: 0.02197\n",
            "Epoch: 176 | Iteration: 50 | Classification loss: 0.00003 | Regression loss: 0.01025 | Running loss: 0.02194\n",
            "Epoch: 176 | Iteration: 51 | Classification loss: 0.00007 | Regression loss: 0.01586 | Running loss: 0.02193\n",
            "Epoch: 176 | Iteration: 52 | Classification loss: 0.00030 | Regression loss: 0.02592 | Running loss: 0.02194\n",
            "Epoch: 176 | Iteration: 53 | Classification loss: 0.00025 | Regression loss: 0.03887 | Running loss: 0.02198\n",
            "Epoch: 176 | Iteration: 54 | Classification loss: 0.00030 | Regression loss: 0.02054 | Running loss: 0.02199\n",
            "Epoch: 176 | Iteration: 55 | Classification loss: 0.00023 | Regression loss: 0.02159 | Running loss: 0.02200\n",
            "Epoch: 176 | Iteration: 56 | Classification loss: 0.00016 | Regression loss: 0.03585 | Running loss: 0.02204\n",
            "Epoch: 176 | Iteration: 57 | Classification loss: 0.00006 | Regression loss: 0.00503 | Running loss: 0.02202\n",
            "Epoch: 176 | Iteration: 58 | Classification loss: 0.00052 | Regression loss: 0.03680 | Running loss: 0.02202\n",
            "Epoch: 176 | Iteration: 59 | Classification loss: 0.00036 | Regression loss: 0.03085 | Running loss: 0.02206\n",
            "Epoch: 176 | Iteration: 60 | Classification loss: 0.00018 | Regression loss: 0.02384 | Running loss: 0.02207\n",
            "Epoch: 176 | Iteration: 61 | Classification loss: 0.00011 | Regression loss: 0.01042 | Running loss: 0.02201\n",
            "Epoch: 176 | Iteration: 62 | Classification loss: 0.00010 | Regression loss: 0.02032 | Running loss: 0.02200\n",
            "Epoch: 176 | Iteration: 63 | Classification loss: 0.00010 | Regression loss: 0.01493 | Running loss: 0.02200\n",
            "Epoch: 176 | Iteration: 64 | Classification loss: 0.00004 | Regression loss: 0.00626 | Running loss: 0.02194\n",
            "Epoch: 176 | Iteration: 65 | Classification loss: 0.00040 | Regression loss: 0.02260 | Running loss: 0.02195\n",
            "Epoch: 176 | Iteration: 66 | Classification loss: 0.00019 | Regression loss: 0.02373 | Running loss: 0.02191\n",
            "Epoch: 176 | Iteration: 67 | Classification loss: 0.00028 | Regression loss: 0.02834 | Running loss: 0.02189\n",
            "Epoch: 176 | Iteration: 68 | Classification loss: 0.00024 | Regression loss: 0.02960 | Running loss: 0.02192\n",
            "Epoch: 176 | Iteration: 69 | Classification loss: 0.00009 | Regression loss: 0.01467 | Running loss: 0.02189\n",
            "Epoch: 176 | Iteration: 70 | Classification loss: 0.00005 | Regression loss: 0.02207 | Running loss: 0.02190\n",
            "Epoch: 176 | Iteration: 71 | Classification loss: 0.00006 | Regression loss: 0.02019 | Running loss: 0.02194\n",
            "Epoch: 176 | Iteration: 72 | Classification loss: 0.00022 | Regression loss: 0.01361 | Running loss: 0.02194\n",
            "Epoch: 176 | Iteration: 73 | Classification loss: 0.00011 | Regression loss: 0.01817 | Running loss: 0.02194\n",
            "Epoch: 176 | Iteration: 74 | Classification loss: 0.00016 | Regression loss: 0.02809 | Running loss: 0.02194\n",
            "Epoch: 176 | Iteration: 75 | Classification loss: 0.00002 | Regression loss: 0.00396 | Running loss: 0.02187\n",
            "Epoch: 176 | Iteration: 76 | Classification loss: 0.00013 | Regression loss: 0.01908 | Running loss: 0.02187\n",
            "Epoch: 176 | Iteration: 77 | Classification loss: 0.00025 | Regression loss: 0.03466 | Running loss: 0.02191\n",
            "Epoch: 176 | Iteration: 78 | Classification loss: 0.00014 | Regression loss: 0.01280 | Running loss: 0.02189\n",
            "Epoch: 176 | Iteration: 79 | Classification loss: 0.00035 | Regression loss: 0.04445 | Running loss: 0.02192\n",
            "Evaluating dataset\n",
            "\n",
            "mAP:\n",
            "person: 0.7281551087798303\n",
            "Precision:  0.5764705882352941\n",
            "Recall:  0.7945945945945946\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}, {0: (0.7229906021615096, 185.0)}, {0: (0.7259378939307871, 185.0)}, {0: (0.7306172160652471, 185.0)}, {0: (0.7210590702449308, 185.0)}, {0: (0.7214498931770392, 185.0)}, {0: (0.7272071618498421, 185.0)}, {0: (0.725966639522496, 185.0)}, {0: (0.7208028358685962, 185.0)}, {0: (0.7255636845600553, 185.0)}, {0: (0.7261742190616329, 185.0)}, {0: (0.7217332811149212, 185.0)}, {0: (0.7261653370413381, 185.0)}, {0: (0.7215292823555803, 185.0)}, {0: (0.7277253912015531, 185.0)}, {0: (0.7219177066363771, 185.0)}, {0: (0.7274783215794732, 185.0)}, {0: (0.7218983427777539, 185.0)}, {0: (0.7270037695200344, 185.0)}, {0: (0.7272851879977948, 185.0)}, {0: (0.7221052381441093, 185.0)}, {0: (0.7218191972667742, 185.0)}, {0: (0.7259991707220859, 185.0)}, {0: (0.7281551087798303, 185.0)}]\n",
            "Epoch: 177 | Iteration: 0 | Classification loss: 0.00010 | Regression loss: 0.01611 | Running loss: 0.02193\n",
            "Epoch: 177 | Iteration: 1 | Classification loss: 0.00007 | Regression loss: 0.01125 | Running loss: 0.02189\n",
            "Epoch: 177 | Iteration: 2 | Classification loss: 0.00015 | Regression loss: 0.01374 | Running loss: 0.02189\n",
            "Epoch: 177 | Iteration: 3 | Classification loss: 0.00004 | Regression loss: 0.01282 | Running loss: 0.02187\n",
            "Epoch: 177 | Iteration: 4 | Classification loss: 0.00037 | Regression loss: 0.03286 | Running loss: 0.02190\n",
            "Epoch: 177 | Iteration: 5 | Classification loss: 0.00018 | Regression loss: 0.02925 | Running loss: 0.02191\n",
            "Epoch: 177 | Iteration: 6 | Classification loss: 0.00010 | Regression loss: 0.02283 | Running loss: 0.02194\n",
            "Epoch: 177 | Iteration: 7 | Classification loss: 0.00010 | Regression loss: 0.02926 | Running loss: 0.02195\n",
            "Epoch: 177 | Iteration: 8 | Classification loss: 0.00016 | Regression loss: 0.01416 | Running loss: 0.02193\n",
            "Epoch: 177 | Iteration: 9 | Classification loss: 0.00006 | Regression loss: 0.02030 | Running loss: 0.02193\n",
            "Epoch: 177 | Iteration: 10 | Classification loss: 0.00019 | Regression loss: 0.03521 | Running loss: 0.02196\n",
            "Epoch: 177 | Iteration: 11 | Classification loss: 0.00011 | Regression loss: 0.02887 | Running loss: 0.02195\n",
            "Epoch: 177 | Iteration: 12 | Classification loss: 0.00018 | Regression loss: 0.02182 | Running loss: 0.02198\n",
            "Epoch: 177 | Iteration: 13 | Classification loss: 0.00013 | Regression loss: 0.01670 | Running loss: 0.02196\n",
            "Epoch: 177 | Iteration: 14 | Classification loss: 0.00014 | Regression loss: 0.00921 | Running loss: 0.02195\n",
            "Epoch: 177 | Iteration: 15 | Classification loss: 0.00008 | Regression loss: 0.01466 | Running loss: 0.02191\n",
            "Epoch: 177 | Iteration: 16 | Classification loss: 0.00026 | Regression loss: 0.03322 | Running loss: 0.02192\n",
            "Epoch: 177 | Iteration: 17 | Classification loss: 0.00011 | Regression loss: 0.01654 | Running loss: 0.02192\n",
            "Epoch: 177 | Iteration: 18 | Classification loss: 0.00016 | Regression loss: 0.01752 | Running loss: 0.02193\n",
            "Epoch: 177 | Iteration: 19 | Classification loss: 0.00014 | Regression loss: 0.02002 | Running loss: 0.02194\n",
            "Epoch: 177 | Iteration: 20 | Classification loss: 0.00042 | Regression loss: 0.03552 | Running loss: 0.02196\n",
            "Epoch: 177 | Iteration: 21 | Classification loss: 0.00048 | Regression loss: 0.03678 | Running loss: 0.02199\n",
            "Epoch: 177 | Iteration: 22 | Classification loss: 0.00006 | Regression loss: 0.01969 | Running loss: 0.02202\n",
            "Epoch: 177 | Iteration: 23 | Classification loss: 0.00010 | Regression loss: 0.01817 | Running loss: 0.02201\n",
            "Epoch: 177 | Iteration: 24 | Classification loss: 0.00011 | Regression loss: 0.01670 | Running loss: 0.02200\n",
            "Epoch: 177 | Iteration: 25 | Classification loss: 0.00005 | Regression loss: 0.01737 | Running loss: 0.02201\n",
            "Epoch: 177 | Iteration: 26 | Classification loss: 0.00041 | Regression loss: 0.03942 | Running loss: 0.02207\n",
            "Epoch: 177 | Iteration: 27 | Classification loss: 0.00005 | Regression loss: 0.00762 | Running loss: 0.02199\n",
            "Epoch: 177 | Iteration: 28 | Classification loss: 0.00028 | Regression loss: 0.02562 | Running loss: 0.02199\n",
            "Epoch: 177 | Iteration: 29 | Classification loss: 0.00008 | Regression loss: 0.01260 | Running loss: 0.02198\n",
            "Epoch: 177 | Iteration: 30 | Classification loss: 0.00010 | Regression loss: 0.02096 | Running loss: 0.02195\n",
            "Epoch: 177 | Iteration: 31 | Classification loss: 0.00013 | Regression loss: 0.02483 | Running loss: 0.02195\n",
            "Epoch: 177 | Iteration: 32 | Classification loss: 0.00037 | Regression loss: 0.03981 | Running loss: 0.02197\n",
            "Epoch: 177 | Iteration: 33 | Classification loss: 0.00015 | Regression loss: 0.02055 | Running loss: 0.02195\n",
            "Epoch: 177 | Iteration: 34 | Classification loss: 0.00007 | Regression loss: 0.00791 | Running loss: 0.02189\n",
            "Epoch: 177 | Iteration: 35 | Classification loss: 0.00040 | Regression loss: 0.02067 | Running loss: 0.02189\n",
            "Epoch: 177 | Iteration: 36 | Classification loss: 0.00065 | Regression loss: 0.03413 | Running loss: 0.02195\n",
            "Epoch: 177 | Iteration: 37 | Classification loss: 0.00017 | Regression loss: 0.01777 | Running loss: 0.02197\n",
            "Epoch: 177 | Iteration: 38 | Classification loss: 0.00012 | Regression loss: 0.01338 | Running loss: 0.02196\n",
            "Epoch: 177 | Iteration: 39 | Classification loss: 0.00005 | Regression loss: 0.02929 | Running loss: 0.02198\n",
            "Epoch: 177 | Iteration: 40 | Classification loss: 0.00011 | Regression loss: 0.02922 | Running loss: 0.02201\n",
            "Epoch: 177 | Iteration: 41 | Classification loss: 0.00024 | Regression loss: 0.03011 | Running loss: 0.02205\n",
            "Epoch: 177 | Iteration: 42 | Classification loss: 0.00010 | Regression loss: 0.00986 | Running loss: 0.02204\n",
            "Epoch: 177 | Iteration: 43 | Classification loss: 0.00007 | Regression loss: 0.02077 | Running loss: 0.02203\n",
            "Epoch: 177 | Iteration: 44 | Classification loss: 0.00007 | Regression loss: 0.02511 | Running loss: 0.02205\n",
            "Epoch: 177 | Iteration: 45 | Classification loss: 0.00004 | Regression loss: 0.02533 | Running loss: 0.02205\n",
            "Epoch: 177 | Iteration: 46 | Classification loss: 0.00013 | Regression loss: 0.01420 | Running loss: 0.02204\n",
            "Epoch: 177 | Iteration: 47 | Classification loss: 0.00009 | Regression loss: 0.00944 | Running loss: 0.02199\n",
            "Epoch: 177 | Iteration: 48 | Classification loss: 0.00040 | Regression loss: 0.04987 | Running loss: 0.02205\n",
            "Epoch: 177 | Iteration: 49 | Classification loss: 0.00011 | Regression loss: 0.01442 | Running loss: 0.02199\n",
            "Epoch: 177 | Iteration: 50 | Classification loss: 0.00002 | Regression loss: 0.00410 | Running loss: 0.02197\n",
            "Epoch: 177 | Iteration: 51 | Classification loss: 0.00024 | Regression loss: 0.02096 | Running loss: 0.02192\n",
            "Epoch: 177 | Iteration: 52 | Classification loss: 0.00027 | Regression loss: 0.02621 | Running loss: 0.02196\n",
            "Epoch: 177 | Iteration: 53 | Classification loss: 0.00019 | Regression loss: 0.01844 | Running loss: 0.02193\n",
            "Epoch: 177 | Iteration: 54 | Classification loss: 0.00007 | Regression loss: 0.01573 | Running loss: 0.02193\n",
            "Epoch: 177 | Iteration: 55 | Classification loss: 0.00051 | Regression loss: 0.01999 | Running loss: 0.02194\n",
            "Epoch: 177 | Iteration: 56 | Classification loss: 0.00039 | Regression loss: 0.04138 | Running loss: 0.02199\n",
            "Epoch: 177 | Iteration: 57 | Classification loss: 0.00005 | Regression loss: 0.00545 | Running loss: 0.02193\n",
            "Epoch: 177 | Iteration: 58 | Classification loss: 0.00022 | Regression loss: 0.03195 | Running loss: 0.02192\n",
            "Epoch: 177 | Iteration: 59 | Classification loss: 0.00005 | Regression loss: 0.01558 | Running loss: 0.02191\n",
            "Epoch: 177 | Iteration: 60 | Classification loss: 0.00002 | Regression loss: 0.00910 | Running loss: 0.02190\n",
            "Epoch: 177 | Iteration: 61 | Classification loss: 0.00027 | Regression loss: 0.02865 | Running loss: 0.02192\n",
            "Epoch: 177 | Iteration: 62 | Classification loss: 0.00017 | Regression loss: 0.03461 | Running loss: 0.02193\n",
            "Epoch: 177 | Iteration: 63 | Classification loss: 0.00014 | Regression loss: 0.01946 | Running loss: 0.02193\n",
            "Epoch: 177 | Iteration: 64 | Classification loss: 0.00048 | Regression loss: 0.02188 | Running loss: 0.02195\n",
            "Epoch: 177 | Iteration: 65 | Classification loss: 0.00012 | Regression loss: 0.01794 | Running loss: 0.02194\n",
            "Epoch: 177 | Iteration: 66 | Classification loss: 0.00009 | Regression loss: 0.01478 | Running loss: 0.02196\n",
            "Epoch: 177 | Iteration: 67 | Classification loss: 0.00027 | Regression loss: 0.02326 | Running loss: 0.02195\n",
            "Epoch: 177 | Iteration: 68 | Classification loss: 0.00057 | Regression loss: 0.03677 | Running loss: 0.02197\n",
            "Epoch: 177 | Iteration: 69 | Classification loss: 0.00009 | Regression loss: 0.01692 | Running loss: 0.02197\n",
            "Epoch: 177 | Iteration: 70 | Classification loss: 0.00013 | Regression loss: 0.01509 | Running loss: 0.02198\n",
            "Epoch: 177 | Iteration: 71 | Classification loss: 0.00003 | Regression loss: 0.00700 | Running loss: 0.02196\n",
            "Epoch: 177 | Iteration: 72 | Classification loss: 0.00005 | Regression loss: 0.01493 | Running loss: 0.02195\n",
            "Epoch: 177 | Iteration: 73 | Classification loss: 0.00019 | Regression loss: 0.03237 | Running loss: 0.02200\n",
            "Epoch: 177 | Iteration: 74 | Classification loss: 0.00017 | Regression loss: 0.01715 | Running loss: 0.02200\n",
            "Epoch: 177 | Iteration: 75 | Classification loss: 0.00014 | Regression loss: 0.01866 | Running loss: 0.02200\n",
            "Epoch: 177 | Iteration: 76 | Classification loss: 0.00016 | Regression loss: 0.02727 | Running loss: 0.02203\n",
            "Epoch: 177 | Iteration: 77 | Classification loss: 0.00015 | Regression loss: 0.01304 | Running loss: 0.02201\n",
            "Epoch: 177 | Iteration: 78 | Classification loss: 0.00019 | Regression loss: 0.02173 | Running loss: 0.02200\n",
            "Epoch: 177 | Iteration: 79 | Classification loss: 0.00031 | Regression loss: 0.03166 | Running loss: 0.02198\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.732505495958946\n",
            "Precision:  0.592\n",
            "Recall:  0.8\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}, {0: (0.7229906021615096, 185.0)}, {0: (0.7259378939307871, 185.0)}, {0: (0.7306172160652471, 185.0)}, {0: (0.7210590702449308, 185.0)}, {0: (0.7214498931770392, 185.0)}, {0: (0.7272071618498421, 185.0)}, {0: (0.725966639522496, 185.0)}, {0: (0.7208028358685962, 185.0)}, {0: (0.7255636845600553, 185.0)}, {0: (0.7261742190616329, 185.0)}, {0: (0.7217332811149212, 185.0)}, {0: (0.7261653370413381, 185.0)}, {0: (0.7215292823555803, 185.0)}, {0: (0.7277253912015531, 185.0)}, {0: (0.7219177066363771, 185.0)}, {0: (0.7274783215794732, 185.0)}, {0: (0.7218983427777539, 185.0)}, {0: (0.7270037695200344, 185.0)}, {0: (0.7272851879977948, 185.0)}, {0: (0.7221052381441093, 185.0)}, {0: (0.7218191972667742, 185.0)}, {0: (0.7259991707220859, 185.0)}, {0: (0.7281551087798303, 185.0)}, {0: (0.732505495958946, 185.0)}]\n",
            "Epoch: 178 | Iteration: 0 | Classification loss: 0.00011 | Regression loss: 0.02962 | Running loss: 0.02196\n",
            "Epoch: 178 | Iteration: 1 | Classification loss: 0.00005 | Regression loss: 0.01439 | Running loss: 0.02195\n",
            "Epoch: 178 | Iteration: 2 | Classification loss: 0.00004 | Regression loss: 0.00705 | Running loss: 0.02190\n",
            "Epoch: 178 | Iteration: 3 | Classification loss: 0.00003 | Regression loss: 0.01430 | Running loss: 0.02190\n",
            "Epoch: 178 | Iteration: 4 | Classification loss: 0.00006 | Regression loss: 0.01457 | Running loss: 0.02188\n",
            "Epoch: 178 | Iteration: 5 | Classification loss: 0.00011 | Regression loss: 0.02237 | Running loss: 0.02188\n",
            "Epoch: 178 | Iteration: 6 | Classification loss: 0.00021 | Regression loss: 0.02992 | Running loss: 0.02187\n",
            "Epoch: 178 | Iteration: 7 | Classification loss: 0.00013 | Regression loss: 0.01726 | Running loss: 0.02188\n",
            "Epoch: 178 | Iteration: 8 | Classification loss: 0.00010 | Regression loss: 0.02311 | Running loss: 0.02187\n",
            "Epoch: 178 | Iteration: 9 | Classification loss: 0.00042 | Regression loss: 0.03694 | Running loss: 0.02189\n",
            "Epoch: 178 | Iteration: 10 | Classification loss: 0.00014 | Regression loss: 0.00891 | Running loss: 0.02186\n",
            "Epoch: 178 | Iteration: 11 | Classification loss: 0.00052 | Regression loss: 0.03308 | Running loss: 0.02187\n",
            "Epoch: 178 | Iteration: 12 | Classification loss: 0.00013 | Regression loss: 0.02227 | Running loss: 0.02189\n",
            "Epoch: 178 | Iteration: 13 | Classification loss: 0.00006 | Regression loss: 0.02065 | Running loss: 0.02189\n",
            "Epoch: 178 | Iteration: 14 | Classification loss: 0.00040 | Regression loss: 0.02265 | Running loss: 0.02191\n",
            "Epoch: 178 | Iteration: 15 | Classification loss: 0.00009 | Regression loss: 0.02921 | Running loss: 0.02193\n",
            "Epoch: 178 | Iteration: 16 | Classification loss: 0.00008 | Regression loss: 0.01413 | Running loss: 0.02191\n",
            "Epoch: 178 | Iteration: 17 | Classification loss: 0.00040 | Regression loss: 0.02217 | Running loss: 0.02187\n",
            "Epoch: 178 | Iteration: 18 | Classification loss: 0.00008 | Regression loss: 0.01690 | Running loss: 0.02185\n",
            "Epoch: 178 | Iteration: 19 | Classification loss: 0.00010 | Regression loss: 0.01594 | Running loss: 0.02184\n",
            "Epoch: 178 | Iteration: 20 | Classification loss: 0.00011 | Regression loss: 0.01410 | Running loss: 0.02184\n",
            "Epoch: 178 | Iteration: 21 | Classification loss: 0.00036 | Regression loss: 0.02221 | Running loss: 0.02180\n",
            "Epoch: 178 | Iteration: 22 | Classification loss: 0.00011 | Regression loss: 0.01160 | Running loss: 0.02179\n",
            "Epoch: 178 | Iteration: 23 | Classification loss: 0.00012 | Regression loss: 0.01764 | Running loss: 0.02178\n",
            "Epoch: 178 | Iteration: 24 | Classification loss: 0.00005 | Regression loss: 0.00826 | Running loss: 0.02178\n",
            "Epoch: 178 | Iteration: 25 | Classification loss: 0.00015 | Regression loss: 0.01385 | Running loss: 0.02178\n",
            "Epoch: 178 | Iteration: 26 | Classification loss: 0.00005 | Regression loss: 0.01287 | Running loss: 0.02173\n",
            "Epoch: 178 | Iteration: 27 | Classification loss: 0.00009 | Regression loss: 0.00994 | Running loss: 0.02166\n",
            "Epoch: 178 | Iteration: 28 | Classification loss: 0.00023 | Regression loss: 0.02852 | Running loss: 0.02168\n",
            "Epoch: 178 | Iteration: 29 | Classification loss: 0.00033 | Regression loss: 0.03993 | Running loss: 0.02173\n",
            "Epoch: 178 | Iteration: 30 | Classification loss: 0.00025 | Regression loss: 0.02872 | Running loss: 0.02177\n",
            "Epoch: 178 | Iteration: 31 | Classification loss: 0.00013 | Regression loss: 0.01391 | Running loss: 0.02177\n",
            "Epoch: 178 | Iteration: 32 | Classification loss: 0.00038 | Regression loss: 0.03745 | Running loss: 0.02180\n",
            "Epoch: 178 | Iteration: 33 | Classification loss: 0.00006 | Regression loss: 0.01812 | Running loss: 0.02178\n",
            "Epoch: 178 | Iteration: 34 | Classification loss: 0.00015 | Regression loss: 0.01544 | Running loss: 0.02175\n",
            "Epoch: 178 | Iteration: 35 | Classification loss: 0.00012 | Regression loss: 0.01663 | Running loss: 0.02174\n",
            "Epoch: 178 | Iteration: 36 | Classification loss: 0.00002 | Regression loss: 0.00384 | Running loss: 0.02174\n",
            "Epoch: 178 | Iteration: 37 | Classification loss: 0.00020 | Regression loss: 0.02708 | Running loss: 0.02176\n",
            "Epoch: 178 | Iteration: 38 | Classification loss: 0.00033 | Regression loss: 0.04192 | Running loss: 0.02178\n",
            "Epoch: 178 | Iteration: 39 | Classification loss: 0.00029 | Regression loss: 0.02457 | Running loss: 0.02179\n",
            "Epoch: 178 | Iteration: 40 | Classification loss: 0.00015 | Regression loss: 0.03705 | Running loss: 0.02179\n",
            "Epoch: 178 | Iteration: 41 | Classification loss: 0.00060 | Regression loss: 0.03458 | Running loss: 0.02181\n",
            "Epoch: 178 | Iteration: 42 | Classification loss: 0.00029 | Regression loss: 0.02344 | Running loss: 0.02178\n",
            "Epoch: 178 | Iteration: 43 | Classification loss: 0.00017 | Regression loss: 0.01728 | Running loss: 0.02177\n",
            "Epoch: 178 | Iteration: 44 | Classification loss: 0.00011 | Regression loss: 0.00994 | Running loss: 0.02175\n",
            "Epoch: 178 | Iteration: 45 | Classification loss: 0.00020 | Regression loss: 0.01317 | Running loss: 0.02176\n",
            "Epoch: 178 | Iteration: 46 | Classification loss: 0.00009 | Regression loss: 0.02360 | Running loss: 0.02179\n",
            "Epoch: 178 | Iteration: 47 | Classification loss: 0.00005 | Regression loss: 0.01850 | Running loss: 0.02176\n",
            "Epoch: 178 | Iteration: 48 | Classification loss: 0.00021 | Regression loss: 0.01981 | Running loss: 0.02177\n",
            "Epoch: 178 | Iteration: 49 | Classification loss: 0.00016 | Regression loss: 0.02501 | Running loss: 0.02179\n",
            "Epoch: 178 | Iteration: 50 | Classification loss: 0.00017 | Regression loss: 0.01900 | Running loss: 0.02177\n",
            "Epoch: 178 | Iteration: 51 | Classification loss: 0.00043 | Regression loss: 0.03268 | Running loss: 0.02178\n",
            "Epoch: 178 | Iteration: 52 | Classification loss: 0.00008 | Regression loss: 0.01380 | Running loss: 0.02178\n",
            "Epoch: 178 | Iteration: 53 | Classification loss: 0.00038 | Regression loss: 0.03178 | Running loss: 0.02182\n",
            "Epoch: 178 | Iteration: 54 | Classification loss: 0.00006 | Regression loss: 0.02141 | Running loss: 0.02182\n",
            "Epoch: 178 | Iteration: 55 | Classification loss: 0.00013 | Regression loss: 0.02389 | Running loss: 0.02179\n",
            "Epoch: 178 | Iteration: 56 | Classification loss: 0.00007 | Regression loss: 0.00937 | Running loss: 0.02177\n",
            "Epoch: 178 | Iteration: 57 | Classification loss: 0.00006 | Regression loss: 0.02949 | Running loss: 0.02178\n",
            "Epoch: 178 | Iteration: 58 | Classification loss: 0.00015 | Regression loss: 0.02373 | Running loss: 0.02180\n",
            "Epoch: 178 | Iteration: 59 | Classification loss: 0.00021 | Regression loss: 0.02449 | Running loss: 0.02178\n",
            "Epoch: 178 | Iteration: 60 | Classification loss: 0.00008 | Regression loss: 0.01434 | Running loss: 0.02177\n",
            "Epoch: 178 | Iteration: 61 | Classification loss: 0.00009 | Regression loss: 0.01616 | Running loss: 0.02177\n",
            "Epoch: 178 | Iteration: 62 | Classification loss: 0.00013 | Regression loss: 0.01978 | Running loss: 0.02176\n",
            "Epoch: 178 | Iteration: 63 | Classification loss: 0.00010 | Regression loss: 0.01619 | Running loss: 0.02173\n",
            "Epoch: 178 | Iteration: 64 | Classification loss: 0.00013 | Regression loss: 0.01221 | Running loss: 0.02169\n",
            "Epoch: 178 | Iteration: 65 | Classification loss: 0.00015 | Regression loss: 0.01484 | Running loss: 0.02170\n",
            "Epoch: 178 | Iteration: 66 | Classification loss: 0.00033 | Regression loss: 0.04775 | Running loss: 0.02176\n",
            "Epoch: 178 | Iteration: 67 | Classification loss: 0.00019 | Regression loss: 0.03466 | Running loss: 0.02180\n",
            "Epoch: 178 | Iteration: 68 | Classification loss: 0.00033 | Regression loss: 0.03087 | Running loss: 0.02182\n",
            "Epoch: 178 | Iteration: 69 | Classification loss: 0.00017 | Regression loss: 0.01799 | Running loss: 0.02182\n",
            "Epoch: 178 | Iteration: 70 | Classification loss: 0.00005 | Regression loss: 0.00855 | Running loss: 0.02178\n",
            "Epoch: 178 | Iteration: 71 | Classification loss: 0.00012 | Regression loss: 0.01668 | Running loss: 0.02178\n",
            "Epoch: 178 | Iteration: 72 | Classification loss: 0.00020 | Regression loss: 0.03391 | Running loss: 0.02182\n",
            "Epoch: 178 | Iteration: 73 | Classification loss: 0.00008 | Regression loss: 0.02082 | Running loss: 0.02183\n",
            "Epoch: 178 | Iteration: 74 | Classification loss: 0.00020 | Regression loss: 0.01702 | Running loss: 0.02180\n",
            "Epoch: 178 | Iteration: 75 | Classification loss: 0.00051 | Regression loss: 0.03629 | Running loss: 0.02179\n",
            "Epoch: 178 | Iteration: 76 | Classification loss: 0.00015 | Regression loss: 0.02483 | Running loss: 0.02181\n",
            "Epoch: 178 | Iteration: 77 | Classification loss: 0.00002 | Regression loss: 0.00900 | Running loss: 0.02182\n",
            "Epoch: 178 | Iteration: 78 | Classification loss: 0.00021 | Regression loss: 0.01934 | Running loss: 0.02179\n",
            "Epoch: 178 | Iteration: 79 | Classification loss: 0.00058 | Regression loss: 0.02290 | Running loss: 0.02180\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7266266562021302\n",
            "Precision:  0.5833333333333334\n",
            "Recall:  0.7945945945945946\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}, {0: (0.7229906021615096, 185.0)}, {0: (0.7259378939307871, 185.0)}, {0: (0.7306172160652471, 185.0)}, {0: (0.7210590702449308, 185.0)}, {0: (0.7214498931770392, 185.0)}, {0: (0.7272071618498421, 185.0)}, {0: (0.725966639522496, 185.0)}, {0: (0.7208028358685962, 185.0)}, {0: (0.7255636845600553, 185.0)}, {0: (0.7261742190616329, 185.0)}, {0: (0.7217332811149212, 185.0)}, {0: (0.7261653370413381, 185.0)}, {0: (0.7215292823555803, 185.0)}, {0: (0.7277253912015531, 185.0)}, {0: (0.7219177066363771, 185.0)}, {0: (0.7274783215794732, 185.0)}, {0: (0.7218983427777539, 185.0)}, {0: (0.7270037695200344, 185.0)}, {0: (0.7272851879977948, 185.0)}, {0: (0.7221052381441093, 185.0)}, {0: (0.7218191972667742, 185.0)}, {0: (0.7259991707220859, 185.0)}, {0: (0.7281551087798303, 185.0)}, {0: (0.732505495958946, 185.0)}, {0: (0.7266266562021302, 185.0)}]\n",
            "Epoch: 179 | Iteration: 0 | Classification loss: 0.00010 | Regression loss: 0.02096 | Running loss: 0.02180\n",
            "Epoch: 179 | Iteration: 1 | Classification loss: 0.00008 | Regression loss: 0.02134 | Running loss: 0.02180\n",
            "Epoch: 179 | Iteration: 2 | Classification loss: 0.00022 | Regression loss: 0.03940 | Running loss: 0.02177\n",
            "Epoch: 179 | Iteration: 3 | Classification loss: 0.00030 | Regression loss: 0.02998 | Running loss: 0.02178\n",
            "Epoch: 179 | Iteration: 4 | Classification loss: 0.00008 | Regression loss: 0.00780 | Running loss: 0.02172\n",
            "Epoch: 179 | Iteration: 5 | Classification loss: 0.00018 | Regression loss: 0.03132 | Running loss: 0.02177\n",
            "Epoch: 179 | Iteration: 6 | Classification loss: 0.00018 | Regression loss: 0.01768 | Running loss: 0.02174\n",
            "Epoch: 179 | Iteration: 7 | Classification loss: 0.00011 | Regression loss: 0.01575 | Running loss: 0.02173\n",
            "Epoch: 179 | Iteration: 8 | Classification loss: 0.00013 | Regression loss: 0.01695 | Running loss: 0.02173\n",
            "Epoch: 179 | Iteration: 9 | Classification loss: 0.00009 | Regression loss: 0.01012 | Running loss: 0.02167\n",
            "Epoch: 179 | Iteration: 10 | Classification loss: 0.00006 | Regression loss: 0.01669 | Running loss: 0.02166\n",
            "Epoch: 179 | Iteration: 11 | Classification loss: 0.00026 | Regression loss: 0.03017 | Running loss: 0.02169\n",
            "Epoch: 179 | Iteration: 12 | Classification loss: 0.00036 | Regression loss: 0.02075 | Running loss: 0.02170\n",
            "Epoch: 179 | Iteration: 13 | Classification loss: 0.00018 | Regression loss: 0.03024 | Running loss: 0.02171\n",
            "Epoch: 179 | Iteration: 14 | Classification loss: 0.00015 | Regression loss: 0.02096 | Running loss: 0.02171\n",
            "Epoch: 179 | Iteration: 15 | Classification loss: 0.00046 | Regression loss: 0.01786 | Running loss: 0.02170\n",
            "Epoch: 179 | Iteration: 16 | Classification loss: 0.00034 | Regression loss: 0.03892 | Running loss: 0.02174\n",
            "Epoch: 179 | Iteration: 17 | Classification loss: 0.00004 | Regression loss: 0.02159 | Running loss: 0.02174\n",
            "Epoch: 179 | Iteration: 18 | Classification loss: 0.00003 | Regression loss: 0.00399 | Running loss: 0.02172\n",
            "Epoch: 179 | Iteration: 19 | Classification loss: 0.00004 | Regression loss: 0.00716 | Running loss: 0.02169\n",
            "Epoch: 179 | Iteration: 20 | Classification loss: 0.00006 | Regression loss: 0.02053 | Running loss: 0.02168\n",
            "Epoch: 179 | Iteration: 21 | Classification loss: 0.00006 | Regression loss: 0.01407 | Running loss: 0.02168\n",
            "Epoch: 179 | Iteration: 22 | Classification loss: 0.00008 | Regression loss: 0.01346 | Running loss: 0.02168\n",
            "Epoch: 179 | Iteration: 23 | Classification loss: 0.00005 | Regression loss: 0.02775 | Running loss: 0.02171\n",
            "Epoch: 179 | Iteration: 24 | Classification loss: 0.00011 | Regression loss: 0.01355 | Running loss: 0.02168\n",
            "Epoch: 179 | Iteration: 25 | Classification loss: 0.00010 | Regression loss: 0.01438 | Running loss: 0.02168\n",
            "Epoch: 179 | Iteration: 26 | Classification loss: 0.00026 | Regression loss: 0.02544 | Running loss: 0.02166\n",
            "Epoch: 179 | Iteration: 27 | Classification loss: 0.00030 | Regression loss: 0.03764 | Running loss: 0.02170\n",
            "Epoch: 179 | Iteration: 28 | Classification loss: 0.00054 | Regression loss: 0.03396 | Running loss: 0.02171\n",
            "Epoch: 179 | Iteration: 29 | Classification loss: 0.00008 | Regression loss: 0.01593 | Running loss: 0.02169\n",
            "Epoch: 179 | Iteration: 30 | Classification loss: 0.00018 | Regression loss: 0.03806 | Running loss: 0.02172\n",
            "Epoch: 179 | Iteration: 31 | Classification loss: 0.00038 | Regression loss: 0.03638 | Running loss: 0.02174\n",
            "Epoch: 179 | Iteration: 32 | Classification loss: 0.00009 | Regression loss: 0.02565 | Running loss: 0.02172\n",
            "Epoch: 179 | Iteration: 33 | Classification loss: 0.00014 | Regression loss: 0.02465 | Running loss: 0.02168\n",
            "Epoch: 179 | Iteration: 34 | Classification loss: 0.00015 | Regression loss: 0.01347 | Running loss: 0.02166\n",
            "Epoch: 179 | Iteration: 35 | Classification loss: 0.00002 | Regression loss: 0.00986 | Running loss: 0.02167\n",
            "Epoch: 179 | Iteration: 36 | Classification loss: 0.00012 | Regression loss: 0.01767 | Running loss: 0.02166\n",
            "Epoch: 179 | Iteration: 37 | Classification loss: 0.00017 | Regression loss: 0.01326 | Running loss: 0.02165\n",
            "Epoch: 179 | Iteration: 38 | Classification loss: 0.00041 | Regression loss: 0.01826 | Running loss: 0.02161\n",
            "Epoch: 179 | Iteration: 39 | Classification loss: 0.00005 | Regression loss: 0.00816 | Running loss: 0.02162\n",
            "Epoch: 179 | Iteration: 40 | Classification loss: 0.00013 | Regression loss: 0.01139 | Running loss: 0.02161\n",
            "Epoch: 179 | Iteration: 41 | Classification loss: 0.00011 | Regression loss: 0.01692 | Running loss: 0.02164\n",
            "Epoch: 179 | Iteration: 42 | Classification loss: 0.00014 | Regression loss: 0.01931 | Running loss: 0.02161\n",
            "Epoch: 179 | Iteration: 43 | Classification loss: 0.00017 | Regression loss: 0.02324 | Running loss: 0.02158\n",
            "Epoch: 179 | Iteration: 44 | Classification loss: 0.00008 | Regression loss: 0.01430 | Running loss: 0.02155\n",
            "Epoch: 179 | Iteration: 45 | Classification loss: 0.00017 | Regression loss: 0.03218 | Running loss: 0.02159\n",
            "Epoch: 179 | Iteration: 46 | Classification loss: 0.00020 | Regression loss: 0.01959 | Running loss: 0.02160\n",
            "Epoch: 179 | Iteration: 47 | Classification loss: 0.00009 | Regression loss: 0.03026 | Running loss: 0.02164\n",
            "Epoch: 179 | Iteration: 48 | Classification loss: 0.00055 | Regression loss: 0.03359 | Running loss: 0.02166\n",
            "Epoch: 179 | Iteration: 49 | Classification loss: 0.00009 | Regression loss: 0.00900 | Running loss: 0.02166\n",
            "Epoch: 179 | Iteration: 50 | Classification loss: 0.00031 | Regression loss: 0.04773 | Running loss: 0.02172\n",
            "Epoch: 179 | Iteration: 51 | Classification loss: 0.00015 | Regression loss: 0.03641 | Running loss: 0.02176\n",
            "Epoch: 179 | Iteration: 52 | Classification loss: 0.00006 | Regression loss: 0.01456 | Running loss: 0.02175\n",
            "Epoch: 179 | Iteration: 53 | Classification loss: 0.00020 | Regression loss: 0.02264 | Running loss: 0.02175\n",
            "Epoch: 179 | Iteration: 54 | Classification loss: 0.00005 | Regression loss: 0.01843 | Running loss: 0.02175\n",
            "Epoch: 179 | Iteration: 55 | Classification loss: 0.00004 | Regression loss: 0.01653 | Running loss: 0.02176\n",
            "Epoch: 179 | Iteration: 56 | Classification loss: 0.00007 | Regression loss: 0.01437 | Running loss: 0.02176\n",
            "Epoch: 179 | Iteration: 57 | Classification loss: 0.00028 | Regression loss: 0.03938 | Running loss: 0.02182\n",
            "Epoch: 179 | Iteration: 58 | Classification loss: 0.00010 | Regression loss: 0.02306 | Running loss: 0.02181\n",
            "Epoch: 179 | Iteration: 59 | Classification loss: 0.00012 | Regression loss: 0.01526 | Running loss: 0.02181\n",
            "Epoch: 179 | Iteration: 60 | Classification loss: 0.00014 | Regression loss: 0.01906 | Running loss: 0.02181\n",
            "Epoch: 179 | Iteration: 61 | Classification loss: 0.00023 | Regression loss: 0.02494 | Running loss: 0.02182\n",
            "Epoch: 179 | Iteration: 62 | Classification loss: 0.00005 | Regression loss: 0.01474 | Running loss: 0.02180\n",
            "Epoch: 179 | Iteration: 63 | Classification loss: 0.00008 | Regression loss: 0.01750 | Running loss: 0.02180\n",
            "Epoch: 179 | Iteration: 64 | Classification loss: 0.00017 | Regression loss: 0.01756 | Running loss: 0.02178\n",
            "Epoch: 179 | Iteration: 65 | Classification loss: 0.00046 | Regression loss: 0.02157 | Running loss: 0.02176\n",
            "Epoch: 179 | Iteration: 66 | Classification loss: 0.00011 | Regression loss: 0.01773 | Running loss: 0.02176\n",
            "Epoch: 179 | Iteration: 67 | Classification loss: 0.00014 | Regression loss: 0.01997 | Running loss: 0.02172\n",
            "Epoch: 179 | Iteration: 68 | Classification loss: 0.00009 | Regression loss: 0.02982 | Running loss: 0.02174\n",
            "Epoch: 179 | Iteration: 69 | Classification loss: 0.00008 | Regression loss: 0.01391 | Running loss: 0.02172\n",
            "Epoch: 179 | Iteration: 70 | Classification loss: 0.00033 | Regression loss: 0.02833 | Running loss: 0.02176\n",
            "Epoch: 179 | Iteration: 71 | Classification loss: 0.00019 | Regression loss: 0.01888 | Running loss: 0.02173\n",
            "Epoch: 179 | Iteration: 72 | Classification loss: 0.00071 | Regression loss: 0.03886 | Running loss: 0.02178\n",
            "Epoch: 179 | Iteration: 73 | Classification loss: 0.00015 | Regression loss: 0.01442 | Running loss: 0.02174\n",
            "Epoch: 179 | Iteration: 74 | Classification loss: 0.00014 | Regression loss: 0.02898 | Running loss: 0.02177\n",
            "Epoch: 179 | Iteration: 75 | Classification loss: 0.00015 | Regression loss: 0.00880 | Running loss: 0.02170\n",
            "Epoch: 179 | Iteration: 76 | Classification loss: 0.00005 | Regression loss: 0.00540 | Running loss: 0.02166\n",
            "Epoch: 179 | Iteration: 77 | Classification loss: 0.00014 | Regression loss: 0.01674 | Running loss: 0.02163\n",
            "Epoch: 179 | Iteration: 78 | Classification loss: 0.00032 | Regression loss: 0.03179 | Running loss: 0.02159\n",
            "Epoch: 179 | Iteration: 79 | Classification loss: 0.00016 | Regression loss: 0.02512 | Running loss: 0.02160\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7320831049102212\n",
            "Precision:  0.5826771653543307\n",
            "Recall:  0.8\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}, {0: (0.7229906021615096, 185.0)}, {0: (0.7259378939307871, 185.0)}, {0: (0.7306172160652471, 185.0)}, {0: (0.7210590702449308, 185.0)}, {0: (0.7214498931770392, 185.0)}, {0: (0.7272071618498421, 185.0)}, {0: (0.725966639522496, 185.0)}, {0: (0.7208028358685962, 185.0)}, {0: (0.7255636845600553, 185.0)}, {0: (0.7261742190616329, 185.0)}, {0: (0.7217332811149212, 185.0)}, {0: (0.7261653370413381, 185.0)}, {0: (0.7215292823555803, 185.0)}, {0: (0.7277253912015531, 185.0)}, {0: (0.7219177066363771, 185.0)}, {0: (0.7274783215794732, 185.0)}, {0: (0.7218983427777539, 185.0)}, {0: (0.7270037695200344, 185.0)}, {0: (0.7272851879977948, 185.0)}, {0: (0.7221052381441093, 185.0)}, {0: (0.7218191972667742, 185.0)}, {0: (0.7259991707220859, 185.0)}, {0: (0.7281551087798303, 185.0)}, {0: (0.732505495958946, 185.0)}, {0: (0.7266266562021302, 185.0)}, {0: (0.7320831049102212, 185.0)}]\n",
            "Epoch: 180 | Iteration: 0 | Classification loss: 0.00018 | Regression loss: 0.01743 | Running loss: 0.02158\n",
            "Epoch: 180 | Iteration: 1 | Classification loss: 0.00024 | Regression loss: 0.03055 | Running loss: 0.02160\n",
            "Epoch: 180 | Iteration: 2 | Classification loss: 0.00023 | Regression loss: 0.02812 | Running loss: 0.02160\n",
            "Epoch: 180 | Iteration: 3 | Classification loss: 0.00016 | Regression loss: 0.02786 | Running loss: 0.02162\n",
            "Epoch: 180 | Iteration: 4 | Classification loss: 0.00014 | Regression loss: 0.03570 | Running loss: 0.02166\n",
            "Epoch: 180 | Iteration: 5 | Classification loss: 0.00044 | Regression loss: 0.01784 | Running loss: 0.02164\n",
            "Epoch: 180 | Iteration: 6 | Classification loss: 0.00007 | Regression loss: 0.01379 | Running loss: 0.02163\n",
            "Epoch: 180 | Iteration: 7 | Classification loss: 0.00005 | Regression loss: 0.01123 | Running loss: 0.02163\n",
            "Epoch: 180 | Iteration: 8 | Classification loss: 0.00013 | Regression loss: 0.01646 | Running loss: 0.02159\n",
            "Epoch: 180 | Iteration: 9 | Classification loss: 0.00006 | Regression loss: 0.02914 | Running loss: 0.02161\n",
            "Epoch: 180 | Iteration: 10 | Classification loss: 0.00038 | Regression loss: 0.03771 | Running loss: 0.02165\n",
            "Epoch: 180 | Iteration: 11 | Classification loss: 0.00013 | Regression loss: 0.01594 | Running loss: 0.02163\n",
            "Epoch: 180 | Iteration: 12 | Classification loss: 0.00036 | Regression loss: 0.01825 | Running loss: 0.02163\n",
            "Epoch: 180 | Iteration: 13 | Classification loss: 0.00009 | Regression loss: 0.00924 | Running loss: 0.02163\n",
            "Epoch: 180 | Iteration: 14 | Classification loss: 0.00048 | Regression loss: 0.03345 | Running loss: 0.02167\n",
            "Epoch: 180 | Iteration: 15 | Classification loss: 0.00018 | Regression loss: 0.03443 | Running loss: 0.02170\n",
            "Epoch: 180 | Iteration: 16 | Classification loss: 0.00014 | Regression loss: 0.01951 | Running loss: 0.02169\n",
            "Epoch: 180 | Iteration: 17 | Classification loss: 0.00018 | Regression loss: 0.02318 | Running loss: 0.02172\n",
            "Epoch: 180 | Iteration: 18 | Classification loss: 0.00006 | Regression loss: 0.00483 | Running loss: 0.02167\n",
            "Epoch: 180 | Iteration: 19 | Classification loss: 0.00003 | Regression loss: 0.01431 | Running loss: 0.02166\n",
            "Epoch: 180 | Iteration: 20 | Classification loss: 0.00010 | Regression loss: 0.01707 | Running loss: 0.02168\n",
            "Epoch: 180 | Iteration: 21 | Classification loss: 0.00005 | Regression loss: 0.01259 | Running loss: 0.02169\n",
            "Epoch: 180 | Iteration: 22 | Classification loss: 0.00004 | Regression loss: 0.01267 | Running loss: 0.02168\n",
            "Epoch: 180 | Iteration: 23 | Classification loss: 0.00009 | Regression loss: 0.01574 | Running loss: 0.02165\n",
            "Epoch: 180 | Iteration: 24 | Classification loss: 0.00009 | Regression loss: 0.00973 | Running loss: 0.02164\n",
            "Epoch: 180 | Iteration: 25 | Classification loss: 0.00007 | Regression loss: 0.02007 | Running loss: 0.02165\n",
            "Epoch: 180 | Iteration: 26 | Classification loss: 0.00011 | Regression loss: 0.02961 | Running loss: 0.02167\n",
            "Epoch: 180 | Iteration: 27 | Classification loss: 0.00066 | Regression loss: 0.03865 | Running loss: 0.02170\n",
            "Epoch: 180 | Iteration: 28 | Classification loss: 0.00016 | Regression loss: 0.01132 | Running loss: 0.02168\n",
            "Epoch: 180 | Iteration: 29 | Classification loss: 0.00012 | Regression loss: 0.01924 | Running loss: 0.02169\n",
            "Epoch: 180 | Iteration: 30 | Classification loss: 0.00011 | Regression loss: 0.02290 | Running loss: 0.02166\n",
            "Epoch: 180 | Iteration: 31 | Classification loss: 0.00006 | Regression loss: 0.02044 | Running loss: 0.02166\n",
            "Epoch: 180 | Iteration: 32 | Classification loss: 0.00026 | Regression loss: 0.03865 | Running loss: 0.02171\n",
            "Epoch: 180 | Iteration: 33 | Classification loss: 0.00015 | Regression loss: 0.01282 | Running loss: 0.02169\n",
            "Epoch: 180 | Iteration: 34 | Classification loss: 0.00009 | Regression loss: 0.02792 | Running loss: 0.02169\n",
            "Epoch: 180 | Iteration: 35 | Classification loss: 0.00025 | Regression loss: 0.03807 | Running loss: 0.02174\n",
            "Epoch: 180 | Iteration: 36 | Classification loss: 0.00039 | Regression loss: 0.02562 | Running loss: 0.02176\n",
            "Epoch: 180 | Iteration: 37 | Classification loss: 0.00035 | Regression loss: 0.03776 | Running loss: 0.02181\n",
            "Epoch: 180 | Iteration: 38 | Classification loss: 0.00011 | Regression loss: 0.01741 | Running loss: 0.02180\n",
            "Epoch: 180 | Iteration: 39 | Classification loss: 0.00038 | Regression loss: 0.02390 | Running loss: 0.02180\n",
            "Epoch: 180 | Iteration: 40 | Classification loss: 0.00009 | Regression loss: 0.01370 | Running loss: 0.02179\n",
            "Epoch: 180 | Iteration: 41 | Classification loss: 0.00002 | Regression loss: 0.00968 | Running loss: 0.02176\n",
            "Epoch: 180 | Iteration: 42 | Classification loss: 0.00018 | Regression loss: 0.01887 | Running loss: 0.02173\n",
            "Epoch: 180 | Iteration: 43 | Classification loss: 0.00013 | Regression loss: 0.02355 | Running loss: 0.02176\n",
            "Epoch: 180 | Iteration: 44 | Classification loss: 0.00035 | Regression loss: 0.03302 | Running loss: 0.02178\n",
            "Epoch: 180 | Iteration: 45 | Classification loss: 0.00017 | Regression loss: 0.01915 | Running loss: 0.02178\n",
            "Epoch: 180 | Iteration: 46 | Classification loss: 0.00012 | Regression loss: 0.02614 | Running loss: 0.02181\n",
            "Epoch: 180 | Iteration: 47 | Classification loss: 0.00006 | Regression loss: 0.01523 | Running loss: 0.02180\n",
            "Epoch: 180 | Iteration: 48 | Classification loss: 0.00002 | Regression loss: 0.00371 | Running loss: 0.02178\n",
            "Epoch: 180 | Iteration: 49 | Classification loss: 0.00011 | Regression loss: 0.01423 | Running loss: 0.02178\n",
            "Epoch: 180 | Iteration: 50 | Classification loss: 0.00037 | Regression loss: 0.04820 | Running loss: 0.02185\n",
            "Epoch: 180 | Iteration: 51 | Classification loss: 0.00008 | Regression loss: 0.01494 | Running loss: 0.02184\n",
            "Epoch: 180 | Iteration: 52 | Classification loss: 0.00009 | Regression loss: 0.01282 | Running loss: 0.02182\n",
            "Epoch: 180 | Iteration: 53 | Classification loss: 0.00020 | Regression loss: 0.03232 | Running loss: 0.02186\n",
            "Epoch: 180 | Iteration: 54 | Classification loss: 0.00013 | Regression loss: 0.00891 | Running loss: 0.02179\n",
            "Epoch: 180 | Iteration: 55 | Classification loss: 0.00011 | Regression loss: 0.01459 | Running loss: 0.02177\n",
            "Epoch: 180 | Iteration: 56 | Classification loss: 0.00016 | Regression loss: 0.02247 | Running loss: 0.02176\n",
            "Epoch: 180 | Iteration: 57 | Classification loss: 0.00008 | Regression loss: 0.01234 | Running loss: 0.02175\n",
            "Epoch: 180 | Iteration: 58 | Classification loss: 0.00006 | Regression loss: 0.02539 | Running loss: 0.02176\n",
            "Epoch: 180 | Iteration: 59 | Classification loss: 0.00017 | Regression loss: 0.01290 | Running loss: 0.02172\n",
            "Epoch: 180 | Iteration: 60 | Classification loss: 0.00019 | Regression loss: 0.03801 | Running loss: 0.02172\n",
            "Epoch: 180 | Iteration: 61 | Classification loss: 0.00022 | Regression loss: 0.02166 | Running loss: 0.02174\n",
            "Epoch: 180 | Iteration: 62 | Classification loss: 0.00011 | Regression loss: 0.01279 | Running loss: 0.02171\n",
            "Epoch: 180 | Iteration: 63 | Classification loss: 0.00008 | Regression loss: 0.02216 | Running loss: 0.02172\n",
            "Epoch: 180 | Iteration: 64 | Classification loss: 0.00015 | Regression loss: 0.01404 | Running loss: 0.02170\n",
            "Epoch: 180 | Iteration: 65 | Classification loss: 0.00004 | Regression loss: 0.00778 | Running loss: 0.02165\n",
            "Epoch: 180 | Iteration: 66 | Classification loss: 0.00029 | Regression loss: 0.02379 | Running loss: 0.02167\n",
            "Epoch: 180 | Iteration: 67 | Classification loss: 0.00017 | Regression loss: 0.01800 | Running loss: 0.02167\n",
            "Epoch: 180 | Iteration: 68 | Classification loss: 0.00005 | Regression loss: 0.02487 | Running loss: 0.02168\n",
            "Epoch: 180 | Iteration: 69 | Classification loss: 0.00006 | Regression loss: 0.01512 | Running loss: 0.02165\n",
            "Epoch: 180 | Iteration: 70 | Classification loss: 0.00011 | Regression loss: 0.01564 | Running loss: 0.02164\n",
            "Epoch: 180 | Iteration: 71 | Classification loss: 0.00008 | Regression loss: 0.01646 | Running loss: 0.02159\n",
            "Epoch: 180 | Iteration: 72 | Classification loss: 0.00004 | Regression loss: 0.01886 | Running loss: 0.02158\n",
            "Epoch: 180 | Iteration: 73 | Classification loss: 0.00004 | Regression loss: 0.00740 | Running loss: 0.02157\n",
            "Epoch: 180 | Iteration: 74 | Classification loss: 0.00020 | Regression loss: 0.02028 | Running loss: 0.02157\n",
            "Epoch: 180 | Iteration: 75 | Classification loss: 0.00008 | Regression loss: 0.02178 | Running loss: 0.02155\n",
            "Epoch: 180 | Iteration: 76 | Classification loss: 0.00063 | Regression loss: 0.03412 | Running loss: 0.02158\n",
            "Epoch: 180 | Iteration: 77 | Classification loss: 0.00014 | Regression loss: 0.02432 | Running loss: 0.02160\n",
            "Epoch: 180 | Iteration: 78 | Classification loss: 0.00026 | Regression loss: 0.02539 | Running loss: 0.02158\n",
            "Epoch: 180 | Iteration: 79 | Classification loss: 0.00030 | Regression loss: 0.02925 | Running loss: 0.02160\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7278242123515042\n",
            "Precision:  0.5856573705179283\n",
            "Recall:  0.7945945945945946\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}, {0: (0.7229906021615096, 185.0)}, {0: (0.7259378939307871, 185.0)}, {0: (0.7306172160652471, 185.0)}, {0: (0.7210590702449308, 185.0)}, {0: (0.7214498931770392, 185.0)}, {0: (0.7272071618498421, 185.0)}, {0: (0.725966639522496, 185.0)}, {0: (0.7208028358685962, 185.0)}, {0: (0.7255636845600553, 185.0)}, {0: (0.7261742190616329, 185.0)}, {0: (0.7217332811149212, 185.0)}, {0: (0.7261653370413381, 185.0)}, {0: (0.7215292823555803, 185.0)}, {0: (0.7277253912015531, 185.0)}, {0: (0.7219177066363771, 185.0)}, {0: (0.7274783215794732, 185.0)}, {0: (0.7218983427777539, 185.0)}, {0: (0.7270037695200344, 185.0)}, {0: (0.7272851879977948, 185.0)}, {0: (0.7221052381441093, 185.0)}, {0: (0.7218191972667742, 185.0)}, {0: (0.7259991707220859, 185.0)}, {0: (0.7281551087798303, 185.0)}, {0: (0.732505495958946, 185.0)}, {0: (0.7266266562021302, 185.0)}, {0: (0.7320831049102212, 185.0)}, {0: (0.7278242123515042, 185.0)}]\n",
            "Epoch: 181 | Iteration: 0 | Classification loss: 0.00005 | Regression loss: 0.02050 | Running loss: 0.02161\n",
            "Epoch: 181 | Iteration: 1 | Classification loss: 0.00016 | Regression loss: 0.02771 | Running loss: 0.02165\n",
            "Epoch: 181 | Iteration: 2 | Classification loss: 0.00049 | Regression loss: 0.03800 | Running loss: 0.02168\n",
            "Epoch: 181 | Iteration: 3 | Classification loss: 0.00035 | Regression loss: 0.03597 | Running loss: 0.02172\n",
            "Epoch: 181 | Iteration: 4 | Classification loss: 0.00004 | Regression loss: 0.02561 | Running loss: 0.02171\n",
            "Epoch: 181 | Iteration: 5 | Classification loss: 0.00004 | Regression loss: 0.01877 | Running loss: 0.02171\n",
            "Epoch: 181 | Iteration: 6 | Classification loss: 0.00008 | Regression loss: 0.00776 | Running loss: 0.02165\n",
            "Epoch: 181 | Iteration: 7 | Classification loss: 0.00030 | Regression loss: 0.03079 | Running loss: 0.02165\n",
            "Epoch: 181 | Iteration: 8 | Classification loss: 0.00014 | Regression loss: 0.01220 | Running loss: 0.02166\n",
            "Epoch: 181 | Iteration: 9 | Classification loss: 0.00011 | Regression loss: 0.02204 | Running loss: 0.02168\n",
            "Epoch: 181 | Iteration: 10 | Classification loss: 0.00011 | Regression loss: 0.01762 | Running loss: 0.02161\n",
            "Epoch: 181 | Iteration: 11 | Classification loss: 0.00017 | Regression loss: 0.01756 | Running loss: 0.02157\n",
            "Epoch: 181 | Iteration: 12 | Classification loss: 0.00042 | Regression loss: 0.02255 | Running loss: 0.02159\n",
            "Epoch: 181 | Iteration: 13 | Classification loss: 0.00007 | Regression loss: 0.02075 | Running loss: 0.02156\n",
            "Epoch: 181 | Iteration: 14 | Classification loss: 0.00008 | Regression loss: 0.00875 | Running loss: 0.02157\n",
            "Epoch: 181 | Iteration: 15 | Classification loss: 0.00028 | Regression loss: 0.03957 | Running loss: 0.02162\n",
            "Epoch: 181 | Iteration: 16 | Classification loss: 0.00009 | Regression loss: 0.01028 | Running loss: 0.02156\n",
            "Epoch: 181 | Iteration: 17 | Classification loss: 0.00013 | Regression loss: 0.01490 | Running loss: 0.02155\n",
            "Epoch: 181 | Iteration: 18 | Classification loss: 0.00004 | Regression loss: 0.00676 | Running loss: 0.02154\n",
            "Epoch: 181 | Iteration: 19 | Classification loss: 0.00008 | Regression loss: 0.02659 | Running loss: 0.02156\n",
            "Epoch: 181 | Iteration: 20 | Classification loss: 0.00006 | Regression loss: 0.01405 | Running loss: 0.02152\n",
            "Epoch: 181 | Iteration: 21 | Classification loss: 0.00019 | Regression loss: 0.01998 | Running loss: 0.02152\n",
            "Epoch: 181 | Iteration: 22 | Classification loss: 0.00005 | Regression loss: 0.02595 | Running loss: 0.02153\n",
            "Epoch: 181 | Iteration: 23 | Classification loss: 0.00025 | Regression loss: 0.02261 | Running loss: 0.02155\n",
            "Epoch: 181 | Iteration: 24 | Classification loss: 0.00013 | Regression loss: 0.01602 | Running loss: 0.02155\n",
            "Epoch: 181 | Iteration: 25 | Classification loss: 0.00021 | Regression loss: 0.02020 | Running loss: 0.02155\n",
            "Epoch: 181 | Iteration: 26 | Classification loss: 0.00003 | Regression loss: 0.01417 | Running loss: 0.02155\n",
            "Epoch: 181 | Iteration: 27 | Classification loss: 0.00012 | Regression loss: 0.02991 | Running loss: 0.02156\n",
            "Epoch: 181 | Iteration: 28 | Classification loss: 0.00007 | Regression loss: 0.01276 | Running loss: 0.02155\n",
            "Epoch: 181 | Iteration: 29 | Classification loss: 0.00021 | Regression loss: 0.01773 | Running loss: 0.02154\n",
            "Epoch: 181 | Iteration: 30 | Classification loss: 0.00009 | Regression loss: 0.02131 | Running loss: 0.02152\n",
            "Epoch: 181 | Iteration: 31 | Classification loss: 0.00016 | Regression loss: 0.02114 | Running loss: 0.02153\n",
            "Epoch: 181 | Iteration: 32 | Classification loss: 0.00044 | Regression loss: 0.02001 | Running loss: 0.02152\n",
            "Epoch: 181 | Iteration: 33 | Classification loss: 0.00031 | Regression loss: 0.02211 | Running loss: 0.02152\n",
            "Epoch: 181 | Iteration: 34 | Classification loss: 0.00005 | Regression loss: 0.00503 | Running loss: 0.02147\n",
            "Epoch: 181 | Iteration: 35 | Classification loss: 0.00013 | Regression loss: 0.02423 | Running loss: 0.02146\n",
            "Epoch: 181 | Iteration: 36 | Classification loss: 0.00036 | Regression loss: 0.04837 | Running loss: 0.02150\n",
            "Epoch: 181 | Iteration: 37 | Classification loss: 0.00048 | Regression loss: 0.03210 | Running loss: 0.02151\n",
            "Epoch: 181 | Iteration: 38 | Classification loss: 0.00014 | Regression loss: 0.03612 | Running loss: 0.02155\n",
            "Epoch: 181 | Iteration: 39 | Classification loss: 0.00010 | Regression loss: 0.01714 | Running loss: 0.02151\n",
            "Epoch: 181 | Iteration: 40 | Classification loss: 0.00020 | Regression loss: 0.03157 | Running loss: 0.02156\n",
            "Epoch: 181 | Iteration: 41 | Classification loss: 0.00036 | Regression loss: 0.01925 | Running loss: 0.02157\n",
            "Epoch: 181 | Iteration: 42 | Classification loss: 0.00006 | Regression loss: 0.01772 | Running loss: 0.02158\n",
            "Epoch: 181 | Iteration: 43 | Classification loss: 0.00018 | Regression loss: 0.01662 | Running loss: 0.02157\n",
            "Epoch: 181 | Iteration: 44 | Classification loss: 0.00007 | Regression loss: 0.01363 | Running loss: 0.02158\n",
            "Epoch: 181 | Iteration: 45 | Classification loss: 0.00008 | Regression loss: 0.01623 | Running loss: 0.02156\n",
            "Epoch: 181 | Iteration: 46 | Classification loss: 0.00013 | Regression loss: 0.01307 | Running loss: 0.02154\n",
            "Epoch: 181 | Iteration: 47 | Classification loss: 0.00016 | Regression loss: 0.02565 | Running loss: 0.02155\n",
            "Epoch: 181 | Iteration: 48 | Classification loss: 0.00037 | Regression loss: 0.03513 | Running loss: 0.02159\n",
            "Epoch: 181 | Iteration: 49 | Classification loss: 0.00017 | Regression loss: 0.01733 | Running loss: 0.02155\n",
            "Epoch: 181 | Iteration: 50 | Classification loss: 0.00015 | Regression loss: 0.01493 | Running loss: 0.02154\n",
            "Epoch: 181 | Iteration: 51 | Classification loss: 0.00023 | Regression loss: 0.03383 | Running loss: 0.02156\n",
            "Epoch: 181 | Iteration: 52 | Classification loss: 0.00011 | Regression loss: 0.01525 | Running loss: 0.02157\n",
            "Epoch: 181 | Iteration: 53 | Classification loss: 0.00006 | Regression loss: 0.01936 | Running loss: 0.02157\n",
            "Epoch: 181 | Iteration: 54 | Classification loss: 0.00006 | Regression loss: 0.01300 | Running loss: 0.02155\n",
            "Epoch: 181 | Iteration: 55 | Classification loss: 0.00012 | Regression loss: 0.01754 | Running loss: 0.02155\n",
            "Epoch: 181 | Iteration: 56 | Classification loss: 0.00015 | Regression loss: 0.03642 | Running loss: 0.02159\n",
            "Epoch: 181 | Iteration: 57 | Classification loss: 0.00012 | Regression loss: 0.01217 | Running loss: 0.02159\n",
            "Epoch: 181 | Iteration: 58 | Classification loss: 0.00010 | Regression loss: 0.02555 | Running loss: 0.02161\n",
            "Epoch: 181 | Iteration: 59 | Classification loss: 0.00008 | Regression loss: 0.02854 | Running loss: 0.02160\n",
            "Epoch: 181 | Iteration: 60 | Classification loss: 0.00010 | Regression loss: 0.02278 | Running loss: 0.02162\n",
            "Epoch: 181 | Iteration: 61 | Classification loss: 0.00013 | Regression loss: 0.01266 | Running loss: 0.02159\n",
            "Epoch: 181 | Iteration: 62 | Classification loss: 0.00018 | Regression loss: 0.02384 | Running loss: 0.02157\n",
            "Epoch: 181 | Iteration: 63 | Classification loss: 0.00023 | Regression loss: 0.03032 | Running loss: 0.02160\n",
            "Epoch: 181 | Iteration: 64 | Classification loss: 0.00029 | Regression loss: 0.02900 | Running loss: 0.02163\n",
            "Epoch: 181 | Iteration: 65 | Classification loss: 0.00004 | Regression loss: 0.00726 | Running loss: 0.02160\n",
            "Epoch: 181 | Iteration: 66 | Classification loss: 0.00009 | Regression loss: 0.01175 | Running loss: 0.02154\n",
            "Epoch: 181 | Iteration: 67 | Classification loss: 0.00005 | Regression loss: 0.01427 | Running loss: 0.02153\n",
            "Epoch: 181 | Iteration: 68 | Classification loss: 0.00010 | Regression loss: 0.01420 | Running loss: 0.02154\n",
            "Epoch: 181 | Iteration: 69 | Classification loss: 0.00010 | Regression loss: 0.02856 | Running loss: 0.02153\n",
            "Epoch: 181 | Iteration: 70 | Classification loss: 0.00007 | Regression loss: 0.01395 | Running loss: 0.02147\n",
            "Epoch: 181 | Iteration: 71 | Classification loss: 0.00002 | Regression loss: 0.00375 | Running loss: 0.02146\n",
            "Epoch: 181 | Iteration: 72 | Classification loss: 0.00032 | Regression loss: 0.02828 | Running loss: 0.02143\n",
            "Epoch: 181 | Iteration: 73 | Classification loss: 0.00056 | Regression loss: 0.03622 | Running loss: 0.02146\n",
            "Epoch: 181 | Iteration: 74 | Classification loss: 0.00040 | Regression loss: 0.03612 | Running loss: 0.02149\n",
            "Epoch: 181 | Iteration: 75 | Classification loss: 0.00027 | Regression loss: 0.02629 | Running loss: 0.02150\n",
            "Epoch: 181 | Iteration: 76 | Classification loss: 0.00012 | Regression loss: 0.02408 | Running loss: 0.02148\n",
            "Epoch: 181 | Iteration: 77 | Classification loss: 0.00011 | Regression loss: 0.01638 | Running loss: 0.02148\n",
            "Epoch: 181 | Iteration: 78 | Classification loss: 0.00006 | Regression loss: 0.01363 | Running loss: 0.02143\n",
            "Epoch: 181 | Iteration: 79 | Classification loss: 0.00003 | Regression loss: 0.01011 | Running loss: 0.02138\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7278307869685627\n",
            "Precision:  0.5810276679841897\n",
            "Recall:  0.7945945945945946\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}, {0: (0.7229906021615096, 185.0)}, {0: (0.7259378939307871, 185.0)}, {0: (0.7306172160652471, 185.0)}, {0: (0.7210590702449308, 185.0)}, {0: (0.7214498931770392, 185.0)}, {0: (0.7272071618498421, 185.0)}, {0: (0.725966639522496, 185.0)}, {0: (0.7208028358685962, 185.0)}, {0: (0.7255636845600553, 185.0)}, {0: (0.7261742190616329, 185.0)}, {0: (0.7217332811149212, 185.0)}, {0: (0.7261653370413381, 185.0)}, {0: (0.7215292823555803, 185.0)}, {0: (0.7277253912015531, 185.0)}, {0: (0.7219177066363771, 185.0)}, {0: (0.7274783215794732, 185.0)}, {0: (0.7218983427777539, 185.0)}, {0: (0.7270037695200344, 185.0)}, {0: (0.7272851879977948, 185.0)}, {0: (0.7221052381441093, 185.0)}, {0: (0.7218191972667742, 185.0)}, {0: (0.7259991707220859, 185.0)}, {0: (0.7281551087798303, 185.0)}, {0: (0.732505495958946, 185.0)}, {0: (0.7266266562021302, 185.0)}, {0: (0.7320831049102212, 185.0)}, {0: (0.7278242123515042, 185.0)}, {0: (0.7278307869685627, 185.0)}]\n",
            "Epoch: 182 | Iteration: 0 | Classification loss: 0.00008 | Regression loss: 0.02872 | Running loss: 0.02143\n",
            "Epoch: 182 | Iteration: 1 | Classification loss: 0.00017 | Regression loss: 0.03416 | Running loss: 0.02147\n",
            "Epoch: 182 | Iteration: 2 | Classification loss: 0.00004 | Regression loss: 0.01285 | Running loss: 0.02148\n",
            "Epoch: 182 | Iteration: 3 | Classification loss: 0.00006 | Regression loss: 0.01450 | Running loss: 0.02147\n",
            "Epoch: 182 | Iteration: 4 | Classification loss: 0.00010 | Regression loss: 0.02819 | Running loss: 0.02149\n",
            "Epoch: 182 | Iteration: 5 | Classification loss: 0.00014 | Regression loss: 0.03325 | Running loss: 0.02152\n",
            "Epoch: 182 | Iteration: 6 | Classification loss: 0.00016 | Regression loss: 0.01135 | Running loss: 0.02152\n",
            "Epoch: 182 | Iteration: 7 | Classification loss: 0.00008 | Regression loss: 0.01621 | Running loss: 0.02150\n",
            "Epoch: 182 | Iteration: 8 | Classification loss: 0.00012 | Regression loss: 0.00814 | Running loss: 0.02148\n",
            "Epoch: 182 | Iteration: 9 | Classification loss: 0.00006 | Regression loss: 0.00896 | Running loss: 0.02147\n",
            "Epoch: 182 | Iteration: 10 | Classification loss: 0.00010 | Regression loss: 0.01723 | Running loss: 0.02147\n",
            "Epoch: 182 | Iteration: 11 | Classification loss: 0.00042 | Regression loss: 0.04788 | Running loss: 0.02151\n",
            "Epoch: 182 | Iteration: 12 | Classification loss: 0.00004 | Regression loss: 0.00697 | Running loss: 0.02150\n",
            "Epoch: 182 | Iteration: 13 | Classification loss: 0.00010 | Regression loss: 0.01668 | Running loss: 0.02148\n",
            "Epoch: 182 | Iteration: 14 | Classification loss: 0.00017 | Regression loss: 0.01236 | Running loss: 0.02144\n",
            "Epoch: 182 | Iteration: 15 | Classification loss: 0.00017 | Regression loss: 0.03432 | Running loss: 0.02148\n",
            "Epoch: 182 | Iteration: 16 | Classification loss: 0.00015 | Regression loss: 0.03153 | Running loss: 0.02151\n",
            "Epoch: 182 | Iteration: 17 | Classification loss: 0.00007 | Regression loss: 0.01337 | Running loss: 0.02143\n",
            "Epoch: 182 | Iteration: 18 | Classification loss: 0.00023 | Regression loss: 0.02339 | Running loss: 0.02145\n",
            "Epoch: 182 | Iteration: 19 | Classification loss: 0.00045 | Regression loss: 0.03703 | Running loss: 0.02151\n",
            "Epoch: 182 | Iteration: 20 | Classification loss: 0.00006 | Regression loss: 0.01855 | Running loss: 0.02150\n",
            "Epoch: 182 | Iteration: 21 | Classification loss: 0.00002 | Regression loss: 0.00884 | Running loss: 0.02148\n",
            "Epoch: 182 | Iteration: 22 | Classification loss: 0.00008 | Regression loss: 0.01206 | Running loss: 0.02148\n",
            "Epoch: 182 | Iteration: 23 | Classification loss: 0.00034 | Regression loss: 0.03557 | Running loss: 0.02151\n",
            "Epoch: 182 | Iteration: 24 | Classification loss: 0.00010 | Regression loss: 0.01694 | Running loss: 0.02146\n",
            "Epoch: 182 | Iteration: 25 | Classification loss: 0.00051 | Regression loss: 0.03327 | Running loss: 0.02146\n",
            "Epoch: 182 | Iteration: 26 | Classification loss: 0.00009 | Regression loss: 0.01137 | Running loss: 0.02142\n",
            "Epoch: 182 | Iteration: 27 | Classification loss: 0.00009 | Regression loss: 0.01794 | Running loss: 0.02140\n",
            "Epoch: 182 | Iteration: 28 | Classification loss: 0.00002 | Regression loss: 0.00419 | Running loss: 0.02134\n",
            "Epoch: 182 | Iteration: 29 | Classification loss: 0.00017 | Regression loss: 0.01689 | Running loss: 0.02134\n",
            "Epoch: 182 | Iteration: 30 | Classification loss: 0.00011 | Regression loss: 0.02806 | Running loss: 0.02137\n",
            "Epoch: 182 | Iteration: 31 | Classification loss: 0.00042 | Regression loss: 0.03179 | Running loss: 0.02140\n",
            "Epoch: 182 | Iteration: 32 | Classification loss: 0.00005 | Regression loss: 0.00785 | Running loss: 0.02138\n",
            "Epoch: 182 | Iteration: 33 | Classification loss: 0.00008 | Regression loss: 0.02203 | Running loss: 0.02138\n",
            "Epoch: 182 | Iteration: 34 | Classification loss: 0.00043 | Regression loss: 0.02113 | Running loss: 0.02138\n",
            "Epoch: 182 | Iteration: 35 | Classification loss: 0.00010 | Regression loss: 0.01786 | Running loss: 0.02138\n",
            "Epoch: 182 | Iteration: 36 | Classification loss: 0.00035 | Regression loss: 0.01789 | Running loss: 0.02135\n",
            "Epoch: 182 | Iteration: 37 | Classification loss: 0.00042 | Regression loss: 0.03163 | Running loss: 0.02138\n",
            "Epoch: 182 | Iteration: 38 | Classification loss: 0.00015 | Regression loss: 0.01462 | Running loss: 0.02136\n",
            "Epoch: 182 | Iteration: 39 | Classification loss: 0.00027 | Regression loss: 0.02116 | Running loss: 0.02136\n",
            "Epoch: 182 | Iteration: 40 | Classification loss: 0.00004 | Regression loss: 0.02175 | Running loss: 0.02138\n",
            "Epoch: 182 | Iteration: 41 | Classification loss: 0.00004 | Regression loss: 0.01817 | Running loss: 0.02137\n",
            "Epoch: 182 | Iteration: 42 | Classification loss: 0.00007 | Regression loss: 0.01908 | Running loss: 0.02139\n",
            "Epoch: 182 | Iteration: 43 | Classification loss: 0.00017 | Regression loss: 0.01809 | Running loss: 0.02139\n",
            "Epoch: 182 | Iteration: 44 | Classification loss: 0.00011 | Regression loss: 0.01436 | Running loss: 0.02139\n",
            "Epoch: 182 | Iteration: 45 | Classification loss: 0.00007 | Regression loss: 0.01343 | Running loss: 0.02139\n",
            "Epoch: 182 | Iteration: 46 | Classification loss: 0.00013 | Regression loss: 0.01194 | Running loss: 0.02138\n",
            "Epoch: 182 | Iteration: 47 | Classification loss: 0.00022 | Regression loss: 0.02964 | Running loss: 0.02140\n",
            "Epoch: 182 | Iteration: 48 | Classification loss: 0.00014 | Regression loss: 0.03038 | Running loss: 0.02142\n",
            "Epoch: 182 | Iteration: 49 | Classification loss: 0.00049 | Regression loss: 0.03099 | Running loss: 0.02143\n",
            "Epoch: 182 | Iteration: 50 | Classification loss: 0.00005 | Regression loss: 0.01718 | Running loss: 0.02140\n",
            "Epoch: 182 | Iteration: 51 | Classification loss: 0.00012 | Regression loss: 0.01547 | Running loss: 0.02140\n",
            "Epoch: 182 | Iteration: 52 | Classification loss: 0.00035 | Regression loss: 0.04308 | Running loss: 0.02145\n",
            "Epoch: 182 | Iteration: 53 | Classification loss: 0.00015 | Regression loss: 0.02132 | Running loss: 0.02146\n",
            "Epoch: 182 | Iteration: 54 | Classification loss: 0.00010 | Regression loss: 0.01435 | Running loss: 0.02139\n",
            "Epoch: 182 | Iteration: 55 | Classification loss: 0.00011 | Regression loss: 0.02316 | Running loss: 0.02139\n",
            "Epoch: 182 | Iteration: 56 | Classification loss: 0.00057 | Regression loss: 0.02064 | Running loss: 0.02141\n",
            "Epoch: 182 | Iteration: 57 | Classification loss: 0.00014 | Regression loss: 0.01471 | Running loss: 0.02140\n",
            "Epoch: 182 | Iteration: 58 | Classification loss: 0.00012 | Regression loss: 0.01425 | Running loss: 0.02141\n",
            "Epoch: 182 | Iteration: 59 | Classification loss: 0.00008 | Regression loss: 0.01939 | Running loss: 0.02138\n",
            "Epoch: 182 | Iteration: 60 | Classification loss: 0.00013 | Regression loss: 0.02236 | Running loss: 0.02139\n",
            "Epoch: 182 | Iteration: 61 | Classification loss: 0.00004 | Regression loss: 0.01428 | Running loss: 0.02139\n",
            "Epoch: 182 | Iteration: 62 | Classification loss: 0.00006 | Regression loss: 0.02377 | Running loss: 0.02137\n",
            "Epoch: 182 | Iteration: 63 | Classification loss: 0.00004 | Regression loss: 0.00812 | Running loss: 0.02137\n",
            "Epoch: 182 | Iteration: 64 | Classification loss: 0.00036 | Regression loss: 0.02103 | Running loss: 0.02137\n",
            "Epoch: 182 | Iteration: 65 | Classification loss: 0.00016 | Regression loss: 0.02187 | Running loss: 0.02138\n",
            "Epoch: 182 | Iteration: 66 | Classification loss: 0.00026 | Regression loss: 0.02487 | Running loss: 0.02138\n",
            "Epoch: 182 | Iteration: 67 | Classification loss: 0.00012 | Regression loss: 0.02427 | Running loss: 0.02140\n",
            "Epoch: 182 | Iteration: 68 | Classification loss: 0.00025 | Regression loss: 0.02769 | Running loss: 0.02142\n",
            "Epoch: 182 | Iteration: 69 | Classification loss: 0.00005 | Regression loss: 0.01888 | Running loss: 0.02142\n",
            "Epoch: 182 | Iteration: 70 | Classification loss: 0.00015 | Regression loss: 0.01653 | Running loss: 0.02143\n",
            "Epoch: 182 | Iteration: 71 | Classification loss: 0.00024 | Regression loss: 0.02815 | Running loss: 0.02146\n",
            "Epoch: 182 | Iteration: 72 | Classification loss: 0.00017 | Regression loss: 0.02301 | Running loss: 0.02145\n",
            "Epoch: 182 | Iteration: 73 | Classification loss: 0.00019 | Regression loss: 0.02331 | Running loss: 0.02142\n",
            "Epoch: 182 | Iteration: 74 | Classification loss: 0.00036 | Regression loss: 0.03923 | Running loss: 0.02146\n",
            "Epoch: 182 | Iteration: 75 | Classification loss: 0.00008 | Regression loss: 0.00927 | Running loss: 0.02143\n",
            "Epoch: 182 | Iteration: 76 | Classification loss: 0.00008 | Regression loss: 0.01924 | Running loss: 0.02140\n",
            "Epoch: 182 | Iteration: 77 | Classification loss: 0.00029 | Regression loss: 0.03652 | Running loss: 0.02146\n",
            "Epoch: 182 | Iteration: 78 | Classification loss: 0.00016 | Regression loss: 0.01787 | Running loss: 0.02142\n",
            "Epoch: 182 | Iteration: 79 | Classification loss: 0.00021 | Regression loss: 0.02027 | Running loss: 0.02140\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7325520428180083\n",
            "Precision:  0.5873015873015873\n",
            "Recall:  0.8\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}, {0: (0.7229906021615096, 185.0)}, {0: (0.7259378939307871, 185.0)}, {0: (0.7306172160652471, 185.0)}, {0: (0.7210590702449308, 185.0)}, {0: (0.7214498931770392, 185.0)}, {0: (0.7272071618498421, 185.0)}, {0: (0.725966639522496, 185.0)}, {0: (0.7208028358685962, 185.0)}, {0: (0.7255636845600553, 185.0)}, {0: (0.7261742190616329, 185.0)}, {0: (0.7217332811149212, 185.0)}, {0: (0.7261653370413381, 185.0)}, {0: (0.7215292823555803, 185.0)}, {0: (0.7277253912015531, 185.0)}, {0: (0.7219177066363771, 185.0)}, {0: (0.7274783215794732, 185.0)}, {0: (0.7218983427777539, 185.0)}, {0: (0.7270037695200344, 185.0)}, {0: (0.7272851879977948, 185.0)}, {0: (0.7221052381441093, 185.0)}, {0: (0.7218191972667742, 185.0)}, {0: (0.7259991707220859, 185.0)}, {0: (0.7281551087798303, 185.0)}, {0: (0.732505495958946, 185.0)}, {0: (0.7266266562021302, 185.0)}, {0: (0.7320831049102212, 185.0)}, {0: (0.7278242123515042, 185.0)}, {0: (0.7278307869685627, 185.0)}, {0: (0.7325520428180083, 185.0)}]\n",
            "Epoch: 183 | Iteration: 0 | Classification loss: 0.00009 | Regression loss: 0.02225 | Running loss: 0.02140\n",
            "Epoch: 183 | Iteration: 1 | Classification loss: 0.00010 | Regression loss: 0.01715 | Running loss: 0.02141\n",
            "Epoch: 183 | Iteration: 2 | Classification loss: 0.00011 | Regression loss: 0.02810 | Running loss: 0.02143\n",
            "Epoch: 183 | Iteration: 3 | Classification loss: 0.00014 | Regression loss: 0.01691 | Running loss: 0.02143\n",
            "Epoch: 183 | Iteration: 4 | Classification loss: 0.00018 | Regression loss: 0.02027 | Running loss: 0.02146\n",
            "Epoch: 183 | Iteration: 5 | Classification loss: 0.00008 | Regression loss: 0.02196 | Running loss: 0.02146\n",
            "Epoch: 183 | Iteration: 6 | Classification loss: 0.00009 | Regression loss: 0.01419 | Running loss: 0.02144\n",
            "Epoch: 183 | Iteration: 7 | Classification loss: 0.00026 | Regression loss: 0.03128 | Running loss: 0.02144\n",
            "Epoch: 183 | Iteration: 8 | Classification loss: 0.00013 | Regression loss: 0.01462 | Running loss: 0.02141\n",
            "Epoch: 183 | Iteration: 9 | Classification loss: 0.00015 | Regression loss: 0.02343 | Running loss: 0.02143\n",
            "Epoch: 183 | Iteration: 10 | Classification loss: 0.00010 | Regression loss: 0.01669 | Running loss: 0.02142\n",
            "Epoch: 183 | Iteration: 11 | Classification loss: 0.00016 | Regression loss: 0.02011 | Running loss: 0.02142\n",
            "Epoch: 183 | Iteration: 12 | Classification loss: 0.00008 | Regression loss: 0.02553 | Running loss: 0.02144\n",
            "Epoch: 183 | Iteration: 13 | Classification loss: 0.00014 | Regression loss: 0.01339 | Running loss: 0.02143\n",
            "Epoch: 183 | Iteration: 14 | Classification loss: 0.00028 | Regression loss: 0.02278 | Running loss: 0.02142\n",
            "Epoch: 183 | Iteration: 15 | Classification loss: 0.00044 | Regression loss: 0.03930 | Running loss: 0.02150\n",
            "Epoch: 183 | Iteration: 16 | Classification loss: 0.00019 | Regression loss: 0.03849 | Running loss: 0.02153\n",
            "Epoch: 183 | Iteration: 17 | Classification loss: 0.00032 | Regression loss: 0.01893 | Running loss: 0.02150\n",
            "Epoch: 183 | Iteration: 18 | Classification loss: 0.00063 | Regression loss: 0.03818 | Running loss: 0.02155\n",
            "Epoch: 183 | Iteration: 19 | Classification loss: 0.00003 | Regression loss: 0.01831 | Running loss: 0.02150\n",
            "Epoch: 183 | Iteration: 20 | Classification loss: 0.00040 | Regression loss: 0.02227 | Running loss: 0.02151\n",
            "Epoch: 183 | Iteration: 21 | Classification loss: 0.00005 | Regression loss: 0.01273 | Running loss: 0.02152\n",
            "Epoch: 183 | Iteration: 22 | Classification loss: 0.00005 | Regression loss: 0.02124 | Running loss: 0.02153\n",
            "Epoch: 183 | Iteration: 23 | Classification loss: 0.00049 | Regression loss: 0.03317 | Running loss: 0.02157\n",
            "Epoch: 183 | Iteration: 24 | Classification loss: 0.00007 | Regression loss: 0.01861 | Running loss: 0.02155\n",
            "Epoch: 183 | Iteration: 25 | Classification loss: 0.00009 | Regression loss: 0.02019 | Running loss: 0.02153\n",
            "Epoch: 183 | Iteration: 26 | Classification loss: 0.00008 | Regression loss: 0.01416 | Running loss: 0.02151\n",
            "Epoch: 183 | Iteration: 27 | Classification loss: 0.00037 | Regression loss: 0.04807 | Running loss: 0.02155\n",
            "Epoch: 183 | Iteration: 28 | Classification loss: 0.00016 | Regression loss: 0.02771 | Running loss: 0.02157\n",
            "Epoch: 183 | Iteration: 29 | Classification loss: 0.00022 | Regression loss: 0.02540 | Running loss: 0.02159\n",
            "Epoch: 183 | Iteration: 30 | Classification loss: 0.00019 | Regression loss: 0.01806 | Running loss: 0.02155\n",
            "Epoch: 183 | Iteration: 31 | Classification loss: 0.00034 | Regression loss: 0.02540 | Running loss: 0.02154\n",
            "Epoch: 183 | Iteration: 32 | Classification loss: 0.00010 | Regression loss: 0.01385 | Running loss: 0.02153\n",
            "Epoch: 183 | Iteration: 33 | Classification loss: 0.00007 | Regression loss: 0.01231 | Running loss: 0.02152\n",
            "Epoch: 183 | Iteration: 34 | Classification loss: 0.00036 | Regression loss: 0.03335 | Running loss: 0.02157\n",
            "Epoch: 183 | Iteration: 35 | Classification loss: 0.00011 | Regression loss: 0.01777 | Running loss: 0.02157\n",
            "Epoch: 183 | Iteration: 36 | Classification loss: 0.00022 | Regression loss: 0.02929 | Running loss: 0.02157\n",
            "Epoch: 183 | Iteration: 37 | Classification loss: 0.00044 | Regression loss: 0.01960 | Running loss: 0.02157\n",
            "Epoch: 183 | Iteration: 38 | Classification loss: 0.00032 | Regression loss: 0.03590 | Running loss: 0.02161\n",
            "Epoch: 183 | Iteration: 39 | Classification loss: 0.00013 | Regression loss: 0.01607 | Running loss: 0.02160\n",
            "Epoch: 183 | Iteration: 40 | Classification loss: 0.00011 | Regression loss: 0.01846 | Running loss: 0.02157\n",
            "Epoch: 183 | Iteration: 41 | Classification loss: 0.00006 | Regression loss: 0.00915 | Running loss: 0.02151\n",
            "Epoch: 183 | Iteration: 42 | Classification loss: 0.00008 | Regression loss: 0.01026 | Running loss: 0.02149\n",
            "Epoch: 183 | Iteration: 43 | Classification loss: 0.00005 | Regression loss: 0.02190 | Running loss: 0.02150\n",
            "Epoch: 183 | Iteration: 44 | Classification loss: 0.00013 | Regression loss: 0.01498 | Running loss: 0.02150\n",
            "Epoch: 183 | Iteration: 45 | Classification loss: 0.00012 | Regression loss: 0.01983 | Running loss: 0.02150\n",
            "Epoch: 183 | Iteration: 46 | Classification loss: 0.00006 | Regression loss: 0.01504 | Running loss: 0.02145\n",
            "Epoch: 183 | Iteration: 47 | Classification loss: 0.00014 | Regression loss: 0.03577 | Running loss: 0.02151\n",
            "Epoch: 183 | Iteration: 48 | Classification loss: 0.00005 | Regression loss: 0.02032 | Running loss: 0.02150\n",
            "Epoch: 183 | Iteration: 49 | Classification loss: 0.00012 | Regression loss: 0.01267 | Running loss: 0.02150\n",
            "Epoch: 183 | Iteration: 50 | Classification loss: 0.00009 | Regression loss: 0.01466 | Running loss: 0.02149\n",
            "Epoch: 183 | Iteration: 51 | Classification loss: 0.00009 | Regression loss: 0.01586 | Running loss: 0.02147\n",
            "Epoch: 183 | Iteration: 52 | Classification loss: 0.00033 | Regression loss: 0.02088 | Running loss: 0.02143\n",
            "Epoch: 183 | Iteration: 53 | Classification loss: 0.00012 | Regression loss: 0.02306 | Running loss: 0.02143\n",
            "Epoch: 183 | Iteration: 54 | Classification loss: 0.00030 | Regression loss: 0.02881 | Running loss: 0.02148\n",
            "Epoch: 183 | Iteration: 55 | Classification loss: 0.00005 | Regression loss: 0.00731 | Running loss: 0.02145\n",
            "Epoch: 183 | Iteration: 56 | Classification loss: 0.00030 | Regression loss: 0.04397 | Running loss: 0.02147\n",
            "Epoch: 183 | Iteration: 57 | Classification loss: 0.00014 | Regression loss: 0.01164 | Running loss: 0.02146\n",
            "Epoch: 183 | Iteration: 58 | Classification loss: 0.00005 | Regression loss: 0.00761 | Running loss: 0.02144\n",
            "Epoch: 183 | Iteration: 59 | Classification loss: 0.00015 | Regression loss: 0.01723 | Running loss: 0.02142\n",
            "Epoch: 183 | Iteration: 60 | Classification loss: 0.00002 | Regression loss: 0.00883 | Running loss: 0.02138\n",
            "Epoch: 183 | Iteration: 61 | Classification loss: 0.00003 | Regression loss: 0.01419 | Running loss: 0.02135\n",
            "Epoch: 183 | Iteration: 62 | Classification loss: 0.00013 | Regression loss: 0.03320 | Running loss: 0.02139\n",
            "Epoch: 183 | Iteration: 63 | Classification loss: 0.00004 | Regression loss: 0.00727 | Running loss: 0.02137\n",
            "Epoch: 183 | Iteration: 64 | Classification loss: 0.00017 | Regression loss: 0.02461 | Running loss: 0.02137\n",
            "Epoch: 183 | Iteration: 65 | Classification loss: 0.00012 | Regression loss: 0.01204 | Running loss: 0.02134\n",
            "Epoch: 183 | Iteration: 66 | Classification loss: 0.00008 | Regression loss: 0.02618 | Running loss: 0.02136\n",
            "Epoch: 183 | Iteration: 67 | Classification loss: 0.00020 | Regression loss: 0.02014 | Running loss: 0.02138\n",
            "Epoch: 183 | Iteration: 68 | Classification loss: 0.00002 | Regression loss: 0.00366 | Running loss: 0.02129\n",
            "Epoch: 183 | Iteration: 69 | Classification loss: 0.00007 | Regression loss: 0.01457 | Running loss: 0.02129\n",
            "Epoch: 183 | Iteration: 70 | Classification loss: 0.00012 | Regression loss: 0.01683 | Running loss: 0.02132\n",
            "Epoch: 183 | Iteration: 71 | Classification loss: 0.00016 | Regression loss: 0.03115 | Running loss: 0.02134\n",
            "Epoch: 183 | Iteration: 72 | Classification loss: 0.00022 | Regression loss: 0.03070 | Running loss: 0.02135\n",
            "Epoch: 183 | Iteration: 73 | Classification loss: 0.00064 | Regression loss: 0.03470 | Running loss: 0.02138\n",
            "Epoch: 183 | Iteration: 74 | Classification loss: 0.00009 | Regression loss: 0.00758 | Running loss: 0.02136\n",
            "Epoch: 183 | Iteration: 75 | Classification loss: 0.00010 | Regression loss: 0.01533 | Running loss: 0.02135\n",
            "Epoch: 183 | Iteration: 76 | Classification loss: 0.00005 | Regression loss: 0.01139 | Running loss: 0.02129\n",
            "Epoch: 183 | Iteration: 77 | Classification loss: 0.00006 | Regression loss: 0.01734 | Running loss: 0.02132\n",
            "Epoch: 183 | Iteration: 78 | Classification loss: 0.00005 | Regression loss: 0.02051 | Running loss: 0.02129\n",
            "Epoch: 183 | Iteration: 79 | Classification loss: 0.00013 | Regression loss: 0.01603 | Running loss: 0.02129\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7216918061885735\n",
            "Precision:  0.5910931174089069\n",
            "Recall:  0.7891891891891892\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}, {0: (0.7229906021615096, 185.0)}, {0: (0.7259378939307871, 185.0)}, {0: (0.7306172160652471, 185.0)}, {0: (0.7210590702449308, 185.0)}, {0: (0.7214498931770392, 185.0)}, {0: (0.7272071618498421, 185.0)}, {0: (0.725966639522496, 185.0)}, {0: (0.7208028358685962, 185.0)}, {0: (0.7255636845600553, 185.0)}, {0: (0.7261742190616329, 185.0)}, {0: (0.7217332811149212, 185.0)}, {0: (0.7261653370413381, 185.0)}, {0: (0.7215292823555803, 185.0)}, {0: (0.7277253912015531, 185.0)}, {0: (0.7219177066363771, 185.0)}, {0: (0.7274783215794732, 185.0)}, {0: (0.7218983427777539, 185.0)}, {0: (0.7270037695200344, 185.0)}, {0: (0.7272851879977948, 185.0)}, {0: (0.7221052381441093, 185.0)}, {0: (0.7218191972667742, 185.0)}, {0: (0.7259991707220859, 185.0)}, {0: (0.7281551087798303, 185.0)}, {0: (0.732505495958946, 185.0)}, {0: (0.7266266562021302, 185.0)}, {0: (0.7320831049102212, 185.0)}, {0: (0.7278242123515042, 185.0)}, {0: (0.7278307869685627, 185.0)}, {0: (0.7325520428180083, 185.0)}, {0: (0.7216918061885735, 185.0)}]\n",
            "Epoch: 184 | Iteration: 0 | Classification loss: 0.00018 | Regression loss: 0.03616 | Running loss: 0.02135\n",
            "Epoch: 184 | Iteration: 1 | Classification loss: 0.00007 | Regression loss: 0.00747 | Running loss: 0.02131\n",
            "Epoch: 184 | Iteration: 2 | Classification loss: 0.00003 | Regression loss: 0.01443 | Running loss: 0.02127\n",
            "Epoch: 184 | Iteration: 3 | Classification loss: 0.00032 | Regression loss: 0.03519 | Running loss: 0.02130\n",
            "Epoch: 184 | Iteration: 4 | Classification loss: 0.00024 | Regression loss: 0.02147 | Running loss: 0.02130\n",
            "Epoch: 184 | Iteration: 5 | Classification loss: 0.00016 | Regression loss: 0.02065 | Running loss: 0.02130\n",
            "Epoch: 184 | Iteration: 6 | Classification loss: 0.00014 | Regression loss: 0.02882 | Running loss: 0.02133\n",
            "Epoch: 184 | Iteration: 7 | Classification loss: 0.00015 | Regression loss: 0.01657 | Running loss: 0.02132\n",
            "Epoch: 184 | Iteration: 8 | Classification loss: 0.00010 | Regression loss: 0.01182 | Running loss: 0.02127\n",
            "Epoch: 184 | Iteration: 9 | Classification loss: 0.00032 | Regression loss: 0.04136 | Running loss: 0.02131\n",
            "Epoch: 184 | Iteration: 10 | Classification loss: 0.00010 | Regression loss: 0.01365 | Running loss: 0.02131\n",
            "Epoch: 184 | Iteration: 11 | Classification loss: 0.00009 | Regression loss: 0.02224 | Running loss: 0.02134\n",
            "Epoch: 184 | Iteration: 12 | Classification loss: 0.00033 | Regression loss: 0.04733 | Running loss: 0.02141\n",
            "Epoch: 184 | Iteration: 13 | Classification loss: 0.00015 | Regression loss: 0.01657 | Running loss: 0.02138\n",
            "Epoch: 184 | Iteration: 14 | Classification loss: 0.00015 | Regression loss: 0.01268 | Running loss: 0.02137\n",
            "Epoch: 184 | Iteration: 15 | Classification loss: 0.00005 | Regression loss: 0.01367 | Running loss: 0.02136\n",
            "Epoch: 184 | Iteration: 16 | Classification loss: 0.00010 | Regression loss: 0.02475 | Running loss: 0.02135\n",
            "Epoch: 184 | Iteration: 17 | Classification loss: 0.00004 | Regression loss: 0.01708 | Running loss: 0.02136\n",
            "Epoch: 184 | Iteration: 18 | Classification loss: 0.00036 | Regression loss: 0.02051 | Running loss: 0.02136\n",
            "Epoch: 184 | Iteration: 19 | Classification loss: 0.00032 | Regression loss: 0.03366 | Running loss: 0.02136\n",
            "Epoch: 184 | Iteration: 20 | Classification loss: 0.00054 | Regression loss: 0.02267 | Running loss: 0.02135\n",
            "Epoch: 184 | Iteration: 21 | Classification loss: 0.00005 | Regression loss: 0.02077 | Running loss: 0.02136\n",
            "Epoch: 184 | Iteration: 22 | Classification loss: 0.00034 | Regression loss: 0.03586 | Running loss: 0.02142\n",
            "Epoch: 184 | Iteration: 23 | Classification loss: 0.00008 | Regression loss: 0.02571 | Running loss: 0.02144\n",
            "Epoch: 184 | Iteration: 24 | Classification loss: 0.00024 | Regression loss: 0.02614 | Running loss: 0.02147\n",
            "Epoch: 184 | Iteration: 25 | Classification loss: 0.00005 | Regression loss: 0.01470 | Running loss: 0.02145\n",
            "Epoch: 184 | Iteration: 26 | Classification loss: 0.00016 | Regression loss: 0.01305 | Running loss: 0.02142\n",
            "Epoch: 184 | Iteration: 27 | Classification loss: 0.00018 | Regression loss: 0.01945 | Running loss: 0.02142\n",
            "Epoch: 184 | Iteration: 28 | Classification loss: 0.00003 | Regression loss: 0.00711 | Running loss: 0.02139\n",
            "Epoch: 184 | Iteration: 29 | Classification loss: 0.00016 | Regression loss: 0.01638 | Running loss: 0.02135\n",
            "Epoch: 184 | Iteration: 30 | Classification loss: 0.00015 | Regression loss: 0.02706 | Running loss: 0.02138\n",
            "Epoch: 184 | Iteration: 31 | Classification loss: 0.00006 | Regression loss: 0.01237 | Running loss: 0.02134\n",
            "Epoch: 184 | Iteration: 32 | Classification loss: 0.00049 | Regression loss: 0.03229 | Running loss: 0.02136\n",
            "Epoch: 184 | Iteration: 33 | Classification loss: 0.00008 | Regression loss: 0.01226 | Running loss: 0.02135\n",
            "Epoch: 184 | Iteration: 34 | Classification loss: 0.00016 | Regression loss: 0.01127 | Running loss: 0.02132\n",
            "Epoch: 184 | Iteration: 35 | Classification loss: 0.00029 | Regression loss: 0.04306 | Running loss: 0.02135\n",
            "Epoch: 184 | Iteration: 36 | Classification loss: 0.00014 | Regression loss: 0.02767 | Running loss: 0.02138\n",
            "Epoch: 184 | Iteration: 37 | Classification loss: 0.00005 | Regression loss: 0.01915 | Running loss: 0.02137\n",
            "Epoch: 184 | Iteration: 38 | Classification loss: 0.00030 | Regression loss: 0.02997 | Running loss: 0.02140\n",
            "Epoch: 184 | Iteration: 39 | Classification loss: 0.00010 | Regression loss: 0.01621 | Running loss: 0.02140\n",
            "Epoch: 184 | Iteration: 40 | Classification loss: 0.00014 | Regression loss: 0.02344 | Running loss: 0.02142\n",
            "Epoch: 184 | Iteration: 41 | Classification loss: 0.00011 | Regression loss: 0.01504 | Running loss: 0.02140\n",
            "Epoch: 184 | Iteration: 42 | Classification loss: 0.00046 | Regression loss: 0.03296 | Running loss: 0.02145\n",
            "Epoch: 184 | Iteration: 43 | Classification loss: 0.00011 | Regression loss: 0.01205 | Running loss: 0.02143\n",
            "Epoch: 184 | Iteration: 44 | Classification loss: 0.00014 | Regression loss: 0.03454 | Running loss: 0.02149\n",
            "Epoch: 184 | Iteration: 45 | Classification loss: 0.00012 | Regression loss: 0.01969 | Running loss: 0.02150\n",
            "Epoch: 184 | Iteration: 46 | Classification loss: 0.00023 | Regression loss: 0.02956 | Running loss: 0.02153\n",
            "Epoch: 184 | Iteration: 47 | Classification loss: 0.00012 | Regression loss: 0.01571 | Running loss: 0.02154\n",
            "Epoch: 184 | Iteration: 48 | Classification loss: 0.00008 | Regression loss: 0.02667 | Running loss: 0.02154\n",
            "Epoch: 184 | Iteration: 49 | Classification loss: 0.00009 | Regression loss: 0.01729 | Running loss: 0.02149\n",
            "Epoch: 184 | Iteration: 50 | Classification loss: 0.00022 | Regression loss: 0.02175 | Running loss: 0.02148\n",
            "Epoch: 184 | Iteration: 51 | Classification loss: 0.00008 | Regression loss: 0.02020 | Running loss: 0.02149\n",
            "Epoch: 184 | Iteration: 52 | Classification loss: 0.00014 | Regression loss: 0.02011 | Running loss: 0.02146\n",
            "Epoch: 184 | Iteration: 53 | Classification loss: 0.00006 | Regression loss: 0.01330 | Running loss: 0.02145\n",
            "Epoch: 184 | Iteration: 54 | Classification loss: 0.00004 | Regression loss: 0.00828 | Running loss: 0.02143\n",
            "Epoch: 184 | Iteration: 55 | Classification loss: 0.00007 | Regression loss: 0.01523 | Running loss: 0.02143\n",
            "Epoch: 184 | Iteration: 56 | Classification loss: 0.00012 | Regression loss: 0.02311 | Running loss: 0.02147\n",
            "Epoch: 184 | Iteration: 57 | Classification loss: 0.00010 | Regression loss: 0.01714 | Running loss: 0.02145\n",
            "Epoch: 184 | Iteration: 58 | Classification loss: 0.00013 | Regression loss: 0.01383 | Running loss: 0.02139\n",
            "Epoch: 184 | Iteration: 59 | Classification loss: 0.00013 | Regression loss: 0.01223 | Running loss: 0.02137\n",
            "Epoch: 184 | Iteration: 60 | Classification loss: 0.00006 | Regression loss: 0.02615 | Running loss: 0.02135\n",
            "Epoch: 184 | Iteration: 61 | Classification loss: 0.00038 | Regression loss: 0.02278 | Running loss: 0.02132\n",
            "Epoch: 184 | Iteration: 62 | Classification loss: 0.00006 | Regression loss: 0.00459 | Running loss: 0.02128\n",
            "Epoch: 184 | Iteration: 63 | Classification loss: 0.00007 | Regression loss: 0.00913 | Running loss: 0.02127\n",
            "Epoch: 184 | Iteration: 64 | Classification loss: 0.00024 | Regression loss: 0.02385 | Running loss: 0.02129\n",
            "Epoch: 184 | Iteration: 65 | Classification loss: 0.00006 | Regression loss: 0.02045 | Running loss: 0.02131\n",
            "Epoch: 184 | Iteration: 66 | Classification loss: 0.00027 | Regression loss: 0.02898 | Running loss: 0.02132\n",
            "Epoch: 184 | Iteration: 67 | Classification loss: 0.00017 | Regression loss: 0.01865 | Running loss: 0.02132\n",
            "Epoch: 184 | Iteration: 68 | Classification loss: 0.00008 | Regression loss: 0.01234 | Running loss: 0.02131\n",
            "Epoch: 184 | Iteration: 69 | Classification loss: 0.00017 | Regression loss: 0.01784 | Running loss: 0.02129\n",
            "Epoch: 184 | Iteration: 70 | Classification loss: 0.00023 | Regression loss: 0.03142 | Running loss: 0.02132\n",
            "Epoch: 184 | Iteration: 71 | Classification loss: 0.00002 | Regression loss: 0.00395 | Running loss: 0.02126\n",
            "Epoch: 184 | Iteration: 72 | Classification loss: 0.00006 | Regression loss: 0.01503 | Running loss: 0.02126\n",
            "Epoch: 184 | Iteration: 73 | Classification loss: 0.00002 | Regression loss: 0.00867 | Running loss: 0.02121\n",
            "Epoch: 184 | Iteration: 74 | Classification loss: 0.00009 | Regression loss: 0.01376 | Running loss: 0.02120\n",
            "Epoch: 184 | Iteration: 75 | Classification loss: 0.00012 | Regression loss: 0.02089 | Running loss: 0.02119\n",
            "Epoch: 184 | Iteration: 76 | Classification loss: 0.00018 | Regression loss: 0.03080 | Running loss: 0.02124\n",
            "Epoch: 184 | Iteration: 77 | Classification loss: 0.00004 | Regression loss: 0.02788 | Running loss: 0.02123\n",
            "Epoch: 184 | Iteration: 78 | Classification loss: 0.00009 | Regression loss: 0.01665 | Running loss: 0.02122\n",
            "Epoch: 184 | Iteration: 79 | Classification loss: 0.00060 | Regression loss: 0.03931 | Running loss: 0.02125\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.722396113600594\n",
            "Precision:  0.584\n",
            "Recall:  0.7891891891891892\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}, {0: (0.7229906021615096, 185.0)}, {0: (0.7259378939307871, 185.0)}, {0: (0.7306172160652471, 185.0)}, {0: (0.7210590702449308, 185.0)}, {0: (0.7214498931770392, 185.0)}, {0: (0.7272071618498421, 185.0)}, {0: (0.725966639522496, 185.0)}, {0: (0.7208028358685962, 185.0)}, {0: (0.7255636845600553, 185.0)}, {0: (0.7261742190616329, 185.0)}, {0: (0.7217332811149212, 185.0)}, {0: (0.7261653370413381, 185.0)}, {0: (0.7215292823555803, 185.0)}, {0: (0.7277253912015531, 185.0)}, {0: (0.7219177066363771, 185.0)}, {0: (0.7274783215794732, 185.0)}, {0: (0.7218983427777539, 185.0)}, {0: (0.7270037695200344, 185.0)}, {0: (0.7272851879977948, 185.0)}, {0: (0.7221052381441093, 185.0)}, {0: (0.7218191972667742, 185.0)}, {0: (0.7259991707220859, 185.0)}, {0: (0.7281551087798303, 185.0)}, {0: (0.732505495958946, 185.0)}, {0: (0.7266266562021302, 185.0)}, {0: (0.7320831049102212, 185.0)}, {0: (0.7278242123515042, 185.0)}, {0: (0.7278307869685627, 185.0)}, {0: (0.7325520428180083, 185.0)}, {0: (0.7216918061885735, 185.0)}, {0: (0.722396113600594, 185.0)}]\n",
            "Epoch: 185 | Iteration: 0 | Classification loss: 0.00017 | Regression loss: 0.03134 | Running loss: 0.02128\n",
            "Epoch: 185 | Iteration: 1 | Classification loss: 0.00025 | Regression loss: 0.02856 | Running loss: 0.02131\n",
            "Epoch: 185 | Iteration: 2 | Classification loss: 0.00013 | Regression loss: 0.03176 | Running loss: 0.02133\n",
            "Epoch: 185 | Iteration: 3 | Classification loss: 0.00016 | Regression loss: 0.01623 | Running loss: 0.02133\n",
            "Epoch: 185 | Iteration: 4 | Classification loss: 0.00011 | Regression loss: 0.01484 | Running loss: 0.02134\n",
            "Epoch: 185 | Iteration: 5 | Classification loss: 0.00021 | Regression loss: 0.02816 | Running loss: 0.02136\n",
            "Epoch: 185 | Iteration: 6 | Classification loss: 0.00022 | Regression loss: 0.02456 | Running loss: 0.02132\n",
            "Epoch: 185 | Iteration: 7 | Classification loss: 0.00044 | Regression loss: 0.03548 | Running loss: 0.02132\n",
            "Epoch: 185 | Iteration: 8 | Classification loss: 0.00004 | Regression loss: 0.01231 | Running loss: 0.02128\n",
            "Epoch: 185 | Iteration: 9 | Classification loss: 0.00011 | Regression loss: 0.02169 | Running loss: 0.02129\n",
            "Epoch: 185 | Iteration: 10 | Classification loss: 0.00009 | Regression loss: 0.01680 | Running loss: 0.02131\n",
            "Epoch: 185 | Iteration: 11 | Classification loss: 0.00007 | Regression loss: 0.01134 | Running loss: 0.02129\n",
            "Epoch: 185 | Iteration: 12 | Classification loss: 0.00005 | Regression loss: 0.02342 | Running loss: 0.02127\n",
            "Epoch: 185 | Iteration: 13 | Classification loss: 0.00026 | Regression loss: 0.02026 | Running loss: 0.02127\n",
            "Epoch: 185 | Iteration: 14 | Classification loss: 0.00015 | Regression loss: 0.02969 | Running loss: 0.02130\n",
            "Epoch: 185 | Iteration: 15 | Classification loss: 0.00011 | Regression loss: 0.01875 | Running loss: 0.02126\n",
            "Epoch: 185 | Iteration: 16 | Classification loss: 0.00009 | Regression loss: 0.01603 | Running loss: 0.02124\n",
            "Epoch: 185 | Iteration: 17 | Classification loss: 0.00049 | Regression loss: 0.03349 | Running loss: 0.02129\n",
            "Epoch: 185 | Iteration: 18 | Classification loss: 0.00006 | Regression loss: 0.01181 | Running loss: 0.02128\n",
            "Epoch: 185 | Iteration: 19 | Classification loss: 0.00030 | Regression loss: 0.04145 | Running loss: 0.02132\n",
            "Epoch: 185 | Iteration: 20 | Classification loss: 0.00014 | Regression loss: 0.01126 | Running loss: 0.02130\n",
            "Epoch: 185 | Iteration: 21 | Classification loss: 0.00007 | Regression loss: 0.02464 | Running loss: 0.02130\n",
            "Epoch: 185 | Iteration: 22 | Classification loss: 0.00012 | Regression loss: 0.01603 | Running loss: 0.02126\n",
            "Epoch: 185 | Iteration: 23 | Classification loss: 0.00004 | Regression loss: 0.00793 | Running loss: 0.02121\n",
            "Epoch: 185 | Iteration: 24 | Classification loss: 0.00010 | Regression loss: 0.01720 | Running loss: 0.02123\n",
            "Epoch: 185 | Iteration: 25 | Classification loss: 0.00005 | Regression loss: 0.02065 | Running loss: 0.02121\n",
            "Epoch: 185 | Iteration: 26 | Classification loss: 0.00004 | Regression loss: 0.01487 | Running loss: 0.02120\n",
            "Epoch: 185 | Iteration: 27 | Classification loss: 0.00032 | Regression loss: 0.04698 | Running loss: 0.02126\n",
            "Epoch: 185 | Iteration: 28 | Classification loss: 0.00002 | Regression loss: 0.00386 | Running loss: 0.02124\n",
            "Epoch: 185 | Iteration: 29 | Classification loss: 0.00028 | Regression loss: 0.03656 | Running loss: 0.02129\n",
            "Epoch: 185 | Iteration: 30 | Classification loss: 0.00013 | Regression loss: 0.01244 | Running loss: 0.02128\n",
            "Epoch: 185 | Iteration: 31 | Classification loss: 0.00011 | Regression loss: 0.01157 | Running loss: 0.02125\n",
            "Epoch: 185 | Iteration: 32 | Classification loss: 0.00033 | Regression loss: 0.01940 | Running loss: 0.02124\n",
            "Epoch: 185 | Iteration: 33 | Classification loss: 0.00005 | Regression loss: 0.00495 | Running loss: 0.02119\n",
            "Epoch: 185 | Iteration: 34 | Classification loss: 0.00022 | Regression loss: 0.02447 | Running loss: 0.02120\n",
            "Epoch: 185 | Iteration: 35 | Classification loss: 0.00005 | Regression loss: 0.01326 | Running loss: 0.02119\n",
            "Epoch: 185 | Iteration: 36 | Classification loss: 0.00033 | Regression loss: 0.03357 | Running loss: 0.02118\n",
            "Epoch: 185 | Iteration: 37 | Classification loss: 0.00012 | Regression loss: 0.03446 | Running loss: 0.02120\n",
            "Epoch: 185 | Iteration: 38 | Classification loss: 0.00024 | Regression loss: 0.03095 | Running loss: 0.02126\n",
            "Epoch: 185 | Iteration: 39 | Classification loss: 0.00009 | Regression loss: 0.01473 | Running loss: 0.02127\n",
            "Epoch: 185 | Iteration: 40 | Classification loss: 0.00006 | Regression loss: 0.02900 | Running loss: 0.02129\n",
            "Epoch: 185 | Iteration: 41 | Classification loss: 0.00015 | Regression loss: 0.01630 | Running loss: 0.02130\n",
            "Epoch: 185 | Iteration: 42 | Classification loss: 0.00014 | Regression loss: 0.01623 | Running loss: 0.02130\n",
            "Epoch: 185 | Iteration: 43 | Classification loss: 0.00021 | Regression loss: 0.03055 | Running loss: 0.02131\n",
            "Epoch: 185 | Iteration: 44 | Classification loss: 0.00054 | Regression loss: 0.03396 | Running loss: 0.02135\n",
            "Epoch: 185 | Iteration: 45 | Classification loss: 0.00005 | Regression loss: 0.01881 | Running loss: 0.02136\n",
            "Epoch: 185 | Iteration: 46 | Classification loss: 0.00012 | Regression loss: 0.02379 | Running loss: 0.02135\n",
            "Epoch: 185 | Iteration: 47 | Classification loss: 0.00015 | Regression loss: 0.01922 | Running loss: 0.02132\n",
            "Epoch: 185 | Iteration: 48 | Classification loss: 0.00009 | Regression loss: 0.00939 | Running loss: 0.02127\n",
            "Epoch: 185 | Iteration: 49 | Classification loss: 0.00002 | Regression loss: 0.01389 | Running loss: 0.02126\n",
            "Epoch: 185 | Iteration: 50 | Classification loss: 0.00032 | Regression loss: 0.02223 | Running loss: 0.02123\n",
            "Epoch: 185 | Iteration: 51 | Classification loss: 0.00012 | Regression loss: 0.02393 | Running loss: 0.02121\n",
            "Epoch: 185 | Iteration: 52 | Classification loss: 0.00032 | Regression loss: 0.02757 | Running loss: 0.02121\n",
            "Epoch: 185 | Iteration: 53 | Classification loss: 0.00033 | Regression loss: 0.03749 | Running loss: 0.02124\n",
            "Epoch: 185 | Iteration: 54 | Classification loss: 0.00008 | Regression loss: 0.01357 | Running loss: 0.02124\n",
            "Epoch: 185 | Iteration: 55 | Classification loss: 0.00019 | Regression loss: 0.01488 | Running loss: 0.02125\n",
            "Epoch: 185 | Iteration: 56 | Classification loss: 0.00011 | Regression loss: 0.01564 | Running loss: 0.02124\n",
            "Epoch: 185 | Iteration: 57 | Classification loss: 0.00007 | Regression loss: 0.01383 | Running loss: 0.02124\n",
            "Epoch: 185 | Iteration: 58 | Classification loss: 0.00009 | Regression loss: 0.02962 | Running loss: 0.02127\n",
            "Epoch: 185 | Iteration: 59 | Classification loss: 0.00002 | Regression loss: 0.00857 | Running loss: 0.02127\n",
            "Epoch: 185 | Iteration: 60 | Classification loss: 0.00012 | Regression loss: 0.02187 | Running loss: 0.02129\n",
            "Epoch: 185 | Iteration: 61 | Classification loss: 0.00004 | Regression loss: 0.01996 | Running loss: 0.02129\n",
            "Epoch: 185 | Iteration: 62 | Classification loss: 0.00008 | Regression loss: 0.01241 | Running loss: 0.02128\n",
            "Epoch: 185 | Iteration: 63 | Classification loss: 0.00008 | Regression loss: 0.01602 | Running loss: 0.02126\n",
            "Epoch: 185 | Iteration: 64 | Classification loss: 0.00023 | Regression loss: 0.02207 | Running loss: 0.02128\n",
            "Epoch: 185 | Iteration: 65 | Classification loss: 0.00013 | Regression loss: 0.02363 | Running loss: 0.02126\n",
            "Epoch: 185 | Iteration: 66 | Classification loss: 0.00015 | Regression loss: 0.01198 | Running loss: 0.02125\n",
            "Epoch: 185 | Iteration: 67 | Classification loss: 0.00003 | Regression loss: 0.00723 | Running loss: 0.02120\n",
            "Epoch: 185 | Iteration: 68 | Classification loss: 0.00053 | Regression loss: 0.02221 | Running loss: 0.02118\n",
            "Epoch: 185 | Iteration: 69 | Classification loss: 0.00020 | Regression loss: 0.01803 | Running loss: 0.02120\n",
            "Epoch: 185 | Iteration: 70 | Classification loss: 0.00009 | Regression loss: 0.02116 | Running loss: 0.02114\n",
            "Epoch: 185 | Iteration: 71 | Classification loss: 0.00011 | Regression loss: 0.01207 | Running loss: 0.02110\n",
            "Epoch: 185 | Iteration: 72 | Classification loss: 0.00036 | Regression loss: 0.02280 | Running loss: 0.02111\n",
            "Epoch: 185 | Iteration: 73 | Classification loss: 0.00008 | Regression loss: 0.01796 | Running loss: 0.02110\n",
            "Epoch: 185 | Iteration: 74 | Classification loss: 0.00006 | Regression loss: 0.00882 | Running loss: 0.02108\n",
            "Epoch: 185 | Iteration: 75 | Classification loss: 0.00038 | Regression loss: 0.03896 | Running loss: 0.02113\n",
            "Epoch: 185 | Iteration: 76 | Classification loss: 0.00006 | Regression loss: 0.02391 | Running loss: 0.02115\n",
            "Epoch: 185 | Iteration: 77 | Classification loss: 0.00008 | Regression loss: 0.01892 | Running loss: 0.02111\n",
            "Epoch: 185 | Iteration: 78 | Classification loss: 0.00011 | Regression loss: 0.02300 | Running loss: 0.02111\n",
            "Epoch: 185 | Iteration: 79 | Classification loss: 0.00006 | Regression loss: 0.01419 | Running loss: 0.02110\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7220707898936956\n",
            "Precision:  0.5863453815261044\n",
            "Recall:  0.7891891891891892\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}, {0: (0.7229906021615096, 185.0)}, {0: (0.7259378939307871, 185.0)}, {0: (0.7306172160652471, 185.0)}, {0: (0.7210590702449308, 185.0)}, {0: (0.7214498931770392, 185.0)}, {0: (0.7272071618498421, 185.0)}, {0: (0.725966639522496, 185.0)}, {0: (0.7208028358685962, 185.0)}, {0: (0.7255636845600553, 185.0)}, {0: (0.7261742190616329, 185.0)}, {0: (0.7217332811149212, 185.0)}, {0: (0.7261653370413381, 185.0)}, {0: (0.7215292823555803, 185.0)}, {0: (0.7277253912015531, 185.0)}, {0: (0.7219177066363771, 185.0)}, {0: (0.7274783215794732, 185.0)}, {0: (0.7218983427777539, 185.0)}, {0: (0.7270037695200344, 185.0)}, {0: (0.7272851879977948, 185.0)}, {0: (0.7221052381441093, 185.0)}, {0: (0.7218191972667742, 185.0)}, {0: (0.7259991707220859, 185.0)}, {0: (0.7281551087798303, 185.0)}, {0: (0.732505495958946, 185.0)}, {0: (0.7266266562021302, 185.0)}, {0: (0.7320831049102212, 185.0)}, {0: (0.7278242123515042, 185.0)}, {0: (0.7278307869685627, 185.0)}, {0: (0.7325520428180083, 185.0)}, {0: (0.7216918061885735, 185.0)}, {0: (0.722396113600594, 185.0)}, {0: (0.7220707898936956, 185.0)}]\n",
            "Epoch: 186 | Iteration: 0 | Classification loss: 0.00023 | Regression loss: 0.02821 | Running loss: 0.02112\n",
            "Epoch: 186 | Iteration: 1 | Classification loss: 0.00003 | Regression loss: 0.01915 | Running loss: 0.02111\n",
            "Epoch: 186 | Iteration: 2 | Classification loss: 0.00014 | Regression loss: 0.01742 | Running loss: 0.02112\n",
            "Epoch: 186 | Iteration: 3 | Classification loss: 0.00004 | Regression loss: 0.01236 | Running loss: 0.02111\n",
            "Epoch: 186 | Iteration: 4 | Classification loss: 0.00040 | Regression loss: 0.01730 | Running loss: 0.02111\n",
            "Epoch: 186 | Iteration: 5 | Classification loss: 0.00058 | Regression loss: 0.03834 | Running loss: 0.02114\n",
            "Epoch: 186 | Iteration: 6 | Classification loss: 0.00013 | Regression loss: 0.02025 | Running loss: 0.02114\n",
            "Epoch: 186 | Iteration: 7 | Classification loss: 0.00022 | Regression loss: 0.02058 | Running loss: 0.02115\n",
            "Epoch: 186 | Iteration: 8 | Classification loss: 0.00016 | Regression loss: 0.01708 | Running loss: 0.02112\n",
            "Epoch: 186 | Iteration: 9 | Classification loss: 0.00015 | Regression loss: 0.03441 | Running loss: 0.02116\n",
            "Epoch: 186 | Iteration: 10 | Classification loss: 0.00011 | Regression loss: 0.01552 | Running loss: 0.02114\n",
            "Epoch: 186 | Iteration: 11 | Classification loss: 0.00008 | Regression loss: 0.01974 | Running loss: 0.02114\n",
            "Epoch: 186 | Iteration: 12 | Classification loss: 0.00023 | Regression loss: 0.02227 | Running loss: 0.02110\n",
            "Epoch: 186 | Iteration: 13 | Classification loss: 0.00039 | Regression loss: 0.04035 | Running loss: 0.02116\n",
            "Epoch: 186 | Iteration: 14 | Classification loss: 0.00029 | Regression loss: 0.03238 | Running loss: 0.02116\n",
            "Epoch: 186 | Iteration: 15 | Classification loss: 0.00013 | Regression loss: 0.01192 | Running loss: 0.02117\n",
            "Epoch: 186 | Iteration: 16 | Classification loss: 0.00016 | Regression loss: 0.01772 | Running loss: 0.02119\n",
            "Epoch: 186 | Iteration: 17 | Classification loss: 0.00014 | Regression loss: 0.02662 | Running loss: 0.02121\n",
            "Epoch: 186 | Iteration: 18 | Classification loss: 0.00004 | Regression loss: 0.00709 | Running loss: 0.02116\n",
            "Epoch: 186 | Iteration: 19 | Classification loss: 0.00007 | Regression loss: 0.01338 | Running loss: 0.02114\n",
            "Epoch: 186 | Iteration: 20 | Classification loss: 0.00018 | Regression loss: 0.02292 | Running loss: 0.02115\n",
            "Epoch: 186 | Iteration: 21 | Classification loss: 0.00021 | Regression loss: 0.02878 | Running loss: 0.02115\n",
            "Epoch: 186 | Iteration: 22 | Classification loss: 0.00015 | Regression loss: 0.01325 | Running loss: 0.02112\n",
            "Epoch: 186 | Iteration: 23 | Classification loss: 0.00006 | Regression loss: 0.01805 | Running loss: 0.02110\n",
            "Epoch: 186 | Iteration: 24 | Classification loss: 0.00007 | Regression loss: 0.00902 | Running loss: 0.02104\n",
            "Epoch: 186 | Iteration: 25 | Classification loss: 0.00002 | Regression loss: 0.00836 | Running loss: 0.02102\n",
            "Epoch: 186 | Iteration: 26 | Classification loss: 0.00019 | Regression loss: 0.03240 | Running loss: 0.02106\n",
            "Epoch: 186 | Iteration: 27 | Classification loss: 0.00011 | Regression loss: 0.01699 | Running loss: 0.02107\n",
            "Epoch: 186 | Iteration: 28 | Classification loss: 0.00005 | Regression loss: 0.00537 | Running loss: 0.02105\n",
            "Epoch: 186 | Iteration: 29 | Classification loss: 0.00009 | Regression loss: 0.01606 | Running loss: 0.02102\n",
            "Epoch: 186 | Iteration: 30 | Classification loss: 0.00002 | Regression loss: 0.00410 | Running loss: 0.02096\n",
            "Epoch: 186 | Iteration: 31 | Classification loss: 0.00013 | Regression loss: 0.01497 | Running loss: 0.02095\n",
            "Epoch: 186 | Iteration: 32 | Classification loss: 0.00030 | Regression loss: 0.03929 | Running loss: 0.02100\n",
            "Epoch: 186 | Iteration: 33 | Classification loss: 0.00036 | Regression loss: 0.02131 | Running loss: 0.02102\n",
            "Epoch: 186 | Iteration: 34 | Classification loss: 0.00009 | Regression loss: 0.02201 | Running loss: 0.02100\n",
            "Epoch: 186 | Iteration: 35 | Classification loss: 0.00013 | Regression loss: 0.01880 | Running loss: 0.02097\n",
            "Epoch: 186 | Iteration: 36 | Classification loss: 0.00011 | Regression loss: 0.02441 | Running loss: 0.02098\n",
            "Epoch: 186 | Iteration: 37 | Classification loss: 0.00010 | Regression loss: 0.01179 | Running loss: 0.02095\n",
            "Epoch: 186 | Iteration: 38 | Classification loss: 0.00010 | Regression loss: 0.01696 | Running loss: 0.02098\n",
            "Epoch: 186 | Iteration: 39 | Classification loss: 0.00021 | Regression loss: 0.03202 | Running loss: 0.02101\n",
            "Epoch: 186 | Iteration: 40 | Classification loss: 0.00025 | Regression loss: 0.02365 | Running loss: 0.02103\n",
            "Epoch: 186 | Iteration: 41 | Classification loss: 0.00012 | Regression loss: 0.01282 | Running loss: 0.02103\n",
            "Epoch: 186 | Iteration: 42 | Classification loss: 0.00009 | Regression loss: 0.02066 | Running loss: 0.02104\n",
            "Epoch: 186 | Iteration: 43 | Classification loss: 0.00014 | Regression loss: 0.02377 | Running loss: 0.02106\n",
            "Epoch: 186 | Iteration: 44 | Classification loss: 0.00007 | Regression loss: 0.01353 | Running loss: 0.02107\n",
            "Epoch: 186 | Iteration: 45 | Classification loss: 0.00019 | Regression loss: 0.01654 | Running loss: 0.02106\n",
            "Epoch: 186 | Iteration: 46 | Classification loss: 0.00036 | Regression loss: 0.03549 | Running loss: 0.02107\n",
            "Epoch: 186 | Iteration: 47 | Classification loss: 0.00003 | Regression loss: 0.00791 | Running loss: 0.02101\n",
            "Epoch: 186 | Iteration: 48 | Classification loss: 0.00007 | Regression loss: 0.02818 | Running loss: 0.02104\n",
            "Epoch: 186 | Iteration: 49 | Classification loss: 0.00035 | Regression loss: 0.01958 | Running loss: 0.02104\n",
            "Epoch: 186 | Iteration: 50 | Classification loss: 0.00008 | Regression loss: 0.01463 | Running loss: 0.02103\n",
            "Epoch: 186 | Iteration: 51 | Classification loss: 0.00007 | Regression loss: 0.01415 | Running loss: 0.02102\n",
            "Epoch: 186 | Iteration: 52 | Classification loss: 0.00007 | Regression loss: 0.02431 | Running loss: 0.02099\n",
            "Epoch: 186 | Iteration: 53 | Classification loss: 0.00006 | Regression loss: 0.01094 | Running loss: 0.02098\n",
            "Epoch: 186 | Iteration: 54 | Classification loss: 0.00005 | Regression loss: 0.01280 | Running loss: 0.02095\n",
            "Epoch: 186 | Iteration: 55 | Classification loss: 0.00009 | Regression loss: 0.01676 | Running loss: 0.02091\n",
            "Epoch: 186 | Iteration: 56 | Classification loss: 0.00052 | Regression loss: 0.03375 | Running loss: 0.02093\n",
            "Epoch: 186 | Iteration: 57 | Classification loss: 0.00003 | Regression loss: 0.01422 | Running loss: 0.02088\n",
            "Epoch: 186 | Iteration: 58 | Classification loss: 0.00011 | Regression loss: 0.02408 | Running loss: 0.02089\n",
            "Epoch: 186 | Iteration: 59 | Classification loss: 0.00048 | Regression loss: 0.03279 | Running loss: 0.02091\n",
            "Epoch: 186 | Iteration: 60 | Classification loss: 0.00011 | Regression loss: 0.01655 | Running loss: 0.02092\n",
            "Epoch: 186 | Iteration: 61 | Classification loss: 0.00005 | Regression loss: 0.00823 | Running loss: 0.02091\n",
            "Epoch: 186 | Iteration: 62 | Classification loss: 0.00009 | Regression loss: 0.01211 | Running loss: 0.02090\n",
            "Epoch: 186 | Iteration: 63 | Classification loss: 0.00009 | Regression loss: 0.01530 | Running loss: 0.02088\n",
            "Epoch: 186 | Iteration: 64 | Classification loss: 0.00011 | Regression loss: 0.01429 | Running loss: 0.02084\n",
            "Epoch: 186 | Iteration: 65 | Classification loss: 0.00015 | Regression loss: 0.02532 | Running loss: 0.02086\n",
            "Epoch: 186 | Iteration: 66 | Classification loss: 0.00005 | Regression loss: 0.02823 | Running loss: 0.02086\n",
            "Epoch: 186 | Iteration: 67 | Classification loss: 0.00008 | Regression loss: 0.01931 | Running loss: 0.02087\n",
            "Epoch: 186 | Iteration: 68 | Classification loss: 0.00030 | Regression loss: 0.04615 | Running loss: 0.02095\n",
            "Epoch: 186 | Iteration: 69 | Classification loss: 0.00013 | Regression loss: 0.01898 | Running loss: 0.02096\n",
            "Epoch: 186 | Iteration: 70 | Classification loss: 0.00009 | Regression loss: 0.01539 | Running loss: 0.02090\n",
            "Epoch: 186 | Iteration: 71 | Classification loss: 0.00005 | Regression loss: 0.01469 | Running loss: 0.02090\n",
            "Epoch: 186 | Iteration: 72 | Classification loss: 0.00028 | Regression loss: 0.03912 | Running loss: 0.02095\n",
            "Epoch: 186 | Iteration: 73 | Classification loss: 0.00004 | Regression loss: 0.01815 | Running loss: 0.02092\n",
            "Epoch: 186 | Iteration: 74 | Classification loss: 0.00009 | Regression loss: 0.00993 | Running loss: 0.02092\n",
            "Epoch: 186 | Iteration: 75 | Classification loss: 0.00039 | Regression loss: 0.02664 | Running loss: 0.02095\n",
            "Epoch: 186 | Iteration: 76 | Classification loss: 0.00014 | Regression loss: 0.03410 | Running loss: 0.02097\n",
            "Epoch: 186 | Iteration: 77 | Classification loss: 0.00012 | Regression loss: 0.01405 | Running loss: 0.02097\n",
            "Epoch: 186 | Iteration: 78 | Classification loss: 0.00005 | Regression loss: 0.02134 | Running loss: 0.02097\n",
            "Epoch: 186 | Iteration: 79 | Classification loss: 0.00036 | Regression loss: 0.03695 | Running loss: 0.02101\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7217779820060826\n",
            "Precision:  0.5910931174089069\n",
            "Recall:  0.7891891891891892\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}, {0: (0.7229906021615096, 185.0)}, {0: (0.7259378939307871, 185.0)}, {0: (0.7306172160652471, 185.0)}, {0: (0.7210590702449308, 185.0)}, {0: (0.7214498931770392, 185.0)}, {0: (0.7272071618498421, 185.0)}, {0: (0.725966639522496, 185.0)}, {0: (0.7208028358685962, 185.0)}, {0: (0.7255636845600553, 185.0)}, {0: (0.7261742190616329, 185.0)}, {0: (0.7217332811149212, 185.0)}, {0: (0.7261653370413381, 185.0)}, {0: (0.7215292823555803, 185.0)}, {0: (0.7277253912015531, 185.0)}, {0: (0.7219177066363771, 185.0)}, {0: (0.7274783215794732, 185.0)}, {0: (0.7218983427777539, 185.0)}, {0: (0.7270037695200344, 185.0)}, {0: (0.7272851879977948, 185.0)}, {0: (0.7221052381441093, 185.0)}, {0: (0.7218191972667742, 185.0)}, {0: (0.7259991707220859, 185.0)}, {0: (0.7281551087798303, 185.0)}, {0: (0.732505495958946, 185.0)}, {0: (0.7266266562021302, 185.0)}, {0: (0.7320831049102212, 185.0)}, {0: (0.7278242123515042, 185.0)}, {0: (0.7278307869685627, 185.0)}, {0: (0.7325520428180083, 185.0)}, {0: (0.7216918061885735, 185.0)}, {0: (0.722396113600594, 185.0)}, {0: (0.7220707898936956, 185.0)}, {0: (0.7217779820060826, 185.0)}]\n",
            "Epoch: 187 | Iteration: 0 | Classification loss: 0.00028 | Regression loss: 0.02962 | Running loss: 0.02100\n",
            "Epoch: 187 | Iteration: 1 | Classification loss: 0.00006 | Regression loss: 0.00740 | Running loss: 0.02097\n",
            "Epoch: 187 | Iteration: 2 | Classification loss: 0.00005 | Regression loss: 0.02123 | Running loss: 0.02099\n",
            "Epoch: 187 | Iteration: 3 | Classification loss: 0.00008 | Regression loss: 0.02489 | Running loss: 0.02099\n",
            "Epoch: 187 | Iteration: 4 | Classification loss: 0.00009 | Regression loss: 0.02285 | Running loss: 0.02101\n",
            "Epoch: 187 | Iteration: 5 | Classification loss: 0.00004 | Regression loss: 0.01435 | Running loss: 0.02102\n",
            "Epoch: 187 | Iteration: 6 | Classification loss: 0.00024 | Regression loss: 0.02489 | Running loss: 0.02102\n",
            "Epoch: 187 | Iteration: 7 | Classification loss: 0.00008 | Regression loss: 0.01233 | Running loss: 0.02101\n",
            "Epoch: 187 | Iteration: 8 | Classification loss: 0.00018 | Regression loss: 0.02987 | Running loss: 0.02102\n",
            "Epoch: 187 | Iteration: 9 | Classification loss: 0.00015 | Regression loss: 0.01773 | Running loss: 0.02103\n",
            "Epoch: 187 | Iteration: 10 | Classification loss: 0.00033 | Regression loss: 0.01863 | Running loss: 0.02104\n",
            "Epoch: 187 | Iteration: 11 | Classification loss: 0.00008 | Regression loss: 0.01635 | Running loss: 0.02103\n",
            "Epoch: 187 | Iteration: 12 | Classification loss: 0.00011 | Regression loss: 0.02481 | Running loss: 0.02105\n",
            "Epoch: 187 | Iteration: 13 | Classification loss: 0.00013 | Regression loss: 0.03312 | Running loss: 0.02110\n",
            "Epoch: 187 | Iteration: 14 | Classification loss: 0.00008 | Regression loss: 0.00854 | Running loss: 0.02107\n",
            "Epoch: 187 | Iteration: 15 | Classification loss: 0.00049 | Regression loss: 0.02170 | Running loss: 0.02108\n",
            "Epoch: 187 | Iteration: 16 | Classification loss: 0.00030 | Regression loss: 0.04594 | Running loss: 0.02110\n",
            "Epoch: 187 | Iteration: 17 | Classification loss: 0.00002 | Regression loss: 0.00839 | Running loss: 0.02107\n",
            "Epoch: 187 | Iteration: 18 | Classification loss: 0.00015 | Regression loss: 0.01757 | Running loss: 0.02105\n",
            "Epoch: 187 | Iteration: 19 | Classification loss: 0.00020 | Regression loss: 0.02042 | Running loss: 0.02103\n",
            "Epoch: 187 | Iteration: 20 | Classification loss: 0.00023 | Regression loss: 0.02326 | Running loss: 0.02104\n",
            "Epoch: 187 | Iteration: 21 | Classification loss: 0.00006 | Regression loss: 0.01541 | Running loss: 0.02101\n",
            "Epoch: 187 | Iteration: 22 | Classification loss: 0.00037 | Regression loss: 0.02136 | Running loss: 0.02098\n",
            "Epoch: 187 | Iteration: 23 | Classification loss: 0.00022 | Regression loss: 0.02380 | Running loss: 0.02096\n",
            "Epoch: 187 | Iteration: 24 | Classification loss: 0.00004 | Regression loss: 0.00766 | Running loss: 0.02092\n",
            "Epoch: 187 | Iteration: 25 | Classification loss: 0.00047 | Regression loss: 0.03311 | Running loss: 0.02095\n",
            "Epoch: 187 | Iteration: 26 | Classification loss: 0.00012 | Regression loss: 0.02115 | Running loss: 0.02098\n",
            "Epoch: 187 | Iteration: 27 | Classification loss: 0.00008 | Regression loss: 0.01398 | Running loss: 0.02094\n",
            "Epoch: 187 | Iteration: 28 | Classification loss: 0.00012 | Regression loss: 0.01382 | Running loss: 0.02095\n",
            "Epoch: 187 | Iteration: 29 | Classification loss: 0.00013 | Regression loss: 0.01621 | Running loss: 0.02093\n",
            "Epoch: 187 | Iteration: 30 | Classification loss: 0.00012 | Regression loss: 0.01195 | Running loss: 0.02092\n",
            "Epoch: 187 | Iteration: 31 | Classification loss: 0.00034 | Regression loss: 0.03448 | Running loss: 0.02096\n",
            "Epoch: 187 | Iteration: 32 | Classification loss: 0.00033 | Regression loss: 0.03939 | Running loss: 0.02099\n",
            "Epoch: 187 | Iteration: 33 | Classification loss: 0.00010 | Regression loss: 0.01193 | Running loss: 0.02097\n",
            "Epoch: 187 | Iteration: 34 | Classification loss: 0.00004 | Regression loss: 0.01811 | Running loss: 0.02099\n",
            "Epoch: 187 | Iteration: 35 | Classification loss: 0.00017 | Regression loss: 0.03277 | Running loss: 0.02098\n",
            "Epoch: 187 | Iteration: 36 | Classification loss: 0.00022 | Regression loss: 0.02888 | Running loss: 0.02101\n",
            "Epoch: 187 | Iteration: 37 | Classification loss: 0.00013 | Regression loss: 0.01136 | Running loss: 0.02101\n",
            "Epoch: 187 | Iteration: 38 | Classification loss: 0.00012 | Regression loss: 0.01407 | Running loss: 0.02102\n",
            "Epoch: 187 | Iteration: 39 | Classification loss: 0.00010 | Regression loss: 0.01564 | Running loss: 0.02100\n",
            "Epoch: 187 | Iteration: 40 | Classification loss: 0.00005 | Regression loss: 0.00465 | Running loss: 0.02098\n",
            "Epoch: 187 | Iteration: 41 | Classification loss: 0.00007 | Regression loss: 0.01542 | Running loss: 0.02097\n",
            "Epoch: 187 | Iteration: 42 | Classification loss: 0.00005 | Regression loss: 0.01307 | Running loss: 0.02095\n",
            "Epoch: 187 | Iteration: 43 | Classification loss: 0.00027 | Regression loss: 0.03536 | Running loss: 0.02097\n",
            "Epoch: 187 | Iteration: 44 | Classification loss: 0.00005 | Regression loss: 0.01341 | Running loss: 0.02097\n",
            "Epoch: 187 | Iteration: 45 | Classification loss: 0.00028 | Regression loss: 0.02095 | Running loss: 0.02097\n",
            "Epoch: 187 | Iteration: 46 | Classification loss: 0.00009 | Regression loss: 0.01575 | Running loss: 0.02097\n",
            "Epoch: 187 | Iteration: 47 | Classification loss: 0.00008 | Regression loss: 0.02289 | Running loss: 0.02096\n",
            "Epoch: 187 | Iteration: 48 | Classification loss: 0.00002 | Regression loss: 0.00383 | Running loss: 0.02094\n",
            "Epoch: 187 | Iteration: 49 | Classification loss: 0.00011 | Regression loss: 0.01690 | Running loss: 0.02094\n",
            "Epoch: 187 | Iteration: 50 | Classification loss: 0.00045 | Regression loss: 0.03649 | Running loss: 0.02097\n",
            "Epoch: 187 | Iteration: 51 | Classification loss: 0.00014 | Regression loss: 0.01647 | Running loss: 0.02096\n",
            "Epoch: 187 | Iteration: 52 | Classification loss: 0.00011 | Regression loss: 0.02712 | Running loss: 0.02097\n",
            "Epoch: 187 | Iteration: 53 | Classification loss: 0.00026 | Regression loss: 0.02515 | Running loss: 0.02098\n",
            "Epoch: 187 | Iteration: 54 | Classification loss: 0.00003 | Regression loss: 0.00669 | Running loss: 0.02098\n",
            "Epoch: 187 | Iteration: 55 | Classification loss: 0.00013 | Regression loss: 0.01282 | Running loss: 0.02096\n",
            "Epoch: 187 | Iteration: 56 | Classification loss: 0.00007 | Regression loss: 0.01164 | Running loss: 0.02089\n",
            "Epoch: 187 | Iteration: 57 | Classification loss: 0.00023 | Regression loss: 0.02746 | Running loss: 0.02088\n",
            "Epoch: 187 | Iteration: 58 | Classification loss: 0.00003 | Regression loss: 0.01383 | Running loss: 0.02083\n",
            "Epoch: 187 | Iteration: 59 | Classification loss: 0.00014 | Regression loss: 0.01084 | Running loss: 0.02082\n",
            "Epoch: 187 | Iteration: 60 | Classification loss: 0.00017 | Regression loss: 0.01928 | Running loss: 0.02079\n",
            "Epoch: 187 | Iteration: 61 | Classification loss: 0.00014 | Regression loss: 0.03366 | Running loss: 0.02082\n",
            "Epoch: 187 | Iteration: 62 | Classification loss: 0.00003 | Regression loss: 0.01602 | Running loss: 0.02082\n",
            "Epoch: 187 | Iteration: 63 | Classification loss: 0.00006 | Regression loss: 0.01194 | Running loss: 0.02081\n",
            "Epoch: 187 | Iteration: 64 | Classification loss: 0.00010 | Regression loss: 0.02335 | Running loss: 0.02083\n",
            "Epoch: 187 | Iteration: 65 | Classification loss: 0.00030 | Regression loss: 0.04263 | Running loss: 0.02088\n",
            "Epoch: 187 | Iteration: 66 | Classification loss: 0.00004 | Regression loss: 0.01907 | Running loss: 0.02089\n",
            "Epoch: 187 | Iteration: 67 | Classification loss: 0.00005 | Regression loss: 0.02498 | Running loss: 0.02089\n",
            "Epoch: 187 | Iteration: 68 | Classification loss: 0.00011 | Regression loss: 0.01477 | Running loss: 0.02085\n",
            "Epoch: 187 | Iteration: 69 | Classification loss: 0.00012 | Regression loss: 0.01499 | Running loss: 0.02085\n",
            "Epoch: 187 | Iteration: 70 | Classification loss: 0.00019 | Regression loss: 0.01267 | Running loss: 0.02084\n",
            "Epoch: 187 | Iteration: 71 | Classification loss: 0.00007 | Regression loss: 0.02092 | Running loss: 0.02082\n",
            "Epoch: 187 | Iteration: 72 | Classification loss: 0.00014 | Regression loss: 0.02636 | Running loss: 0.02084\n",
            "Epoch: 187 | Iteration: 73 | Classification loss: 0.00011 | Regression loss: 0.02330 | Running loss: 0.02085\n",
            "Epoch: 187 | Iteration: 74 | Classification loss: 0.00033 | Regression loss: 0.03437 | Running loss: 0.02089\n",
            "Epoch: 187 | Iteration: 75 | Classification loss: 0.00008 | Regression loss: 0.01851 | Running loss: 0.02089\n",
            "Epoch: 187 | Iteration: 76 | Classification loss: 0.00009 | Regression loss: 0.01935 | Running loss: 0.02086\n",
            "Epoch: 187 | Iteration: 77 | Classification loss: 0.00011 | Regression loss: 0.01622 | Running loss: 0.02086\n",
            "Epoch: 187 | Iteration: 78 | Classification loss: 0.00006 | Regression loss: 0.01310 | Running loss: 0.02084\n",
            "Epoch: 187 | Iteration: 79 | Classification loss: 0.00048 | Regression loss: 0.03514 | Running loss: 0.02085\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7230066917234796\n",
            "Precision:  0.5887096774193549\n",
            "Recall:  0.7891891891891892\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}, {0: (0.7229906021615096, 185.0)}, {0: (0.7259378939307871, 185.0)}, {0: (0.7306172160652471, 185.0)}, {0: (0.7210590702449308, 185.0)}, {0: (0.7214498931770392, 185.0)}, {0: (0.7272071618498421, 185.0)}, {0: (0.725966639522496, 185.0)}, {0: (0.7208028358685962, 185.0)}, {0: (0.7255636845600553, 185.0)}, {0: (0.7261742190616329, 185.0)}, {0: (0.7217332811149212, 185.0)}, {0: (0.7261653370413381, 185.0)}, {0: (0.7215292823555803, 185.0)}, {0: (0.7277253912015531, 185.0)}, {0: (0.7219177066363771, 185.0)}, {0: (0.7274783215794732, 185.0)}, {0: (0.7218983427777539, 185.0)}, {0: (0.7270037695200344, 185.0)}, {0: (0.7272851879977948, 185.0)}, {0: (0.7221052381441093, 185.0)}, {0: (0.7218191972667742, 185.0)}, {0: (0.7259991707220859, 185.0)}, {0: (0.7281551087798303, 185.0)}, {0: (0.732505495958946, 185.0)}, {0: (0.7266266562021302, 185.0)}, {0: (0.7320831049102212, 185.0)}, {0: (0.7278242123515042, 185.0)}, {0: (0.7278307869685627, 185.0)}, {0: (0.7325520428180083, 185.0)}, {0: (0.7216918061885735, 185.0)}, {0: (0.722396113600594, 185.0)}, {0: (0.7220707898936956, 185.0)}, {0: (0.7217779820060826, 185.0)}, {0: (0.7230066917234796, 185.0)}]\n",
            "Epoch: 188 | Iteration: 0 | Classification loss: 0.00004 | Regression loss: 0.01767 | Running loss: 0.02084\n",
            "Epoch: 188 | Iteration: 1 | Classification loss: 0.00003 | Regression loss: 0.01186 | Running loss: 0.02084\n",
            "Epoch: 188 | Iteration: 2 | Classification loss: 0.00030 | Regression loss: 0.01725 | Running loss: 0.02083\n",
            "Epoch: 188 | Iteration: 3 | Classification loss: 0.00045 | Regression loss: 0.03316 | Running loss: 0.02083\n",
            "Epoch: 188 | Iteration: 4 | Classification loss: 0.00006 | Regression loss: 0.02581 | Running loss: 0.02083\n",
            "Epoch: 188 | Iteration: 5 | Classification loss: 0.00010 | Regression loss: 0.01598 | Running loss: 0.02085\n",
            "Epoch: 188 | Iteration: 6 | Classification loss: 0.00018 | Regression loss: 0.03091 | Running loss: 0.02088\n",
            "Epoch: 188 | Iteration: 7 | Classification loss: 0.00008 | Regression loss: 0.01422 | Running loss: 0.02088\n",
            "Epoch: 188 | Iteration: 8 | Classification loss: 0.00023 | Regression loss: 0.02822 | Running loss: 0.02091\n",
            "Epoch: 188 | Iteration: 9 | Classification loss: 0.00004 | Regression loss: 0.00532 | Running loss: 0.02087\n",
            "Epoch: 188 | Iteration: 10 | Classification loss: 0.00010 | Regression loss: 0.00940 | Running loss: 0.02086\n",
            "Epoch: 188 | Iteration: 11 | Classification loss: 0.00011 | Regression loss: 0.01606 | Running loss: 0.02088\n",
            "Epoch: 188 | Iteration: 12 | Classification loss: 0.00011 | Regression loss: 0.01168 | Running loss: 0.02085\n",
            "Epoch: 188 | Iteration: 13 | Classification loss: 0.00003 | Regression loss: 0.01583 | Running loss: 0.02081\n",
            "Epoch: 188 | Iteration: 14 | Classification loss: 0.00008 | Regression loss: 0.01625 | Running loss: 0.02077\n",
            "Epoch: 188 | Iteration: 15 | Classification loss: 0.00015 | Regression loss: 0.01833 | Running loss: 0.02075\n",
            "Epoch: 188 | Iteration: 16 | Classification loss: 0.00008 | Regression loss: 0.02457 | Running loss: 0.02075\n",
            "Epoch: 188 | Iteration: 17 | Classification loss: 0.00018 | Regression loss: 0.02927 | Running loss: 0.02078\n",
            "Epoch: 188 | Iteration: 18 | Classification loss: 0.00010 | Regression loss: 0.01560 | Running loss: 0.02078\n",
            "Epoch: 188 | Iteration: 19 | Classification loss: 0.00002 | Regression loss: 0.01381 | Running loss: 0.02079\n",
            "Epoch: 188 | Iteration: 20 | Classification loss: 0.00012 | Regression loss: 0.03509 | Running loss: 0.02080\n",
            "Epoch: 188 | Iteration: 21 | Classification loss: 0.00010 | Regression loss: 0.03153 | Running loss: 0.02079\n",
            "Epoch: 188 | Iteration: 22 | Classification loss: 0.00008 | Regression loss: 0.01424 | Running loss: 0.02080\n",
            "Epoch: 188 | Iteration: 23 | Classification loss: 0.00004 | Regression loss: 0.02051 | Running loss: 0.02081\n",
            "Epoch: 188 | Iteration: 24 | Classification loss: 0.00013 | Regression loss: 0.02751 | Running loss: 0.02081\n",
            "Epoch: 188 | Iteration: 25 | Classification loss: 0.00014 | Regression loss: 0.03185 | Running loss: 0.02081\n",
            "Epoch: 188 | Iteration: 26 | Classification loss: 0.00008 | Regression loss: 0.01660 | Running loss: 0.02082\n",
            "Epoch: 188 | Iteration: 27 | Classification loss: 0.00002 | Regression loss: 0.00848 | Running loss: 0.02080\n",
            "Epoch: 188 | Iteration: 28 | Classification loss: 0.00047 | Regression loss: 0.02015 | Running loss: 0.02083\n",
            "Epoch: 188 | Iteration: 29 | Classification loss: 0.00042 | Regression loss: 0.03312 | Running loss: 0.02087\n",
            "Epoch: 188 | Iteration: 30 | Classification loss: 0.00002 | Regression loss: 0.00400 | Running loss: 0.02085\n",
            "Epoch: 188 | Iteration: 31 | Classification loss: 0.00011 | Regression loss: 0.01176 | Running loss: 0.02077\n",
            "Epoch: 188 | Iteration: 32 | Classification loss: 0.00004 | Regression loss: 0.00612 | Running loss: 0.02077\n",
            "Epoch: 188 | Iteration: 33 | Classification loss: 0.00006 | Regression loss: 0.02077 | Running loss: 0.02078\n",
            "Epoch: 188 | Iteration: 34 | Classification loss: 0.00023 | Regression loss: 0.03360 | Running loss: 0.02082\n",
            "Epoch: 188 | Iteration: 35 | Classification loss: 0.00004 | Regression loss: 0.00707 | Running loss: 0.02077\n",
            "Epoch: 188 | Iteration: 36 | Classification loss: 0.00021 | Regression loss: 0.02147 | Running loss: 0.02075\n",
            "Epoch: 188 | Iteration: 37 | Classification loss: 0.00008 | Regression loss: 0.00904 | Running loss: 0.02074\n",
            "Epoch: 188 | Iteration: 38 | Classification loss: 0.00014 | Regression loss: 0.02391 | Running loss: 0.02074\n",
            "Epoch: 188 | Iteration: 39 | Classification loss: 0.00025 | Regression loss: 0.02534 | Running loss: 0.02072\n",
            "Epoch: 188 | Iteration: 40 | Classification loss: 0.00008 | Regression loss: 0.00970 | Running loss: 0.02070\n",
            "Epoch: 188 | Iteration: 41 | Classification loss: 0.00008 | Regression loss: 0.02094 | Running loss: 0.02072\n",
            "Epoch: 188 | Iteration: 42 | Classification loss: 0.00006 | Regression loss: 0.01349 | Running loss: 0.02073\n",
            "Epoch: 188 | Iteration: 43 | Classification loss: 0.00009 | Regression loss: 0.02281 | Running loss: 0.02070\n",
            "Epoch: 188 | Iteration: 44 | Classification loss: 0.00014 | Regression loss: 0.01736 | Running loss: 0.02070\n",
            "Epoch: 188 | Iteration: 45 | Classification loss: 0.00004 | Regression loss: 0.02561 | Running loss: 0.02069\n",
            "Epoch: 188 | Iteration: 46 | Classification loss: 0.00005 | Regression loss: 0.01432 | Running loss: 0.02069\n",
            "Epoch: 188 | Iteration: 47 | Classification loss: 0.00020 | Regression loss: 0.02842 | Running loss: 0.02071\n",
            "Epoch: 188 | Iteration: 48 | Classification loss: 0.00005 | Regression loss: 0.01824 | Running loss: 0.02074\n",
            "Epoch: 188 | Iteration: 49 | Classification loss: 0.00020 | Regression loss: 0.02052 | Running loss: 0.02075\n",
            "Epoch: 188 | Iteration: 50 | Classification loss: 0.00011 | Regression loss: 0.01329 | Running loss: 0.02072\n",
            "Epoch: 188 | Iteration: 51 | Classification loss: 0.00011 | Regression loss: 0.02362 | Running loss: 0.02070\n",
            "Epoch: 188 | Iteration: 52 | Classification loss: 0.00020 | Regression loss: 0.03713 | Running loss: 0.02076\n",
            "Epoch: 188 | Iteration: 53 | Classification loss: 0.00013 | Regression loss: 0.00899 | Running loss: 0.02073\n",
            "Epoch: 188 | Iteration: 54 | Classification loss: 0.00020 | Regression loss: 0.01857 | Running loss: 0.02073\n",
            "Epoch: 188 | Iteration: 55 | Classification loss: 0.00028 | Regression loss: 0.04743 | Running loss: 0.02079\n",
            "Epoch: 188 | Iteration: 56 | Classification loss: 0.00006 | Regression loss: 0.02779 | Running loss: 0.02081\n",
            "Epoch: 188 | Iteration: 57 | Classification loss: 0.00009 | Regression loss: 0.01633 | Running loss: 0.02078\n",
            "Epoch: 188 | Iteration: 58 | Classification loss: 0.00041 | Regression loss: 0.03703 | Running loss: 0.02082\n",
            "Epoch: 188 | Iteration: 59 | Classification loss: 0.00016 | Regression loss: 0.01638 | Running loss: 0.02081\n",
            "Epoch: 188 | Iteration: 60 | Classification loss: 0.00008 | Regression loss: 0.01270 | Running loss: 0.02079\n",
            "Epoch: 188 | Iteration: 61 | Classification loss: 0.00010 | Regression loss: 0.02032 | Running loss: 0.02080\n",
            "Epoch: 188 | Iteration: 62 | Classification loss: 0.00004 | Regression loss: 0.01276 | Running loss: 0.02079\n",
            "Epoch: 188 | Iteration: 63 | Classification loss: 0.00015 | Regression loss: 0.02213 | Running loss: 0.02079\n",
            "Epoch: 188 | Iteration: 64 | Classification loss: 0.00014 | Regression loss: 0.02132 | Running loss: 0.02081\n",
            "Epoch: 188 | Iteration: 65 | Classification loss: 0.00035 | Regression loss: 0.03648 | Running loss: 0.02085\n",
            "Epoch: 188 | Iteration: 66 | Classification loss: 0.00014 | Regression loss: 0.01166 | Running loss: 0.02085\n",
            "Epoch: 188 | Iteration: 67 | Classification loss: 0.00017 | Regression loss: 0.01954 | Running loss: 0.02083\n",
            "Epoch: 188 | Iteration: 68 | Classification loss: 0.00034 | Regression loss: 0.02421 | Running loss: 0.02082\n",
            "Epoch: 188 | Iteration: 69 | Classification loss: 0.00009 | Regression loss: 0.01671 | Running loss: 0.02079\n",
            "Epoch: 188 | Iteration: 70 | Classification loss: 0.00009 | Regression loss: 0.01455 | Running loss: 0.02079\n",
            "Epoch: 188 | Iteration: 71 | Classification loss: 0.00028 | Regression loss: 0.03916 | Running loss: 0.02083\n",
            "Epoch: 188 | Iteration: 72 | Classification loss: 0.00027 | Regression loss: 0.03677 | Running loss: 0.02082\n",
            "Epoch: 188 | Iteration: 73 | Classification loss: 0.00010 | Regression loss: 0.01432 | Running loss: 0.02081\n",
            "Epoch: 188 | Iteration: 74 | Classification loss: 0.00012 | Regression loss: 0.02280 | Running loss: 0.02082\n",
            "Epoch: 188 | Iteration: 75 | Classification loss: 0.00032 | Regression loss: 0.03731 | Running loss: 0.02085\n",
            "Epoch: 188 | Iteration: 76 | Classification loss: 0.00025 | Regression loss: 0.02072 | Running loss: 0.02085\n",
            "Epoch: 188 | Iteration: 77 | Classification loss: 0.00025 | Regression loss: 0.02517 | Running loss: 0.02087\n",
            "Epoch: 188 | Iteration: 78 | Classification loss: 0.00020 | Regression loss: 0.01204 | Running loss: 0.02087\n",
            "Epoch: 188 | Iteration: 79 | Classification loss: 0.00006 | Regression loss: 0.01122 | Running loss: 0.02085\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.727859800128543\n",
            "Precision:  0.5903614457831325\n",
            "Recall:  0.7945945945945946\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}, {0: (0.7229906021615096, 185.0)}, {0: (0.7259378939307871, 185.0)}, {0: (0.7306172160652471, 185.0)}, {0: (0.7210590702449308, 185.0)}, {0: (0.7214498931770392, 185.0)}, {0: (0.7272071618498421, 185.0)}, {0: (0.725966639522496, 185.0)}, {0: (0.7208028358685962, 185.0)}, {0: (0.7255636845600553, 185.0)}, {0: (0.7261742190616329, 185.0)}, {0: (0.7217332811149212, 185.0)}, {0: (0.7261653370413381, 185.0)}, {0: (0.7215292823555803, 185.0)}, {0: (0.7277253912015531, 185.0)}, {0: (0.7219177066363771, 185.0)}, {0: (0.7274783215794732, 185.0)}, {0: (0.7218983427777539, 185.0)}, {0: (0.7270037695200344, 185.0)}, {0: (0.7272851879977948, 185.0)}, {0: (0.7221052381441093, 185.0)}, {0: (0.7218191972667742, 185.0)}, {0: (0.7259991707220859, 185.0)}, {0: (0.7281551087798303, 185.0)}, {0: (0.732505495958946, 185.0)}, {0: (0.7266266562021302, 185.0)}, {0: (0.7320831049102212, 185.0)}, {0: (0.7278242123515042, 185.0)}, {0: (0.7278307869685627, 185.0)}, {0: (0.7325520428180083, 185.0)}, {0: (0.7216918061885735, 185.0)}, {0: (0.722396113600594, 185.0)}, {0: (0.7220707898936956, 185.0)}, {0: (0.7217779820060826, 185.0)}, {0: (0.7230066917234796, 185.0)}, {0: (0.727859800128543, 185.0)}]\n",
            "Epoch: 189 | Iteration: 0 | Classification loss: 0.00017 | Regression loss: 0.01710 | Running loss: 0.02084\n",
            "Epoch: 189 | Iteration: 1 | Classification loss: 0.00010 | Regression loss: 0.01555 | Running loss: 0.02085\n",
            "Epoch: 189 | Iteration: 2 | Classification loss: 0.00006 | Regression loss: 0.01323 | Running loss: 0.02082\n",
            "Epoch: 189 | Iteration: 3 | Classification loss: 0.00002 | Regression loss: 0.00378 | Running loss: 0.02082\n",
            "Epoch: 189 | Iteration: 4 | Classification loss: 0.00003 | Regression loss: 0.00602 | Running loss: 0.02079\n",
            "Epoch: 189 | Iteration: 5 | Classification loss: 0.00014 | Regression loss: 0.01148 | Running loss: 0.02076\n",
            "Epoch: 189 | Iteration: 6 | Classification loss: 0.00005 | Regression loss: 0.02311 | Running loss: 0.02076\n",
            "Epoch: 189 | Iteration: 7 | Classification loss: 0.00017 | Regression loss: 0.03395 | Running loss: 0.02078\n",
            "Epoch: 189 | Iteration: 8 | Classification loss: 0.00013 | Regression loss: 0.03011 | Running loss: 0.02078\n",
            "Epoch: 189 | Iteration: 9 | Classification loss: 0.00017 | Regression loss: 0.03344 | Running loss: 0.02081\n",
            "Epoch: 189 | Iteration: 10 | Classification loss: 0.00008 | Regression loss: 0.02586 | Running loss: 0.02083\n",
            "Epoch: 189 | Iteration: 11 | Classification loss: 0.00007 | Regression loss: 0.02017 | Running loss: 0.02082\n",
            "Epoch: 189 | Iteration: 12 | Classification loss: 0.00031 | Regression loss: 0.03234 | Running loss: 0.02083\n",
            "Epoch: 189 | Iteration: 13 | Classification loss: 0.00009 | Regression loss: 0.02076 | Running loss: 0.02083\n",
            "Epoch: 189 | Iteration: 14 | Classification loss: 0.00012 | Regression loss: 0.01062 | Running loss: 0.02077\n",
            "Epoch: 189 | Iteration: 15 | Classification loss: 0.00006 | Regression loss: 0.01947 | Running loss: 0.02079\n",
            "Epoch: 189 | Iteration: 16 | Classification loss: 0.00028 | Regression loss: 0.02768 | Running loss: 0.02081\n",
            "Epoch: 189 | Iteration: 17 | Classification loss: 0.00028 | Regression loss: 0.04707 | Running loss: 0.02083\n",
            "Epoch: 189 | Iteration: 18 | Classification loss: 0.00017 | Regression loss: 0.03727 | Running loss: 0.02087\n",
            "Epoch: 189 | Iteration: 19 | Classification loss: 0.00012 | Regression loss: 0.02756 | Running loss: 0.02088\n",
            "Epoch: 189 | Iteration: 20 | Classification loss: 0.00005 | Regression loss: 0.01358 | Running loss: 0.02087\n",
            "Epoch: 189 | Iteration: 21 | Classification loss: 0.00010 | Regression loss: 0.01765 | Running loss: 0.02087\n",
            "Epoch: 189 | Iteration: 22 | Classification loss: 0.00004 | Regression loss: 0.02012 | Running loss: 0.02085\n",
            "Epoch: 189 | Iteration: 23 | Classification loss: 0.00015 | Regression loss: 0.01698 | Running loss: 0.02085\n",
            "Epoch: 189 | Iteration: 24 | Classification loss: 0.00007 | Regression loss: 0.01372 | Running loss: 0.02084\n",
            "Epoch: 189 | Iteration: 25 | Classification loss: 0.00040 | Regression loss: 0.03609 | Running loss: 0.02087\n",
            "Epoch: 189 | Iteration: 26 | Classification loss: 0.00008 | Regression loss: 0.02004 | Running loss: 0.02088\n",
            "Epoch: 189 | Iteration: 27 | Classification loss: 0.00022 | Regression loss: 0.03971 | Running loss: 0.02090\n",
            "Epoch: 189 | Iteration: 28 | Classification loss: 0.00007 | Regression loss: 0.00873 | Running loss: 0.02088\n",
            "Epoch: 189 | Iteration: 29 | Classification loss: 0.00005 | Regression loss: 0.01362 | Running loss: 0.02086\n",
            "Epoch: 189 | Iteration: 30 | Classification loss: 0.00008 | Regression loss: 0.01521 | Running loss: 0.02086\n",
            "Epoch: 189 | Iteration: 31 | Classification loss: 0.00004 | Regression loss: 0.01782 | Running loss: 0.02086\n",
            "Epoch: 189 | Iteration: 32 | Classification loss: 0.00056 | Regression loss: 0.03306 | Running loss: 0.02087\n",
            "Epoch: 189 | Iteration: 33 | Classification loss: 0.00010 | Regression loss: 0.01599 | Running loss: 0.02088\n",
            "Epoch: 189 | Iteration: 34 | Classification loss: 0.00009 | Regression loss: 0.01619 | Running loss: 0.02086\n",
            "Epoch: 189 | Iteration: 35 | Classification loss: 0.00022 | Regression loss: 0.02396 | Running loss: 0.02083\n",
            "Epoch: 189 | Iteration: 36 | Classification loss: 0.00027 | Regression loss: 0.02757 | Running loss: 0.02081\n",
            "Epoch: 189 | Iteration: 37 | Classification loss: 0.00009 | Regression loss: 0.01726 | Running loss: 0.02081\n",
            "Epoch: 189 | Iteration: 38 | Classification loss: 0.00013 | Regression loss: 0.02161 | Running loss: 0.02077\n",
            "Epoch: 189 | Iteration: 39 | Classification loss: 0.00009 | Regression loss: 0.01111 | Running loss: 0.02076\n",
            "Epoch: 189 | Iteration: 40 | Classification loss: 0.00033 | Regression loss: 0.02885 | Running loss: 0.02077\n",
            "Epoch: 189 | Iteration: 41 | Classification loss: 0.00010 | Regression loss: 0.01807 | Running loss: 0.02078\n",
            "Epoch: 189 | Iteration: 42 | Classification loss: 0.00008 | Regression loss: 0.01281 | Running loss: 0.02077\n",
            "Epoch: 189 | Iteration: 43 | Classification loss: 0.00020 | Regression loss: 0.02822 | Running loss: 0.02075\n",
            "Epoch: 189 | Iteration: 44 | Classification loss: 0.00008 | Regression loss: 0.01341 | Running loss: 0.02074\n",
            "Epoch: 189 | Iteration: 45 | Classification loss: 0.00011 | Regression loss: 0.01838 | Running loss: 0.02074\n",
            "Epoch: 189 | Iteration: 46 | Classification loss: 0.00004 | Regression loss: 0.01309 | Running loss: 0.02074\n",
            "Epoch: 189 | Iteration: 47 | Classification loss: 0.00012 | Regression loss: 0.01278 | Running loss: 0.02067\n",
            "Epoch: 189 | Iteration: 48 | Classification loss: 0.00035 | Regression loss: 0.02173 | Running loss: 0.02066\n",
            "Epoch: 189 | Iteration: 49 | Classification loss: 0.00021 | Regression loss: 0.02342 | Running loss: 0.02065\n",
            "Epoch: 189 | Iteration: 50 | Classification loss: 0.00006 | Regression loss: 0.01271 | Running loss: 0.02064\n",
            "Epoch: 189 | Iteration: 51 | Classification loss: 0.00002 | Regression loss: 0.01371 | Running loss: 0.02062\n",
            "Epoch: 189 | Iteration: 52 | Classification loss: 0.00012 | Regression loss: 0.02176 | Running loss: 0.02063\n",
            "Epoch: 189 | Iteration: 53 | Classification loss: 0.00004 | Regression loss: 0.00726 | Running loss: 0.02062\n",
            "Epoch: 189 | Iteration: 54 | Classification loss: 0.00014 | Regression loss: 0.01858 | Running loss: 0.02059\n",
            "Epoch: 189 | Iteration: 55 | Classification loss: 0.00001 | Regression loss: 0.00797 | Running loss: 0.02057\n",
            "Epoch: 189 | Iteration: 56 | Classification loss: 0.00014 | Regression loss: 0.02640 | Running loss: 0.02057\n",
            "Epoch: 189 | Iteration: 57 | Classification loss: 0.00038 | Regression loss: 0.01712 | Running loss: 0.02056\n",
            "Epoch: 189 | Iteration: 58 | Classification loss: 0.00012 | Regression loss: 0.01186 | Running loss: 0.02051\n",
            "Epoch: 189 | Iteration: 59 | Classification loss: 0.00005 | Regression loss: 0.00861 | Running loss: 0.02050\n",
            "Epoch: 189 | Iteration: 60 | Classification loss: 0.00008 | Regression loss: 0.01209 | Running loss: 0.02049\n",
            "Epoch: 189 | Iteration: 61 | Classification loss: 0.00008 | Regression loss: 0.01652 | Running loss: 0.02050\n",
            "Epoch: 189 | Iteration: 62 | Classification loss: 0.00015 | Regression loss: 0.01882 | Running loss: 0.02052\n",
            "Epoch: 189 | Iteration: 63 | Classification loss: 0.00005 | Regression loss: 0.01707 | Running loss: 0.02051\n",
            "Epoch: 189 | Iteration: 64 | Classification loss: 0.00047 | Regression loss: 0.03023 | Running loss: 0.02054\n",
            "Epoch: 189 | Iteration: 65 | Classification loss: 0.00030 | Regression loss: 0.03632 | Running loss: 0.02057\n",
            "Epoch: 189 | Iteration: 66 | Classification loss: 0.00026 | Regression loss: 0.03855 | Running loss: 0.02062\n",
            "Epoch: 189 | Iteration: 67 | Classification loss: 0.00029 | Regression loss: 0.02056 | Running loss: 0.02059\n",
            "Epoch: 189 | Iteration: 68 | Classification loss: 0.00015 | Regression loss: 0.02231 | Running loss: 0.02059\n",
            "Epoch: 189 | Iteration: 69 | Classification loss: 0.00004 | Regression loss: 0.00462 | Running loss: 0.02058\n",
            "Epoch: 189 | Iteration: 70 | Classification loss: 0.00015 | Regression loss: 0.02417 | Running loss: 0.02060\n",
            "Epoch: 189 | Iteration: 71 | Classification loss: 0.00013 | Regression loss: 0.01761 | Running loss: 0.02060\n",
            "Epoch: 189 | Iteration: 72 | Classification loss: 0.00011 | Regression loss: 0.01556 | Running loss: 0.02059\n",
            "Epoch: 189 | Iteration: 73 | Classification loss: 0.00007 | Regression loss: 0.02576 | Running loss: 0.02059\n",
            "Epoch: 189 | Iteration: 74 | Classification loss: 0.00010 | Regression loss: 0.00811 | Running loss: 0.02055\n",
            "Epoch: 189 | Iteration: 75 | Classification loss: 0.00005 | Regression loss: 0.02452 | Running loss: 0.02059\n",
            "Epoch: 189 | Iteration: 76 | Classification loss: 0.00033 | Regression loss: 0.03626 | Running loss: 0.02057\n",
            "Epoch: 189 | Iteration: 77 | Classification loss: 0.00003 | Regression loss: 0.01227 | Running loss: 0.02057\n",
            "Epoch: 189 | Iteration: 78 | Classification loss: 0.00032 | Regression loss: 0.02221 | Running loss: 0.02060\n",
            "Epoch: 189 | Iteration: 79 | Classification loss: 0.00013 | Regression loss: 0.02006 | Running loss: 0.02061\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.722455689942733\n",
            "Precision:  0.5816733067729084\n",
            "Recall:  0.7891891891891892\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}, {0: (0.7229906021615096, 185.0)}, {0: (0.7259378939307871, 185.0)}, {0: (0.7306172160652471, 185.0)}, {0: (0.7210590702449308, 185.0)}, {0: (0.7214498931770392, 185.0)}, {0: (0.7272071618498421, 185.0)}, {0: (0.725966639522496, 185.0)}, {0: (0.7208028358685962, 185.0)}, {0: (0.7255636845600553, 185.0)}, {0: (0.7261742190616329, 185.0)}, {0: (0.7217332811149212, 185.0)}, {0: (0.7261653370413381, 185.0)}, {0: (0.7215292823555803, 185.0)}, {0: (0.7277253912015531, 185.0)}, {0: (0.7219177066363771, 185.0)}, {0: (0.7274783215794732, 185.0)}, {0: (0.7218983427777539, 185.0)}, {0: (0.7270037695200344, 185.0)}, {0: (0.7272851879977948, 185.0)}, {0: (0.7221052381441093, 185.0)}, {0: (0.7218191972667742, 185.0)}, {0: (0.7259991707220859, 185.0)}, {0: (0.7281551087798303, 185.0)}, {0: (0.732505495958946, 185.0)}, {0: (0.7266266562021302, 185.0)}, {0: (0.7320831049102212, 185.0)}, {0: (0.7278242123515042, 185.0)}, {0: (0.7278307869685627, 185.0)}, {0: (0.7325520428180083, 185.0)}, {0: (0.7216918061885735, 185.0)}, {0: (0.722396113600594, 185.0)}, {0: (0.7220707898936956, 185.0)}, {0: (0.7217779820060826, 185.0)}, {0: (0.7230066917234796, 185.0)}, {0: (0.727859800128543, 185.0)}, {0: (0.722455689942733, 185.0)}]\n",
            "Epoch: 190 | Iteration: 0 | Classification loss: 0.00005 | Regression loss: 0.01792 | Running loss: 0.02063\n",
            "Epoch: 190 | Iteration: 1 | Classification loss: 0.00019 | Regression loss: 0.02093 | Running loss: 0.02064\n",
            "Epoch: 190 | Iteration: 2 | Classification loss: 0.00047 | Regression loss: 0.03211 | Running loss: 0.02064\n",
            "Epoch: 190 | Iteration: 3 | Classification loss: 0.00008 | Regression loss: 0.02755 | Running loss: 0.02068\n",
            "Epoch: 190 | Iteration: 4 | Classification loss: 0.00011 | Regression loss: 0.01440 | Running loss: 0.02066\n",
            "Epoch: 190 | Iteration: 5 | Classification loss: 0.00011 | Regression loss: 0.02714 | Running loss: 0.02069\n",
            "Epoch: 190 | Iteration: 6 | Classification loss: 0.00024 | Regression loss: 0.02458 | Running loss: 0.02069\n",
            "Epoch: 190 | Iteration: 7 | Classification loss: 0.00013 | Regression loss: 0.01928 | Running loss: 0.02068\n",
            "Epoch: 190 | Iteration: 8 | Classification loss: 0.00022 | Regression loss: 0.02064 | Running loss: 0.02072\n",
            "Epoch: 190 | Iteration: 9 | Classification loss: 0.00035 | Regression loss: 0.03491 | Running loss: 0.02076\n",
            "Epoch: 190 | Iteration: 10 | Classification loss: 0.00013 | Regression loss: 0.02096 | Running loss: 0.02077\n",
            "Epoch: 190 | Iteration: 11 | Classification loss: 0.00010 | Regression loss: 0.01456 | Running loss: 0.02073\n",
            "Epoch: 190 | Iteration: 12 | Classification loss: 0.00007 | Regression loss: 0.02501 | Running loss: 0.02072\n",
            "Epoch: 190 | Iteration: 13 | Classification loss: 0.00008 | Regression loss: 0.01409 | Running loss: 0.02068\n",
            "Epoch: 190 | Iteration: 14 | Classification loss: 0.00015 | Regression loss: 0.03415 | Running loss: 0.02073\n",
            "Epoch: 190 | Iteration: 15 | Classification loss: 0.00018 | Regression loss: 0.03234 | Running loss: 0.02077\n",
            "Epoch: 190 | Iteration: 16 | Classification loss: 0.00010 | Regression loss: 0.02051 | Running loss: 0.02079\n",
            "Epoch: 190 | Iteration: 17 | Classification loss: 0.00009 | Regression loss: 0.01721 | Running loss: 0.02079\n",
            "Epoch: 190 | Iteration: 18 | Classification loss: 0.00008 | Regression loss: 0.01531 | Running loss: 0.02078\n",
            "Epoch: 190 | Iteration: 19 | Classification loss: 0.00013 | Regression loss: 0.01625 | Running loss: 0.02078\n",
            "Epoch: 190 | Iteration: 20 | Classification loss: 0.00004 | Regression loss: 0.02287 | Running loss: 0.02075\n",
            "Epoch: 190 | Iteration: 21 | Classification loss: 0.00008 | Regression loss: 0.01574 | Running loss: 0.02077\n",
            "Epoch: 190 | Iteration: 22 | Classification loss: 0.00007 | Regression loss: 0.02883 | Running loss: 0.02080\n",
            "Epoch: 190 | Iteration: 23 | Classification loss: 0.00005 | Regression loss: 0.00844 | Running loss: 0.02074\n",
            "Epoch: 190 | Iteration: 24 | Classification loss: 0.00009 | Regression loss: 0.02239 | Running loss: 0.02074\n",
            "Epoch: 190 | Iteration: 25 | Classification loss: 0.00020 | Regression loss: 0.01886 | Running loss: 0.02074\n",
            "Epoch: 190 | Iteration: 26 | Classification loss: 0.00007 | Regression loss: 0.01124 | Running loss: 0.02070\n",
            "Epoch: 190 | Iteration: 27 | Classification loss: 0.00013 | Regression loss: 0.01836 | Running loss: 0.02071\n",
            "Epoch: 190 | Iteration: 28 | Classification loss: 0.00008 | Regression loss: 0.02085 | Running loss: 0.02073\n",
            "Epoch: 190 | Iteration: 29 | Classification loss: 0.00027 | Regression loss: 0.03195 | Running loss: 0.02071\n",
            "Epoch: 190 | Iteration: 30 | Classification loss: 0.00013 | Regression loss: 0.01700 | Running loss: 0.02071\n",
            "Epoch: 190 | Iteration: 31 | Classification loss: 0.00005 | Regression loss: 0.01343 | Running loss: 0.02070\n",
            "Epoch: 190 | Iteration: 32 | Classification loss: 0.00009 | Regression loss: 0.01391 | Running loss: 0.02063\n",
            "Epoch: 190 | Iteration: 33 | Classification loss: 0.00006 | Regression loss: 0.00746 | Running loss: 0.02061\n",
            "Epoch: 190 | Iteration: 34 | Classification loss: 0.00041 | Regression loss: 0.03512 | Running loss: 0.02066\n",
            "Epoch: 190 | Iteration: 35 | Classification loss: 0.00019 | Regression loss: 0.02432 | Running loss: 0.02068\n",
            "Epoch: 190 | Iteration: 36 | Classification loss: 0.00016 | Regression loss: 0.01734 | Running loss: 0.02066\n",
            "Epoch: 190 | Iteration: 37 | Classification loss: 0.00005 | Regression loss: 0.00658 | Running loss: 0.02064\n",
            "Epoch: 190 | Iteration: 38 | Classification loss: 0.00011 | Regression loss: 0.02182 | Running loss: 0.02064\n",
            "Epoch: 190 | Iteration: 39 | Classification loss: 0.00021 | Regression loss: 0.02353 | Running loss: 0.02062\n",
            "Epoch: 190 | Iteration: 40 | Classification loss: 0.00035 | Regression loss: 0.01994 | Running loss: 0.02062\n",
            "Epoch: 190 | Iteration: 41 | Classification loss: 0.00012 | Regression loss: 0.03282 | Running loss: 0.02064\n",
            "Epoch: 190 | Iteration: 42 | Classification loss: 0.00018 | Regression loss: 0.03107 | Running loss: 0.02063\n",
            "Epoch: 190 | Iteration: 43 | Classification loss: 0.00005 | Regression loss: 0.01489 | Running loss: 0.02061\n",
            "Epoch: 190 | Iteration: 44 | Classification loss: 0.00046 | Regression loss: 0.03180 | Running loss: 0.02062\n",
            "Epoch: 190 | Iteration: 45 | Classification loss: 0.00006 | Regression loss: 0.01469 | Running loss: 0.02062\n",
            "Epoch: 190 | Iteration: 46 | Classification loss: 0.00022 | Regression loss: 0.03828 | Running loss: 0.02067\n",
            "Epoch: 190 | Iteration: 47 | Classification loss: 0.00004 | Regression loss: 0.00748 | Running loss: 0.02065\n",
            "Epoch: 190 | Iteration: 48 | Classification loss: 0.00044 | Regression loss: 0.01945 | Running loss: 0.02067\n",
            "Epoch: 190 | Iteration: 49 | Classification loss: 0.00006 | Regression loss: 0.01327 | Running loss: 0.02067\n",
            "Epoch: 190 | Iteration: 50 | Classification loss: 0.00036 | Regression loss: 0.04733 | Running loss: 0.02071\n",
            "Epoch: 190 | Iteration: 51 | Classification loss: 0.00011 | Regression loss: 0.02674 | Running loss: 0.02074\n",
            "Epoch: 190 | Iteration: 52 | Classification loss: 0.00011 | Regression loss: 0.00812 | Running loss: 0.02069\n",
            "Epoch: 190 | Iteration: 53 | Classification loss: 0.00002 | Regression loss: 0.00920 | Running loss: 0.02068\n",
            "Epoch: 190 | Iteration: 54 | Classification loss: 0.00013 | Regression loss: 0.01441 | Running loss: 0.02069\n",
            "Epoch: 190 | Iteration: 55 | Classification loss: 0.00003 | Regression loss: 0.02433 | Running loss: 0.02065\n",
            "Epoch: 190 | Iteration: 56 | Classification loss: 0.00002 | Regression loss: 0.00573 | Running loss: 0.02061\n",
            "Epoch: 190 | Iteration: 57 | Classification loss: 0.00008 | Regression loss: 0.01211 | Running loss: 0.02059\n",
            "Epoch: 190 | Iteration: 58 | Classification loss: 0.00017 | Regression loss: 0.01656 | Running loss: 0.02056\n",
            "Epoch: 190 | Iteration: 59 | Classification loss: 0.00015 | Regression loss: 0.01187 | Running loss: 0.02056\n",
            "Epoch: 190 | Iteration: 60 | Classification loss: 0.00008 | Regression loss: 0.00968 | Running loss: 0.02053\n",
            "Epoch: 190 | Iteration: 61 | Classification loss: 0.00026 | Regression loss: 0.02768 | Running loss: 0.02055\n",
            "Epoch: 190 | Iteration: 62 | Classification loss: 0.00007 | Regression loss: 0.01526 | Running loss: 0.02052\n",
            "Epoch: 190 | Iteration: 63 | Classification loss: 0.00006 | Regression loss: 0.01630 | Running loss: 0.02053\n",
            "Epoch: 190 | Iteration: 64 | Classification loss: 0.00026 | Regression loss: 0.02887 | Running loss: 0.02051\n",
            "Epoch: 190 | Iteration: 65 | Classification loss: 0.00006 | Regression loss: 0.01147 | Running loss: 0.02050\n",
            "Epoch: 190 | Iteration: 66 | Classification loss: 0.00008 | Regression loss: 0.01619 | Running loss: 0.02047\n",
            "Epoch: 190 | Iteration: 67 | Classification loss: 0.00012 | Regression loss: 0.01388 | Running loss: 0.02047\n",
            "Epoch: 190 | Iteration: 68 | Classification loss: 0.00001 | Regression loss: 0.00376 | Running loss: 0.02042\n",
            "Epoch: 190 | Iteration: 69 | Classification loss: 0.00012 | Regression loss: 0.02025 | Running loss: 0.02043\n",
            "Epoch: 190 | Iteration: 70 | Classification loss: 0.00031 | Regression loss: 0.01680 | Running loss: 0.02042\n",
            "Epoch: 190 | Iteration: 71 | Classification loss: 0.00010 | Regression loss: 0.01110 | Running loss: 0.02040\n",
            "Epoch: 190 | Iteration: 72 | Classification loss: 0.00005 | Regression loss: 0.01810 | Running loss: 0.02040\n",
            "Epoch: 190 | Iteration: 73 | Classification loss: 0.00004 | Regression loss: 0.01452 | Running loss: 0.02040\n",
            "Epoch: 190 | Iteration: 74 | Classification loss: 0.00035 | Regression loss: 0.03802 | Running loss: 0.02046\n",
            "Epoch: 190 | Iteration: 75 | Classification loss: 0.00014 | Regression loss: 0.02191 | Running loss: 0.02047\n",
            "Epoch: 190 | Iteration: 76 | Classification loss: 0.00024 | Regression loss: 0.03725 | Running loss: 0.02050\n",
            "Epoch: 190 | Iteration: 77 | Classification loss: 0.00021 | Regression loss: 0.02769 | Running loss: 0.02052\n",
            "Epoch: 190 | Iteration: 78 | Classification loss: 0.00006 | Regression loss: 0.01865 | Running loss: 0.02053\n",
            "Epoch: 190 | Iteration: 79 | Classification loss: 0.00003 | Regression loss: 0.01834 | Running loss: 0.02054\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7295061049671745\n",
            "Precision:  0.5856573705179283\n",
            "Recall:  0.7945945945945946\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}, {0: (0.7229906021615096, 185.0)}, {0: (0.7259378939307871, 185.0)}, {0: (0.7306172160652471, 185.0)}, {0: (0.7210590702449308, 185.0)}, {0: (0.7214498931770392, 185.0)}, {0: (0.7272071618498421, 185.0)}, {0: (0.725966639522496, 185.0)}, {0: (0.7208028358685962, 185.0)}, {0: (0.7255636845600553, 185.0)}, {0: (0.7261742190616329, 185.0)}, {0: (0.7217332811149212, 185.0)}, {0: (0.7261653370413381, 185.0)}, {0: (0.7215292823555803, 185.0)}, {0: (0.7277253912015531, 185.0)}, {0: (0.7219177066363771, 185.0)}, {0: (0.7274783215794732, 185.0)}, {0: (0.7218983427777539, 185.0)}, {0: (0.7270037695200344, 185.0)}, {0: (0.7272851879977948, 185.0)}, {0: (0.7221052381441093, 185.0)}, {0: (0.7218191972667742, 185.0)}, {0: (0.7259991707220859, 185.0)}, {0: (0.7281551087798303, 185.0)}, {0: (0.732505495958946, 185.0)}, {0: (0.7266266562021302, 185.0)}, {0: (0.7320831049102212, 185.0)}, {0: (0.7278242123515042, 185.0)}, {0: (0.7278307869685627, 185.0)}, {0: (0.7325520428180083, 185.0)}, {0: (0.7216918061885735, 185.0)}, {0: (0.722396113600594, 185.0)}, {0: (0.7220707898936956, 185.0)}, {0: (0.7217779820060826, 185.0)}, {0: (0.7230066917234796, 185.0)}, {0: (0.727859800128543, 185.0)}, {0: (0.722455689942733, 185.0)}, {0: (0.7295061049671745, 185.0)}]\n",
            "Epoch: 191 | Iteration: 0 | Classification loss: 0.00021 | Regression loss: 0.03427 | Running loss: 0.02056\n",
            "Epoch: 191 | Iteration: 1 | Classification loss: 0.00005 | Regression loss: 0.00656 | Running loss: 0.02053\n",
            "Epoch: 191 | Iteration: 2 | Classification loss: 0.00043 | Regression loss: 0.03130 | Running loss: 0.02058\n",
            "Epoch: 191 | Iteration: 3 | Classification loss: 0.00006 | Regression loss: 0.02779 | Running loss: 0.02062\n",
            "Epoch: 191 | Iteration: 4 | Classification loss: 0.00012 | Regression loss: 0.02089 | Running loss: 0.02061\n",
            "Epoch: 191 | Iteration: 5 | Classification loss: 0.00031 | Regression loss: 0.01790 | Running loss: 0.02061\n",
            "Epoch: 191 | Iteration: 6 | Classification loss: 0.00019 | Regression loss: 0.02840 | Running loss: 0.02061\n",
            "Epoch: 191 | Iteration: 7 | Classification loss: 0.00013 | Regression loss: 0.02850 | Running loss: 0.02063\n",
            "Epoch: 191 | Iteration: 8 | Classification loss: 0.00011 | Regression loss: 0.02254 | Running loss: 0.02065\n",
            "Epoch: 191 | Iteration: 9 | Classification loss: 0.00004 | Regression loss: 0.01407 | Running loss: 0.02064\n",
            "Epoch: 191 | Iteration: 10 | Classification loss: 0.00008 | Regression loss: 0.01448 | Running loss: 0.02060\n",
            "Epoch: 191 | Iteration: 11 | Classification loss: 0.00010 | Regression loss: 0.01496 | Running loss: 0.02063\n",
            "Epoch: 191 | Iteration: 12 | Classification loss: 0.00004 | Regression loss: 0.01197 | Running loss: 0.02062\n",
            "Epoch: 191 | Iteration: 13 | Classification loss: 0.00008 | Regression loss: 0.01498 | Running loss: 0.02063\n",
            "Epoch: 191 | Iteration: 14 | Classification loss: 0.00003 | Regression loss: 0.01596 | Running loss: 0.02064\n",
            "Epoch: 191 | Iteration: 15 | Classification loss: 0.00041 | Regression loss: 0.03468 | Running loss: 0.02066\n",
            "Epoch: 191 | Iteration: 16 | Classification loss: 0.00009 | Regression loss: 0.02399 | Running loss: 0.02065\n",
            "Epoch: 191 | Iteration: 17 | Classification loss: 0.00058 | Regression loss: 0.03862 | Running loss: 0.02067\n",
            "Epoch: 191 | Iteration: 18 | Classification loss: 0.00027 | Regression loss: 0.03496 | Running loss: 0.02071\n",
            "Epoch: 191 | Iteration: 19 | Classification loss: 0.00026 | Regression loss: 0.02683 | Running loss: 0.02069\n",
            "Epoch: 191 | Iteration: 20 | Classification loss: 0.00005 | Regression loss: 0.02013 | Running loss: 0.02066\n",
            "Epoch: 191 | Iteration: 21 | Classification loss: 0.00006 | Regression loss: 0.01306 | Running loss: 0.02063\n",
            "Epoch: 191 | Iteration: 22 | Classification loss: 0.00007 | Regression loss: 0.01623 | Running loss: 0.02060\n",
            "Epoch: 191 | Iteration: 23 | Classification loss: 0.00002 | Regression loss: 0.01364 | Running loss: 0.02059\n",
            "Epoch: 191 | Iteration: 24 | Classification loss: 0.00015 | Regression loss: 0.03086 | Running loss: 0.02063\n",
            "Epoch: 191 | Iteration: 25 | Classification loss: 0.00004 | Regression loss: 0.00737 | Running loss: 0.02058\n",
            "Epoch: 191 | Iteration: 26 | Classification loss: 0.00013 | Regression loss: 0.03447 | Running loss: 0.02060\n",
            "Epoch: 191 | Iteration: 27 | Classification loss: 0.00029 | Regression loss: 0.04607 | Running loss: 0.02063\n",
            "Epoch: 191 | Iteration: 28 | Classification loss: 0.00013 | Regression loss: 0.01572 | Running loss: 0.02063\n",
            "Epoch: 191 | Iteration: 29 | Classification loss: 0.00010 | Regression loss: 0.00761 | Running loss: 0.02060\n",
            "Epoch: 191 | Iteration: 30 | Classification loss: 0.00014 | Regression loss: 0.01623 | Running loss: 0.02060\n",
            "Epoch: 191 | Iteration: 31 | Classification loss: 0.00004 | Regression loss: 0.02722 | Running loss: 0.02063\n",
            "Epoch: 191 | Iteration: 32 | Classification loss: 0.00008 | Regression loss: 0.01909 | Running loss: 0.02063\n",
            "Epoch: 191 | Iteration: 33 | Classification loss: 0.00020 | Regression loss: 0.02542 | Running loss: 0.02064\n",
            "Epoch: 191 | Iteration: 34 | Classification loss: 0.00004 | Regression loss: 0.01478 | Running loss: 0.02061\n",
            "Epoch: 191 | Iteration: 35 | Classification loss: 0.00012 | Regression loss: 0.01404 | Running loss: 0.02060\n",
            "Epoch: 191 | Iteration: 36 | Classification loss: 0.00033 | Regression loss: 0.03499 | Running loss: 0.02064\n",
            "Epoch: 191 | Iteration: 37 | Classification loss: 0.00008 | Regression loss: 0.00863 | Running loss: 0.02058\n",
            "Epoch: 191 | Iteration: 38 | Classification loss: 0.00001 | Regression loss: 0.00827 | Running loss: 0.02058\n",
            "Epoch: 191 | Iteration: 39 | Classification loss: 0.00007 | Regression loss: 0.01922 | Running loss: 0.02053\n",
            "Epoch: 191 | Iteration: 40 | Classification loss: 0.00011 | Regression loss: 0.01834 | Running loss: 0.02055\n",
            "Epoch: 191 | Iteration: 41 | Classification loss: 0.00013 | Regression loss: 0.01842 | Running loss: 0.02053\n",
            "Epoch: 191 | Iteration: 42 | Classification loss: 0.00002 | Regression loss: 0.00371 | Running loss: 0.02051\n",
            "Epoch: 191 | Iteration: 43 | Classification loss: 0.00012 | Regression loss: 0.01074 | Running loss: 0.02052\n",
            "Epoch: 191 | Iteration: 44 | Classification loss: 0.00005 | Regression loss: 0.02529 | Running loss: 0.02053\n",
            "Epoch: 191 | Iteration: 45 | Classification loss: 0.00012 | Regression loss: 0.01132 | Running loss: 0.02051\n",
            "Epoch: 191 | Iteration: 46 | Classification loss: 0.00020 | Regression loss: 0.02405 | Running loss: 0.02053\n",
            "Epoch: 191 | Iteration: 47 | Classification loss: 0.00007 | Regression loss: 0.01233 | Running loss: 0.02046\n",
            "Epoch: 191 | Iteration: 48 | Classification loss: 0.00011 | Regression loss: 0.01752 | Running loss: 0.02049\n",
            "Epoch: 191 | Iteration: 49 | Classification loss: 0.00012 | Regression loss: 0.01613 | Running loss: 0.02045\n",
            "Epoch: 191 | Iteration: 50 | Classification loss: 0.00004 | Regression loss: 0.01744 | Running loss: 0.02046\n",
            "Epoch: 191 | Iteration: 51 | Classification loss: 0.00009 | Regression loss: 0.01385 | Running loss: 0.02046\n",
            "Epoch: 191 | Iteration: 52 | Classification loss: 0.00012 | Regression loss: 0.01120 | Running loss: 0.02045\n",
            "Epoch: 191 | Iteration: 53 | Classification loss: 0.00020 | Regression loss: 0.02757 | Running loss: 0.02049\n",
            "Epoch: 191 | Iteration: 54 | Classification loss: 0.00005 | Regression loss: 0.01370 | Running loss: 0.02047\n",
            "Epoch: 191 | Iteration: 55 | Classification loss: 0.00008 | Regression loss: 0.00887 | Running loss: 0.02046\n",
            "Epoch: 191 | Iteration: 56 | Classification loss: 0.00004 | Regression loss: 0.00694 | Running loss: 0.02041\n",
            "Epoch: 191 | Iteration: 57 | Classification loss: 0.00027 | Regression loss: 0.04189 | Running loss: 0.02042\n",
            "Epoch: 191 | Iteration: 58 | Classification loss: 0.00010 | Regression loss: 0.01586 | Running loss: 0.02039\n",
            "Epoch: 191 | Iteration: 59 | Classification loss: 0.00009 | Regression loss: 0.02484 | Running loss: 0.02041\n",
            "Epoch: 191 | Iteration: 60 | Classification loss: 0.00006 | Regression loss: 0.01303 | Running loss: 0.02038\n",
            "Epoch: 191 | Iteration: 61 | Classification loss: 0.00008 | Regression loss: 0.01642 | Running loss: 0.02038\n",
            "Epoch: 191 | Iteration: 62 | Classification loss: 0.00012 | Regression loss: 0.01589 | Running loss: 0.02038\n",
            "Epoch: 191 | Iteration: 63 | Classification loss: 0.00011 | Regression loss: 0.02340 | Running loss: 0.02036\n",
            "Epoch: 191 | Iteration: 64 | Classification loss: 0.00016 | Regression loss: 0.03028 | Running loss: 0.02036\n",
            "Epoch: 191 | Iteration: 65 | Classification loss: 0.00008 | Regression loss: 0.01051 | Running loss: 0.02034\n",
            "Epoch: 191 | Iteration: 66 | Classification loss: 0.00022 | Regression loss: 0.03115 | Running loss: 0.02035\n",
            "Epoch: 191 | Iteration: 67 | Classification loss: 0.00034 | Regression loss: 0.02229 | Running loss: 0.02036\n",
            "Epoch: 191 | Iteration: 68 | Classification loss: 0.00019 | Regression loss: 0.02112 | Running loss: 0.02038\n",
            "Epoch: 191 | Iteration: 69 | Classification loss: 0.00016 | Regression loss: 0.01947 | Running loss: 0.02040\n",
            "Epoch: 191 | Iteration: 70 | Classification loss: 0.00019 | Regression loss: 0.03039 | Running loss: 0.02041\n",
            "Epoch: 191 | Iteration: 71 | Classification loss: 0.00046 | Regression loss: 0.02122 | Running loss: 0.02041\n",
            "Epoch: 191 | Iteration: 72 | Classification loss: 0.00008 | Regression loss: 0.01135 | Running loss: 0.02037\n",
            "Epoch: 191 | Iteration: 73 | Classification loss: 0.00005 | Regression loss: 0.01897 | Running loss: 0.02034\n",
            "Epoch: 191 | Iteration: 74 | Classification loss: 0.00007 | Regression loss: 0.01503 | Running loss: 0.02034\n",
            "Epoch: 191 | Iteration: 75 | Classification loss: 0.00018 | Regression loss: 0.02073 | Running loss: 0.02035\n",
            "Epoch: 191 | Iteration: 76 | Classification loss: 0.00010 | Regression loss: 0.02069 | Running loss: 0.02036\n",
            "Epoch: 191 | Iteration: 77 | Classification loss: 0.00009 | Regression loss: 0.02891 | Running loss: 0.02039\n",
            "Epoch: 191 | Iteration: 78 | Classification loss: 0.00013 | Regression loss: 0.01729 | Running loss: 0.02037\n",
            "Epoch: 191 | Iteration: 79 | Classification loss: 0.00030 | Regression loss: 0.03618 | Running loss: 0.02042\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7289767970839213\n",
            "Precision:  0.5856573705179283\n",
            "Recall:  0.7945945945945946\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}, {0: (0.7229906021615096, 185.0)}, {0: (0.7259378939307871, 185.0)}, {0: (0.7306172160652471, 185.0)}, {0: (0.7210590702449308, 185.0)}, {0: (0.7214498931770392, 185.0)}, {0: (0.7272071618498421, 185.0)}, {0: (0.725966639522496, 185.0)}, {0: (0.7208028358685962, 185.0)}, {0: (0.7255636845600553, 185.0)}, {0: (0.7261742190616329, 185.0)}, {0: (0.7217332811149212, 185.0)}, {0: (0.7261653370413381, 185.0)}, {0: (0.7215292823555803, 185.0)}, {0: (0.7277253912015531, 185.0)}, {0: (0.7219177066363771, 185.0)}, {0: (0.7274783215794732, 185.0)}, {0: (0.7218983427777539, 185.0)}, {0: (0.7270037695200344, 185.0)}, {0: (0.7272851879977948, 185.0)}, {0: (0.7221052381441093, 185.0)}, {0: (0.7218191972667742, 185.0)}, {0: (0.7259991707220859, 185.0)}, {0: (0.7281551087798303, 185.0)}, {0: (0.732505495958946, 185.0)}, {0: (0.7266266562021302, 185.0)}, {0: (0.7320831049102212, 185.0)}, {0: (0.7278242123515042, 185.0)}, {0: (0.7278307869685627, 185.0)}, {0: (0.7325520428180083, 185.0)}, {0: (0.7216918061885735, 185.0)}, {0: (0.722396113600594, 185.0)}, {0: (0.7220707898936956, 185.0)}, {0: (0.7217779820060826, 185.0)}, {0: (0.7230066917234796, 185.0)}, {0: (0.727859800128543, 185.0)}, {0: (0.722455689942733, 185.0)}, {0: (0.7295061049671745, 185.0)}, {0: (0.7289767970839213, 185.0)}]\n",
            "Epoch: 192 | Iteration: 0 | Classification loss: 0.00021 | Regression loss: 0.02817 | Running loss: 0.02044\n",
            "Epoch: 192 | Iteration: 1 | Classification loss: 0.00007 | Regression loss: 0.00811 | Running loss: 0.02041\n",
            "Epoch: 192 | Iteration: 2 | Classification loss: 0.00011 | Regression loss: 0.01130 | Running loss: 0.02041\n",
            "Epoch: 192 | Iteration: 3 | Classification loss: 0.00008 | Regression loss: 0.02156 | Running loss: 0.02042\n",
            "Epoch: 192 | Iteration: 4 | Classification loss: 0.00011 | Regression loss: 0.01293 | Running loss: 0.02040\n",
            "Epoch: 192 | Iteration: 5 | Classification loss: 0.00028 | Regression loss: 0.03234 | Running loss: 0.02042\n",
            "Epoch: 192 | Iteration: 6 | Classification loss: 0.00005 | Regression loss: 0.01230 | Running loss: 0.02042\n",
            "Epoch: 192 | Iteration: 7 | Classification loss: 0.00019 | Regression loss: 0.02200 | Running loss: 0.02045\n",
            "Epoch: 192 | Iteration: 8 | Classification loss: 0.00042 | Regression loss: 0.01949 | Running loss: 0.02045\n",
            "Epoch: 192 | Iteration: 9 | Classification loss: 0.00004 | Regression loss: 0.02521 | Running loss: 0.02046\n",
            "Epoch: 192 | Iteration: 10 | Classification loss: 0.00040 | Regression loss: 0.03169 | Running loss: 0.02048\n",
            "Epoch: 192 | Iteration: 11 | Classification loss: 0.00008 | Regression loss: 0.02414 | Running loss: 0.02050\n",
            "Epoch: 192 | Iteration: 12 | Classification loss: 0.00011 | Regression loss: 0.01627 | Running loss: 0.02049\n",
            "Epoch: 192 | Iteration: 13 | Classification loss: 0.00008 | Regression loss: 0.01139 | Running loss: 0.02048\n",
            "Epoch: 192 | Iteration: 14 | Classification loss: 0.00007 | Regression loss: 0.02700 | Running loss: 0.02051\n",
            "Epoch: 192 | Iteration: 15 | Classification loss: 0.00026 | Regression loss: 0.03984 | Running loss: 0.02052\n",
            "Epoch: 192 | Iteration: 16 | Classification loss: 0.00001 | Regression loss: 0.00893 | Running loss: 0.02049\n",
            "Epoch: 192 | Iteration: 17 | Classification loss: 0.00012 | Regression loss: 0.02290 | Running loss: 0.02049\n",
            "Epoch: 192 | Iteration: 18 | Classification loss: 0.00008 | Regression loss: 0.01576 | Running loss: 0.02048\n",
            "Epoch: 192 | Iteration: 19 | Classification loss: 0.00007 | Regression loss: 0.01906 | Running loss: 0.02049\n",
            "Epoch: 192 | Iteration: 20 | Classification loss: 0.00030 | Regression loss: 0.03256 | Running loss: 0.02050\n",
            "Epoch: 192 | Iteration: 21 | Classification loss: 0.00013 | Regression loss: 0.01611 | Running loss: 0.02049\n",
            "Epoch: 192 | Iteration: 22 | Classification loss: 0.00011 | Regression loss: 0.01582 | Running loss: 0.02049\n",
            "Epoch: 192 | Iteration: 23 | Classification loss: 0.00011 | Regression loss: 0.03260 | Running loss: 0.02053\n",
            "Epoch: 192 | Iteration: 24 | Classification loss: 0.00011 | Regression loss: 0.01501 | Running loss: 0.02052\n",
            "Epoch: 192 | Iteration: 25 | Classification loss: 0.00008 | Regression loss: 0.01581 | Running loss: 0.02048\n",
            "Epoch: 192 | Iteration: 26 | Classification loss: 0.00035 | Regression loss: 0.02888 | Running loss: 0.02050\n",
            "Epoch: 192 | Iteration: 27 | Classification loss: 0.00009 | Regression loss: 0.01227 | Running loss: 0.02048\n",
            "Epoch: 192 | Iteration: 28 | Classification loss: 0.00016 | Regression loss: 0.02303 | Running loss: 0.02049\n",
            "Epoch: 192 | Iteration: 29 | Classification loss: 0.00012 | Regression loss: 0.01680 | Running loss: 0.02046\n",
            "Epoch: 192 | Iteration: 30 | Classification loss: 0.00004 | Regression loss: 0.00762 | Running loss: 0.02044\n",
            "Epoch: 192 | Iteration: 31 | Classification loss: 0.00009 | Regression loss: 0.01624 | Running loss: 0.02043\n",
            "Epoch: 192 | Iteration: 32 | Classification loss: 0.00045 | Regression loss: 0.03249 | Running loss: 0.02045\n",
            "Epoch: 192 | Iteration: 33 | Classification loss: 0.00004 | Regression loss: 0.00733 | Running loss: 0.02039\n",
            "Epoch: 192 | Iteration: 34 | Classification loss: 0.00014 | Regression loss: 0.02874 | Running loss: 0.02038\n",
            "Epoch: 192 | Iteration: 35 | Classification loss: 0.00019 | Regression loss: 0.02438 | Running loss: 0.02040\n",
            "Epoch: 192 | Iteration: 36 | Classification loss: 0.00055 | Regression loss: 0.03664 | Running loss: 0.02044\n",
            "Epoch: 192 | Iteration: 37 | Classification loss: 0.00017 | Regression loss: 0.02952 | Running loss: 0.02045\n",
            "Epoch: 192 | Iteration: 38 | Classification loss: 0.00005 | Regression loss: 0.01816 | Running loss: 0.02047\n",
            "Epoch: 192 | Iteration: 39 | Classification loss: 0.00010 | Regression loss: 0.01908 | Running loss: 0.02048\n",
            "Epoch: 192 | Iteration: 40 | Classification loss: 0.00019 | Regression loss: 0.01927 | Running loss: 0.02048\n",
            "Epoch: 192 | Iteration: 41 | Classification loss: 0.00010 | Regression loss: 0.02287 | Running loss: 0.02046\n",
            "Epoch: 192 | Iteration: 42 | Classification loss: 0.00011 | Regression loss: 0.00814 | Running loss: 0.02045\n",
            "Epoch: 192 | Iteration: 43 | Classification loss: 0.00001 | Regression loss: 0.00378 | Running loss: 0.02042\n",
            "Epoch: 192 | Iteration: 44 | Classification loss: 0.00004 | Regression loss: 0.01608 | Running loss: 0.02044\n",
            "Epoch: 192 | Iteration: 45 | Classification loss: 0.00008 | Regression loss: 0.01403 | Running loss: 0.02045\n",
            "Epoch: 192 | Iteration: 46 | Classification loss: 0.00005 | Regression loss: 0.00684 | Running loss: 0.02040\n",
            "Epoch: 192 | Iteration: 47 | Classification loss: 0.00011 | Regression loss: 0.01731 | Running loss: 0.02040\n",
            "Epoch: 192 | Iteration: 48 | Classification loss: 0.00026 | Regression loss: 0.04671 | Running loss: 0.02048\n",
            "Epoch: 192 | Iteration: 49 | Classification loss: 0.00003 | Regression loss: 0.01180 | Running loss: 0.02047\n",
            "Epoch: 192 | Iteration: 50 | Classification loss: 0.00003 | Regression loss: 0.01391 | Running loss: 0.02049\n",
            "Epoch: 192 | Iteration: 51 | Classification loss: 0.00009 | Regression loss: 0.01603 | Running loss: 0.02050\n",
            "Epoch: 192 | Iteration: 52 | Classification loss: 0.00030 | Regression loss: 0.03780 | Running loss: 0.02049\n",
            "Epoch: 192 | Iteration: 53 | Classification loss: 0.00010 | Regression loss: 0.01346 | Running loss: 0.02048\n",
            "Epoch: 192 | Iteration: 54 | Classification loss: 0.00032 | Regression loss: 0.02292 | Running loss: 0.02048\n",
            "Epoch: 192 | Iteration: 55 | Classification loss: 0.00015 | Regression loss: 0.02301 | Running loss: 0.02049\n",
            "Epoch: 192 | Iteration: 56 | Classification loss: 0.00004 | Regression loss: 0.02071 | Running loss: 0.02048\n",
            "Epoch: 192 | Iteration: 57 | Classification loss: 0.00016 | Regression loss: 0.03037 | Running loss: 0.02052\n",
            "Epoch: 192 | Iteration: 58 | Classification loss: 0.00009 | Regression loss: 0.00961 | Running loss: 0.02050\n",
            "Epoch: 192 | Iteration: 59 | Classification loss: 0.00008 | Regression loss: 0.02085 | Running loss: 0.02048\n",
            "Epoch: 192 | Iteration: 60 | Classification loss: 0.00031 | Regression loss: 0.02059 | Running loss: 0.02047\n",
            "Epoch: 192 | Iteration: 61 | Classification loss: 0.00012 | Regression loss: 0.01967 | Running loss: 0.02049\n",
            "Epoch: 192 | Iteration: 62 | Classification loss: 0.00003 | Regression loss: 0.00554 | Running loss: 0.02046\n",
            "Epoch: 192 | Iteration: 63 | Classification loss: 0.00015 | Regression loss: 0.03011 | Running loss: 0.02047\n",
            "Epoch: 192 | Iteration: 64 | Classification loss: 0.00008 | Regression loss: 0.01507 | Running loss: 0.02047\n",
            "Epoch: 192 | Iteration: 65 | Classification loss: 0.00009 | Regression loss: 0.01677 | Running loss: 0.02047\n",
            "Epoch: 192 | Iteration: 66 | Classification loss: 0.00022 | Regression loss: 0.02727 | Running loss: 0.02046\n",
            "Epoch: 192 | Iteration: 67 | Classification loss: 0.00018 | Regression loss: 0.03196 | Running loss: 0.02050\n",
            "Epoch: 192 | Iteration: 68 | Classification loss: 0.00012 | Regression loss: 0.01579 | Running loss: 0.02048\n",
            "Epoch: 192 | Iteration: 69 | Classification loss: 0.00015 | Regression loss: 0.01741 | Running loss: 0.02047\n",
            "Epoch: 192 | Iteration: 70 | Classification loss: 0.00005 | Regression loss: 0.01093 | Running loss: 0.02047\n",
            "Epoch: 192 | Iteration: 71 | Classification loss: 0.00004 | Regression loss: 0.01855 | Running loss: 0.02048\n",
            "Epoch: 192 | Iteration: 72 | Classification loss: 0.00011 | Regression loss: 0.02690 | Running loss: 0.02048\n",
            "Epoch: 192 | Iteration: 73 | Classification loss: 0.00006 | Regression loss: 0.01287 | Running loss: 0.02049\n",
            "Epoch: 192 | Iteration: 74 | Classification loss: 0.00011 | Regression loss: 0.01237 | Running loss: 0.02048\n",
            "Epoch: 192 | Iteration: 75 | Classification loss: 0.00004 | Regression loss: 0.01444 | Running loss: 0.02048\n",
            "Epoch: 192 | Iteration: 76 | Classification loss: 0.00019 | Regression loss: 0.03833 | Running loss: 0.02049\n",
            "Epoch: 192 | Iteration: 77 | Classification loss: 0.00014 | Regression loss: 0.01928 | Running loss: 0.02050\n",
            "Epoch: 192 | Iteration: 78 | Classification loss: 0.00006 | Regression loss: 0.01918 | Running loss: 0.02049\n",
            "Epoch: 192 | Iteration: 79 | Classification loss: 0.00031 | Regression loss: 0.01913 | Running loss: 0.02046\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7277669415378738\n",
            "Precision:  0.588\n",
            "Recall:  0.7945945945945946\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}, {0: (0.7229906021615096, 185.0)}, {0: (0.7259378939307871, 185.0)}, {0: (0.7306172160652471, 185.0)}, {0: (0.7210590702449308, 185.0)}, {0: (0.7214498931770392, 185.0)}, {0: (0.7272071618498421, 185.0)}, {0: (0.725966639522496, 185.0)}, {0: (0.7208028358685962, 185.0)}, {0: (0.7255636845600553, 185.0)}, {0: (0.7261742190616329, 185.0)}, {0: (0.7217332811149212, 185.0)}, {0: (0.7261653370413381, 185.0)}, {0: (0.7215292823555803, 185.0)}, {0: (0.7277253912015531, 185.0)}, {0: (0.7219177066363771, 185.0)}, {0: (0.7274783215794732, 185.0)}, {0: (0.7218983427777539, 185.0)}, {0: (0.7270037695200344, 185.0)}, {0: (0.7272851879977948, 185.0)}, {0: (0.7221052381441093, 185.0)}, {0: (0.7218191972667742, 185.0)}, {0: (0.7259991707220859, 185.0)}, {0: (0.7281551087798303, 185.0)}, {0: (0.732505495958946, 185.0)}, {0: (0.7266266562021302, 185.0)}, {0: (0.7320831049102212, 185.0)}, {0: (0.7278242123515042, 185.0)}, {0: (0.7278307869685627, 185.0)}, {0: (0.7325520428180083, 185.0)}, {0: (0.7216918061885735, 185.0)}, {0: (0.722396113600594, 185.0)}, {0: (0.7220707898936956, 185.0)}, {0: (0.7217779820060826, 185.0)}, {0: (0.7230066917234796, 185.0)}, {0: (0.727859800128543, 185.0)}, {0: (0.722455689942733, 185.0)}, {0: (0.7295061049671745, 185.0)}, {0: (0.7289767970839213, 185.0)}, {0: (0.7277669415378738, 185.0)}]\n",
            "Epoch: 193 | Iteration: 0 | Classification loss: 0.00017 | Regression loss: 0.02700 | Running loss: 0.02048\n",
            "Epoch: 193 | Iteration: 1 | Classification loss: 0.00005 | Regression loss: 0.01869 | Running loss: 0.02050\n",
            "Epoch: 193 | Iteration: 2 | Classification loss: 0.00006 | Regression loss: 0.01301 | Running loss: 0.02050\n",
            "Epoch: 193 | Iteration: 3 | Classification loss: 0.00054 | Regression loss: 0.03429 | Running loss: 0.02054\n",
            "Epoch: 193 | Iteration: 4 | Classification loss: 0.00008 | Regression loss: 0.01586 | Running loss: 0.02055\n",
            "Epoch: 193 | Iteration: 5 | Classification loss: 0.00013 | Regression loss: 0.03003 | Running loss: 0.02056\n",
            "Epoch: 193 | Iteration: 6 | Classification loss: 0.00021 | Regression loss: 0.02705 | Running loss: 0.02055\n",
            "Epoch: 193 | Iteration: 7 | Classification loss: 0.00022 | Regression loss: 0.03497 | Running loss: 0.02059\n",
            "Epoch: 193 | Iteration: 8 | Classification loss: 0.00007 | Regression loss: 0.01127 | Running loss: 0.02052\n",
            "Epoch: 193 | Iteration: 9 | Classification loss: 0.00033 | Regression loss: 0.04444 | Running loss: 0.02057\n",
            "Epoch: 193 | Iteration: 10 | Classification loss: 0.00009 | Regression loss: 0.01438 | Running loss: 0.02056\n",
            "Epoch: 193 | Iteration: 11 | Classification loss: 0.00009 | Regression loss: 0.01483 | Running loss: 0.02057\n",
            "Epoch: 193 | Iteration: 12 | Classification loss: 0.00010 | Regression loss: 0.02086 | Running loss: 0.02053\n",
            "Epoch: 193 | Iteration: 13 | Classification loss: 0.00003 | Regression loss: 0.00674 | Running loss: 0.02051\n",
            "Epoch: 193 | Iteration: 14 | Classification loss: 0.00010 | Regression loss: 0.02378 | Running loss: 0.02053\n",
            "Epoch: 193 | Iteration: 15 | Classification loss: 0.00025 | Regression loss: 0.03932 | Running loss: 0.02056\n",
            "Epoch: 193 | Iteration: 16 | Classification loss: 0.00049 | Regression loss: 0.03684 | Running loss: 0.02056\n",
            "Epoch: 193 | Iteration: 17 | Classification loss: 0.00015 | Regression loss: 0.02428 | Running loss: 0.02059\n",
            "Epoch: 193 | Iteration: 18 | Classification loss: 0.00014 | Regression loss: 0.01694 | Running loss: 0.02058\n",
            "Epoch: 193 | Iteration: 19 | Classification loss: 0.00017 | Regression loss: 0.02177 | Running loss: 0.02055\n",
            "Epoch: 193 | Iteration: 20 | Classification loss: 0.00011 | Regression loss: 0.01143 | Running loss: 0.02051\n",
            "Epoch: 193 | Iteration: 21 | Classification loss: 0.00017 | Regression loss: 0.02146 | Running loss: 0.02054\n",
            "Epoch: 193 | Iteration: 22 | Classification loss: 0.00007 | Regression loss: 0.01991 | Running loss: 0.02053\n",
            "Epoch: 193 | Iteration: 23 | Classification loss: 0.00014 | Regression loss: 0.02336 | Running loss: 0.02053\n",
            "Epoch: 193 | Iteration: 24 | Classification loss: 0.00008 | Regression loss: 0.01097 | Running loss: 0.02051\n",
            "Epoch: 193 | Iteration: 25 | Classification loss: 0.00017 | Regression loss: 0.03444 | Running loss: 0.02055\n",
            "Epoch: 193 | Iteration: 26 | Classification loss: 0.00028 | Regression loss: 0.02658 | Running loss: 0.02055\n",
            "Epoch: 193 | Iteration: 27 | Classification loss: 0.00006 | Regression loss: 0.02491 | Running loss: 0.02058\n",
            "Epoch: 193 | Iteration: 28 | Classification loss: 0.00013 | Regression loss: 0.02298 | Running loss: 0.02056\n",
            "Epoch: 193 | Iteration: 29 | Classification loss: 0.00015 | Regression loss: 0.01841 | Running loss: 0.02056\n",
            "Epoch: 193 | Iteration: 30 | Classification loss: 0.00005 | Regression loss: 0.02870 | Running loss: 0.02058\n",
            "Epoch: 193 | Iteration: 31 | Classification loss: 0.00031 | Regression loss: 0.02084 | Running loss: 0.02059\n",
            "Epoch: 193 | Iteration: 32 | Classification loss: 0.00003 | Regression loss: 0.01534 | Running loss: 0.02057\n",
            "Epoch: 193 | Iteration: 33 | Classification loss: 0.00031 | Regression loss: 0.03527 | Running loss: 0.02058\n",
            "Epoch: 193 | Iteration: 34 | Classification loss: 0.00008 | Regression loss: 0.01549 | Running loss: 0.02059\n",
            "Epoch: 193 | Iteration: 35 | Classification loss: 0.00013 | Regression loss: 0.01845 | Running loss: 0.02059\n",
            "Epoch: 193 | Iteration: 36 | Classification loss: 0.00002 | Regression loss: 0.00376 | Running loss: 0.02050\n",
            "Epoch: 193 | Iteration: 37 | Classification loss: 0.00012 | Regression loss: 0.01181 | Running loss: 0.02051\n",
            "Epoch: 193 | Iteration: 38 | Classification loss: 0.00004 | Regression loss: 0.00506 | Running loss: 0.02048\n",
            "Epoch: 193 | Iteration: 39 | Classification loss: 0.00019 | Regression loss: 0.01971 | Running loss: 0.02048\n",
            "Epoch: 193 | Iteration: 40 | Classification loss: 0.00041 | Regression loss: 0.02128 | Running loss: 0.02048\n",
            "Epoch: 193 | Iteration: 41 | Classification loss: 0.00015 | Regression loss: 0.01704 | Running loss: 0.02048\n",
            "Epoch: 193 | Iteration: 42 | Classification loss: 0.00005 | Regression loss: 0.00885 | Running loss: 0.02046\n",
            "Epoch: 193 | Iteration: 43 | Classification loss: 0.00014 | Regression loss: 0.03302 | Running loss: 0.02047\n",
            "Epoch: 193 | Iteration: 44 | Classification loss: 0.00016 | Regression loss: 0.03459 | Running loss: 0.02053\n",
            "Epoch: 193 | Iteration: 45 | Classification loss: 0.00008 | Regression loss: 0.01504 | Running loss: 0.02049\n",
            "Epoch: 193 | Iteration: 46 | Classification loss: 0.00003 | Regression loss: 0.00734 | Running loss: 0.02046\n",
            "Epoch: 193 | Iteration: 47 | Classification loss: 0.00005 | Regression loss: 0.01088 | Running loss: 0.02046\n",
            "Epoch: 193 | Iteration: 48 | Classification loss: 0.00019 | Regression loss: 0.02278 | Running loss: 0.02047\n",
            "Epoch: 193 | Iteration: 49 | Classification loss: 0.00003 | Regression loss: 0.01405 | Running loss: 0.02047\n",
            "Epoch: 193 | Iteration: 50 | Classification loss: 0.00004 | Regression loss: 0.01772 | Running loss: 0.02048\n",
            "Epoch: 193 | Iteration: 51 | Classification loss: 0.00009 | Regression loss: 0.01577 | Running loss: 0.02044\n",
            "Epoch: 193 | Iteration: 52 | Classification loss: 0.00012 | Regression loss: 0.01065 | Running loss: 0.02039\n",
            "Epoch: 193 | Iteration: 53 | Classification loss: 0.00013 | Regression loss: 0.01665 | Running loss: 0.02040\n",
            "Epoch: 193 | Iteration: 54 | Classification loss: 0.00043 | Regression loss: 0.03425 | Running loss: 0.02043\n",
            "Epoch: 193 | Iteration: 55 | Classification loss: 0.00029 | Regression loss: 0.01817 | Running loss: 0.02040\n",
            "Epoch: 193 | Iteration: 56 | Classification loss: 0.00007 | Regression loss: 0.01239 | Running loss: 0.02037\n",
            "Epoch: 193 | Iteration: 57 | Classification loss: 0.00009 | Regression loss: 0.02077 | Running loss: 0.02038\n",
            "Epoch: 193 | Iteration: 58 | Classification loss: 0.00006 | Regression loss: 0.02736 | Running loss: 0.02041\n",
            "Epoch: 193 | Iteration: 59 | Classification loss: 0.00009 | Regression loss: 0.01731 | Running loss: 0.02041\n",
            "Epoch: 193 | Iteration: 60 | Classification loss: 0.00005 | Regression loss: 0.01468 | Running loss: 0.02043\n",
            "Epoch: 193 | Iteration: 61 | Classification loss: 0.00008 | Regression loss: 0.02818 | Running loss: 0.02046\n",
            "Epoch: 193 | Iteration: 62 | Classification loss: 0.00009 | Regression loss: 0.01675 | Running loss: 0.02047\n",
            "Epoch: 193 | Iteration: 63 | Classification loss: 0.00027 | Regression loss: 0.03963 | Running loss: 0.02048\n",
            "Epoch: 193 | Iteration: 64 | Classification loss: 0.00005 | Regression loss: 0.01889 | Running loss: 0.02049\n",
            "Epoch: 193 | Iteration: 65 | Classification loss: 0.00006 | Regression loss: 0.00698 | Running loss: 0.02046\n",
            "Epoch: 193 | Iteration: 66 | Classification loss: 0.00012 | Regression loss: 0.02325 | Running loss: 0.02047\n",
            "Epoch: 193 | Iteration: 67 | Classification loss: 0.00007 | Regression loss: 0.01527 | Running loss: 0.02046\n",
            "Epoch: 193 | Iteration: 68 | Classification loss: 0.00006 | Regression loss: 0.01348 | Running loss: 0.02048\n",
            "Epoch: 193 | Iteration: 69 | Classification loss: 0.00003 | Regression loss: 0.01141 | Running loss: 0.02047\n",
            "Epoch: 193 | Iteration: 70 | Classification loss: 0.00006 | Regression loss: 0.02007 | Running loss: 0.02043\n",
            "Epoch: 193 | Iteration: 71 | Classification loss: 0.00002 | Regression loss: 0.00851 | Running loss: 0.02042\n",
            "Epoch: 193 | Iteration: 72 | Classification loss: 0.00011 | Regression loss: 0.01287 | Running loss: 0.02039\n",
            "Epoch: 193 | Iteration: 73 | Classification loss: 0.00007 | Regression loss: 0.02142 | Running loss: 0.02038\n",
            "Epoch: 193 | Iteration: 74 | Classification loss: 0.00027 | Regression loss: 0.02893 | Running loss: 0.02043\n",
            "Epoch: 193 | Iteration: 75 | Classification loss: 0.00008 | Regression loss: 0.01324 | Running loss: 0.02043\n",
            "Epoch: 193 | Iteration: 76 | Classification loss: 0.00013 | Regression loss: 0.02719 | Running loss: 0.02046\n",
            "Epoch: 193 | Iteration: 77 | Classification loss: 0.00013 | Regression loss: 0.02898 | Running loss: 0.02046\n",
            "Epoch: 193 | Iteration: 78 | Classification loss: 0.00003 | Regression loss: 0.01243 | Running loss: 0.02046\n",
            "Epoch: 193 | Iteration: 79 | Classification loss: 0.00007 | Regression loss: 0.01077 | Running loss: 0.02046\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7230921594743263\n",
            "Precision:  0.5959183673469388\n",
            "Recall:  0.7891891891891892\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}, {0: (0.7229906021615096, 185.0)}, {0: (0.7259378939307871, 185.0)}, {0: (0.7306172160652471, 185.0)}, {0: (0.7210590702449308, 185.0)}, {0: (0.7214498931770392, 185.0)}, {0: (0.7272071618498421, 185.0)}, {0: (0.725966639522496, 185.0)}, {0: (0.7208028358685962, 185.0)}, {0: (0.7255636845600553, 185.0)}, {0: (0.7261742190616329, 185.0)}, {0: (0.7217332811149212, 185.0)}, {0: (0.7261653370413381, 185.0)}, {0: (0.7215292823555803, 185.0)}, {0: (0.7277253912015531, 185.0)}, {0: (0.7219177066363771, 185.0)}, {0: (0.7274783215794732, 185.0)}, {0: (0.7218983427777539, 185.0)}, {0: (0.7270037695200344, 185.0)}, {0: (0.7272851879977948, 185.0)}, {0: (0.7221052381441093, 185.0)}, {0: (0.7218191972667742, 185.0)}, {0: (0.7259991707220859, 185.0)}, {0: (0.7281551087798303, 185.0)}, {0: (0.732505495958946, 185.0)}, {0: (0.7266266562021302, 185.0)}, {0: (0.7320831049102212, 185.0)}, {0: (0.7278242123515042, 185.0)}, {0: (0.7278307869685627, 185.0)}, {0: (0.7325520428180083, 185.0)}, {0: (0.7216918061885735, 185.0)}, {0: (0.722396113600594, 185.0)}, {0: (0.7220707898936956, 185.0)}, {0: (0.7217779820060826, 185.0)}, {0: (0.7230066917234796, 185.0)}, {0: (0.727859800128543, 185.0)}, {0: (0.722455689942733, 185.0)}, {0: (0.7295061049671745, 185.0)}, {0: (0.7289767970839213, 185.0)}, {0: (0.7277669415378738, 185.0)}, {0: (0.7230921594743263, 185.0)}]\n",
            "Epoch: 194 | Iteration: 0 | Classification loss: 0.00003 | Regression loss: 0.00766 | Running loss: 0.02043\n",
            "Epoch: 194 | Iteration: 1 | Classification loss: 0.00011 | Regression loss: 0.01204 | Running loss: 0.02039\n",
            "Epoch: 194 | Iteration: 2 | Classification loss: 0.00006 | Regression loss: 0.00677 | Running loss: 0.02037\n",
            "Epoch: 194 | Iteration: 3 | Classification loss: 0.00008 | Regression loss: 0.02005 | Running loss: 0.02039\n",
            "Epoch: 194 | Iteration: 4 | Classification loss: 0.00009 | Regression loss: 0.01550 | Running loss: 0.02037\n",
            "Epoch: 194 | Iteration: 5 | Classification loss: 0.00029 | Regression loss: 0.02225 | Running loss: 0.02033\n",
            "Epoch: 194 | Iteration: 6 | Classification loss: 0.00003 | Regression loss: 0.00658 | Running loss: 0.02031\n",
            "Epoch: 194 | Iteration: 7 | Classification loss: 0.00004 | Regression loss: 0.01522 | Running loss: 0.02029\n",
            "Epoch: 194 | Iteration: 8 | Classification loss: 0.00004 | Regression loss: 0.00707 | Running loss: 0.02027\n",
            "Epoch: 194 | Iteration: 9 | Classification loss: 0.00008 | Regression loss: 0.01678 | Running loss: 0.02028\n",
            "Epoch: 194 | Iteration: 10 | Classification loss: 0.00028 | Regression loss: 0.02014 | Running loss: 0.02029\n",
            "Epoch: 194 | Iteration: 11 | Classification loss: 0.00011 | Regression loss: 0.01066 | Running loss: 0.02027\n",
            "Epoch: 194 | Iteration: 12 | Classification loss: 0.00015 | Regression loss: 0.01622 | Running loss: 0.02025\n",
            "Epoch: 194 | Iteration: 13 | Classification loss: 0.00016 | Regression loss: 0.01780 | Running loss: 0.02024\n",
            "Epoch: 194 | Iteration: 14 | Classification loss: 0.00016 | Regression loss: 0.03054 | Running loss: 0.02023\n",
            "Epoch: 194 | Iteration: 15 | Classification loss: 0.00012 | Regression loss: 0.02543 | Running loss: 0.02024\n",
            "Epoch: 194 | Iteration: 16 | Classification loss: 0.00024 | Regression loss: 0.02404 | Running loss: 0.02025\n",
            "Epoch: 194 | Iteration: 17 | Classification loss: 0.00034 | Regression loss: 0.03477 | Running loss: 0.02029\n",
            "Epoch: 194 | Iteration: 18 | Classification loss: 0.00012 | Regression loss: 0.02024 | Running loss: 0.02031\n",
            "Epoch: 194 | Iteration: 19 | Classification loss: 0.00006 | Regression loss: 0.01187 | Running loss: 0.02026\n",
            "Epoch: 194 | Iteration: 20 | Classification loss: 0.00010 | Regression loss: 0.03417 | Running loss: 0.02029\n",
            "Epoch: 194 | Iteration: 21 | Classification loss: 0.00004 | Regression loss: 0.01254 | Running loss: 0.02029\n",
            "Epoch: 194 | Iteration: 22 | Classification loss: 0.00007 | Regression loss: 0.00883 | Running loss: 0.02028\n",
            "Epoch: 194 | Iteration: 23 | Classification loss: 0.00009 | Regression loss: 0.01431 | Running loss: 0.02024\n",
            "Epoch: 194 | Iteration: 24 | Classification loss: 0.00007 | Regression loss: 0.01434 | Running loss: 0.02021\n",
            "Epoch: 194 | Iteration: 25 | Classification loss: 0.00010 | Regression loss: 0.02290 | Running loss: 0.02023\n",
            "Epoch: 194 | Iteration: 26 | Classification loss: 0.00021 | Regression loss: 0.02291 | Running loss: 0.02021\n",
            "Epoch: 194 | Iteration: 27 | Classification loss: 0.00009 | Regression loss: 0.00802 | Running loss: 0.02020\n",
            "Epoch: 194 | Iteration: 28 | Classification loss: 0.00014 | Regression loss: 0.02686 | Running loss: 0.02020\n",
            "Epoch: 194 | Iteration: 29 | Classification loss: 0.00020 | Regression loss: 0.02034 | Running loss: 0.02023\n",
            "Epoch: 194 | Iteration: 30 | Classification loss: 0.00042 | Regression loss: 0.01936 | Running loss: 0.02025\n",
            "Epoch: 194 | Iteration: 31 | Classification loss: 0.00009 | Regression loss: 0.01214 | Running loss: 0.02024\n",
            "Epoch: 194 | Iteration: 32 | Classification loss: 0.00032 | Regression loss: 0.03765 | Running loss: 0.02029\n",
            "Epoch: 194 | Iteration: 33 | Classification loss: 0.00008 | Regression loss: 0.01405 | Running loss: 0.02029\n",
            "Epoch: 194 | Iteration: 34 | Classification loss: 0.00014 | Regression loss: 0.02132 | Running loss: 0.02030\n",
            "Epoch: 194 | Iteration: 35 | Classification loss: 0.00004 | Regression loss: 0.01651 | Running loss: 0.02030\n",
            "Epoch: 194 | Iteration: 36 | Classification loss: 0.00021 | Regression loss: 0.02302 | Running loss: 0.02029\n",
            "Epoch: 194 | Iteration: 37 | Classification loss: 0.00005 | Regression loss: 0.02085 | Running loss: 0.02028\n",
            "Epoch: 194 | Iteration: 38 | Classification loss: 0.00028 | Regression loss: 0.03762 | Running loss: 0.02032\n",
            "Epoch: 194 | Iteration: 39 | Classification loss: 0.00019 | Regression loss: 0.01237 | Running loss: 0.02032\n",
            "Epoch: 194 | Iteration: 40 | Classification loss: 0.00008 | Regression loss: 0.01577 | Running loss: 0.02028\n",
            "Epoch: 194 | Iteration: 41 | Classification loss: 0.00005 | Regression loss: 0.01347 | Running loss: 0.02024\n",
            "Epoch: 194 | Iteration: 42 | Classification loss: 0.00030 | Regression loss: 0.01835 | Running loss: 0.02025\n",
            "Epoch: 194 | Iteration: 43 | Classification loss: 0.00004 | Regression loss: 0.02081 | Running loss: 0.02025\n",
            "Epoch: 194 | Iteration: 44 | Classification loss: 0.00014 | Regression loss: 0.01948 | Running loss: 0.02024\n",
            "Epoch: 194 | Iteration: 45 | Classification loss: 0.00010 | Regression loss: 0.01234 | Running loss: 0.02020\n",
            "Epoch: 194 | Iteration: 46 | Classification loss: 0.00030 | Regression loss: 0.04634 | Running loss: 0.02026\n",
            "Epoch: 194 | Iteration: 47 | Classification loss: 0.00039 | Regression loss: 0.03123 | Running loss: 0.02030\n",
            "Epoch: 194 | Iteration: 48 | Classification loss: 0.00010 | Regression loss: 0.01118 | Running loss: 0.02028\n",
            "Epoch: 194 | Iteration: 49 | Classification loss: 0.00006 | Regression loss: 0.01480 | Running loss: 0.02025\n",
            "Epoch: 194 | Iteration: 50 | Classification loss: 0.00009 | Regression loss: 0.02197 | Running loss: 0.02028\n",
            "Epoch: 194 | Iteration: 51 | Classification loss: 0.00008 | Regression loss: 0.01565 | Running loss: 0.02029\n",
            "Epoch: 194 | Iteration: 52 | Classification loss: 0.00001 | Regression loss: 0.00885 | Running loss: 0.02030\n",
            "Epoch: 194 | Iteration: 53 | Classification loss: 0.00012 | Regression loss: 0.03069 | Running loss: 0.02032\n",
            "Epoch: 194 | Iteration: 54 | Classification loss: 0.00008 | Regression loss: 0.01566 | Running loss: 0.02028\n",
            "Epoch: 194 | Iteration: 55 | Classification loss: 0.00003 | Regression loss: 0.01362 | Running loss: 0.02029\n",
            "Epoch: 194 | Iteration: 56 | Classification loss: 0.00011 | Regression loss: 0.02312 | Running loss: 0.02030\n",
            "Epoch: 194 | Iteration: 57 | Classification loss: 0.00013 | Regression loss: 0.01509 | Running loss: 0.02031\n",
            "Epoch: 194 | Iteration: 58 | Classification loss: 0.00018 | Regression loss: 0.02713 | Running loss: 0.02032\n",
            "Epoch: 194 | Iteration: 59 | Classification loss: 0.00012 | Regression loss: 0.01856 | Running loss: 0.02030\n",
            "Epoch: 194 | Iteration: 60 | Classification loss: 0.00013 | Regression loss: 0.01584 | Running loss: 0.02031\n",
            "Epoch: 194 | Iteration: 61 | Classification loss: 0.00001 | Regression loss: 0.00381 | Running loss: 0.02028\n",
            "Epoch: 194 | Iteration: 62 | Classification loss: 0.00016 | Regression loss: 0.03218 | Running loss: 0.02032\n",
            "Epoch: 194 | Iteration: 63 | Classification loss: 0.00006 | Regression loss: 0.02249 | Running loss: 0.02032\n",
            "Epoch: 194 | Iteration: 64 | Classification loss: 0.00008 | Regression loss: 0.01975 | Running loss: 0.02032\n",
            "Epoch: 194 | Iteration: 65 | Classification loss: 0.00008 | Regression loss: 0.01724 | Running loss: 0.02030\n",
            "Epoch: 194 | Iteration: 66 | Classification loss: 0.00005 | Regression loss: 0.01733 | Running loss: 0.02031\n",
            "Epoch: 194 | Iteration: 67 | Classification loss: 0.00031 | Regression loss: 0.03108 | Running loss: 0.02032\n",
            "Epoch: 194 | Iteration: 68 | Classification loss: 0.00005 | Regression loss: 0.01436 | Running loss: 0.02031\n",
            "Epoch: 194 | Iteration: 69 | Classification loss: 0.00049 | Regression loss: 0.03172 | Running loss: 0.02033\n",
            "Epoch: 194 | Iteration: 70 | Classification loss: 0.00020 | Regression loss: 0.02651 | Running loss: 0.02036\n",
            "Epoch: 194 | Iteration: 71 | Classification loss: 0.00008 | Regression loss: 0.01478 | Running loss: 0.02034\n",
            "Epoch: 194 | Iteration: 72 | Classification loss: 0.00039 | Regression loss: 0.03417 | Running loss: 0.02033\n",
            "Epoch: 194 | Iteration: 73 | Classification loss: 0.00008 | Regression loss: 0.02528 | Running loss: 0.02037\n",
            "Epoch: 194 | Iteration: 74 | Classification loss: 0.00005 | Regression loss: 0.02380 | Running loss: 0.02038\n",
            "Epoch: 194 | Iteration: 75 | Classification loss: 0.00009 | Regression loss: 0.01413 | Running loss: 0.02031\n",
            "Epoch: 194 | Iteration: 76 | Classification loss: 0.00004 | Regression loss: 0.01790 | Running loss: 0.02029\n",
            "Epoch: 194 | Iteration: 77 | Classification loss: 0.00005 | Regression loss: 0.01086 | Running loss: 0.02028\n",
            "Epoch: 194 | Iteration: 78 | Classification loss: 0.00023 | Regression loss: 0.03926 | Running loss: 0.02028\n",
            "Epoch: 194 | Iteration: 79 | Classification loss: 0.00006 | Regression loss: 0.01937 | Running loss: 0.02029\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7224723887683493\n",
            "Precision:  0.5910931174089069\n",
            "Recall:  0.7891891891891892\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}, {0: (0.7229906021615096, 185.0)}, {0: (0.7259378939307871, 185.0)}, {0: (0.7306172160652471, 185.0)}, {0: (0.7210590702449308, 185.0)}, {0: (0.7214498931770392, 185.0)}, {0: (0.7272071618498421, 185.0)}, {0: (0.725966639522496, 185.0)}, {0: (0.7208028358685962, 185.0)}, {0: (0.7255636845600553, 185.0)}, {0: (0.7261742190616329, 185.0)}, {0: (0.7217332811149212, 185.0)}, {0: (0.7261653370413381, 185.0)}, {0: (0.7215292823555803, 185.0)}, {0: (0.7277253912015531, 185.0)}, {0: (0.7219177066363771, 185.0)}, {0: (0.7274783215794732, 185.0)}, {0: (0.7218983427777539, 185.0)}, {0: (0.7270037695200344, 185.0)}, {0: (0.7272851879977948, 185.0)}, {0: (0.7221052381441093, 185.0)}, {0: (0.7218191972667742, 185.0)}, {0: (0.7259991707220859, 185.0)}, {0: (0.7281551087798303, 185.0)}, {0: (0.732505495958946, 185.0)}, {0: (0.7266266562021302, 185.0)}, {0: (0.7320831049102212, 185.0)}, {0: (0.7278242123515042, 185.0)}, {0: (0.7278307869685627, 185.0)}, {0: (0.7325520428180083, 185.0)}, {0: (0.7216918061885735, 185.0)}, {0: (0.722396113600594, 185.0)}, {0: (0.7220707898936956, 185.0)}, {0: (0.7217779820060826, 185.0)}, {0: (0.7230066917234796, 185.0)}, {0: (0.727859800128543, 185.0)}, {0: (0.722455689942733, 185.0)}, {0: (0.7295061049671745, 185.0)}, {0: (0.7289767970839213, 185.0)}, {0: (0.7277669415378738, 185.0)}, {0: (0.7230921594743263, 185.0)}, {0: (0.7224723887683493, 185.0)}]\n",
            "Epoch: 195 | Iteration: 0 | Classification loss: 0.00002 | Regression loss: 0.00564 | Running loss: 0.02027\n",
            "Epoch: 195 | Iteration: 1 | Classification loss: 0.00010 | Regression loss: 0.00998 | Running loss: 0.02025\n",
            "Epoch: 195 | Iteration: 2 | Classification loss: 0.00025 | Regression loss: 0.03212 | Running loss: 0.02029\n",
            "Epoch: 195 | Iteration: 3 | Classification loss: 0.00007 | Regression loss: 0.01554 | Running loss: 0.02028\n",
            "Epoch: 195 | Iteration: 4 | Classification loss: 0.00016 | Regression loss: 0.03286 | Running loss: 0.02030\n",
            "Epoch: 195 | Iteration: 5 | Classification loss: 0.00012 | Regression loss: 0.02935 | Running loss: 0.02029\n",
            "Epoch: 195 | Iteration: 6 | Classification loss: 0.00010 | Regression loss: 0.01061 | Running loss: 0.02029\n",
            "Epoch: 195 | Iteration: 7 | Classification loss: 0.00018 | Regression loss: 0.02637 | Running loss: 0.02030\n",
            "Epoch: 195 | Iteration: 8 | Classification loss: 0.00011 | Regression loss: 0.01635 | Running loss: 0.02028\n",
            "Epoch: 195 | Iteration: 9 | Classification loss: 0.00005 | Regression loss: 0.00852 | Running loss: 0.02027\n",
            "Epoch: 195 | Iteration: 10 | Classification loss: 0.00007 | Regression loss: 0.01605 | Running loss: 0.02027\n",
            "Epoch: 195 | Iteration: 11 | Classification loss: 0.00042 | Regression loss: 0.03271 | Running loss: 0.02026\n",
            "Epoch: 195 | Iteration: 12 | Classification loss: 0.00006 | Regression loss: 0.01181 | Running loss: 0.02021\n",
            "Epoch: 195 | Iteration: 13 | Classification loss: 0.00018 | Regression loss: 0.02459 | Running loss: 0.02023\n",
            "Epoch: 195 | Iteration: 14 | Classification loss: 0.00051 | Regression loss: 0.03757 | Running loss: 0.02026\n",
            "Epoch: 195 | Iteration: 15 | Classification loss: 0.00009 | Regression loss: 0.02235 | Running loss: 0.02023\n",
            "Epoch: 195 | Iteration: 16 | Classification loss: 0.00005 | Regression loss: 0.01314 | Running loss: 0.02021\n",
            "Epoch: 195 | Iteration: 17 | Classification loss: 0.00008 | Regression loss: 0.01640 | Running loss: 0.02019\n",
            "Epoch: 195 | Iteration: 18 | Classification loss: 0.00007 | Regression loss: 0.01477 | Running loss: 0.02020\n",
            "Epoch: 195 | Iteration: 19 | Classification loss: 0.00014 | Regression loss: 0.03203 | Running loss: 0.02024\n",
            "Epoch: 195 | Iteration: 20 | Classification loss: 0.00020 | Regression loss: 0.02827 | Running loss: 0.02026\n",
            "Epoch: 195 | Iteration: 21 | Classification loss: 0.00010 | Regression loss: 0.01570 | Running loss: 0.02026\n",
            "Epoch: 195 | Iteration: 22 | Classification loss: 0.00017 | Regression loss: 0.01912 | Running loss: 0.02028\n",
            "Epoch: 195 | Iteration: 23 | Classification loss: 0.00007 | Regression loss: 0.02364 | Running loss: 0.02032\n",
            "Epoch: 195 | Iteration: 24 | Classification loss: 0.00031 | Regression loss: 0.03496 | Running loss: 0.02037\n",
            "Epoch: 195 | Iteration: 25 | Classification loss: 0.00004 | Regression loss: 0.00453 | Running loss: 0.02036\n",
            "Epoch: 195 | Iteration: 26 | Classification loss: 0.00010 | Regression loss: 0.01295 | Running loss: 0.02034\n",
            "Epoch: 195 | Iteration: 27 | Classification loss: 0.00002 | Regression loss: 0.00350 | Running loss: 0.02028\n",
            "Epoch: 195 | Iteration: 28 | Classification loss: 0.00008 | Regression loss: 0.02455 | Running loss: 0.02027\n",
            "Epoch: 195 | Iteration: 29 | Classification loss: 0.00005 | Regression loss: 0.01151 | Running loss: 0.02022\n",
            "Epoch: 195 | Iteration: 30 | Classification loss: 0.00023 | Regression loss: 0.04651 | Running loss: 0.02026\n",
            "Epoch: 195 | Iteration: 31 | Classification loss: 0.00014 | Regression loss: 0.01777 | Running loss: 0.02026\n",
            "Epoch: 195 | Iteration: 32 | Classification loss: 0.00020 | Regression loss: 0.02314 | Running loss: 0.02024\n",
            "Epoch: 195 | Iteration: 33 | Classification loss: 0.00013 | Regression loss: 0.01158 | Running loss: 0.02022\n",
            "Epoch: 195 | Iteration: 34 | Classification loss: 0.00012 | Regression loss: 0.01550 | Running loss: 0.02023\n",
            "Epoch: 195 | Iteration: 35 | Classification loss: 0.00007 | Regression loss: 0.01588 | Running loss: 0.02023\n",
            "Epoch: 195 | Iteration: 36 | Classification loss: 0.00005 | Regression loss: 0.01822 | Running loss: 0.02021\n",
            "Epoch: 195 | Iteration: 37 | Classification loss: 0.00010 | Regression loss: 0.02180 | Running loss: 0.02016\n",
            "Epoch: 195 | Iteration: 38 | Classification loss: 0.00027 | Regression loss: 0.01953 | Running loss: 0.02012\n",
            "Epoch: 195 | Iteration: 39 | Classification loss: 0.00004 | Regression loss: 0.02358 | Running loss: 0.02011\n",
            "Epoch: 195 | Iteration: 40 | Classification loss: 0.00007 | Regression loss: 0.02269 | Running loss: 0.02013\n",
            "Epoch: 195 | Iteration: 41 | Classification loss: 0.00014 | Regression loss: 0.01614 | Running loss: 0.02013\n",
            "Epoch: 195 | Iteration: 42 | Classification loss: 0.00004 | Regression loss: 0.01428 | Running loss: 0.02012\n",
            "Epoch: 195 | Iteration: 43 | Classification loss: 0.00004 | Regression loss: 0.01972 | Running loss: 0.02012\n",
            "Epoch: 195 | Iteration: 44 | Classification loss: 0.00005 | Regression loss: 0.01211 | Running loss: 0.02012\n",
            "Epoch: 195 | Iteration: 45 | Classification loss: 0.00006 | Regression loss: 0.01162 | Running loss: 0.02007\n",
            "Epoch: 195 | Iteration: 46 | Classification loss: 0.00013 | Regression loss: 0.01682 | Running loss: 0.02006\n",
            "Epoch: 195 | Iteration: 47 | Classification loss: 0.00002 | Regression loss: 0.01331 | Running loss: 0.02001\n",
            "Epoch: 195 | Iteration: 48 | Classification loss: 0.00003 | Regression loss: 0.01510 | Running loss: 0.02002\n",
            "Epoch: 195 | Iteration: 49 | Classification loss: 0.00012 | Regression loss: 0.03090 | Running loss: 0.02006\n",
            "Epoch: 195 | Iteration: 50 | Classification loss: 0.00012 | Regression loss: 0.03436 | Running loss: 0.02009\n",
            "Epoch: 195 | Iteration: 51 | Classification loss: 0.00022 | Regression loss: 0.02742 | Running loss: 0.02011\n",
            "Epoch: 195 | Iteration: 52 | Classification loss: 0.00005 | Regression loss: 0.02806 | Running loss: 0.02010\n",
            "Epoch: 195 | Iteration: 53 | Classification loss: 0.00001 | Regression loss: 0.00803 | Running loss: 0.02009\n",
            "Epoch: 195 | Iteration: 54 | Classification loss: 0.00007 | Regression loss: 0.00893 | Running loss: 0.02007\n",
            "Epoch: 195 | Iteration: 55 | Classification loss: 0.00006 | Regression loss: 0.02021 | Running loss: 0.02006\n",
            "Epoch: 195 | Iteration: 56 | Classification loss: 0.00009 | Regression loss: 0.02072 | Running loss: 0.02005\n",
            "Epoch: 195 | Iteration: 57 | Classification loss: 0.00007 | Regression loss: 0.01831 | Running loss: 0.02005\n",
            "Epoch: 195 | Iteration: 58 | Classification loss: 0.00024 | Regression loss: 0.03656 | Running loss: 0.02008\n",
            "Epoch: 195 | Iteration: 59 | Classification loss: 0.00022 | Regression loss: 0.03952 | Running loss: 0.02014\n",
            "Epoch: 195 | Iteration: 60 | Classification loss: 0.00006 | Regression loss: 0.01260 | Running loss: 0.02011\n",
            "Epoch: 195 | Iteration: 61 | Classification loss: 0.00012 | Regression loss: 0.02376 | Running loss: 0.02012\n",
            "Epoch: 195 | Iteration: 62 | Classification loss: 0.00042 | Regression loss: 0.02109 | Running loss: 0.02014\n",
            "Epoch: 195 | Iteration: 63 | Classification loss: 0.00021 | Regression loss: 0.02099 | Running loss: 0.02012\n",
            "Epoch: 195 | Iteration: 64 | Classification loss: 0.00013 | Regression loss: 0.01826 | Running loss: 0.02013\n",
            "Epoch: 195 | Iteration: 65 | Classification loss: 0.00010 | Regression loss: 0.01429 | Running loss: 0.02012\n",
            "Epoch: 195 | Iteration: 66 | Classification loss: 0.00008 | Regression loss: 0.01404 | Running loss: 0.02012\n",
            "Epoch: 195 | Iteration: 67 | Classification loss: 0.00018 | Regression loss: 0.02016 | Running loss: 0.02014\n",
            "Epoch: 195 | Iteration: 68 | Classification loss: 0.00049 | Regression loss: 0.03119 | Running loss: 0.02016\n",
            "Epoch: 195 | Iteration: 69 | Classification loss: 0.00013 | Regression loss: 0.02236 | Running loss: 0.02016\n",
            "Epoch: 195 | Iteration: 70 | Classification loss: 0.00024 | Regression loss: 0.02849 | Running loss: 0.02019\n",
            "Epoch: 195 | Iteration: 71 | Classification loss: 0.00003 | Regression loss: 0.01169 | Running loss: 0.02018\n",
            "Epoch: 195 | Iteration: 72 | Classification loss: 0.00004 | Regression loss: 0.01903 | Running loss: 0.02018\n",
            "Epoch: 195 | Iteration: 73 | Classification loss: 0.00026 | Regression loss: 0.04085 | Running loss: 0.02025\n",
            "Epoch: 195 | Iteration: 74 | Classification loss: 0.00010 | Regression loss: 0.00849 | Running loss: 0.02023\n",
            "Epoch: 195 | Iteration: 75 | Classification loss: 0.00003 | Regression loss: 0.00682 | Running loss: 0.02022\n",
            "Epoch: 195 | Iteration: 76 | Classification loss: 0.00009 | Regression loss: 0.01307 | Running loss: 0.02020\n",
            "Epoch: 195 | Iteration: 77 | Classification loss: 0.00007 | Regression loss: 0.01394 | Running loss: 0.02019\n",
            "Epoch: 195 | Iteration: 78 | Classification loss: 0.00012 | Regression loss: 0.01252 | Running loss: 0.02019\n",
            "Epoch: 195 | Iteration: 79 | Classification loss: 0.00029 | Regression loss: 0.02343 | Running loss: 0.02022\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7270613820183327\n",
            "Precision:  0.5903614457831325\n",
            "Recall:  0.7945945945945946\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}, {0: (0.7229906021615096, 185.0)}, {0: (0.7259378939307871, 185.0)}, {0: (0.7306172160652471, 185.0)}, {0: (0.7210590702449308, 185.0)}, {0: (0.7214498931770392, 185.0)}, {0: (0.7272071618498421, 185.0)}, {0: (0.725966639522496, 185.0)}, {0: (0.7208028358685962, 185.0)}, {0: (0.7255636845600553, 185.0)}, {0: (0.7261742190616329, 185.0)}, {0: (0.7217332811149212, 185.0)}, {0: (0.7261653370413381, 185.0)}, {0: (0.7215292823555803, 185.0)}, {0: (0.7277253912015531, 185.0)}, {0: (0.7219177066363771, 185.0)}, {0: (0.7274783215794732, 185.0)}, {0: (0.7218983427777539, 185.0)}, {0: (0.7270037695200344, 185.0)}, {0: (0.7272851879977948, 185.0)}, {0: (0.7221052381441093, 185.0)}, {0: (0.7218191972667742, 185.0)}, {0: (0.7259991707220859, 185.0)}, {0: (0.7281551087798303, 185.0)}, {0: (0.732505495958946, 185.0)}, {0: (0.7266266562021302, 185.0)}, {0: (0.7320831049102212, 185.0)}, {0: (0.7278242123515042, 185.0)}, {0: (0.7278307869685627, 185.0)}, {0: (0.7325520428180083, 185.0)}, {0: (0.7216918061885735, 185.0)}, {0: (0.722396113600594, 185.0)}, {0: (0.7220707898936956, 185.0)}, {0: (0.7217779820060826, 185.0)}, {0: (0.7230066917234796, 185.0)}, {0: (0.727859800128543, 185.0)}, {0: (0.722455689942733, 185.0)}, {0: (0.7295061049671745, 185.0)}, {0: (0.7289767970839213, 185.0)}, {0: (0.7277669415378738, 185.0)}, {0: (0.7230921594743263, 185.0)}, {0: (0.7224723887683493, 185.0)}, {0: (0.7270613820183327, 185.0)}]\n",
            "Epoch: 196 | Iteration: 0 | Classification loss: 0.00010 | Regression loss: 0.01206 | Running loss: 0.02022\n",
            "Epoch: 196 | Iteration: 1 | Classification loss: 0.00031 | Regression loss: 0.03382 | Running loss: 0.02026\n",
            "Epoch: 196 | Iteration: 2 | Classification loss: 0.00009 | Regression loss: 0.02755 | Running loss: 0.02027\n",
            "Epoch: 196 | Iteration: 3 | Classification loss: 0.00009 | Regression loss: 0.01353 | Running loss: 0.02027\n",
            "Epoch: 196 | Iteration: 4 | Classification loss: 0.00006 | Regression loss: 0.02222 | Running loss: 0.02025\n",
            "Epoch: 196 | Iteration: 5 | Classification loss: 0.00045 | Regression loss: 0.03637 | Running loss: 0.02025\n",
            "Epoch: 196 | Iteration: 6 | Classification loss: 0.00013 | Regression loss: 0.01670 | Running loss: 0.02021\n",
            "Epoch: 196 | Iteration: 7 | Classification loss: 0.00012 | Regression loss: 0.01653 | Running loss: 0.02020\n",
            "Epoch: 196 | Iteration: 8 | Classification loss: 0.00006 | Regression loss: 0.00868 | Running loss: 0.02017\n",
            "Epoch: 196 | Iteration: 9 | Classification loss: 0.00010 | Regression loss: 0.01524 | Running loss: 0.02019\n",
            "Epoch: 196 | Iteration: 10 | Classification loss: 0.00006 | Regression loss: 0.01445 | Running loss: 0.02017\n",
            "Epoch: 196 | Iteration: 11 | Classification loss: 0.00028 | Regression loss: 0.01967 | Running loss: 0.02018\n",
            "Epoch: 196 | Iteration: 12 | Classification loss: 0.00004 | Regression loss: 0.01649 | Running loss: 0.02018\n",
            "Epoch: 196 | Iteration: 13 | Classification loss: 0.00006 | Regression loss: 0.01132 | Running loss: 0.02015\n",
            "Epoch: 196 | Iteration: 14 | Classification loss: 0.00004 | Regression loss: 0.02352 | Running loss: 0.02018\n",
            "Epoch: 196 | Iteration: 15 | Classification loss: 0.00007 | Regression loss: 0.01300 | Running loss: 0.02016\n",
            "Epoch: 196 | Iteration: 16 | Classification loss: 0.00003 | Regression loss: 0.01362 | Running loss: 0.02011\n",
            "Epoch: 196 | Iteration: 17 | Classification loss: 0.00004 | Regression loss: 0.00715 | Running loss: 0.02010\n",
            "Epoch: 196 | Iteration: 18 | Classification loss: 0.00010 | Regression loss: 0.02019 | Running loss: 0.02010\n",
            "Epoch: 196 | Iteration: 19 | Classification loss: 0.00014 | Regression loss: 0.01204 | Running loss: 0.02008\n",
            "Epoch: 196 | Iteration: 20 | Classification loss: 0.00014 | Regression loss: 0.03483 | Running loss: 0.02011\n",
            "Epoch: 196 | Iteration: 21 | Classification loss: 0.00011 | Regression loss: 0.01513 | Running loss: 0.02010\n",
            "Epoch: 196 | Iteration: 22 | Classification loss: 0.00026 | Regression loss: 0.02613 | Running loss: 0.02009\n",
            "Epoch: 196 | Iteration: 23 | Classification loss: 0.00005 | Regression loss: 0.01303 | Running loss: 0.02006\n",
            "Epoch: 196 | Iteration: 24 | Classification loss: 0.00003 | Regression loss: 0.01752 | Running loss: 0.02007\n",
            "Epoch: 196 | Iteration: 25 | Classification loss: 0.00009 | Regression loss: 0.01193 | Running loss: 0.02004\n",
            "Epoch: 196 | Iteration: 26 | Classification loss: 0.00038 | Regression loss: 0.03358 | Running loss: 0.02006\n",
            "Epoch: 196 | Iteration: 27 | Classification loss: 0.00009 | Regression loss: 0.01579 | Running loss: 0.02005\n",
            "Epoch: 196 | Iteration: 28 | Classification loss: 0.00009 | Regression loss: 0.01042 | Running loss: 0.02003\n",
            "Epoch: 196 | Iteration: 29 | Classification loss: 0.00007 | Regression loss: 0.01665 | Running loss: 0.01999\n",
            "Epoch: 196 | Iteration: 30 | Classification loss: 0.00028 | Regression loss: 0.03223 | Running loss: 0.02001\n",
            "Epoch: 196 | Iteration: 31 | Classification loss: 0.00002 | Regression loss: 0.00359 | Running loss: 0.01999\n",
            "Epoch: 196 | Iteration: 32 | Classification loss: 0.00005 | Regression loss: 0.01481 | Running loss: 0.01997\n",
            "Epoch: 196 | Iteration: 33 | Classification loss: 0.00006 | Regression loss: 0.02793 | Running loss: 0.02000\n",
            "Epoch: 196 | Iteration: 34 | Classification loss: 0.00005 | Regression loss: 0.00709 | Running loss: 0.01994\n",
            "Epoch: 196 | Iteration: 35 | Classification loss: 0.00008 | Regression loss: 0.01589 | Running loss: 0.01991\n",
            "Epoch: 196 | Iteration: 36 | Classification loss: 0.00007 | Regression loss: 0.01340 | Running loss: 0.01990\n",
            "Epoch: 196 | Iteration: 37 | Classification loss: 0.00048 | Regression loss: 0.03408 | Running loss: 0.01993\n",
            "Epoch: 196 | Iteration: 38 | Classification loss: 0.00010 | Regression loss: 0.01280 | Running loss: 0.01993\n",
            "Epoch: 196 | Iteration: 39 | Classification loss: 0.00010 | Regression loss: 0.03339 | Running loss: 0.01996\n",
            "Epoch: 196 | Iteration: 40 | Classification loss: 0.00016 | Regression loss: 0.03181 | Running loss: 0.01998\n",
            "Epoch: 196 | Iteration: 41 | Classification loss: 0.00017 | Regression loss: 0.02747 | Running loss: 0.02000\n",
            "Epoch: 196 | Iteration: 42 | Classification loss: 0.00009 | Regression loss: 0.01746 | Running loss: 0.01998\n",
            "Epoch: 196 | Iteration: 43 | Classification loss: 0.00024 | Regression loss: 0.04456 | Running loss: 0.02005\n",
            "Epoch: 196 | Iteration: 44 | Classification loss: 0.00003 | Regression loss: 0.00571 | Running loss: 0.02002\n",
            "Epoch: 196 | Iteration: 45 | Classification loss: 0.00004 | Regression loss: 0.02445 | Running loss: 0.02003\n",
            "Epoch: 196 | Iteration: 46 | Classification loss: 0.00008 | Regression loss: 0.02060 | Running loss: 0.02005\n",
            "Epoch: 196 | Iteration: 47 | Classification loss: 0.00015 | Regression loss: 0.01794 | Running loss: 0.02005\n",
            "Epoch: 196 | Iteration: 48 | Classification loss: 0.00013 | Regression loss: 0.02351 | Running loss: 0.02005\n",
            "Epoch: 196 | Iteration: 49 | Classification loss: 0.00010 | Regression loss: 0.02239 | Running loss: 0.02003\n",
            "Epoch: 196 | Iteration: 50 | Classification loss: 0.00011 | Regression loss: 0.01535 | Running loss: 0.02003\n",
            "Epoch: 196 | Iteration: 51 | Classification loss: 0.00018 | Regression loss: 0.02459 | Running loss: 0.02005\n",
            "Epoch: 196 | Iteration: 52 | Classification loss: 0.00008 | Regression loss: 0.02754 | Running loss: 0.02008\n",
            "Epoch: 196 | Iteration: 53 | Classification loss: 0.00010 | Regression loss: 0.01975 | Running loss: 0.02010\n",
            "Epoch: 196 | Iteration: 54 | Classification loss: 0.00003 | Regression loss: 0.00710 | Running loss: 0.02005\n",
            "Epoch: 196 | Iteration: 55 | Classification loss: 0.00011 | Regression loss: 0.01030 | Running loss: 0.02002\n",
            "Epoch: 196 | Iteration: 56 | Classification loss: 0.00007 | Regression loss: 0.01868 | Running loss: 0.02002\n",
            "Epoch: 196 | Iteration: 57 | Classification loss: 0.00006 | Regression loss: 0.02025 | Running loss: 0.02005\n",
            "Epoch: 196 | Iteration: 58 | Classification loss: 0.00010 | Regression loss: 0.01124 | Running loss: 0.02003\n",
            "Epoch: 196 | Iteration: 59 | Classification loss: 0.00009 | Regression loss: 0.02655 | Running loss: 0.02003\n",
            "Epoch: 196 | Iteration: 60 | Classification loss: 0.00022 | Regression loss: 0.02682 | Running loss: 0.02005\n",
            "Epoch: 196 | Iteration: 61 | Classification loss: 0.00011 | Regression loss: 0.01878 | Running loss: 0.02002\n",
            "Epoch: 196 | Iteration: 62 | Classification loss: 0.00029 | Regression loss: 0.03570 | Running loss: 0.02003\n",
            "Epoch: 196 | Iteration: 63 | Classification loss: 0.00003 | Regression loss: 0.01375 | Running loss: 0.02003\n",
            "Epoch: 196 | Iteration: 64 | Classification loss: 0.00027 | Regression loss: 0.02542 | Running loss: 0.02001\n",
            "Epoch: 196 | Iteration: 65 | Classification loss: 0.00031 | Regression loss: 0.03005 | Running loss: 0.02004\n",
            "Epoch: 196 | Iteration: 66 | Classification loss: 0.00027 | Regression loss: 0.02080 | Running loss: 0.02001\n",
            "Epoch: 196 | Iteration: 67 | Classification loss: 0.00001 | Regression loss: 0.00903 | Running loss: 0.02001\n",
            "Epoch: 196 | Iteration: 68 | Classification loss: 0.00010 | Regression loss: 0.01267 | Running loss: 0.02000\n",
            "Epoch: 196 | Iteration: 69 | Classification loss: 0.00003 | Regression loss: 0.01836 | Running loss: 0.02001\n",
            "Epoch: 196 | Iteration: 70 | Classification loss: 0.00036 | Regression loss: 0.01692 | Running loss: 0.01995\n",
            "Epoch: 196 | Iteration: 71 | Classification loss: 0.00021 | Regression loss: 0.03782 | Running loss: 0.01997\n",
            "Epoch: 196 | Iteration: 72 | Classification loss: 0.00014 | Regression loss: 0.01628 | Running loss: 0.01999\n",
            "Epoch: 196 | Iteration: 73 | Classification loss: 0.00020 | Regression loss: 0.02259 | Running loss: 0.02001\n",
            "Epoch: 196 | Iteration: 74 | Classification loss: 0.00008 | Regression loss: 0.01576 | Running loss: 0.02002\n",
            "Epoch: 196 | Iteration: 75 | Classification loss: 0.00007 | Regression loss: 0.01280 | Running loss: 0.01999\n",
            "Epoch: 196 | Iteration: 76 | Classification loss: 0.00005 | Regression loss: 0.01293 | Running loss: 0.02001\n",
            "Epoch: 196 | Iteration: 77 | Classification loss: 0.00012 | Regression loss: 0.02975 | Running loss: 0.02004\n",
            "Epoch: 196 | Iteration: 78 | Classification loss: 0.00021 | Regression loss: 0.02220 | Running loss: 0.02005\n",
            "Epoch: 196 | Iteration: 79 | Classification loss: 0.00027 | Regression loss: 0.02293 | Running loss: 0.02008\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7281272722727319\n",
            "Precision:  0.592741935483871\n",
            "Recall:  0.7945945945945946\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}, {0: (0.7229906021615096, 185.0)}, {0: (0.7259378939307871, 185.0)}, {0: (0.7306172160652471, 185.0)}, {0: (0.7210590702449308, 185.0)}, {0: (0.7214498931770392, 185.0)}, {0: (0.7272071618498421, 185.0)}, {0: (0.725966639522496, 185.0)}, {0: (0.7208028358685962, 185.0)}, {0: (0.7255636845600553, 185.0)}, {0: (0.7261742190616329, 185.0)}, {0: (0.7217332811149212, 185.0)}, {0: (0.7261653370413381, 185.0)}, {0: (0.7215292823555803, 185.0)}, {0: (0.7277253912015531, 185.0)}, {0: (0.7219177066363771, 185.0)}, {0: (0.7274783215794732, 185.0)}, {0: (0.7218983427777539, 185.0)}, {0: (0.7270037695200344, 185.0)}, {0: (0.7272851879977948, 185.0)}, {0: (0.7221052381441093, 185.0)}, {0: (0.7218191972667742, 185.0)}, {0: (0.7259991707220859, 185.0)}, {0: (0.7281551087798303, 185.0)}, {0: (0.732505495958946, 185.0)}, {0: (0.7266266562021302, 185.0)}, {0: (0.7320831049102212, 185.0)}, {0: (0.7278242123515042, 185.0)}, {0: (0.7278307869685627, 185.0)}, {0: (0.7325520428180083, 185.0)}, {0: (0.7216918061885735, 185.0)}, {0: (0.722396113600594, 185.0)}, {0: (0.7220707898936956, 185.0)}, {0: (0.7217779820060826, 185.0)}, {0: (0.7230066917234796, 185.0)}, {0: (0.727859800128543, 185.0)}, {0: (0.722455689942733, 185.0)}, {0: (0.7295061049671745, 185.0)}, {0: (0.7289767970839213, 185.0)}, {0: (0.7277669415378738, 185.0)}, {0: (0.7230921594743263, 185.0)}, {0: (0.7224723887683493, 185.0)}, {0: (0.7270613820183327, 185.0)}, {0: (0.7281272722727319, 185.0)}]\n",
            "Epoch: 197 | Iteration: 0 | Classification loss: 0.00007 | Regression loss: 0.01520 | Running loss: 0.02009\n",
            "Epoch: 197 | Iteration: 1 | Classification loss: 0.00012 | Regression loss: 0.02451 | Running loss: 0.02008\n",
            "Epoch: 197 | Iteration: 2 | Classification loss: 0.00009 | Regression loss: 0.01022 | Running loss: 0.02007\n",
            "Epoch: 197 | Iteration: 3 | Classification loss: 0.00017 | Regression loss: 0.02298 | Running loss: 0.02008\n",
            "Epoch: 197 | Iteration: 4 | Classification loss: 0.00008 | Regression loss: 0.01471 | Running loss: 0.02006\n",
            "Epoch: 197 | Iteration: 5 | Classification loss: 0.00005 | Regression loss: 0.02614 | Running loss: 0.02009\n",
            "Epoch: 197 | Iteration: 6 | Classification loss: 0.00011 | Regression loss: 0.01504 | Running loss: 0.02008\n",
            "Epoch: 197 | Iteration: 7 | Classification loss: 0.00010 | Regression loss: 0.01390 | Running loss: 0.02008\n",
            "Epoch: 197 | Iteration: 8 | Classification loss: 0.00011 | Regression loss: 0.01209 | Running loss: 0.02010\n",
            "Epoch: 197 | Iteration: 9 | Classification loss: 0.00014 | Regression loss: 0.01579 | Running loss: 0.02009\n",
            "Epoch: 197 | Iteration: 10 | Classification loss: 0.00004 | Regression loss: 0.00630 | Running loss: 0.02007\n",
            "Epoch: 197 | Iteration: 11 | Classification loss: 0.00012 | Regression loss: 0.02685 | Running loss: 0.02010\n",
            "Epoch: 197 | Iteration: 12 | Classification loss: 0.00019 | Regression loss: 0.03782 | Running loss: 0.02014\n",
            "Epoch: 197 | Iteration: 13 | Classification loss: 0.00007 | Regression loss: 0.01273 | Running loss: 0.02014\n",
            "Epoch: 197 | Iteration: 14 | Classification loss: 0.00005 | Regression loss: 0.02382 | Running loss: 0.02011\n",
            "Epoch: 197 | Iteration: 15 | Classification loss: 0.00003 | Regression loss: 0.00658 | Running loss: 0.02008\n",
            "Epoch: 197 | Iteration: 16 | Classification loss: 0.00009 | Regression loss: 0.01380 | Running loss: 0.02003\n",
            "Epoch: 197 | Iteration: 17 | Classification loss: 0.00009 | Regression loss: 0.01890 | Running loss: 0.02001\n",
            "Epoch: 197 | Iteration: 18 | Classification loss: 0.00009 | Regression loss: 0.01736 | Running loss: 0.02001\n",
            "Epoch: 197 | Iteration: 19 | Classification loss: 0.00012 | Regression loss: 0.01922 | Running loss: 0.02001\n",
            "Epoch: 197 | Iteration: 20 | Classification loss: 0.00002 | Regression loss: 0.00830 | Running loss: 0.01996\n",
            "Epoch: 197 | Iteration: 21 | Classification loss: 0.00008 | Regression loss: 0.01459 | Running loss: 0.01998\n",
            "Epoch: 197 | Iteration: 22 | Classification loss: 0.00009 | Regression loss: 0.01826 | Running loss: 0.01995\n",
            "Epoch: 197 | Iteration: 23 | Classification loss: 0.00015 | Regression loss: 0.01191 | Running loss: 0.01992\n",
            "Epoch: 197 | Iteration: 24 | Classification loss: 0.00025 | Regression loss: 0.02239 | Running loss: 0.01992\n",
            "Epoch: 197 | Iteration: 25 | Classification loss: 0.00005 | Regression loss: 0.00853 | Running loss: 0.01990\n",
            "Epoch: 197 | Iteration: 26 | Classification loss: 0.00003 | Regression loss: 0.01178 | Running loss: 0.01987\n",
            "Epoch: 197 | Iteration: 27 | Classification loss: 0.00006 | Regression loss: 0.02006 | Running loss: 0.01985\n",
            "Epoch: 197 | Iteration: 28 | Classification loss: 0.00011 | Regression loss: 0.02332 | Running loss: 0.01985\n",
            "Epoch: 197 | Iteration: 29 | Classification loss: 0.00008 | Regression loss: 0.02529 | Running loss: 0.01988\n",
            "Epoch: 197 | Iteration: 30 | Classification loss: 0.00013 | Regression loss: 0.01843 | Running loss: 0.01988\n",
            "Epoch: 197 | Iteration: 31 | Classification loss: 0.00003 | Regression loss: 0.01781 | Running loss: 0.01989\n",
            "Epoch: 197 | Iteration: 32 | Classification loss: 0.00037 | Regression loss: 0.01906 | Running loss: 0.01990\n",
            "Epoch: 197 | Iteration: 33 | Classification loss: 0.00007 | Regression loss: 0.02123 | Running loss: 0.01992\n",
            "Epoch: 197 | Iteration: 34 | Classification loss: 0.00010 | Regression loss: 0.01178 | Running loss: 0.01991\n",
            "Epoch: 197 | Iteration: 35 | Classification loss: 0.00002 | Regression loss: 0.01362 | Running loss: 0.01986\n",
            "Epoch: 197 | Iteration: 36 | Classification loss: 0.00012 | Regression loss: 0.03007 | Running loss: 0.01988\n",
            "Epoch: 197 | Iteration: 37 | Classification loss: 0.00005 | Regression loss: 0.01772 | Running loss: 0.01983\n",
            "Epoch: 197 | Iteration: 38 | Classification loss: 0.00019 | Regression loss: 0.02945 | Running loss: 0.01982\n",
            "Epoch: 197 | Iteration: 39 | Classification loss: 0.00045 | Regression loss: 0.03691 | Running loss: 0.01984\n",
            "Epoch: 197 | Iteration: 40 | Classification loss: 0.00026 | Regression loss: 0.03633 | Running loss: 0.01988\n",
            "Epoch: 197 | Iteration: 41 | Classification loss: 0.00006 | Regression loss: 0.01384 | Running loss: 0.01988\n",
            "Epoch: 197 | Iteration: 42 | Classification loss: 0.00010 | Regression loss: 0.02070 | Running loss: 0.01989\n",
            "Epoch: 197 | Iteration: 43 | Classification loss: 0.00026 | Regression loss: 0.04512 | Running loss: 0.01995\n",
            "Epoch: 197 | Iteration: 44 | Classification loss: 0.00016 | Regression loss: 0.03319 | Running loss: 0.01995\n",
            "Epoch: 197 | Iteration: 45 | Classification loss: 0.00010 | Regression loss: 0.02672 | Running loss: 0.01999\n",
            "Epoch: 197 | Iteration: 46 | Classification loss: 0.00006 | Regression loss: 0.01822 | Running loss: 0.01996\n",
            "Epoch: 197 | Iteration: 47 | Classification loss: 0.00011 | Regression loss: 0.01687 | Running loss: 0.01990\n",
            "Epoch: 197 | Iteration: 48 | Classification loss: 0.00007 | Regression loss: 0.01200 | Running loss: 0.01989\n",
            "Epoch: 197 | Iteration: 49 | Classification loss: 0.00005 | Regression loss: 0.01101 | Running loss: 0.01990\n",
            "Epoch: 197 | Iteration: 50 | Classification loss: 0.00003 | Regression loss: 0.00638 | Running loss: 0.01988\n",
            "Epoch: 197 | Iteration: 51 | Classification loss: 0.00006 | Regression loss: 0.01136 | Running loss: 0.01985\n",
            "Epoch: 197 | Iteration: 52 | Classification loss: 0.00036 | Regression loss: 0.02983 | Running loss: 0.01987\n",
            "Epoch: 197 | Iteration: 53 | Classification loss: 0.00010 | Regression loss: 0.02283 | Running loss: 0.01987\n",
            "Epoch: 197 | Iteration: 54 | Classification loss: 0.00005 | Regression loss: 0.01062 | Running loss: 0.01986\n",
            "Epoch: 197 | Iteration: 55 | Classification loss: 0.00044 | Regression loss: 0.03160 | Running loss: 0.01989\n",
            "Epoch: 197 | Iteration: 56 | Classification loss: 0.00004 | Regression loss: 0.01181 | Running loss: 0.01985\n",
            "Epoch: 197 | Iteration: 57 | Classification loss: 0.00018 | Regression loss: 0.02119 | Running loss: 0.01987\n",
            "Epoch: 197 | Iteration: 58 | Classification loss: 0.00022 | Regression loss: 0.02294 | Running loss: 0.01990\n",
            "Epoch: 197 | Iteration: 59 | Classification loss: 0.00023 | Regression loss: 0.03302 | Running loss: 0.01993\n",
            "Epoch: 197 | Iteration: 60 | Classification loss: 0.00011 | Regression loss: 0.01189 | Running loss: 0.01992\n",
            "Epoch: 197 | Iteration: 61 | Classification loss: 0.00002 | Regression loss: 0.00363 | Running loss: 0.01989\n",
            "Epoch: 197 | Iteration: 62 | Classification loss: 0.00043 | Regression loss: 0.03151 | Running loss: 0.01994\n",
            "Epoch: 197 | Iteration: 63 | Classification loss: 0.00002 | Regression loss: 0.01512 | Running loss: 0.01995\n",
            "Epoch: 197 | Iteration: 64 | Classification loss: 0.00013 | Regression loss: 0.03284 | Running loss: 0.01997\n",
            "Epoch: 197 | Iteration: 65 | Classification loss: 0.00012 | Regression loss: 0.01663 | Running loss: 0.01998\n",
            "Epoch: 197 | Iteration: 66 | Classification loss: 0.00029 | Regression loss: 0.03913 | Running loss: 0.02001\n",
            "Epoch: 197 | Iteration: 67 | Classification loss: 0.00019 | Regression loss: 0.02639 | Running loss: 0.02004\n",
            "Epoch: 197 | Iteration: 68 | Classification loss: 0.00019 | Regression loss: 0.02835 | Running loss: 0.02006\n",
            "Epoch: 197 | Iteration: 69 | Classification loss: 0.00006 | Regression loss: 0.01408 | Running loss: 0.02005\n",
            "Epoch: 197 | Iteration: 70 | Classification loss: 0.00010 | Regression loss: 0.01574 | Running loss: 0.02005\n",
            "Epoch: 197 | Iteration: 71 | Classification loss: 0.00027 | Regression loss: 0.01775 | Running loss: 0.02006\n",
            "Epoch: 197 | Iteration: 72 | Classification loss: 0.00011 | Regression loss: 0.01038 | Running loss: 0.02006\n",
            "Epoch: 197 | Iteration: 73 | Classification loss: 0.00005 | Regression loss: 0.01537 | Running loss: 0.02003\n",
            "Epoch: 197 | Iteration: 74 | Classification loss: 0.00029 | Regression loss: 0.03367 | Running loss: 0.02007\n",
            "Epoch: 197 | Iteration: 75 | Classification loss: 0.00007 | Regression loss: 0.02093 | Running loss: 0.02010\n",
            "Epoch: 197 | Iteration: 76 | Classification loss: 0.00005 | Regression loss: 0.00735 | Running loss: 0.02010\n",
            "Epoch: 197 | Iteration: 77 | Classification loss: 0.00011 | Regression loss: 0.02833 | Running loss: 0.02007\n",
            "Epoch: 197 | Iteration: 78 | Classification loss: 0.00018 | Regression loss: 0.01821 | Running loss: 0.02008\n",
            "Epoch: 197 | Iteration: 79 | Classification loss: 0.00004 | Regression loss: 0.02086 | Running loss: 0.02007\n",
            "Evaluating dataset\n",
            "\n",
            "mAP:\n",
            "person: 0.7229311041250756\n",
            "Precision:  0.5887096774193549\n",
            "Recall:  0.7891891891891892\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}, {0: (0.7229906021615096, 185.0)}, {0: (0.7259378939307871, 185.0)}, {0: (0.7306172160652471, 185.0)}, {0: (0.7210590702449308, 185.0)}, {0: (0.7214498931770392, 185.0)}, {0: (0.7272071618498421, 185.0)}, {0: (0.725966639522496, 185.0)}, {0: (0.7208028358685962, 185.0)}, {0: (0.7255636845600553, 185.0)}, {0: (0.7261742190616329, 185.0)}, {0: (0.7217332811149212, 185.0)}, {0: (0.7261653370413381, 185.0)}, {0: (0.7215292823555803, 185.0)}, {0: (0.7277253912015531, 185.0)}, {0: (0.7219177066363771, 185.0)}, {0: (0.7274783215794732, 185.0)}, {0: (0.7218983427777539, 185.0)}, {0: (0.7270037695200344, 185.0)}, {0: (0.7272851879977948, 185.0)}, {0: (0.7221052381441093, 185.0)}, {0: (0.7218191972667742, 185.0)}, {0: (0.7259991707220859, 185.0)}, {0: (0.7281551087798303, 185.0)}, {0: (0.732505495958946, 185.0)}, {0: (0.7266266562021302, 185.0)}, {0: (0.7320831049102212, 185.0)}, {0: (0.7278242123515042, 185.0)}, {0: (0.7278307869685627, 185.0)}, {0: (0.7325520428180083, 185.0)}, {0: (0.7216918061885735, 185.0)}, {0: (0.722396113600594, 185.0)}, {0: (0.7220707898936956, 185.0)}, {0: (0.7217779820060826, 185.0)}, {0: (0.7230066917234796, 185.0)}, {0: (0.727859800128543, 185.0)}, {0: (0.722455689942733, 185.0)}, {0: (0.7295061049671745, 185.0)}, {0: (0.7289767970839213, 185.0)}, {0: (0.7277669415378738, 185.0)}, {0: (0.7230921594743263, 185.0)}, {0: (0.7224723887683493, 185.0)}, {0: (0.7270613820183327, 185.0)}, {0: (0.7281272722727319, 185.0)}, {0: (0.7229311041250756, 185.0)}]\n",
            "Epoch: 198 | Iteration: 0 | Classification loss: 0.00006 | Regression loss: 0.02029 | Running loss: 0.02008\n",
            "Epoch: 198 | Iteration: 1 | Classification loss: 0.00033 | Regression loss: 0.02097 | Running loss: 0.02009\n",
            "Epoch: 198 | Iteration: 2 | Classification loss: 0.00006 | Regression loss: 0.01166 | Running loss: 0.02008\n",
            "Epoch: 198 | Iteration: 3 | Classification loss: 0.00007 | Regression loss: 0.02053 | Running loss: 0.02008\n",
            "Epoch: 198 | Iteration: 4 | Classification loss: 0.00009 | Regression loss: 0.01608 | Running loss: 0.02005\n",
            "Epoch: 198 | Iteration: 5 | Classification loss: 0.00004 | Regression loss: 0.01429 | Running loss: 0.02006\n",
            "Epoch: 198 | Iteration: 6 | Classification loss: 0.00017 | Regression loss: 0.02806 | Running loss: 0.02005\n",
            "Epoch: 198 | Iteration: 7 | Classification loss: 0.00003 | Regression loss: 0.01818 | Running loss: 0.02004\n",
            "Epoch: 198 | Iteration: 8 | Classification loss: 0.00018 | Regression loss: 0.03331 | Running loss: 0.02007\n",
            "Epoch: 198 | Iteration: 9 | Classification loss: 0.00017 | Regression loss: 0.01983 | Running loss: 0.02007\n",
            "Epoch: 198 | Iteration: 10 | Classification loss: 0.00011 | Regression loss: 0.01548 | Running loss: 0.02004\n",
            "Epoch: 198 | Iteration: 11 | Classification loss: 0.00009 | Regression loss: 0.02247 | Running loss: 0.02004\n",
            "Epoch: 198 | Iteration: 12 | Classification loss: 0.00020 | Regression loss: 0.02278 | Running loss: 0.02006\n",
            "Epoch: 198 | Iteration: 13 | Classification loss: 0.00037 | Regression loss: 0.02104 | Running loss: 0.02007\n",
            "Epoch: 198 | Iteration: 14 | Classification loss: 0.00011 | Regression loss: 0.01118 | Running loss: 0.02006\n",
            "Epoch: 198 | Iteration: 15 | Classification loss: 0.00006 | Regression loss: 0.01579 | Running loss: 0.02005\n",
            "Epoch: 198 | Iteration: 16 | Classification loss: 0.00026 | Regression loss: 0.04457 | Running loss: 0.02010\n",
            "Epoch: 198 | Iteration: 17 | Classification loss: 0.00008 | Regression loss: 0.02276 | Running loss: 0.02008\n",
            "Epoch: 198 | Iteration: 18 | Classification loss: 0.00009 | Regression loss: 0.00805 | Running loss: 0.02007\n",
            "Epoch: 198 | Iteration: 19 | Classification loss: 0.00006 | Regression loss: 0.00859 | Running loss: 0.02001\n",
            "Epoch: 198 | Iteration: 20 | Classification loss: 0.00004 | Regression loss: 0.01279 | Running loss: 0.01998\n",
            "Epoch: 198 | Iteration: 21 | Classification loss: 0.00002 | Regression loss: 0.00808 | Running loss: 0.01998\n",
            "Epoch: 198 | Iteration: 22 | Classification loss: 0.00009 | Regression loss: 0.01507 | Running loss: 0.01999\n",
            "Epoch: 198 | Iteration: 23 | Classification loss: 0.00003 | Regression loss: 0.00738 | Running loss: 0.01996\n",
            "Epoch: 198 | Iteration: 24 | Classification loss: 0.00012 | Regression loss: 0.01572 | Running loss: 0.01996\n",
            "Epoch: 198 | Iteration: 25 | Classification loss: 0.00010 | Regression loss: 0.01871 | Running loss: 0.01994\n",
            "Epoch: 198 | Iteration: 26 | Classification loss: 0.00011 | Regression loss: 0.02414 | Running loss: 0.01996\n",
            "Epoch: 198 | Iteration: 27 | Classification loss: 0.00009 | Regression loss: 0.02763 | Running loss: 0.01997\n",
            "Epoch: 198 | Iteration: 28 | Classification loss: 0.00017 | Regression loss: 0.02985 | Running loss: 0.01999\n",
            "Epoch: 198 | Iteration: 29 | Classification loss: 0.00007 | Regression loss: 0.01544 | Running loss: 0.01997\n",
            "Epoch: 198 | Iteration: 30 | Classification loss: 0.00006 | Regression loss: 0.01382 | Running loss: 0.01993\n",
            "Epoch: 198 | Iteration: 31 | Classification loss: 0.00007 | Regression loss: 0.02720 | Running loss: 0.01994\n",
            "Epoch: 198 | Iteration: 32 | Classification loss: 0.00003 | Regression loss: 0.01783 | Running loss: 0.01994\n",
            "Epoch: 198 | Iteration: 33 | Classification loss: 0.00010 | Regression loss: 0.01247 | Running loss: 0.01995\n",
            "Epoch: 198 | Iteration: 34 | Classification loss: 0.00005 | Regression loss: 0.01399 | Running loss: 0.01992\n",
            "Epoch: 198 | Iteration: 35 | Classification loss: 0.00003 | Regression loss: 0.02379 | Running loss: 0.01989\n",
            "Epoch: 198 | Iteration: 36 | Classification loss: 0.00038 | Regression loss: 0.03231 | Running loss: 0.01994\n",
            "Epoch: 198 | Iteration: 37 | Classification loss: 0.00025 | Regression loss: 0.02863 | Running loss: 0.01995\n",
            "Epoch: 198 | Iteration: 38 | Classification loss: 0.00009 | Regression loss: 0.01956 | Running loss: 0.01995\n",
            "Epoch: 198 | Iteration: 39 | Classification loss: 0.00005 | Regression loss: 0.02648 | Running loss: 0.01997\n",
            "Epoch: 198 | Iteration: 40 | Classification loss: 0.00003 | Regression loss: 0.00599 | Running loss: 0.01992\n",
            "Epoch: 198 | Iteration: 41 | Classification loss: 0.00006 | Regression loss: 0.00680 | Running loss: 0.01990\n",
            "Epoch: 198 | Iteration: 42 | Classification loss: 0.00015 | Regression loss: 0.01217 | Running loss: 0.01989\n",
            "Epoch: 198 | Iteration: 43 | Classification loss: 0.00003 | Regression loss: 0.01143 | Running loss: 0.01985\n",
            "Epoch: 198 | Iteration: 44 | Classification loss: 0.00009 | Regression loss: 0.01541 | Running loss: 0.01985\n",
            "Epoch: 198 | Iteration: 45 | Classification loss: 0.00028 | Regression loss: 0.01641 | Running loss: 0.01985\n",
            "Epoch: 198 | Iteration: 46 | Classification loss: 0.00018 | Regression loss: 0.02157 | Running loss: 0.01983\n",
            "Epoch: 198 | Iteration: 47 | Classification loss: 0.00003 | Regression loss: 0.01731 | Running loss: 0.01984\n",
            "Epoch: 198 | Iteration: 48 | Classification loss: 0.00021 | Regression loss: 0.02642 | Running loss: 0.01985\n",
            "Epoch: 198 | Iteration: 49 | Classification loss: 0.00013 | Regression loss: 0.01814 | Running loss: 0.01985\n",
            "Epoch: 198 | Iteration: 50 | Classification loss: 0.00015 | Regression loss: 0.02856 | Running loss: 0.01990\n",
            "Epoch: 198 | Iteration: 51 | Classification loss: 0.00042 | Regression loss: 0.03182 | Running loss: 0.01993\n",
            "Epoch: 198 | Iteration: 52 | Classification loss: 0.00007 | Regression loss: 0.01813 | Running loss: 0.01990\n",
            "Epoch: 198 | Iteration: 53 | Classification loss: 0.00010 | Regression loss: 0.01989 | Running loss: 0.01992\n",
            "Epoch: 198 | Iteration: 54 | Classification loss: 0.00023 | Regression loss: 0.03986 | Running loss: 0.01995\n",
            "Epoch: 198 | Iteration: 55 | Classification loss: 0.00002 | Regression loss: 0.01309 | Running loss: 0.01992\n",
            "Epoch: 198 | Iteration: 56 | Classification loss: 0.00005 | Regression loss: 0.01218 | Running loss: 0.01987\n",
            "Epoch: 198 | Iteration: 57 | Classification loss: 0.00001 | Regression loss: 0.00324 | Running loss: 0.01982\n",
            "Epoch: 198 | Iteration: 58 | Classification loss: 0.00008 | Regression loss: 0.00938 | Running loss: 0.01980\n",
            "Epoch: 198 | Iteration: 59 | Classification loss: 0.00008 | Regression loss: 0.01347 | Running loss: 0.01979\n",
            "Epoch: 198 | Iteration: 60 | Classification loss: 0.00006 | Regression loss: 0.02006 | Running loss: 0.01979\n",
            "Epoch: 198 | Iteration: 61 | Classification loss: 0.00016 | Regression loss: 0.02274 | Running loss: 0.01979\n",
            "Epoch: 198 | Iteration: 62 | Classification loss: 0.00038 | Regression loss: 0.03515 | Running loss: 0.01985\n",
            "Epoch: 198 | Iteration: 63 | Classification loss: 0.00013 | Regression loss: 0.03429 | Running loss: 0.01991\n",
            "Epoch: 198 | Iteration: 64 | Classification loss: 0.00013 | Regression loss: 0.03276 | Running loss: 0.01994\n",
            "Epoch: 198 | Iteration: 65 | Classification loss: 0.00008 | Regression loss: 0.01678 | Running loss: 0.01995\n",
            "Epoch: 198 | Iteration: 66 | Classification loss: 0.00022 | Regression loss: 0.03319 | Running loss: 0.02000\n",
            "Epoch: 198 | Iteration: 67 | Classification loss: 0.00016 | Regression loss: 0.03102 | Running loss: 0.02003\n",
            "Epoch: 198 | Iteration: 68 | Classification loss: 0.00012 | Regression loss: 0.01832 | Running loss: 0.01997\n",
            "Epoch: 198 | Iteration: 69 | Classification loss: 0.00029 | Regression loss: 0.03156 | Running loss: 0.02001\n",
            "Epoch: 198 | Iteration: 70 | Classification loss: 0.00013 | Regression loss: 0.01712 | Running loss: 0.02002\n",
            "Epoch: 198 | Iteration: 71 | Classification loss: 0.00010 | Regression loss: 0.01229 | Running loss: 0.02001\n",
            "Epoch: 198 | Iteration: 72 | Classification loss: 0.00004 | Regression loss: 0.02079 | Running loss: 0.01998\n",
            "Epoch: 198 | Iteration: 73 | Classification loss: 0.00012 | Regression loss: 0.01682 | Running loss: 0.01998\n",
            "Epoch: 198 | Iteration: 74 | Classification loss: 0.00008 | Regression loss: 0.01091 | Running loss: 0.01996\n",
            "Epoch: 198 | Iteration: 75 | Classification loss: 0.00016 | Regression loss: 0.01861 | Running loss: 0.01995\n",
            "Epoch: 198 | Iteration: 76 | Classification loss: 0.00027 | Regression loss: 0.03889 | Running loss: 0.01999\n",
            "Epoch: 198 | Iteration: 77 | Classification loss: 0.00002 | Regression loss: 0.00703 | Running loss: 0.01994\n",
            "Epoch: 198 | Iteration: 78 | Classification loss: 0.00006 | Regression loss: 0.01337 | Running loss: 0.01995\n",
            "Epoch: 198 | Iteration: 79 | Classification loss: 0.00006 | Regression loss: 0.01111 | Running loss: 0.01993\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7286498942410554\n",
            "Precision:  0.592741935483871\n",
            "Recall:  0.7945945945945946\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}, {0: (0.7229906021615096, 185.0)}, {0: (0.7259378939307871, 185.0)}, {0: (0.7306172160652471, 185.0)}, {0: (0.7210590702449308, 185.0)}, {0: (0.7214498931770392, 185.0)}, {0: (0.7272071618498421, 185.0)}, {0: (0.725966639522496, 185.0)}, {0: (0.7208028358685962, 185.0)}, {0: (0.7255636845600553, 185.0)}, {0: (0.7261742190616329, 185.0)}, {0: (0.7217332811149212, 185.0)}, {0: (0.7261653370413381, 185.0)}, {0: (0.7215292823555803, 185.0)}, {0: (0.7277253912015531, 185.0)}, {0: (0.7219177066363771, 185.0)}, {0: (0.7274783215794732, 185.0)}, {0: (0.7218983427777539, 185.0)}, {0: (0.7270037695200344, 185.0)}, {0: (0.7272851879977948, 185.0)}, {0: (0.7221052381441093, 185.0)}, {0: (0.7218191972667742, 185.0)}, {0: (0.7259991707220859, 185.0)}, {0: (0.7281551087798303, 185.0)}, {0: (0.732505495958946, 185.0)}, {0: (0.7266266562021302, 185.0)}, {0: (0.7320831049102212, 185.0)}, {0: (0.7278242123515042, 185.0)}, {0: (0.7278307869685627, 185.0)}, {0: (0.7325520428180083, 185.0)}, {0: (0.7216918061885735, 185.0)}, {0: (0.722396113600594, 185.0)}, {0: (0.7220707898936956, 185.0)}, {0: (0.7217779820060826, 185.0)}, {0: (0.7230066917234796, 185.0)}, {0: (0.727859800128543, 185.0)}, {0: (0.722455689942733, 185.0)}, {0: (0.7295061049671745, 185.0)}, {0: (0.7289767970839213, 185.0)}, {0: (0.7277669415378738, 185.0)}, {0: (0.7230921594743263, 185.0)}, {0: (0.7224723887683493, 185.0)}, {0: (0.7270613820183327, 185.0)}, {0: (0.7281272722727319, 185.0)}, {0: (0.7229311041250756, 185.0)}, {0: (0.7286498942410554, 185.0)}]\n",
            "Epoch 00199: reducing learning rate of group 0 to 1.0000e-07.\n",
            "Epoch: 199 | Iteration: 0 | Classification loss: 0.00006 | Regression loss: 0.01374 | Running loss: 0.01991\n",
            "Epoch: 199 | Iteration: 1 | Classification loss: 0.00020 | Regression loss: 0.02589 | Running loss: 0.01993\n",
            "Epoch: 199 | Iteration: 2 | Classification loss: 0.00031 | Regression loss: 0.02720 | Running loss: 0.01997\n",
            "Epoch: 199 | Iteration: 3 | Classification loss: 0.00005 | Regression loss: 0.02706 | Running loss: 0.01996\n",
            "Epoch: 199 | Iteration: 4 | Classification loss: 0.00005 | Regression loss: 0.01285 | Running loss: 0.01996\n",
            "Epoch: 199 | Iteration: 5 | Classification loss: 0.00010 | Regression loss: 0.02246 | Running loss: 0.01997\n",
            "Epoch: 199 | Iteration: 6 | Classification loss: 0.00004 | Regression loss: 0.01149 | Running loss: 0.01994\n",
            "Epoch: 199 | Iteration: 7 | Classification loss: 0.00007 | Regression loss: 0.01863 | Running loss: 0.01991\n",
            "Epoch: 199 | Iteration: 8 | Classification loss: 0.00004 | Regression loss: 0.01734 | Running loss: 0.01991\n",
            "Epoch: 199 | Iteration: 9 | Classification loss: 0.00009 | Regression loss: 0.01340 | Running loss: 0.01991\n",
            "Epoch: 199 | Iteration: 10 | Classification loss: 0.00005 | Regression loss: 0.01205 | Running loss: 0.01991\n",
            "Epoch: 199 | Iteration: 11 | Classification loss: 0.00003 | Regression loss: 0.00700 | Running loss: 0.01989\n",
            "Epoch: 199 | Iteration: 12 | Classification loss: 0.00010 | Regression loss: 0.01208 | Running loss: 0.01986\n",
            "Epoch: 199 | Iteration: 13 | Classification loss: 0.00007 | Regression loss: 0.00892 | Running loss: 0.01985\n",
            "Epoch: 199 | Iteration: 14 | Classification loss: 0.00005 | Regression loss: 0.02363 | Running loss: 0.01987\n",
            "Epoch: 199 | Iteration: 15 | Classification loss: 0.00013 | Regression loss: 0.02336 | Running loss: 0.01989\n",
            "Epoch: 199 | Iteration: 16 | Classification loss: 0.00017 | Regression loss: 0.03314 | Running loss: 0.01988\n",
            "Epoch: 199 | Iteration: 17 | Classification loss: 0.00044 | Regression loss: 0.03529 | Running loss: 0.01991\n",
            "Epoch: 199 | Iteration: 18 | Classification loss: 0.00036 | Regression loss: 0.01804 | Running loss: 0.01991\n",
            "Epoch: 199 | Iteration: 19 | Classification loss: 0.00020 | Regression loss: 0.03512 | Running loss: 0.01994\n",
            "Epoch: 199 | Iteration: 20 | Classification loss: 0.00001 | Regression loss: 0.00766 | Running loss: 0.01990\n",
            "Epoch: 199 | Iteration: 21 | Classification loss: 0.00009 | Regression loss: 0.01061 | Running loss: 0.01989\n",
            "Epoch: 199 | Iteration: 22 | Classification loss: 0.00002 | Regression loss: 0.00619 | Running loss: 0.01987\n",
            "Epoch: 199 | Iteration: 23 | Classification loss: 0.00013 | Regression loss: 0.01758 | Running loss: 0.01984\n",
            "Epoch: 199 | Iteration: 24 | Classification loss: 0.00004 | Regression loss: 0.01385 | Running loss: 0.01983\n",
            "Epoch: 199 | Iteration: 25 | Classification loss: 0.00007 | Regression loss: 0.01440 | Running loss: 0.01980\n",
            "Epoch: 199 | Iteration: 26 | Classification loss: 0.00003 | Regression loss: 0.01319 | Running loss: 0.01977\n",
            "Epoch: 199 | Iteration: 27 | Classification loss: 0.00007 | Regression loss: 0.01985 | Running loss: 0.01974\n",
            "Epoch: 199 | Iteration: 28 | Classification loss: 0.00003 | Regression loss: 0.00852 | Running loss: 0.01974\n",
            "Epoch: 199 | Iteration: 29 | Classification loss: 0.00006 | Regression loss: 0.01117 | Running loss: 0.01967\n",
            "Epoch: 199 | Iteration: 30 | Classification loss: 0.00024 | Regression loss: 0.02160 | Running loss: 0.01969\n",
            "Epoch: 199 | Iteration: 31 | Classification loss: 0.00009 | Regression loss: 0.02310 | Running loss: 0.01970\n",
            "Epoch: 199 | Iteration: 32 | Classification loss: 0.00009 | Regression loss: 0.01408 | Running loss: 0.01969\n",
            "Epoch: 199 | Iteration: 33 | Classification loss: 0.00008 | Regression loss: 0.01473 | Running loss: 0.01970\n",
            "Epoch: 199 | Iteration: 34 | Classification loss: 0.00009 | Regression loss: 0.01125 | Running loss: 0.01968\n",
            "Epoch: 199 | Iteration: 35 | Classification loss: 0.00007 | Regression loss: 0.01610 | Running loss: 0.01963\n",
            "Epoch: 199 | Iteration: 36 | Classification loss: 0.00017 | Regression loss: 0.02675 | Running loss: 0.01961\n",
            "Epoch: 199 | Iteration: 37 | Classification loss: 0.00025 | Regression loss: 0.03896 | Running loss: 0.01964\n",
            "Epoch: 199 | Iteration: 38 | Classification loss: 0.00015 | Regression loss: 0.03159 | Running loss: 0.01967\n",
            "Epoch: 199 | Iteration: 39 | Classification loss: 0.00005 | Regression loss: 0.01293 | Running loss: 0.01965\n",
            "Epoch: 199 | Iteration: 40 | Classification loss: 0.00038 | Regression loss: 0.03082 | Running loss: 0.01969\n",
            "Epoch: 199 | Iteration: 41 | Classification loss: 0.00017 | Regression loss: 0.02321 | Running loss: 0.01970\n",
            "Epoch: 199 | Iteration: 42 | Classification loss: 0.00010 | Regression loss: 0.02418 | Running loss: 0.01970\n",
            "Epoch: 199 | Iteration: 43 | Classification loss: 0.00012 | Regression loss: 0.01746 | Running loss: 0.01969\n",
            "Epoch: 199 | Iteration: 44 | Classification loss: 0.00022 | Regression loss: 0.03519 | Running loss: 0.01974\n",
            "Epoch: 199 | Iteration: 45 | Classification loss: 0.00010 | Regression loss: 0.01039 | Running loss: 0.01969\n",
            "Epoch: 199 | Iteration: 46 | Classification loss: 0.00004 | Regression loss: 0.00782 | Running loss: 0.01965\n",
            "Epoch: 199 | Iteration: 47 | Classification loss: 0.00007 | Regression loss: 0.01872 | Running loss: 0.01964\n",
            "Epoch: 199 | Iteration: 48 | Classification loss: 0.00026 | Regression loss: 0.01988 | Running loss: 0.01964\n",
            "Epoch: 199 | Iteration: 49 | Classification loss: 0.00009 | Regression loss: 0.02210 | Running loss: 0.01964\n",
            "Epoch: 199 | Iteration: 50 | Classification loss: 0.00009 | Regression loss: 0.01742 | Running loss: 0.01962\n",
            "Epoch: 199 | Iteration: 51 | Classification loss: 0.00001 | Regression loss: 0.00332 | Running loss: 0.01959\n",
            "Epoch: 199 | Iteration: 52 | Classification loss: 0.00004 | Regression loss: 0.02060 | Running loss: 0.01960\n",
            "Epoch: 199 | Iteration: 53 | Classification loss: 0.00012 | Regression loss: 0.02842 | Running loss: 0.01958\n",
            "Epoch: 199 | Iteration: 54 | Classification loss: 0.00003 | Regression loss: 0.01464 | Running loss: 0.01958\n",
            "Epoch: 199 | Iteration: 55 | Classification loss: 0.00005 | Regression loss: 0.01456 | Running loss: 0.01957\n",
            "Epoch: 199 | Iteration: 56 | Classification loss: 0.00007 | Regression loss: 0.02692 | Running loss: 0.01962\n",
            "Epoch: 199 | Iteration: 57 | Classification loss: 0.00007 | Regression loss: 0.02329 | Running loss: 0.01964\n",
            "Epoch: 199 | Iteration: 58 | Classification loss: 0.00016 | Regression loss: 0.02552 | Running loss: 0.01968\n",
            "Epoch: 199 | Iteration: 59 | Classification loss: 0.00036 | Regression loss: 0.03323 | Running loss: 0.01971\n",
            "Epoch: 199 | Iteration: 60 | Classification loss: 0.00009 | Regression loss: 0.01484 | Running loss: 0.01970\n",
            "Epoch: 199 | Iteration: 61 | Classification loss: 0.00025 | Regression loss: 0.01724 | Running loss: 0.01970\n",
            "Epoch: 199 | Iteration: 62 | Classification loss: 0.00011 | Regression loss: 0.03244 | Running loss: 0.01974\n",
            "Epoch: 199 | Iteration: 63 | Classification loss: 0.00004 | Regression loss: 0.01996 | Running loss: 0.01972\n",
            "Epoch: 199 | Iteration: 64 | Classification loss: 0.00006 | Regression loss: 0.01426 | Running loss: 0.01968\n",
            "Epoch: 199 | Iteration: 65 | Classification loss: 0.00011 | Regression loss: 0.01645 | Running loss: 0.01968\n",
            "Epoch: 199 | Iteration: 66 | Classification loss: 0.00007 | Regression loss: 0.00991 | Running loss: 0.01969\n",
            "Epoch: 199 | Iteration: 67 | Classification loss: 0.00006 | Regression loss: 0.00844 | Running loss: 0.01968\n",
            "Epoch: 199 | Iteration: 68 | Classification loss: 0.00011 | Regression loss: 0.01499 | Running loss: 0.01966\n",
            "Epoch: 199 | Iteration: 69 | Classification loss: 0.00005 | Regression loss: 0.01318 | Running loss: 0.01966\n",
            "Epoch: 199 | Iteration: 70 | Classification loss: 0.00008 | Regression loss: 0.02000 | Running loss: 0.01967\n",
            "Epoch: 199 | Iteration: 71 | Classification loss: 0.00013 | Regression loss: 0.01685 | Running loss: 0.01967\n",
            "Epoch: 199 | Iteration: 72 | Classification loss: 0.00005 | Regression loss: 0.01318 | Running loss: 0.01967\n",
            "Epoch: 199 | Iteration: 73 | Classification loss: 0.00012 | Regression loss: 0.03296 | Running loss: 0.01971\n",
            "Epoch: 199 | Iteration: 74 | Classification loss: 0.00007 | Regression loss: 0.01650 | Running loss: 0.01967\n",
            "Epoch: 199 | Iteration: 75 | Classification loss: 0.00017 | Regression loss: 0.02032 | Running loss: 0.01968\n",
            "Epoch: 199 | Iteration: 76 | Classification loss: 0.00027 | Regression loss: 0.03420 | Running loss: 0.01972\n",
            "Epoch: 199 | Iteration: 77 | Classification loss: 0.00005 | Regression loss: 0.01450 | Running loss: 0.01971\n",
            "Epoch: 199 | Iteration: 78 | Classification loss: 0.00022 | Regression loss: 0.04332 | Running loss: 0.01974\n",
            "Epoch: 199 | Iteration: 79 | Classification loss: 0.00015 | Regression loss: 0.02797 | Running loss: 0.01976\n",
            "Evaluating dataset\n",
            "86/86\n",
            "mAP:\n",
            "person: 0.7247733277573544\n",
            "Precision:  0.5887096774193549\n",
            "Recall:  0.7891891891891892\n",
            "mAps: [{0: (0.25031730257536844, 185.0)}, {0: (0.4115302711379257, 185.0)}, {0: (0.5047689556197569, 185.0)}, {0: (0.5888251714854185, 185.0)}, {0: (0.5957918322147085, 185.0)}, {0: (0.6436317966936427, 185.0)}, {0: (0.6498634504396164, 185.0)}, {0: (0.6752241930823256, 185.0)}, {0: (0.689372637130689, 185.0)}, {0: (0.6828092046817271, 185.0)}, {0: (0.6756160700052309, 185.0)}, {0: (0.6913278958529833, 185.0)}, {0: (0.7053644964717039, 185.0)}, {0: (0.7034563361457843, 185.0)}, {0: (0.6968517192349907, 185.0)}, {0: (0.7151261728562606, 185.0)}, {0: (0.7046683193900111, 185.0)}, {0: (0.7180388037515556, 185.0)}, {0: (0.7227592805354386, 185.0)}, {0: (0.7178707426175974, 185.0)}, {0: (0.711658847541592, 185.0)}, {0: (0.7128570737915658, 185.0)}, {0: (0.7212352712238267, 185.0)}, {0: (0.7164305138447771, 185.0)}, {0: (0.7109494392203506, 185.0)}, {0: (0.7337480663423731, 185.0)}, {0: (0.7379638997937068, 185.0)}, {0: (0.7175806027646381, 185.0)}, {0: (0.7215532296569873, 185.0)}, {0: (0.7269726116703179, 185.0)}, {0: (0.7249783009999836, 185.0)}, {0: (0.7378823558642875, 185.0)}, {0: (0.727992297203059, 185.0)}, {0: (0.7048436295675484, 185.0)}, {0: (0.736095591742638, 185.0)}, {0: (0.7341618352194583, 185.0)}, {0: (0.72160041463, 185.0)}, {0: (0.7151365091535897, 185.0)}, {0: (0.7383038027043096, 185.0)}, {0: (0.743646521119081, 185.0)}, {0: (0.7431394611564032, 185.0)}, {0: (0.7291390876342534, 185.0)}, {0: (0.7299033194216088, 185.0)}, {0: (0.7273539572474641, 185.0)}, {0: (0.7337324995720493, 185.0)}, {0: (0.7400994373907323, 185.0)}, {0: (0.7362549720103133, 185.0)}, {0: (0.7385718840305993, 185.0)}, {0: (0.733780926738186, 185.0)}, {0: (0.7319575948037835, 185.0)}, {0: (0.7322230467831814, 185.0)}, {0: (0.7411338688673046, 185.0)}, {0: (0.7224372900329012, 185.0)}, {0: (0.7422479677801593, 185.0)}, {0: (0.7354257524186182, 185.0)}, {0: (0.7335253134961582, 185.0)}, {0: (0.7474271470733698, 185.0)}, {0: (0.7360976577576471, 185.0)}, {0: (0.7351313007358427, 185.0)}, {0: (0.7372833990501871, 185.0)}, {0: (0.7298858897110738, 185.0)}, {0: (0.7313585116064594, 185.0)}, {0: (0.7248279371069369, 185.0)}, {0: (0.7234584837386684, 185.0)}, {0: (0.730896643083317, 185.0)}, {0: (0.7270590044603292, 185.0)}, {0: (0.7532414872830644, 185.0)}, {0: (0.7368211222459498, 185.0)}, {0: (0.7295566851257791, 185.0)}, {0: (0.7340968580624802, 185.0)}, {0: (0.7295048992558562, 185.0)}, {0: (0.722884762459923, 185.0)}, {0: (0.7225813877874119, 185.0)}, {0: (0.7440264271518768, 185.0)}, {0: (0.7215052068713474, 185.0)}, {0: (0.7378084784536751, 185.0)}, {0: (0.7447087444549974, 185.0)}, {0: (0.7522396978850561, 185.0)}, {0: (0.7340567194567218, 185.0)}, {0: (0.7219629903470353, 185.0)}, {0: (0.7480480938568659, 185.0)}, {0: (0.735426786293127, 185.0)}, {0: (0.7298524441416966, 185.0)}, {0: (0.7317652004701405, 185.0)}, {0: (0.721329282444235, 185.0)}, {0: (0.7371132146313208, 185.0)}, {0: (0.7373016190498781, 185.0)}, {0: (0.734253733578393, 185.0)}, {0: (0.7426235684751035, 185.0)}, {0: (0.7282619320578395, 185.0)}, {0: (0.7340084347143163, 185.0)}, {0: (0.7318748363395795, 185.0)}, {0: (0.738024004926996, 185.0)}, {0: (0.7308972796870135, 185.0)}, {0: (0.7345096450439633, 185.0)}, {0: (0.7360483531958014, 185.0)}, {0: (0.7318569433246045, 185.0)}, {0: (0.7324159789163838, 185.0)}, {0: (0.7414693291451318, 185.0)}, {0: (0.7321045565916446, 185.0)}, {0: (0.7282951661781079, 185.0)}, {0: (0.7322352736253683, 185.0)}, {0: (0.7327302153401717, 185.0)}, {0: (0.7360906325887727, 185.0)}, {0: (0.733738614968539, 185.0)}, {0: (0.7415721933922996, 185.0)}, {0: (0.7371770344838617, 185.0)}, {0: (0.7379615504368779, 185.0)}, {0: (0.7334956771679544, 185.0)}, {0: (0.7401922743153314, 185.0)}, {0: (0.7326103675554838, 185.0)}, {0: (0.7370066017471236, 185.0)}, {0: (0.7339594173171122, 185.0)}, {0: (0.7340079039703112, 185.0)}, {0: (0.7373621962588107, 185.0)}, {0: (0.7337241140617239, 185.0)}, {0: (0.7353844345780804, 185.0)}, {0: (0.7313168456003689, 185.0)}, {0: (0.7268452643244927, 185.0)}, {0: (0.7341073353967792, 185.0)}, {0: (0.7343869354616779, 185.0)}, {0: (0.734387303565357, 185.0)}, {0: (0.7307945655891906, 185.0)}, {0: (0.7308560109638682, 185.0)}, {0: (0.7313621213402994, 185.0)}, {0: (0.7301706011005815, 185.0)}, {0: (0.7354613213438567, 185.0)}, {0: (0.719723915876822, 185.0)}, {0: (0.7308301242503727, 185.0)}, {0: (0.7345782458132508, 185.0)}, {0: (0.7314008002427808, 185.0)}, {0: (0.7300967113286329, 185.0)}, {0: (0.7345960766377658, 185.0)}, {0: (0.7315735715660854, 185.0)}, {0: (0.7232370276982529, 185.0)}, {0: (0.7318520548937836, 185.0)}, {0: (0.7253147560148798, 185.0)}, {0: (0.7248810095273674, 185.0)}, {0: (0.7289387407050175, 185.0)}, {0: (0.731755793693188, 185.0)}, {0: (0.734380694724692, 185.0)}, {0: (0.7175656620918189, 185.0)}, {0: (0.7163095403698455, 185.0)}, {0: (0.7203252039292192, 185.0)}, {0: (0.7425360533360579, 185.0)}, {0: (0.7248766344895328, 185.0)}, {0: (0.7287625569452764, 185.0)}, {0: (0.7203191939832536, 185.0)}, {0: (0.7267137859081885, 185.0)}, {0: (0.7196694265151026, 185.0)}, {0: (0.7248336954943913, 185.0)}, {0: (0.7191814857867086, 185.0)}, {0: (0.7262342939555708, 185.0)}, {0: (0.7257730609271538, 185.0)}, {0: (0.7229906021615096, 185.0)}, {0: (0.7259378939307871, 185.0)}, {0: (0.7306172160652471, 185.0)}, {0: (0.7210590702449308, 185.0)}, {0: (0.7214498931770392, 185.0)}, {0: (0.7272071618498421, 185.0)}, {0: (0.725966639522496, 185.0)}, {0: (0.7208028358685962, 185.0)}, {0: (0.7255636845600553, 185.0)}, {0: (0.7261742190616329, 185.0)}, {0: (0.7217332811149212, 185.0)}, {0: (0.7261653370413381, 185.0)}, {0: (0.7215292823555803, 185.0)}, {0: (0.7277253912015531, 185.0)}, {0: (0.7219177066363771, 185.0)}, {0: (0.7274783215794732, 185.0)}, {0: (0.7218983427777539, 185.0)}, {0: (0.7270037695200344, 185.0)}, {0: (0.7272851879977948, 185.0)}, {0: (0.7221052381441093, 185.0)}, {0: (0.7218191972667742, 185.0)}, {0: (0.7259991707220859, 185.0)}, {0: (0.7281551087798303, 185.0)}, {0: (0.732505495958946, 185.0)}, {0: (0.7266266562021302, 185.0)}, {0: (0.7320831049102212, 185.0)}, {0: (0.7278242123515042, 185.0)}, {0: (0.7278307869685627, 185.0)}, {0: (0.7325520428180083, 185.0)}, {0: (0.7216918061885735, 185.0)}, {0: (0.722396113600594, 185.0)}, {0: (0.7220707898936956, 185.0)}, {0: (0.7217779820060826, 185.0)}, {0: (0.7230066917234796, 185.0)}, {0: (0.727859800128543, 185.0)}, {0: (0.722455689942733, 185.0)}, {0: (0.7295061049671745, 185.0)}, {0: (0.7289767970839213, 185.0)}, {0: (0.7277669415378738, 185.0)}, {0: (0.7230921594743263, 185.0)}, {0: (0.7224723887683493, 185.0)}, {0: (0.7270613820183327, 185.0)}, {0: (0.7281272722727319, 185.0)}, {0: (0.7229311041250756, 185.0)}, {0: (0.7286498942410554, 185.0)}, {0: (0.7247733277573544, 185.0)}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python custom_visualize.py --dataset csv --csv_classes /content/RetinaNET/class_list.csv  --csv_val /content/RetinaNET/Person_Detection-1/valid/_annotations.csv --model /content/RetinaNET/results/model_final.pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtLy2dk3Gw6d",
        "outputId": "1cb9e037-46de-47d3-fc7a-673c0b3b02b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "Elapsed time: 1.431412696838379\n",
            "person\n",
            "Elapsed time: 0.024917125701904297\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.024263620376586914\n",
            "person\n",
            "Elapsed time: 0.024036884307861328\n",
            "person\n",
            "Elapsed time: 0.02431941032409668\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.02454853057861328\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.024615764617919922\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.02871084213256836\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.02418041229248047\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.024262189865112305\n",
            "person\n",
            "Elapsed time: 0.02477741241455078\n",
            "person\n",
            "Elapsed time: 0.0253293514251709\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.024187803268432617\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.024648189544677734\n",
            "person\n",
            "Elapsed time: 0.024007320404052734\n",
            "person\n",
            "Elapsed time: 0.024047136306762695\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.023874521255493164\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.024203062057495117\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.02410149574279785\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.024047136306762695\n",
            "person\n",
            "Elapsed time: 0.023846149444580078\n",
            "person\n",
            "Elapsed time: 0.023972034454345703\n",
            "person\n",
            "Elapsed time: 0.0246427059173584\n",
            "person\n",
            "Elapsed time: 0.023865938186645508\n",
            "person\n",
            "Elapsed time: 0.024433374404907227\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.02396249771118164\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.024906396865844727\n",
            "person\n",
            "Elapsed time: 0.024429798126220703\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.02407050132751465\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.02435898780822754\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.024024248123168945\n",
            "person\n",
            "Elapsed time: 0.024043560028076172\n",
            "person\n",
            "Elapsed time: 0.024242639541625977\n",
            "person\n",
            "Elapsed time: 0.024338722229003906\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.02430248260498047\n",
            "person\n",
            "Elapsed time: 0.024016380310058594\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.024242162704467773\n",
            "person\n",
            "Elapsed time: 0.02394413948059082\n",
            "person\n",
            "Elapsed time: 0.02408623695373535\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.023804426193237305\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.024028778076171875\n",
            "person\n",
            "Elapsed time: 0.024066686630249023\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.024355173110961914\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.024143218994140625\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.024253368377685547\n",
            "person\n",
            "Elapsed time: 0.02404332160949707\n",
            "person\n",
            "Elapsed time: 0.024301528930664062\n",
            "person\n",
            "Elapsed time: 0.024051189422607422\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.023966550827026367\n",
            "person\n",
            "Elapsed time: 0.02449941635131836\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.024420738220214844\n",
            "person\n",
            "Elapsed time: 0.02437114715576172\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.024234533309936523\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.024207115173339844\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.023888111114501953\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.024393796920776367\n",
            "person\n",
            "Elapsed time: 0.023831605911254883\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.024186134338378906\n",
            "person\n",
            "Elapsed time: 0.024155616760253906\n",
            "person\n",
            "Elapsed time: 0.024196147918701172\n",
            "person\n",
            "Elapsed time: 0.024026870727539062\n",
            "person\n",
            "Elapsed time: 0.02405524253845215\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.02386617660522461\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.024282217025756836\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.024306774139404297\n",
            "person\n",
            "Elapsed time: 0.023999691009521484\n",
            "person\n",
            "Elapsed time: 0.024277210235595703\n",
            "person\n",
            "Elapsed time: 0.024617910385131836\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.0240781307220459\n",
            "person\n",
            "Elapsed time: 0.024280071258544922\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.02399444580078125\n",
            "person\n",
            "Elapsed time: 0.024085044860839844\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.024294137954711914\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.02407097816467285\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.023772716522216797\n",
            "person\n",
            "Elapsed time: 0.02760481834411621\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.0242917537689209\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.02442789077758789\n",
            "person\n",
            "Elapsed time: 0.024246692657470703\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.024187564849853516\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.02420639991760254\n",
            "person\n",
            "Elapsed time: 0.024212360382080078\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.024296283721923828\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.024330854415893555\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.024312257766723633\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.024359464645385742\n",
            "person\n",
            "person\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XmY5X7ecG36H"
      }
    }
  ]
}