{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YD73SWu0FhOc",
        "outputId": "b25a8ccd-b8e5-4e8e-ad41-94b918cd73d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'RetinaNET'...\n",
            "remote: Enumerating objects: 39, done.\u001b[K\n",
            "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "remote: Total 39 (delta 8), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (39/39), 967.99 KiB | 20.59 MiB/s, done.\n",
            "Resolving deltas: 100% (8/8), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/huzi1998/RetinaNET.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install roboflow\n",
        "%cd /content/RetinaNET\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"Hh7MsefWobw6AV6RWT5w\")\n",
        "project = rf.workspace(\"person-detection-piykr\").project(\"kaggle-person-detection\")\n",
        "dataset = project.version(2).download(\"retinanet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VV4I1m89GNou",
        "outputId": "787cf153-40aa-4aca-f526-2decba35aa9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting roboflow\n",
            "  Downloading roboflow-1.1.7-py3-none-any.whl (58 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/58.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting certifi==2022.12.7 (from roboflow)\n",
            "  Downloading certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chardet==4.0.0 (from roboflow)\n",
            "  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cycler==0.10.0 (from roboflow)\n",
            "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
            "Collecting idna==2.10 (from roboflow)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.23.5)\n",
            "Collecting opencv-python-headless==4.8.0.74 (from roboflow)\n",
            "  Downloading opencv_python_headless-4.8.0.74-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (9.4.0)\n",
            "Collecting pyparsing==2.4.7 (from roboflow)\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\n",
            "Collecting python-dotenv (from roboflow)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.16.0)\n",
            "Collecting supervision (from roboflow)\n",
            "  Downloading supervision-0.14.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.0.4)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.66.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0.1)\n",
            "Collecting requests-toolbelt (from roboflow)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (1.1.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (4.42.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (23.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (3.2.0)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from supervision->roboflow) (1.11.2)\n",
            "Installing collected packages: python-dotenv, pyparsing, opencv-python-headless, idna, cycler, chardet, certifi, supervision, requests-toolbelt, roboflow\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.1.1\n",
            "    Uninstalling pyparsing-3.1.1:\n",
            "      Successfully uninstalled pyparsing-3.1.1\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.8.0.76\n",
            "    Uninstalling opencv-python-headless-4.8.0.76:\n",
            "      Successfully uninstalled opencv-python-headless-4.8.0.76\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.4\n",
            "    Uninstalling idna-3.4:\n",
            "      Successfully uninstalled idna-3.4\n",
            "  Attempting uninstall: cycler\n",
            "    Found existing installation: cycler 0.11.0\n",
            "    Uninstalling cycler-0.11.0:\n",
            "      Successfully uninstalled cycler-0.11.0\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2023.7.22\n",
            "    Uninstalling certifi-2023.7.22:\n",
            "      Successfully uninstalled certifi-2023.7.22\n",
            "Successfully installed certifi-2022.12.7 chardet-4.0.0 cycler-0.10.0 idna-2.10 opencv-python-headless-4.8.0.74 pyparsing-2.4.7 python-dotenv-1.0.0 requests-toolbelt-1.0.0 roboflow-1.1.7 supervision-0.14.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi",
                  "cycler",
                  "pyparsing"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/RetinaNET\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in Kaggle-Person-Detection-2 to retinanet:: 100%|██████████| 168288/168288 [00:05<00:00, 32124.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to Kaggle-Person-Detection-2 in retinanet:: 100%|██████████| 2610/2610 [00:00<00:00, 4767.94it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/RetinaNET/extract_images_roboflow.py --folder_path /content/RetinaNET/Kaggle-Person-Detection-2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQQq5WUcGgrS",
        "outputId": "7888036f-9753-4847-9fae-871ff59d3189"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images have been copied successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir 'results'"
      ],
      "metadata": {
        "id": "n0jq6tbtGxKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --depth 34 --epochs 150 --dataset csv --csv_train  /content/RetinaNET/Kaggle-Person-Detection-2/train/_annotations.csv --csv_classes /content/RetinaNET/class_list.csv --csv_val /content/RetinaNET/Kaggle-Person-Detection-2/valid/_annotations.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvOrftMvG0-M",
        "outputId": "f5a04c1a-62c2-4023-dc90-3b46a2c325c9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch: 145 | Iteration: 5 | Classification loss: 0.00002 | Regression loss: 0.01098 | Running loss: 0.01431\n",
            "Epoch: 145 | Iteration: 6 | Classification loss: 0.00005 | Regression loss: 0.01472 | Running loss: 0.01432\n",
            "Epoch: 145 | Iteration: 7 | Classification loss: 0.00002 | Regression loss: 0.03244 | Running loss: 0.01437\n",
            "Epoch: 145 | Iteration: 8 | Classification loss: 0.00004 | Regression loss: 0.01635 | Running loss: 0.01436\n",
            "Epoch: 145 | Iteration: 9 | Classification loss: 0.00013 | Regression loss: 0.01798 | Running loss: 0.01436\n",
            "Epoch: 145 | Iteration: 10 | Classification loss: 0.00012 | Regression loss: 0.01517 | Running loss: 0.01437\n",
            "Epoch: 145 | Iteration: 11 | Classification loss: 0.00004 | Regression loss: 0.01475 | Running loss: 0.01437\n",
            "Epoch: 145 | Iteration: 12 | Classification loss: 0.00002 | Regression loss: 0.01278 | Running loss: 0.01436\n",
            "Epoch: 145 | Iteration: 13 | Classification loss: 0.00006 | Regression loss: 0.01337 | Running loss: 0.01434\n",
            "Epoch: 145 | Iteration: 14 | Classification loss: 0.00003 | Regression loss: 0.00694 | Running loss: 0.01433\n",
            "Epoch: 145 | Iteration: 15 | Classification loss: 0.00008 | Regression loss: 0.01973 | Running loss: 0.01432\n",
            "Epoch: 145 | Iteration: 16 | Classification loss: 0.00002 | Regression loss: 0.00540 | Running loss: 0.01429\n",
            "Epoch: 145 | Iteration: 17 | Classification loss: 0.00006 | Regression loss: 0.01453 | Running loss: 0.01429\n",
            "Epoch: 145 | Iteration: 18 | Classification loss: 0.00003 | Regression loss: 0.02652 | Running loss: 0.01431\n",
            "Epoch: 145 | Iteration: 19 | Classification loss: 0.00007 | Regression loss: 0.01239 | Running loss: 0.01432\n",
            "Epoch: 145 | Iteration: 20 | Classification loss: 0.00003 | Regression loss: 0.01507 | Running loss: 0.01431\n",
            "Epoch: 145 | Iteration: 21 | Classification loss: 0.00009 | Regression loss: 0.01170 | Running loss: 0.01433\n",
            "Epoch: 145 | Iteration: 22 | Classification loss: 0.00006 | Regression loss: 0.01215 | Running loss: 0.01432\n",
            "Epoch: 145 | Iteration: 23 | Classification loss: 0.00005 | Regression loss: 0.01562 | Running loss: 0.01433\n",
            "Epoch: 145 | Iteration: 24 | Classification loss: 0.00006 | Regression loss: 0.01821 | Running loss: 0.01433\n",
            "Epoch: 145 | Iteration: 25 | Classification loss: 0.00003 | Regression loss: 0.02120 | Running loss: 0.01431\n",
            "Epoch: 145 | Iteration: 26 | Classification loss: 0.00027 | Regression loss: 0.01041 | Running loss: 0.01431\n",
            "Epoch: 145 | Iteration: 27 | Classification loss: 0.00004 | Regression loss: 0.00740 | Running loss: 0.01428\n",
            "Epoch: 145 | Iteration: 28 | Classification loss: 0.00002 | Regression loss: 0.00713 | Running loss: 0.01429\n",
            "Epoch: 145 | Iteration: 29 | Classification loss: 0.00002 | Regression loss: 0.01359 | Running loss: 0.01429\n",
            "Epoch: 145 | Iteration: 30 | Classification loss: 0.00001 | Regression loss: 0.00401 | Running loss: 0.01426\n",
            "Epoch: 145 | Iteration: 31 | Classification loss: 0.00008 | Regression loss: 0.02337 | Running loss: 0.01428\n",
            "Epoch: 145 | Iteration: 32 | Classification loss: 0.00003 | Regression loss: 0.01279 | Running loss: 0.01428\n",
            "Epoch: 145 | Iteration: 33 | Classification loss: 0.00002 | Regression loss: 0.00497 | Running loss: 0.01428\n",
            "Epoch: 145 | Iteration: 34 | Classification loss: 0.00004 | Regression loss: 0.00708 | Running loss: 0.01427\n",
            "Epoch: 145 | Iteration: 35 | Classification loss: 0.00002 | Regression loss: 0.00530 | Running loss: 0.01425\n",
            "Epoch: 145 | Iteration: 36 | Classification loss: 0.00002 | Regression loss: 0.00596 | Running loss: 0.01422\n",
            "Epoch: 145 | Iteration: 37 | Classification loss: 0.00011 | Regression loss: 0.01380 | Running loss: 0.01424\n",
            "Epoch: 145 | Iteration: 38 | Classification loss: 0.00006 | Regression loss: 0.02068 | Running loss: 0.01426\n",
            "Epoch: 145 | Iteration: 39 | Classification loss: 0.00003 | Regression loss: 0.01463 | Running loss: 0.01427\n",
            "Epoch: 145 | Iteration: 40 | Classification loss: 0.00003 | Regression loss: 0.00718 | Running loss: 0.01428\n",
            "Epoch: 145 | Iteration: 41 | Classification loss: 0.00003 | Regression loss: 0.00868 | Running loss: 0.01423\n",
            "Epoch: 145 | Iteration: 42 | Classification loss: 0.00004 | Regression loss: 0.01386 | Running loss: 0.01423\n",
            "Epoch: 145 | Iteration: 43 | Classification loss: 0.00005 | Regression loss: 0.01384 | Running loss: 0.01425\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 188.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 145 | Iteration: 45 | Classification loss: 0.00023 | Regression loss: 0.05324 | Running loss: 0.01433\n",
            "Epoch: 145 | Iteration: 46 | Classification loss: 0.00002 | Regression loss: 0.03439 | Running loss: 0.01438\n",
            "Epoch: 145 | Iteration: 47 | Classification loss: 0.00006 | Regression loss: 0.01567 | Running loss: 0.01439\n",
            "Epoch: 145 | Iteration: 48 | Classification loss: 0.00023 | Regression loss: 0.02294 | Running loss: 0.01442\n",
            "Epoch: 145 | Iteration: 49 | Classification loss: 0.00004 | Regression loss: 0.01255 | Running loss: 0.01444\n",
            "Epoch: 145 | Iteration: 50 | Classification loss: 0.00004 | Regression loss: 0.01127 | Running loss: 0.01442\n",
            "Epoch: 145 | Iteration: 51 | Classification loss: 0.00023 | Regression loss: 0.01750 | Running loss: 0.01442\n",
            "Epoch: 145 | Iteration: 52 | Classification loss: 0.00006 | Regression loss: 0.01834 | Running loss: 0.01444\n",
            "Epoch: 145 | Iteration: 53 | Classification loss: 0.00008 | Regression loss: 0.00468 | Running loss: 0.01441\n",
            "Epoch: 145 | Iteration: 54 | Classification loss: 0.00005 | Regression loss: 0.01764 | Running loss: 0.01441\n",
            "Epoch: 145 | Iteration: 55 | Classification loss: 0.00003 | Regression loss: 0.01301 | Running loss: 0.01440\n",
            "Epoch: 145 | Iteration: 56 | Classification loss: 0.00002 | Regression loss: 0.00700 | Running loss: 0.01437\n",
            "Epoch: 145 | Iteration: 57 | Classification loss: 0.00014 | Regression loss: 0.01603 | Running loss: 0.01439\n",
            "Epoch: 145 | Iteration: 58 | Classification loss: 0.00007 | Regression loss: 0.01769 | Running loss: 0.01441\n",
            "Epoch: 145 | Iteration: 59 | Classification loss: 0.00009 | Regression loss: 0.01723 | Running loss: 0.01443\n",
            "Epoch: 145 | Iteration: 60 | Classification loss: 0.00002 | Regression loss: 0.00469 | Running loss: 0.01440\n",
            "Epoch: 145 | Iteration: 61 | Classification loss: 0.00004 | Regression loss: 0.01808 | Running loss: 0.01442\n",
            "Epoch: 145 | Iteration: 62 | Classification loss: 0.00014 | Regression loss: 0.01362 | Running loss: 0.01441\n",
            "Epoch: 145 | Iteration: 63 | Classification loss: 0.00004 | Regression loss: 0.00769 | Running loss: 0.01439\n",
            "Epoch: 145 | Iteration: 64 | Classification loss: 0.00004 | Regression loss: 0.00799 | Running loss: 0.01437\n",
            "Epoch: 145 | Iteration: 65 | Classification loss: 0.00010 | Regression loss: 0.01956 | Running loss: 0.01438\n",
            "Epoch: 145 | Iteration: 66 | Classification loss: 0.00004 | Regression loss: 0.00567 | Running loss: 0.01438\n",
            "Epoch: 145 | Iteration: 67 | Classification loss: 0.00004 | Regression loss: 0.01221 | Running loss: 0.01435\n",
            "Epoch: 145 | Iteration: 68 | Classification loss: 0.00011 | Regression loss: 0.01906 | Running loss: 0.01435\n",
            "Epoch: 145 | Iteration: 69 | Classification loss: 0.00004 | Regression loss: 0.01456 | Running loss: 0.01435\n",
            "Epoch: 145 | Iteration: 70 | Classification loss: 0.00005 | Regression loss: 0.01042 | Running loss: 0.01427\n",
            "Epoch: 145 | Iteration: 71 | Classification loss: 0.00005 | Regression loss: 0.01582 | Running loss: 0.01427\n",
            "Epoch: 145 | Iteration: 72 | Classification loss: 0.00002 | Regression loss: 0.01460 | Running loss: 0.01427\n",
            "Epoch: 145 | Iteration: 73 | Classification loss: 0.00004 | Regression loss: 0.00543 | Running loss: 0.01426\n",
            "Epoch: 145 | Iteration: 74 | Classification loss: 0.00007 | Regression loss: 0.01364 | Running loss: 0.01424\n",
            "Epoch: 145 | Iteration: 75 | Classification loss: 0.00003 | Regression loss: 0.02120 | Running loss: 0.01427\n",
            "Epoch: 145 | Iteration: 76 | Classification loss: 0.00008 | Regression loss: 0.01167 | Running loss: 0.01426\n",
            "Epoch: 145 | Iteration: 77 | Classification loss: 0.00011 | Regression loss: 0.02018 | Running loss: 0.01429\n",
            "Epoch: 145 | Iteration: 78 | Classification loss: 0.00003 | Regression loss: 0.01681 | Running loss: 0.01430\n",
            "Epoch: 145 | Iteration: 79 | Classification loss: 0.00001 | Regression loss: 0.01018 | Running loss: 0.01428\n",
            "Epoch: 145 | Iteration: 80 | Classification loss: 0.00001 | Regression loss: 0.00834 | Running loss: 0.01427\n",
            "Epoch: 145 | Iteration: 81 | Classification loss: 0.00003 | Regression loss: 0.00751 | Running loss: 0.01425\n",
            "Epoch: 145 | Iteration: 82 | Classification loss: 0.00006 | Regression loss: 0.00898 | Running loss: 0.01426\n",
            "Epoch: 145 | Iteration: 83 | Classification loss: 0.00006 | Regression loss: 0.02578 | Running loss: 0.01428\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.7561054217186449\n",
            "Precision:  0.5134474327628362\n",
            "Recall:  0.8300395256916996\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}]\n",
            "Epoch: 146 | Iteration: 0 | Classification loss: 0.00002 | Regression loss: 0.01298 | Running loss: 0.01428\n",
            "Epoch: 146 | Iteration: 1 | Classification loss: 0.00001 | Regression loss: 0.00643 | Running loss: 0.01426\n",
            "Epoch: 146 | Iteration: 2 | Classification loss: 0.00005 | Regression loss: 0.00504 | Running loss: 0.01424\n",
            "Epoch: 146 | Iteration: 3 | Classification loss: 0.00006 | Regression loss: 0.00900 | Running loss: 0.01423\n",
            "Epoch: 146 | Iteration: 4 | Classification loss: 0.00007 | Regression loss: 0.01151 | Running loss: 0.01424\n",
            "Epoch: 146 | Iteration: 5 | Classification loss: 0.00021 | Regression loss: 0.01709 | Running loss: 0.01426\n",
            "Epoch: 146 | Iteration: 6 | Classification loss: 0.00002 | Regression loss: 0.00497 | Running loss: 0.01420\n",
            "Epoch: 146 | Iteration: 7 | Classification loss: 0.00002 | Regression loss: 0.00452 | Running loss: 0.01417\n",
            "Epoch: 146 | Iteration: 8 | Classification loss: 0.00006 | Regression loss: 0.01787 | Running loss: 0.01417\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 188.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 146 | Iteration: 10 | Classification loss: 0.00010 | Regression loss: 0.01291 | Running loss: 0.01417\n",
            "Epoch: 146 | Iteration: 11 | Classification loss: 0.00007 | Regression loss: 0.00752 | Running loss: 0.01417\n",
            "Epoch: 146 | Iteration: 12 | Classification loss: 0.00008 | Regression loss: 0.01960 | Running loss: 0.01418\n",
            "Epoch: 146 | Iteration: 13 | Classification loss: 0.00002 | Regression loss: 0.01551 | Running loss: 0.01417\n",
            "Epoch: 146 | Iteration: 14 | Classification loss: 0.00002 | Regression loss: 0.00599 | Running loss: 0.01415\n",
            "Epoch: 146 | Iteration: 15 | Classification loss: 0.00006 | Regression loss: 0.02000 | Running loss: 0.01415\n",
            "Epoch: 146 | Iteration: 16 | Classification loss: 0.00007 | Regression loss: 0.01692 | Running loss: 0.01416\n",
            "Epoch: 146 | Iteration: 17 | Classification loss: 0.00003 | Regression loss: 0.01378 | Running loss: 0.01414\n",
            "Epoch: 146 | Iteration: 18 | Classification loss: 0.00006 | Regression loss: 0.01210 | Running loss: 0.01415\n",
            "Epoch: 146 | Iteration: 19 | Classification loss: 0.00004 | Regression loss: 0.01665 | Running loss: 0.01416\n",
            "Epoch: 146 | Iteration: 20 | Classification loss: 0.00004 | Regression loss: 0.01354 | Running loss: 0.01416\n",
            "Epoch: 146 | Iteration: 21 | Classification loss: 0.00006 | Regression loss: 0.00407 | Running loss: 0.01415\n",
            "Epoch: 146 | Iteration: 22 | Classification loss: 0.00003 | Regression loss: 0.01426 | Running loss: 0.01417\n",
            "Epoch: 146 | Iteration: 23 | Classification loss: 0.00004 | Regression loss: 0.00617 | Running loss: 0.01416\n",
            "Epoch: 146 | Iteration: 24 | Classification loss: 0.00005 | Regression loss: 0.01643 | Running loss: 0.01417\n",
            "Epoch: 146 | Iteration: 25 | Classification loss: 0.00005 | Regression loss: 0.01867 | Running loss: 0.01418\n",
            "Epoch: 146 | Iteration: 26 | Classification loss: 0.00009 | Regression loss: 0.01506 | Running loss: 0.01420\n",
            "Epoch: 146 | Iteration: 27 | Classification loss: 0.00003 | Regression loss: 0.00830 | Running loss: 0.01420\n",
            "Epoch: 146 | Iteration: 28 | Classification loss: 0.00002 | Regression loss: 0.00513 | Running loss: 0.01418\n",
            "Epoch: 146 | Iteration: 29 | Classification loss: 0.00002 | Regression loss: 0.02644 | Running loss: 0.01419\n",
            "Epoch: 146 | Iteration: 30 | Classification loss: 0.00005 | Regression loss: 0.01792 | Running loss: 0.01419\n",
            "Epoch: 146 | Iteration: 31 | Classification loss: 0.00001 | Regression loss: 0.01046 | Running loss: 0.01418\n",
            "Epoch: 146 | Iteration: 32 | Classification loss: 0.00003 | Regression loss: 0.01249 | Running loss: 0.01419\n",
            "Epoch: 146 | Iteration: 33 | Classification loss: 0.00002 | Regression loss: 0.03339 | Running loss: 0.01423\n",
            "Epoch: 146 | Iteration: 34 | Classification loss: 0.00002 | Regression loss: 0.00383 | Running loss: 0.01419\n",
            "Epoch: 146 | Iteration: 35 | Classification loss: 0.00003 | Regression loss: 0.01773 | Running loss: 0.01419\n",
            "Epoch: 146 | Iteration: 36 | Classification loss: 0.00019 | Regression loss: 0.05091 | Running loss: 0.01427\n",
            "Epoch: 146 | Iteration: 37 | Classification loss: 0.00008 | Regression loss: 0.01161 | Running loss: 0.01426\n",
            "Epoch: 146 | Iteration: 38 | Classification loss: 0.00003 | Regression loss: 0.00553 | Running loss: 0.01426\n",
            "Epoch: 146 | Iteration: 39 | Classification loss: 0.00003 | Regression loss: 0.00751 | Running loss: 0.01426\n",
            "Epoch: 146 | Iteration: 40 | Classification loss: 0.00006 | Regression loss: 0.01606 | Running loss: 0.01425\n",
            "Epoch: 146 | Iteration: 41 | Classification loss: 0.00005 | Regression loss: 0.00821 | Running loss: 0.01420\n",
            "Epoch: 146 | Iteration: 42 | Classification loss: 0.00002 | Regression loss: 0.00836 | Running loss: 0.01421\n",
            "Epoch: 146 | Iteration: 43 | Classification loss: 0.00002 | Regression loss: 0.01113 | Running loss: 0.01419\n",
            "Epoch: 146 | Iteration: 44 | Classification loss: 0.00003 | Regression loss: 0.01322 | Running loss: 0.01417\n",
            "Epoch: 146 | Iteration: 45 | Classification loss: 0.00004 | Regression loss: 0.01480 | Running loss: 0.01417\n",
            "Epoch: 146 | Iteration: 46 | Classification loss: 0.00028 | Regression loss: 0.01099 | Running loss: 0.01417\n",
            "Epoch: 146 | Iteration: 47 | Classification loss: 0.00012 | Regression loss: 0.00555 | Running loss: 0.01416\n",
            "Epoch: 146 | Iteration: 48 | Classification loss: 0.00003 | Regression loss: 0.02193 | Running loss: 0.01418\n",
            "Epoch: 146 | Iteration: 49 | Classification loss: 0.00003 | Regression loss: 0.01609 | Running loss: 0.01420\n",
            "Epoch: 146 | Iteration: 50 | Classification loss: 0.00003 | Regression loss: 0.01950 | Running loss: 0.01422\n",
            "Epoch: 146 | Iteration: 51 | Classification loss: 0.00003 | Regression loss: 0.01491 | Running loss: 0.01423\n",
            "Epoch: 146 | Iteration: 52 | Classification loss: 0.00011 | Regression loss: 0.02542 | Running loss: 0.01426\n",
            "Epoch: 146 | Iteration: 53 | Classification loss: 0.00011 | Regression loss: 0.01895 | Running loss: 0.01426\n",
            "Epoch: 146 | Iteration: 54 | Classification loss: 0.00011 | Regression loss: 0.01852 | Running loss: 0.01428\n",
            "Epoch: 146 | Iteration: 55 | Classification loss: 0.00002 | Regression loss: 0.00864 | Running loss: 0.01427\n",
            "Epoch: 146 | Iteration: 56 | Classification loss: 0.00001 | Regression loss: 0.00812 | Running loss: 0.01425\n",
            "Epoch: 146 | Iteration: 57 | Classification loss: 0.00008 | Regression loss: 0.01524 | Running loss: 0.01424\n",
            "Epoch: 146 | Iteration: 58 | Classification loss: 0.00002 | Regression loss: 0.00602 | Running loss: 0.01422\n",
            "Epoch: 146 | Iteration: 59 | Classification loss: 0.00006 | Regression loss: 0.01283 | Running loss: 0.01422\n",
            "Epoch: 146 | Iteration: 60 | Classification loss: 0.00021 | Regression loss: 0.02102 | Running loss: 0.01422\n",
            "Epoch: 146 | Iteration: 61 | Classification loss: 0.00014 | Regression loss: 0.01270 | Running loss: 0.01424\n",
            "Epoch: 146 | Iteration: 62 | Classification loss: 0.00003 | Regression loss: 0.00769 | Running loss: 0.01424\n",
            "Epoch: 146 | Iteration: 63 | Classification loss: 0.00006 | Regression loss: 0.02943 | Running loss: 0.01428\n",
            "Epoch: 146 | Iteration: 64 | Classification loss: 0.00002 | Regression loss: 0.00633 | Running loss: 0.01426\n",
            "Epoch: 146 | Iteration: 65 | Classification loss: 0.00006 | Regression loss: 0.00878 | Running loss: 0.01422\n",
            "Epoch: 146 | Iteration: 66 | Classification loss: 0.00003 | Regression loss: 0.00715 | Running loss: 0.01420\n",
            "Epoch: 146 | Iteration: 67 | Classification loss: 0.00002 | Regression loss: 0.03169 | Running loss: 0.01423\n",
            "Epoch: 146 | Iteration: 68 | Classification loss: 0.00008 | Regression loss: 0.01394 | Running loss: 0.01423\n",
            "Epoch: 146 | Iteration: 69 | Classification loss: 0.00005 | Regression loss: 0.00724 | Running loss: 0.01414\n",
            "Epoch: 146 | Iteration: 70 | Classification loss: 0.00003 | Regression loss: 0.01376 | Running loss: 0.01416\n",
            "Epoch: 146 | Iteration: 71 | Classification loss: 0.00017 | Regression loss: 0.01707 | Running loss: 0.01415\n",
            "Epoch: 146 | Iteration: 72 | Classification loss: 0.00002 | Regression loss: 0.01282 | Running loss: 0.01417\n",
            "Epoch: 146 | Iteration: 73 | Classification loss: 0.00004 | Regression loss: 0.01076 | Running loss: 0.01418\n",
            "Epoch: 146 | Iteration: 74 | Classification loss: 0.00006 | Regression loss: 0.01671 | Running loss: 0.01420\n",
            "Epoch: 146 | Iteration: 75 | Classification loss: 0.00005 | Regression loss: 0.01763 | Running loss: 0.01421\n",
            "Epoch: 146 | Iteration: 76 | Classification loss: 0.00009 | Regression loss: 0.01775 | Running loss: 0.01422\n",
            "Epoch: 146 | Iteration: 77 | Classification loss: 0.00004 | Regression loss: 0.01345 | Running loss: 0.01422\n",
            "Epoch: 146 | Iteration: 78 | Classification loss: 0.00002 | Regression loss: 0.00567 | Running loss: 0.01422\n",
            "Epoch: 146 | Iteration: 79 | Classification loss: 0.00006 | Regression loss: 0.02092 | Running loss: 0.01424\n",
            "Epoch: 146 | Iteration: 80 | Classification loss: 0.00006 | Regression loss: 0.01333 | Running loss: 0.01423\n",
            "Epoch: 146 | Iteration: 81 | Classification loss: 0.00009 | Regression loss: 0.01954 | Running loss: 0.01425\n",
            "Epoch: 146 | Iteration: 82 | Classification loss: 0.00015 | Regression loss: 0.01565 | Running loss: 0.01422\n",
            "Epoch: 146 | Iteration: 83 | Classification loss: 0.00003 | Regression loss: 0.00853 | Running loss: 0.01421\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.7488370631827469\n",
            "Precision:  0.5135802469135803\n",
            "Recall:  0.8221343873517787\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}]\n",
            "Epoch: 147 | Iteration: 0 | Classification loss: 0.00003 | Regression loss: 0.01377 | Running loss: 0.01421\n",
            "Epoch: 147 | Iteration: 1 | Classification loss: 0.00007 | Regression loss: 0.00717 | Running loss: 0.01420\n",
            "Epoch: 147 | Iteration: 2 | Classification loss: 0.00003 | Regression loss: 0.01445 | Running loss: 0.01421\n",
            "Epoch: 147 | Iteration: 3 | Classification loss: 0.00002 | Regression loss: 0.00624 | Running loss: 0.01421\n",
            "Epoch: 147 | Iteration: 4 | Classification loss: 0.00002 | Regression loss: 0.01909 | Running loss: 0.01422\n",
            "Epoch: 147 | Iteration: 5 | Classification loss: 0.00014 | Regression loss: 0.01536 | Running loss: 0.01419\n",
            "Epoch: 147 | Iteration: 6 | Classification loss: 0.00002 | Regression loss: 0.00478 | Running loss: 0.01415\n",
            "Epoch: 147 | Iteration: 7 | Classification loss: 0.00004 | Regression loss: 0.01244 | Running loss: 0.01414\n",
            "Epoch: 147 | Iteration: 8 | Classification loss: 0.00004 | Regression loss: 0.01744 | Running loss: 0.01414\n",
            "Epoch: 147 | Iteration: 9 | Classification loss: 0.00003 | Regression loss: 0.00536 | Running loss: 0.01411\n",
            "Epoch: 147 | Iteration: 10 | Classification loss: 0.00006 | Regression loss: 0.00821 | Running loss: 0.01411\n",
            "Epoch: 147 | Iteration: 11 | Classification loss: 0.00005 | Regression loss: 0.00933 | Running loss: 0.01411\n",
            "Epoch: 147 | Iteration: 12 | Classification loss: 0.00002 | Regression loss: 0.03131 | Running loss: 0.01411\n",
            "Epoch: 147 | Iteration: 13 | Classification loss: 0.00016 | Regression loss: 0.01414 | Running loss: 0.01412\n",
            "Epoch: 147 | Iteration: 14 | Classification loss: 0.00005 | Regression loss: 0.00495 | Running loss: 0.01410\n",
            "Epoch: 147 | Iteration: 15 | Classification loss: 0.00002 | Regression loss: 0.01280 | Running loss: 0.01410\n",
            "Epoch: 147 | Iteration: 16 | Classification loss: 0.00008 | Regression loss: 0.01987 | Running loss: 0.01412\n",
            "Epoch: 147 | Iteration: 17 | Classification loss: 0.00011 | Regression loss: 0.01184 | Running loss: 0.01412\n",
            "Epoch: 147 | Iteration: 18 | Classification loss: 0.00011 | Regression loss: 0.00595 | Running loss: 0.01412\n",
            "Epoch: 147 | Iteration: 19 | Classification loss: 0.00002 | Regression loss: 0.01310 | Running loss: 0.01413\n",
            "Epoch: 147 | Iteration: 20 | Classification loss: 0.00005 | Regression loss: 0.01535 | Running loss: 0.01415\n",
            "Epoch: 147 | Iteration: 21 | Classification loss: 0.00009 | Regression loss: 0.01792 | Running loss: 0.01416\n",
            "Epoch: 147 | Iteration: 22 | Classification loss: 0.00008 | Regression loss: 0.01724 | Running loss: 0.01415\n",
            "Epoch: 147 | Iteration: 23 | Classification loss: 0.00007 | Regression loss: 0.01504 | Running loss: 0.01417\n",
            "Epoch: 147 | Iteration: 24 | Classification loss: 0.00002 | Regression loss: 0.00787 | Running loss: 0.01416\n",
            "Epoch: 147 | Iteration: 25 | Classification loss: 0.00003 | Regression loss: 0.00484 | Running loss: 0.01414\n",
            "Epoch: 147 | Iteration: 26 | Classification loss: 0.00002 | Regression loss: 0.00423 | Running loss: 0.01414\n",
            "Epoch: 147 | Iteration: 27 | Classification loss: 0.00002 | Regression loss: 0.00727 | Running loss: 0.01410\n",
            "Epoch: 147 | Iteration: 28 | Classification loss: 0.00002 | Regression loss: 0.00558 | Running loss: 0.01408\n",
            "Epoch: 147 | Iteration: 29 | Classification loss: 0.00002 | Regression loss: 0.00568 | Running loss: 0.01405\n",
            "Epoch: 147 | Iteration: 30 | Classification loss: 0.00002 | Regression loss: 0.01547 | Running loss: 0.01404\n",
            "Epoch: 147 | Iteration: 31 | Classification loss: 0.00008 | Regression loss: 0.01447 | Running loss: 0.01405\n",
            "Epoch: 147 | Iteration: 32 | Classification loss: 0.00006 | Regression loss: 0.01901 | Running loss: 0.01403\n",
            "Epoch: 147 | Iteration: 33 | Classification loss: 0.00003 | Regression loss: 0.01215 | Running loss: 0.01395\n",
            "Epoch: 147 | Iteration: 34 | Classification loss: 0.00009 | Regression loss: 0.02327 | Running loss: 0.01396\n",
            "Epoch: 147 | Iteration: 35 | Classification loss: 0.00003 | Regression loss: 0.01006 | Running loss: 0.01397\n",
            "Epoch: 147 | Iteration: 36 | Classification loss: 0.00001 | Regression loss: 0.01046 | Running loss: 0.01398\n",
            "Epoch: 147 | Iteration: 37 | Classification loss: 0.00007 | Regression loss: 0.01162 | Running loss: 0.01397\n",
            "Epoch: 147 | Iteration: 38 | Classification loss: 0.00005 | Regression loss: 0.02573 | Running loss: 0.01399\n",
            "Epoch: 147 | Iteration: 39 | Classification loss: 0.00023 | Regression loss: 0.01752 | Running loss: 0.01401\n",
            "Epoch: 147 | Iteration: 40 | Classification loss: 0.00003 | Regression loss: 0.01352 | Running loss: 0.01400\n",
            "Epoch: 147 | Iteration: 41 | Classification loss: 0.00004 | Regression loss: 0.02775 | Running loss: 0.01402\n",
            "Epoch: 147 | Iteration: 42 | Classification loss: 0.00006 | Regression loss: 0.01719 | Running loss: 0.01404\n",
            "Epoch: 147 | Iteration: 43 | Classification loss: 0.00011 | Regression loss: 0.01678 | Running loss: 0.01404\n",
            "Epoch: 147 | Iteration: 44 | Classification loss: 0.00005 | Regression loss: 0.01917 | Running loss: 0.01405\n",
            "Epoch: 147 | Iteration: 45 | Classification loss: 0.00002 | Regression loss: 0.00408 | Running loss: 0.01403\n",
            "Epoch: 147 | Iteration: 46 | Classification loss: 0.00005 | Regression loss: 0.00452 | Running loss: 0.01401\n",
            "Epoch: 147 | Iteration: 47 | Classification loss: 0.00004 | Regression loss: 0.01795 | Running loss: 0.01404\n",
            "Epoch: 147 | Iteration: 48 | Classification loss: 0.00003 | Regression loss: 0.00718 | Running loss: 0.01403\n",
            "Epoch: 147 | Iteration: 49 | Classification loss: 0.00004 | Regression loss: 0.01361 | Running loss: 0.01404\n",
            "Epoch: 147 | Iteration: 50 | Classification loss: 0.00011 | Regression loss: 0.01488 | Running loss: 0.01403\n",
            "Epoch: 147 | Iteration: 51 | Classification loss: 0.00006 | Regression loss: 0.01421 | Running loss: 0.01403\n",
            "Epoch: 147 | Iteration: 52 | Classification loss: 0.00007 | Regression loss: 0.01744 | Running loss: 0.01403\n",
            "Epoch: 147 | Iteration: 53 | Classification loss: 0.00002 | Regression loss: 0.00492 | Running loss: 0.01403\n",
            "Epoch: 147 | Iteration: 54 | Classification loss: 0.00006 | Regression loss: 0.01633 | Running loss: 0.01403\n",
            "Epoch: 147 | Iteration: 55 | Classification loss: 0.00004 | Regression loss: 0.02145 | Running loss: 0.01404\n",
            "Epoch: 147 | Iteration: 56 | Classification loss: 0.00005 | Regression loss: 0.01517 | Running loss: 0.01404\n",
            "Epoch: 147 | Iteration: 57 | Classification loss: 0.00004 | Regression loss: 0.01270 | Running loss: 0.01402\n",
            "Epoch: 147 | Iteration: 58 | Classification loss: 0.00004 | Regression loss: 0.01790 | Running loss: 0.01404\n",
            "Epoch: 147 | Iteration: 59 | Classification loss: 0.00002 | Regression loss: 0.01143 | Running loss: 0.01402\n",
            "Epoch: 147 | Iteration: 60 | Classification loss: 0.00005 | Regression loss: 0.02179 | Running loss: 0.01403\n",
            "Epoch: 147 | Iteration: 61 | Classification loss: 0.00003 | Regression loss: 0.00740 | Running loss: 0.01403\n",
            "Epoch: 147 | Iteration: 62 | Classification loss: 0.00003 | Regression loss: 0.00814 | Running loss: 0.01402\n",
            "Epoch: 147 | Iteration: 63 | Classification loss: 0.00010 | Regression loss: 0.01885 | Running loss: 0.01404\n",
            "Epoch: 147 | Iteration: 64 | Classification loss: 0.00006 | Regression loss: 0.01293 | Running loss: 0.01405\n",
            "Epoch: 147 | Iteration: 65 | Classification loss: 0.00002 | Regression loss: 0.00638 | Running loss: 0.01403\n",
            "Epoch: 147 | Iteration: 66 | Classification loss: 0.00006 | Regression loss: 0.00839 | Running loss: 0.01404\n",
            "Epoch: 147 | Iteration: 67 | Classification loss: 0.00001 | Regression loss: 0.01154 | Running loss: 0.01405\n",
            "Epoch: 147 | Iteration: 68 | Classification loss: 0.00008 | Regression loss: 0.01301 | Running loss: 0.01404\n",
            "Epoch: 147 | Iteration: 69 | Classification loss: 0.00026 | Regression loss: 0.00989 | Running loss: 0.01404\n",
            "Epoch: 147 | Iteration: 70 | Classification loss: 0.00004 | Regression loss: 0.00616 | Running loss: 0.01402\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 188.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 147 | Iteration: 72 | Classification loss: 0.00004 | Regression loss: 0.01404 | Running loss: 0.01398\n",
            "Epoch: 147 | Iteration: 73 | Classification loss: 0.00022 | Regression loss: 0.05178 | Running loss: 0.01408\n",
            "Epoch: 147 | Iteration: 74 | Classification loss: 0.00003 | Regression loss: 0.00781 | Running loss: 0.01405\n",
            "Epoch: 147 | Iteration: 75 | Classification loss: 0.00006 | Regression loss: 0.01502 | Running loss: 0.01407\n",
            "Epoch: 147 | Iteration: 76 | Classification loss: 0.00004 | Regression loss: 0.00674 | Running loss: 0.01405\n",
            "Epoch: 147 | Iteration: 77 | Classification loss: 0.00007 | Regression loss: 0.01123 | Running loss: 0.01404\n",
            "Epoch: 147 | Iteration: 78 | Classification loss: 0.00002 | Regression loss: 0.01327 | Running loss: 0.01403\n",
            "Epoch: 147 | Iteration: 79 | Classification loss: 0.00021 | Regression loss: 0.02184 | Running loss: 0.01404\n",
            "Epoch: 147 | Iteration: 80 | Classification loss: 0.00002 | Regression loss: 0.00553 | Running loss: 0.01402\n",
            "Epoch: 147 | Iteration: 81 | Classification loss: 0.00003 | Regression loss: 0.01508 | Running loss: 0.01403\n",
            "Epoch: 147 | Iteration: 82 | Classification loss: 0.00003 | Regression loss: 0.03342 | Running loss: 0.01407\n",
            "Epoch: 147 | Iteration: 83 | Classification loss: 0.00005 | Regression loss: 0.01257 | Running loss: 0.01405\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.7433985536668353\n",
            "Precision:  0.5160493827160494\n",
            "Recall:  0.8260869565217391\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}]\n",
            "Epoch: 148 | Iteration: 0 | Classification loss: 0.00003 | Regression loss: 0.03478 | Running loss: 0.01411\n",
            "Epoch: 148 | Iteration: 1 | Classification loss: 0.00007 | Regression loss: 0.01159 | Running loss: 0.01412\n",
            "Epoch: 148 | Iteration: 2 | Classification loss: 0.00004 | Regression loss: 0.01257 | Running loss: 0.01412\n",
            "Epoch: 148 | Iteration: 3 | Classification loss: 0.00007 | Regression loss: 0.01692 | Running loss: 0.01409\n",
            "Epoch: 148 | Iteration: 4 | Classification loss: 0.00004 | Regression loss: 0.01319 | Running loss: 0.01408\n",
            "Epoch: 148 | Iteration: 5 | Classification loss: 0.00004 | Regression loss: 0.02778 | Running loss: 0.01409\n",
            "Epoch: 148 | Iteration: 6 | Classification loss: 0.00003 | Regression loss: 0.00509 | Running loss: 0.01406\n",
            "Epoch: 148 | Iteration: 7 | Classification loss: 0.00007 | Regression loss: 0.01956 | Running loss: 0.01407\n",
            "Epoch: 148 | Iteration: 8 | Classification loss: 0.00022 | Regression loss: 0.02143 | Running loss: 0.01409\n",
            "Epoch: 148 | Iteration: 9 | Classification loss: 0.00001 | Regression loss: 0.00430 | Running loss: 0.01409\n",
            "Epoch: 148 | Iteration: 10 | Classification loss: 0.00006 | Regression loss: 0.01571 | Running loss: 0.01410\n",
            "Epoch: 148 | Iteration: 11 | Classification loss: 0.00004 | Regression loss: 0.01946 | Running loss: 0.01413\n",
            "Epoch: 148 | Iteration: 12 | Classification loss: 0.00006 | Regression loss: 0.01599 | Running loss: 0.01413\n",
            "Epoch: 148 | Iteration: 13 | Classification loss: 0.00003 | Regression loss: 0.01610 | Running loss: 0.01412\n",
            "Epoch: 148 | Iteration: 14 | Classification loss: 0.00004 | Regression loss: 0.01451 | Running loss: 0.01413\n",
            "Epoch: 148 | Iteration: 15 | Classification loss: 0.00006 | Regression loss: 0.01147 | Running loss: 0.01414\n",
            "Epoch: 148 | Iteration: 16 | Classification loss: 0.00004 | Regression loss: 0.01238 | Running loss: 0.01413\n",
            "Epoch: 148 | Iteration: 17 | Classification loss: 0.00003 | Regression loss: 0.01018 | Running loss: 0.01412\n",
            "Epoch: 148 | Iteration: 18 | Classification loss: 0.00005 | Regression loss: 0.01788 | Running loss: 0.01413\n",
            "Epoch: 148 | Iteration: 19 | Classification loss: 0.00011 | Regression loss: 0.00525 | Running loss: 0.01413\n",
            "Epoch: 148 | Iteration: 20 | Classification loss: 0.00002 | Regression loss: 0.00653 | Running loss: 0.01411\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 188.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 148 | Iteration: 22 | Classification loss: 0.00004 | Regression loss: 0.00685 | Running loss: 0.01409\n",
            "Epoch: 148 | Iteration: 23 | Classification loss: 0.00012 | Regression loss: 0.01546 | Running loss: 0.01409\n",
            "Epoch: 148 | Iteration: 24 | Classification loss: 0.00003 | Regression loss: 0.01423 | Running loss: 0.01408\n",
            "Epoch: 148 | Iteration: 25 | Classification loss: 0.00005 | Regression loss: 0.00390 | Running loss: 0.01407\n",
            "Epoch: 148 | Iteration: 26 | Classification loss: 0.00004 | Regression loss: 0.01467 | Running loss: 0.01407\n",
            "Epoch: 148 | Iteration: 27 | Classification loss: 0.00004 | Regression loss: 0.00824 | Running loss: 0.01407\n",
            "Epoch: 148 | Iteration: 28 | Classification loss: 0.00004 | Regression loss: 0.01721 | Running loss: 0.01408\n",
            "Epoch: 148 | Iteration: 29 | Classification loss: 0.00003 | Regression loss: 0.01467 | Running loss: 0.01408\n",
            "Epoch: 148 | Iteration: 30 | Classification loss: 0.00004 | Regression loss: 0.00678 | Running loss: 0.01406\n",
            "Epoch: 148 | Iteration: 31 | Classification loss: 0.00012 | Regression loss: 0.01778 | Running loss: 0.01407\n",
            "Epoch: 148 | Iteration: 32 | Classification loss: 0.00005 | Regression loss: 0.00896 | Running loss: 0.01406\n",
            "Epoch: 148 | Iteration: 33 | Classification loss: 0.00003 | Regression loss: 0.00743 | Running loss: 0.01406\n",
            "Epoch: 148 | Iteration: 34 | Classification loss: 0.00008 | Regression loss: 0.02048 | Running loss: 0.01404\n",
            "Epoch: 148 | Iteration: 35 | Classification loss: 0.00002 | Regression loss: 0.01270 | Running loss: 0.01405\n",
            "Epoch: 148 | Iteration: 36 | Classification loss: 0.00005 | Regression loss: 0.01884 | Running loss: 0.01408\n",
            "Epoch: 148 | Iteration: 37 | Classification loss: 0.00002 | Regression loss: 0.01145 | Running loss: 0.01407\n",
            "Epoch: 148 | Iteration: 38 | Classification loss: 0.00007 | Regression loss: 0.01113 | Running loss: 0.01407\n",
            "Epoch: 148 | Iteration: 39 | Classification loss: 0.00005 | Regression loss: 0.00562 | Running loss: 0.01404\n",
            "Epoch: 148 | Iteration: 40 | Classification loss: 0.00002 | Regression loss: 0.00588 | Running loss: 0.01404\n",
            "Epoch: 148 | Iteration: 41 | Classification loss: 0.00009 | Regression loss: 0.01161 | Running loss: 0.01403\n",
            "Epoch: 148 | Iteration: 42 | Classification loss: 0.00005 | Regression loss: 0.01410 | Running loss: 0.01403\n",
            "Epoch: 148 | Iteration: 43 | Classification loss: 0.00002 | Regression loss: 0.00625 | Running loss: 0.01402\n",
            "Epoch: 148 | Iteration: 44 | Classification loss: 0.00002 | Regression loss: 0.00578 | Running loss: 0.01402\n",
            "Epoch: 148 | Iteration: 45 | Classification loss: 0.00018 | Regression loss: 0.01449 | Running loss: 0.01404\n",
            "Epoch: 148 | Iteration: 46 | Classification loss: 0.00007 | Regression loss: 0.02280 | Running loss: 0.01405\n",
            "Epoch: 148 | Iteration: 47 | Classification loss: 0.00002 | Regression loss: 0.00745 | Running loss: 0.01405\n",
            "Epoch: 148 | Iteration: 48 | Classification loss: 0.00013 | Regression loss: 0.01309 | Running loss: 0.01405\n",
            "Epoch: 148 | Iteration: 49 | Classification loss: 0.00002 | Regression loss: 0.03055 | Running loss: 0.01409\n",
            "Epoch: 148 | Iteration: 50 | Classification loss: 0.00005 | Regression loss: 0.00812 | Running loss: 0.01407\n",
            "Epoch: 148 | Iteration: 51 | Classification loss: 0.00005 | Regression loss: 0.01526 | Running loss: 0.01400\n",
            "Epoch: 148 | Iteration: 52 | Classification loss: 0.00005 | Regression loss: 0.01595 | Running loss: 0.01400\n",
            "Epoch: 148 | Iteration: 53 | Classification loss: 0.00018 | Regression loss: 0.05002 | Running loss: 0.01408\n",
            "Epoch: 148 | Iteration: 54 | Classification loss: 0.00007 | Regression loss: 0.01364 | Running loss: 0.01410\n",
            "Epoch: 148 | Iteration: 55 | Classification loss: 0.00002 | Regression loss: 0.00507 | Running loss: 0.01406\n",
            "Epoch: 148 | Iteration: 56 | Classification loss: 0.00007 | Regression loss: 0.01953 | Running loss: 0.01407\n",
            "Epoch: 148 | Iteration: 57 | Classification loss: 0.00007 | Regression loss: 0.01738 | Running loss: 0.01406\n",
            "Epoch: 148 | Iteration: 58 | Classification loss: 0.00005 | Regression loss: 0.01424 | Running loss: 0.01405\n",
            "Epoch: 148 | Iteration: 59 | Classification loss: 0.00001 | Regression loss: 0.00391 | Running loss: 0.01403\n",
            "Epoch: 148 | Iteration: 60 | Classification loss: 0.00002 | Regression loss: 0.00518 | Running loss: 0.01397\n",
            "Epoch: 148 | Iteration: 61 | Classification loss: 0.00002 | Regression loss: 0.00791 | Running loss: 0.01397\n",
            "Epoch: 148 | Iteration: 62 | Classification loss: 0.00003 | Regression loss: 0.01232 | Running loss: 0.01399\n",
            "Epoch: 148 | Iteration: 63 | Classification loss: 0.00002 | Regression loss: 0.01147 | Running loss: 0.01400\n",
            "Epoch: 148 | Iteration: 64 | Classification loss: 0.00027 | Regression loss: 0.01026 | Running loss: 0.01398\n",
            "Epoch: 148 | Iteration: 65 | Classification loss: 0.00001 | Regression loss: 0.00998 | Running loss: 0.01397\n",
            "Epoch: 148 | Iteration: 66 | Classification loss: 0.00005 | Regression loss: 0.01487 | Running loss: 0.01397\n",
            "Epoch: 148 | Iteration: 67 | Classification loss: 0.00009 | Regression loss: 0.01900 | Running loss: 0.01399\n",
            "Epoch: 148 | Iteration: 68 | Classification loss: 0.00002 | Regression loss: 0.00514 | Running loss: 0.01399\n",
            "Epoch: 148 | Iteration: 69 | Classification loss: 0.00004 | Regression loss: 0.01642 | Running loss: 0.01400\n",
            "Epoch: 148 | Iteration: 70 | Classification loss: 0.00013 | Regression loss: 0.01582 | Running loss: 0.01401\n",
            "Epoch: 148 | Iteration: 71 | Classification loss: 0.00002 | Regression loss: 0.00493 | Running loss: 0.01400\n",
            "Epoch: 148 | Iteration: 72 | Classification loss: 0.00001 | Regression loss: 0.00832 | Running loss: 0.01399\n",
            "Epoch: 148 | Iteration: 73 | Classification loss: 0.00003 | Regression loss: 0.02039 | Running loss: 0.01401\n",
            "Epoch: 148 | Iteration: 74 | Classification loss: 0.00003 | Regression loss: 0.01589 | Running loss: 0.01400\n",
            "Epoch: 148 | Iteration: 75 | Classification loss: 0.00006 | Regression loss: 0.01069 | Running loss: 0.01400\n",
            "Epoch: 148 | Iteration: 76 | Classification loss: 0.00003 | Regression loss: 0.01300 | Running loss: 0.01399\n",
            "Epoch: 148 | Iteration: 77 | Classification loss: 0.00004 | Regression loss: 0.00850 | Running loss: 0.01398\n",
            "Epoch: 148 | Iteration: 78 | Classification loss: 0.00006 | Regression loss: 0.02928 | Running loss: 0.01400\n",
            "Epoch: 148 | Iteration: 79 | Classification loss: 0.00002 | Regression loss: 0.01424 | Running loss: 0.01400\n",
            "Epoch: 148 | Iteration: 80 | Classification loss: 0.00005 | Regression loss: 0.01910 | Running loss: 0.01402\n",
            "Epoch: 148 | Iteration: 81 | Classification loss: 0.00003 | Regression loss: 0.00798 | Running loss: 0.01399\n",
            "Epoch: 148 | Iteration: 82 | Classification loss: 0.00005 | Regression loss: 0.00793 | Running loss: 0.01395\n",
            "Epoch: 148 | Iteration: 83 | Classification loss: 0.00008 | Regression loss: 0.01560 | Running loss: 0.01394\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.7385026345668402\n",
            "Precision:  0.52\n",
            "Recall:  0.8221343873517787\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}]\n",
            "Epoch: 149 | Iteration: 0 | Classification loss: 0.00001 | Regression loss: 0.00660 | Running loss: 0.01392\n",
            "Epoch: 149 | Iteration: 1 | Classification loss: 0.00004 | Regression loss: 0.01620 | Running loss: 0.01392\n",
            "Epoch: 149 | Iteration: 2 | Classification loss: 0.00006 | Regression loss: 0.01657 | Running loss: 0.01392\n",
            "Epoch: 149 | Iteration: 3 | Classification loss: 0.00003 | Regression loss: 0.00766 | Running loss: 0.01392\n",
            "Epoch: 149 | Iteration: 4 | Classification loss: 0.00007 | Regression loss: 0.01681 | Running loss: 0.01391\n",
            "Epoch: 149 | Iteration: 5 | Classification loss: 0.00007 | Regression loss: 0.01319 | Running loss: 0.01391\n",
            "Epoch: 149 | Iteration: 6 | Classification loss: 0.00009 | Regression loss: 0.01875 | Running loss: 0.01392\n",
            "Epoch: 149 | Iteration: 7 | Classification loss: 0.00022 | Regression loss: 0.01749 | Running loss: 0.01394\n",
            "Epoch: 149 | Iteration: 8 | Classification loss: 0.00002 | Regression loss: 0.01155 | Running loss: 0.01392\n",
            "Epoch: 149 | Iteration: 9 | Classification loss: 0.00006 | Regression loss: 0.03113 | Running loss: 0.01398\n",
            "Epoch: 149 | Iteration: 10 | Classification loss: 0.00003 | Regression loss: 0.00829 | Running loss: 0.01396\n",
            "Epoch: 149 | Iteration: 11 | Classification loss: 0.00002 | Regression loss: 0.00748 | Running loss: 0.01393\n",
            "Epoch: 149 | Iteration: 12 | Classification loss: 0.00002 | Regression loss: 0.00465 | Running loss: 0.01391\n",
            "Epoch: 149 | Iteration: 13 | Classification loss: 0.00003 | Regression loss: 0.00529 | Running loss: 0.01389\n",
            "Epoch: 149 | Iteration: 14 | Classification loss: 0.00004 | Regression loss: 0.00836 | Running loss: 0.01387\n",
            "Epoch: 149 | Iteration: 15 | Classification loss: 0.00002 | Regression loss: 0.02082 | Running loss: 0.01390\n",
            "Epoch: 149 | Iteration: 16 | Classification loss: 0.00003 | Regression loss: 0.00671 | Running loss: 0.01390\n",
            "Epoch: 149 | Iteration: 17 | Classification loss: 0.00003 | Regression loss: 0.01407 | Running loss: 0.01390\n",
            "Epoch: 149 | Iteration: 18 | Classification loss: 0.00003 | Regression loss: 0.03315 | Running loss: 0.01396\n",
            "Epoch: 149 | Iteration: 19 | Classification loss: 0.00012 | Regression loss: 0.01209 | Running loss: 0.01396\n",
            "Epoch: 149 | Iteration: 20 | Classification loss: 0.00002 | Regression loss: 0.00629 | Running loss: 0.01395\n",
            "Epoch: 149 | Iteration: 21 | Classification loss: 0.00007 | Regression loss: 0.01740 | Running loss: 0.01395\n",
            "Epoch: 149 | Iteration: 22 | Classification loss: 0.00006 | Regression loss: 0.01331 | Running loss: 0.01395\n",
            "Epoch: 149 | Iteration: 23 | Classification loss: 0.00001 | Regression loss: 0.00398 | Running loss: 0.01395\n",
            "Epoch: 149 | Iteration: 24 | Classification loss: 0.00004 | Regression loss: 0.00807 | Running loss: 0.01390\n",
            "Epoch: 149 | Iteration: 25 | Classification loss: 0.00003 | Regression loss: 0.01025 | Running loss: 0.01389\n",
            "Epoch: 149 | Iteration: 26 | Classification loss: 0.00003 | Regression loss: 0.01642 | Running loss: 0.01390\n",
            "Epoch: 149 | Iteration: 27 | Classification loss: 0.00002 | Regression loss: 0.03080 | Running loss: 0.01392\n",
            "Epoch: 149 | Iteration: 28 | Classification loss: 0.00003 | Regression loss: 0.01262 | Running loss: 0.01392\n",
            "Epoch: 149 | Iteration: 29 | Classification loss: 0.00009 | Regression loss: 0.01245 | Running loss: 0.01391\n",
            "Epoch: 149 | Iteration: 30 | Classification loss: 0.00004 | Regression loss: 0.01242 | Running loss: 0.01392\n",
            "Epoch: 149 | Iteration: 31 | Classification loss: 0.00006 | Regression loss: 0.01853 | Running loss: 0.01394\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 190.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 149 | Iteration: 33 | Classification loss: 0.00002 | Regression loss: 0.00445 | Running loss: 0.01394\n",
            "Epoch: 149 | Iteration: 34 | Classification loss: 0.00004 | Regression loss: 0.01255 | Running loss: 0.01393\n",
            "Epoch: 149 | Iteration: 35 | Classification loss: 0.00003 | Regression loss: 0.00596 | Running loss: 0.01390\n",
            "Epoch: 149 | Iteration: 36 | Classification loss: 0.00001 | Regression loss: 0.01020 | Running loss: 0.01389\n",
            "Epoch: 149 | Iteration: 37 | Classification loss: 0.00024 | Regression loss: 0.00865 | Running loss: 0.01387\n",
            "Epoch: 149 | Iteration: 38 | Classification loss: 0.00007 | Regression loss: 0.01629 | Running loss: 0.01388\n",
            "Epoch: 149 | Iteration: 39 | Classification loss: 0.00003 | Regression loss: 0.02024 | Running loss: 0.01389\n",
            "Epoch: 149 | Iteration: 40 | Classification loss: 0.00004 | Regression loss: 0.01482 | Running loss: 0.01388\n",
            "Epoch: 149 | Iteration: 41 | Classification loss: 0.00007 | Regression loss: 0.01912 | Running loss: 0.01391\n",
            "Epoch: 149 | Iteration: 42 | Classification loss: 0.00007 | Regression loss: 0.01574 | Running loss: 0.01392\n",
            "Epoch: 149 | Iteration: 43 | Classification loss: 0.00005 | Regression loss: 0.01479 | Running loss: 0.01394\n",
            "Epoch: 149 | Iteration: 44 | Classification loss: 0.00006 | Regression loss: 0.00855 | Running loss: 0.01394\n",
            "Epoch: 149 | Iteration: 45 | Classification loss: 0.00002 | Regression loss: 0.00552 | Running loss: 0.01393\n",
            "Epoch: 149 | Iteration: 46 | Classification loss: 0.00002 | Regression loss: 0.01499 | Running loss: 0.01393\n",
            "Epoch: 149 | Iteration: 47 | Classification loss: 0.00001 | Regression loss: 0.00410 | Running loss: 0.01393\n",
            "Epoch: 149 | Iteration: 48 | Classification loss: 0.00003 | Regression loss: 0.01253 | Running loss: 0.01394\n",
            "Epoch: 149 | Iteration: 49 | Classification loss: 0.00003 | Regression loss: 0.01314 | Running loss: 0.01394\n",
            "Epoch: 149 | Iteration: 50 | Classification loss: 0.00013 | Regression loss: 0.01525 | Running loss: 0.01396\n",
            "Epoch: 149 | Iteration: 51 | Classification loss: 0.00007 | Regression loss: 0.01449 | Running loss: 0.01394\n",
            "Epoch: 149 | Iteration: 52 | Classification loss: 0.00002 | Regression loss: 0.01243 | Running loss: 0.01393\n",
            "Epoch: 149 | Iteration: 53 | Classification loss: 0.00004 | Regression loss: 0.00783 | Running loss: 0.01391\n",
            "Epoch: 149 | Iteration: 54 | Classification loss: 0.00004 | Regression loss: 0.01583 | Running loss: 0.01391\n",
            "Epoch: 149 | Iteration: 55 | Classification loss: 0.00004 | Regression loss: 0.01418 | Running loss: 0.01390\n",
            "Epoch: 149 | Iteration: 56 | Classification loss: 0.00004 | Regression loss: 0.01749 | Running loss: 0.01391\n",
            "Epoch: 149 | Iteration: 57 | Classification loss: 0.00002 | Regression loss: 0.00514 | Running loss: 0.01385\n",
            "Epoch: 149 | Iteration: 58 | Classification loss: 0.00003 | Regression loss: 0.01338 | Running loss: 0.01378\n",
            "Epoch: 149 | Iteration: 59 | Classification loss: 0.00002 | Regression loss: 0.01235 | Running loss: 0.01377\n",
            "Epoch: 149 | Iteration: 60 | Classification loss: 0.00002 | Regression loss: 0.01044 | Running loss: 0.01376\n",
            "Epoch: 149 | Iteration: 61 | Classification loss: 0.00001 | Regression loss: 0.00786 | Running loss: 0.01377\n",
            "Epoch: 149 | Iteration: 62 | Classification loss: 0.00003 | Regression loss: 0.01883 | Running loss: 0.01379\n",
            "Epoch: 149 | Iteration: 63 | Classification loss: 0.00004 | Regression loss: 0.00639 | Running loss: 0.01375\n",
            "Epoch: 149 | Iteration: 64 | Classification loss: 0.00004 | Regression loss: 0.00507 | Running loss: 0.01373\n",
            "Epoch: 149 | Iteration: 65 | Classification loss: 0.00004 | Regression loss: 0.01200 | Running loss: 0.01372\n",
            "Epoch: 149 | Iteration: 66 | Classification loss: 0.00006 | Regression loss: 0.00864 | Running loss: 0.01367\n",
            "Epoch: 149 | Iteration: 67 | Classification loss: 0.00010 | Regression loss: 0.01597 | Running loss: 0.01368\n",
            "Epoch: 149 | Iteration: 68 | Classification loss: 0.00007 | Regression loss: 0.01174 | Running loss: 0.01369\n",
            "Epoch: 149 | Iteration: 69 | Classification loss: 0.00012 | Regression loss: 0.00515 | Running loss: 0.01367\n",
            "Epoch: 149 | Iteration: 70 | Classification loss: 0.00006 | Regression loss: 0.01661 | Running loss: 0.01369\n",
            "Epoch: 149 | Iteration: 71 | Classification loss: 0.00003 | Regression loss: 0.00869 | Running loss: 0.01368\n",
            "Epoch: 149 | Iteration: 72 | Classification loss: 0.00010 | Regression loss: 0.01482 | Running loss: 0.01369\n",
            "Epoch: 149 | Iteration: 73 | Classification loss: 0.00005 | Regression loss: 0.01295 | Running loss: 0.01371\n",
            "Epoch: 149 | Iteration: 74 | Classification loss: 0.00003 | Regression loss: 0.02108 | Running loss: 0.01372\n",
            "Epoch: 149 | Iteration: 75 | Classification loss: 0.00007 | Regression loss: 0.02304 | Running loss: 0.01375\n",
            "Epoch: 149 | Iteration: 76 | Classification loss: 0.00006 | Regression loss: 0.01143 | Running loss: 0.01375\n",
            "Epoch: 149 | Iteration: 77 | Classification loss: 0.00005 | Regression loss: 0.00449 | Running loss: 0.01372\n",
            "Epoch: 149 | Iteration: 78 | Classification loss: 0.00004 | Regression loss: 0.02823 | Running loss: 0.01374\n",
            "Epoch: 149 | Iteration: 79 | Classification loss: 0.00020 | Regression loss: 0.02225 | Running loss: 0.01376\n",
            "Epoch: 149 | Iteration: 80 | Classification loss: 0.00006 | Regression loss: 0.01617 | Running loss: 0.01376\n",
            "Epoch: 149 | Iteration: 81 | Classification loss: 0.00003 | Regression loss: 0.01596 | Running loss: 0.01377\n",
            "Epoch: 149 | Iteration: 82 | Classification loss: 0.00017 | Regression loss: 0.05007 | Running loss: 0.01385\n",
            "Epoch: 149 | Iteration: 83 | Classification loss: 0.00002 | Regression loss: 0.00569 | Running loss: 0.01383\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.7452146140123328\n",
            "Precision:  0.5186104218362283\n",
            "Recall:  0.8260869565217391\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}]\n",
            "Epoch: 150 | Iteration: 0 | Classification loss: 0.00005 | Regression loss: 0.01419 | Running loss: 0.01383\n",
            "Epoch: 150 | Iteration: 1 | Classification loss: 0.00005 | Regression loss: 0.00825 | Running loss: 0.01382\n",
            "Epoch: 150 | Iteration: 2 | Classification loss: 0.00004 | Regression loss: 0.00795 | Running loss: 0.01381\n",
            "Epoch: 150 | Iteration: 3 | Classification loss: 0.00009 | Regression loss: 0.01206 | Running loss: 0.01378\n",
            "Epoch: 150 | Iteration: 4 | Classification loss: 0.00002 | Regression loss: 0.00708 | Running loss: 0.01375\n",
            "Epoch: 150 | Iteration: 5 | Classification loss: 0.00002 | Regression loss: 0.00399 | Running loss: 0.01374\n",
            "Epoch: 150 | Iteration: 6 | Classification loss: 0.00010 | Regression loss: 0.01425 | Running loss: 0.01373\n",
            "Epoch: 150 | Iteration: 7 | Classification loss: 0.00003 | Regression loss: 0.01602 | Running loss: 0.01373\n",
            "Epoch: 150 | Iteration: 8 | Classification loss: 0.00004 | Regression loss: 0.00809 | Running loss: 0.01372\n",
            "Epoch: 150 | Iteration: 9 | Classification loss: 0.00002 | Regression loss: 0.01281 | Running loss: 0.01371\n",
            "Epoch: 150 | Iteration: 10 | Classification loss: 0.00006 | Regression loss: 0.01190 | Running loss: 0.01370\n",
            "Epoch: 150 | Iteration: 11 | Classification loss: 0.00013 | Regression loss: 0.01489 | Running loss: 0.01371\n",
            "Epoch: 150 | Iteration: 12 | Classification loss: 0.00013 | Regression loss: 0.01328 | Running loss: 0.01370\n",
            "Epoch: 150 | Iteration: 13 | Classification loss: 0.00004 | Regression loss: 0.01182 | Running loss: 0.01370\n",
            "Epoch: 150 | Iteration: 14 | Classification loss: 0.00002 | Regression loss: 0.00514 | Running loss: 0.01367\n",
            "Epoch: 150 | Iteration: 15 | Classification loss: 0.00003 | Regression loss: 0.00742 | Running loss: 0.01365\n",
            "Epoch: 150 | Iteration: 16 | Classification loss: 0.00003 | Regression loss: 0.02125 | Running loss: 0.01367\n",
            "Epoch: 150 | Iteration: 17 | Classification loss: 0.00011 | Regression loss: 0.00502 | Running loss: 0.01365\n",
            "Epoch: 150 | Iteration: 18 | Classification loss: 0.00003 | Regression loss: 0.01531 | Running loss: 0.01366\n",
            "Epoch: 150 | Iteration: 19 | Classification loss: 0.00003 | Regression loss: 0.01391 | Running loss: 0.01368\n",
            "Epoch: 150 | Iteration: 20 | Classification loss: 0.00005 | Regression loss: 0.01823 | Running loss: 0.01368\n",
            "Epoch: 150 | Iteration: 21 | Classification loss: 0.00019 | Regression loss: 0.01656 | Running loss: 0.01370\n",
            "Epoch: 150 | Iteration: 22 | Classification loss: 0.00002 | Regression loss: 0.00610 | Running loss: 0.01365\n",
            "Epoch: 150 | Iteration: 23 | Classification loss: 0.00004 | Regression loss: 0.01596 | Running loss: 0.01368\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 188.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 150 | Iteration: 25 | Classification loss: 0.00004 | Regression loss: 0.01184 | Running loss: 0.01369\n",
            "Epoch: 150 | Iteration: 26 | Classification loss: 0.00005 | Regression loss: 0.01218 | Running loss: 0.01371\n",
            "Epoch: 150 | Iteration: 27 | Classification loss: 0.00005 | Regression loss: 0.01159 | Running loss: 0.01370\n",
            "Epoch: 150 | Iteration: 28 | Classification loss: 0.00004 | Regression loss: 0.00654 | Running loss: 0.01370\n",
            "Epoch: 150 | Iteration: 29 | Classification loss: 0.00006 | Regression loss: 0.02008 | Running loss: 0.01367\n",
            "Epoch: 150 | Iteration: 30 | Classification loss: 0.00003 | Regression loss: 0.03334 | Running loss: 0.01373\n",
            "Epoch: 150 | Iteration: 31 | Classification loss: 0.00007 | Regression loss: 0.01377 | Running loss: 0.01371\n",
            "Epoch: 150 | Iteration: 32 | Classification loss: 0.00002 | Regression loss: 0.00797 | Running loss: 0.01372\n",
            "Epoch: 150 | Iteration: 33 | Classification loss: 0.00006 | Regression loss: 0.00609 | Running loss: 0.01368\n",
            "Epoch: 150 | Iteration: 34 | Classification loss: 0.00003 | Regression loss: 0.01141 | Running loss: 0.01369\n",
            "Epoch: 150 | Iteration: 35 | Classification loss: 0.00002 | Regression loss: 0.00455 | Running loss: 0.01367\n",
            "Epoch: 150 | Iteration: 36 | Classification loss: 0.00001 | Regression loss: 0.00768 | Running loss: 0.01366\n",
            "Epoch: 150 | Iteration: 37 | Classification loss: 0.00018 | Regression loss: 0.02125 | Running loss: 0.01364\n",
            "Epoch: 150 | Iteration: 38 | Classification loss: 0.00006 | Regression loss: 0.01552 | Running loss: 0.01365\n",
            "Epoch: 150 | Iteration: 39 | Classification loss: 0.00003 | Regression loss: 0.00586 | Running loss: 0.01364\n",
            "Epoch: 150 | Iteration: 40 | Classification loss: 0.00003 | Regression loss: 0.01179 | Running loss: 0.01363\n",
            "Epoch: 150 | Iteration: 41 | Classification loss: 0.00003 | Regression loss: 0.00518 | Running loss: 0.01362\n",
            "Epoch: 150 | Iteration: 42 | Classification loss: 0.00005 | Regression loss: 0.01514 | Running loss: 0.01364\n",
            "Epoch: 150 | Iteration: 43 | Classification loss: 0.00006 | Regression loss: 0.01374 | Running loss: 0.01365\n",
            "Epoch: 150 | Iteration: 44 | Classification loss: 0.00001 | Regression loss: 0.01039 | Running loss: 0.01366\n",
            "Epoch: 150 | Iteration: 45 | Classification loss: 0.00006 | Regression loss: 0.01199 | Running loss: 0.01366\n",
            "Epoch: 150 | Iteration: 46 | Classification loss: 0.00007 | Regression loss: 0.01907 | Running loss: 0.01368\n",
            "Epoch: 150 | Iteration: 47 | Classification loss: 0.00006 | Regression loss: 0.00390 | Running loss: 0.01365\n",
            "Epoch: 150 | Iteration: 48 | Classification loss: 0.00006 | Regression loss: 0.01537 | Running loss: 0.01365\n",
            "Epoch: 150 | Iteration: 49 | Classification loss: 0.00003 | Regression loss: 0.01237 | Running loss: 0.01366\n",
            "Epoch: 150 | Iteration: 50 | Classification loss: 0.00001 | Regression loss: 0.00405 | Running loss: 0.01363\n",
            "Epoch: 150 | Iteration: 51 | Classification loss: 0.00002 | Regression loss: 0.01909 | Running loss: 0.01364\n",
            "Epoch: 150 | Iteration: 52 | Classification loss: 0.00003 | Regression loss: 0.00739 | Running loss: 0.01363\n",
            "Epoch: 150 | Iteration: 53 | Classification loss: 0.00002 | Regression loss: 0.01345 | Running loss: 0.01363\n",
            "Epoch: 150 | Iteration: 54 | Classification loss: 0.00005 | Regression loss: 0.02770 | Running loss: 0.01366\n",
            "Epoch: 150 | Iteration: 55 | Classification loss: 0.00004 | Regression loss: 0.01294 | Running loss: 0.01365\n",
            "Epoch: 150 | Iteration: 56 | Classification loss: 0.00002 | Regression loss: 0.00570 | Running loss: 0.01356\n",
            "Epoch: 150 | Iteration: 57 | Classification loss: 0.00004 | Regression loss: 0.02070 | Running loss: 0.01359\n",
            "Epoch: 150 | Iteration: 58 | Classification loss: 0.00001 | Regression loss: 0.00583 | Running loss: 0.01357\n",
            "Epoch: 150 | Iteration: 59 | Classification loss: 0.00003 | Regression loss: 0.01072 | Running loss: 0.01356\n",
            "Epoch: 150 | Iteration: 60 | Classification loss: 0.00003 | Regression loss: 0.01485 | Running loss: 0.01355\n",
            "Epoch: 150 | Iteration: 61 | Classification loss: 0.00008 | Regression loss: 0.02039 | Running loss: 0.01357\n",
            "Epoch: 150 | Iteration: 62 | Classification loss: 0.00003 | Regression loss: 0.00666 | Running loss: 0.01355\n",
            "Epoch: 150 | Iteration: 63 | Classification loss: 0.00004 | Regression loss: 0.01708 | Running loss: 0.01355\n",
            "Epoch: 150 | Iteration: 64 | Classification loss: 0.00007 | Regression loss: 0.01452 | Running loss: 0.01357\n",
            "Epoch: 150 | Iteration: 65 | Classification loss: 0.00024 | Regression loss: 0.00950 | Running loss: 0.01356\n",
            "Epoch: 150 | Iteration: 66 | Classification loss: 0.00018 | Regression loss: 0.04992 | Running loss: 0.01363\n",
            "Epoch: 150 | Iteration: 67 | Classification loss: 0.00003 | Regression loss: 0.01720 | Running loss: 0.01364\n",
            "Epoch: 150 | Iteration: 68 | Classification loss: 0.00005 | Regression loss: 0.01189 | Running loss: 0.01363\n",
            "Epoch: 150 | Iteration: 69 | Classification loss: 0.00003 | Regression loss: 0.00752 | Running loss: 0.01362\n",
            "Epoch: 150 | Iteration: 70 | Classification loss: 0.00012 | Regression loss: 0.02477 | Running loss: 0.01364\n",
            "Epoch: 150 | Iteration: 71 | Classification loss: 0.00002 | Regression loss: 0.01104 | Running loss: 0.01362\n",
            "Epoch: 150 | Iteration: 72 | Classification loss: 0.00004 | Regression loss: 0.02751 | Running loss: 0.01366\n",
            "Epoch: 150 | Iteration: 73 | Classification loss: 0.00006 | Regression loss: 0.01084 | Running loss: 0.01366\n",
            "Epoch: 150 | Iteration: 74 | Classification loss: 0.00005 | Regression loss: 0.01594 | Running loss: 0.01368\n",
            "Epoch: 150 | Iteration: 75 | Classification loss: 0.00002 | Regression loss: 0.00824 | Running loss: 0.01368\n",
            "Epoch: 150 | Iteration: 76 | Classification loss: 0.00002 | Regression loss: 0.01383 | Running loss: 0.01369\n",
            "Epoch: 150 | Iteration: 77 | Classification loss: 0.00003 | Regression loss: 0.00503 | Running loss: 0.01366\n",
            "Epoch: 150 | Iteration: 78 | Classification loss: 0.00002 | Regression loss: 0.03221 | Running loss: 0.01368\n",
            "Epoch: 150 | Iteration: 79 | Classification loss: 0.00005 | Regression loss: 0.01365 | Running loss: 0.01368\n",
            "Epoch: 150 | Iteration: 80 | Classification loss: 0.00003 | Regression loss: 0.01179 | Running loss: 0.01368\n",
            "Epoch: 150 | Iteration: 81 | Classification loss: 0.00002 | Regression loss: 0.00677 | Running loss: 0.01369\n",
            "Epoch: 150 | Iteration: 82 | Classification loss: 0.00009 | Regression loss: 0.01839 | Running loss: 0.01369\n",
            "Epoch: 150 | Iteration: 83 | Classification loss: 0.00009 | Regression loss: 0.01669 | Running loss: 0.01370\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.735336116655414\n",
            "Precision:  0.5135802469135803\n",
            "Recall:  0.8221343873517787\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}]\n",
            "Epoch: 151 | Iteration: 0 | Classification loss: 0.00008 | Regression loss: 0.02328 | Running loss: 0.01373\n",
            "Epoch: 151 | Iteration: 1 | Classification loss: 0.00006 | Regression loss: 0.00745 | Running loss: 0.01371\n",
            "Epoch: 151 | Iteration: 2 | Classification loss: 0.00003 | Regression loss: 0.00984 | Running loss: 0.01372\n",
            "Epoch: 151 | Iteration: 3 | Classification loss: 0.00005 | Regression loss: 0.02028 | Running loss: 0.01375\n",
            "Epoch: 151 | Iteration: 4 | Classification loss: 0.00007 | Regression loss: 0.01520 | Running loss: 0.01374\n",
            "Epoch: 151 | Iteration: 5 | Classification loss: 0.00003 | Regression loss: 0.00763 | Running loss: 0.01374\n",
            "Epoch: 151 | Iteration: 6 | Classification loss: 0.00003 | Regression loss: 0.02708 | Running loss: 0.01379\n",
            "Epoch: 151 | Iteration: 7 | Classification loss: 0.00004 | Regression loss: 0.00804 | Running loss: 0.01378\n",
            "Epoch: 151 | Iteration: 8 | Classification loss: 0.00018 | Regression loss: 0.02109 | Running loss: 0.01379\n",
            "Epoch: 151 | Iteration: 9 | Classification loss: 0.00001 | Regression loss: 0.00507 | Running loss: 0.01374\n",
            "Epoch: 151 | Iteration: 10 | Classification loss: 0.00002 | Regression loss: 0.01460 | Running loss: 0.01373\n",
            "Epoch: 151 | Iteration: 11 | Classification loss: 0.00006 | Regression loss: 0.01634 | Running loss: 0.01373\n",
            "Epoch: 151 | Iteration: 12 | Classification loss: 0.00002 | Regression loss: 0.01287 | Running loss: 0.01373\n",
            "Epoch: 151 | Iteration: 13 | Classification loss: 0.00003 | Regression loss: 0.01681 | Running loss: 0.01373\n",
            "Epoch: 151 | Iteration: 14 | Classification loss: 0.00006 | Regression loss: 0.00893 | Running loss: 0.01372\n",
            "Epoch: 151 | Iteration: 15 | Classification loss: 0.00001 | Regression loss: 0.00999 | Running loss: 0.01372\n",
            "Epoch: 151 | Iteration: 16 | Classification loss: 0.00001 | Regression loss: 0.00416 | Running loss: 0.01371\n",
            "Epoch: 151 | Iteration: 17 | Classification loss: 0.00004 | Regression loss: 0.01444 | Running loss: 0.01370\n",
            "Epoch: 151 | Iteration: 18 | Classification loss: 0.00002 | Regression loss: 0.01299 | Running loss: 0.01371\n",
            "Epoch: 151 | Iteration: 19 | Classification loss: 0.00002 | Regression loss: 0.01620 | Running loss: 0.01372\n",
            "Epoch: 151 | Iteration: 20 | Classification loss: 0.00011 | Regression loss: 0.00527 | Running loss: 0.01368\n",
            "Epoch: 151 | Iteration: 21 | Classification loss: 0.00005 | Regression loss: 0.01436 | Running loss: 0.01368\n",
            "Epoch: 151 | Iteration: 22 | Classification loss: 0.00002 | Regression loss: 0.01100 | Running loss: 0.01367\n",
            "Epoch: 151 | Iteration: 23 | Classification loss: 0.00007 | Regression loss: 0.01205 | Running loss: 0.01367\n",
            "Epoch: 151 | Iteration: 24 | Classification loss: 0.00003 | Regression loss: 0.01127 | Running loss: 0.01367\n",
            "Epoch: 151 | Iteration: 25 | Classification loss: 0.00005 | Regression loss: 0.00833 | Running loss: 0.01366\n",
            "Epoch: 151 | Iteration: 26 | Classification loss: 0.00006 | Regression loss: 0.01813 | Running loss: 0.01366\n",
            "Epoch: 151 | Iteration: 27 | Classification loss: 0.00006 | Regression loss: 0.01880 | Running loss: 0.01365\n",
            "Epoch: 151 | Iteration: 28 | Classification loss: 0.00004 | Regression loss: 0.01606 | Running loss: 0.01366\n",
            "Epoch: 151 | Iteration: 29 | Classification loss: 0.00003 | Regression loss: 0.01866 | Running loss: 0.01368\n",
            "Epoch: 151 | Iteration: 30 | Classification loss: 0.00002 | Regression loss: 0.00790 | Running loss: 0.01369\n",
            "Epoch: 151 | Iteration: 31 | Classification loss: 0.00002 | Regression loss: 0.00402 | Running loss: 0.01367\n",
            "Epoch: 151 | Iteration: 32 | Classification loss: 0.00008 | Regression loss: 0.01813 | Running loss: 0.01369\n",
            "Epoch: 151 | Iteration: 33 | Classification loss: 0.00002 | Regression loss: 0.00466 | Running loss: 0.01366\n",
            "Epoch: 151 | Iteration: 34 | Classification loss: 0.00003 | Regression loss: 0.00733 | Running loss: 0.01365\n",
            "Epoch: 151 | Iteration: 35 | Classification loss: 0.00024 | Regression loss: 0.00985 | Running loss: 0.01366\n",
            "Epoch: 151 | Iteration: 36 | Classification loss: 0.00005 | Regression loss: 0.01639 | Running loss: 0.01368\n",
            "Epoch: 151 | Iteration: 37 | Classification loss: 0.00002 | Regression loss: 0.01289 | Running loss: 0.01369\n",
            "Epoch: 151 | Iteration: 38 | Classification loss: 0.00004 | Regression loss: 0.01196 | Running loss: 0.01370\n",
            "Epoch: 151 | Iteration: 39 | Classification loss: 0.00005 | Regression loss: 0.01699 | Running loss: 0.01371\n",
            "Epoch: 151 | Iteration: 40 | Classification loss: 0.00006 | Regression loss: 0.01136 | Running loss: 0.01369\n",
            "Epoch: 151 | Iteration: 41 | Classification loss: 0.00002 | Regression loss: 0.03129 | Running loss: 0.01372\n",
            "Epoch: 151 | Iteration: 42 | Classification loss: 0.00003 | Regression loss: 0.00491 | Running loss: 0.01372\n",
            "Epoch: 151 | Iteration: 43 | Classification loss: 0.00003 | Regression loss: 0.01275 | Running loss: 0.01373\n",
            "Epoch: 151 | Iteration: 44 | Classification loss: 0.00012 | Regression loss: 0.01245 | Running loss: 0.01372\n",
            "Epoch: 151 | Iteration: 45 | Classification loss: 0.00005 | Regression loss: 0.00364 | Running loss: 0.01370\n",
            "Epoch: 151 | Iteration: 46 | Classification loss: 0.00002 | Regression loss: 0.00501 | Running loss: 0.01361\n",
            "Epoch: 151 | Iteration: 47 | Classification loss: 0.00004 | Regression loss: 0.01565 | Running loss: 0.01357\n",
            "Epoch: 151 | Iteration: 48 | Classification loss: 0.00003 | Regression loss: 0.01377 | Running loss: 0.01357\n",
            "Epoch: 151 | Iteration: 49 | Classification loss: 0.00004 | Regression loss: 0.00809 | Running loss: 0.01354\n",
            "Epoch: 151 | Iteration: 50 | Classification loss: 0.00004 | Regression loss: 0.00656 | Running loss: 0.01352\n",
            "Epoch: 151 | Iteration: 51 | Classification loss: 0.00005 | Regression loss: 0.01948 | Running loss: 0.01354\n",
            "Epoch: 151 | Iteration: 52 | Classification loss: 0.00014 | Regression loss: 0.01551 | Running loss: 0.01354\n",
            "Epoch: 151 | Iteration: 53 | Classification loss: 0.00007 | Regression loss: 0.01615 | Running loss: 0.01353\n",
            "Epoch: 151 | Iteration: 54 | Classification loss: 0.00018 | Regression loss: 0.01581 | Running loss: 0.01355\n",
            "Epoch: 151 | Iteration: 55 | Classification loss: 0.00006 | Regression loss: 0.01290 | Running loss: 0.01354\n",
            "Epoch: 151 | Iteration: 56 | Classification loss: 0.00016 | Regression loss: 0.01648 | Running loss: 0.01355\n",
            "Epoch: 151 | Iteration: 57 | Classification loss: 0.00009 | Regression loss: 0.01537 | Running loss: 0.01357\n",
            "Epoch: 151 | Iteration: 58 | Classification loss: 0.00016 | Regression loss: 0.04890 | Running loss: 0.01363\n",
            "Epoch: 151 | Iteration: 59 | Classification loss: 0.00010 | Regression loss: 0.01861 | Running loss: 0.01364\n",
            "Epoch: 151 | Iteration: 60 | Classification loss: 0.00005 | Regression loss: 0.00579 | Running loss: 0.01361\n",
            "Epoch: 151 | Iteration: 61 | Classification loss: 0.00009 | Regression loss: 0.01266 | Running loss: 0.01363\n",
            "Epoch: 151 | Iteration: 62 | Classification loss: 0.00003 | Regression loss: 0.02058 | Running loss: 0.01363\n",
            "Epoch: 151 | Iteration: 63 | Classification loss: 0.00004 | Regression loss: 0.02900 | Running loss: 0.01366\n",
            "Epoch: 151 | Iteration: 64 | Classification loss: 0.00002 | Regression loss: 0.00727 | Running loss: 0.01366\n",
            "Epoch: 151 | Iteration: 65 | Classification loss: 0.00004 | Regression loss: 0.01247 | Running loss: 0.01367\n",
            "Epoch: 151 | Iteration: 66 | Classification loss: 0.00003 | Regression loss: 0.01894 | Running loss: 0.01367\n",
            "Epoch: 151 | Iteration: 67 | Classification loss: 0.00003 | Regression loss: 0.00793 | Running loss: 0.01368\n",
            "Epoch: 151 | Iteration: 68 | Classification loss: 0.00002 | Regression loss: 0.00446 | Running loss: 0.01366\n",
            "Epoch: 151 | Iteration: 69 | Classification loss: 0.00002 | Regression loss: 0.00705 | Running loss: 0.01364\n",
            "Epoch: 151 | Iteration: 70 | Classification loss: 0.00006 | Regression loss: 0.01353 | Running loss: 0.01363\n",
            "Epoch: 151 | Iteration: 71 | Classification loss: 0.00006 | Regression loss: 0.01058 | Running loss: 0.01363\n",
            "Epoch: 151 | Iteration: 72 | Classification loss: 0.00002 | Regression loss: 0.00621 | Running loss: 0.01362\n",
            "Epoch: 151 | Iteration: 73 | Classification loss: 0.00003 | Regression loss: 0.00751 | Running loss: 0.01360\n",
            "Epoch: 151 | Iteration: 74 | Classification loss: 0.00004 | Regression loss: 0.01211 | Running loss: 0.01361\n",
            "Epoch: 151 | Iteration: 75 | Classification loss: 0.00001 | Regression loss: 0.00842 | Running loss: 0.01360\n",
            "Epoch: 151 | Iteration: 76 | Classification loss: 0.00002 | Regression loss: 0.00560 | Running loss: 0.01357\n",
            "Epoch: 151 | Iteration: 77 | Classification loss: 0.00005 | Regression loss: 0.02096 | Running loss: 0.01359\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 190.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 151 | Iteration: 79 | Classification loss: 0.00004 | Regression loss: 0.01316 | Running loss: 0.01358\n",
            "Epoch: 151 | Iteration: 80 | Classification loss: 0.00002 | Regression loss: 0.03313 | Running loss: 0.01361\n",
            "Epoch: 151 | Iteration: 81 | Classification loss: 0.00005 | Regression loss: 0.01395 | Running loss: 0.01362\n",
            "Epoch: 151 | Iteration: 82 | Classification loss: 0.00004 | Regression loss: 0.01181 | Running loss: 0.01362\n",
            "Epoch: 151 | Iteration: 83 | Classification loss: 0.00002 | Regression loss: 0.00584 | Running loss: 0.01362\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.7383163241711468\n",
            "Precision:  0.52\n",
            "Recall:  0.8221343873517787\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}]\n",
            "Epoch: 152 | Iteration: 0 | Classification loss: 0.00003 | Regression loss: 0.01206 | Running loss: 0.01363\n",
            "Epoch: 152 | Iteration: 1 | Classification loss: 0.00002 | Regression loss: 0.00450 | Running loss: 0.01358\n",
            "Epoch: 152 | Iteration: 2 | Classification loss: 0.00001 | Regression loss: 0.00539 | Running loss: 0.01357\n",
            "Epoch: 152 | Iteration: 3 | Classification loss: 0.00003 | Regression loss: 0.01311 | Running loss: 0.01358\n",
            "Epoch: 152 | Iteration: 4 | Classification loss: 0.00008 | Regression loss: 0.01848 | Running loss: 0.01361\n",
            "Epoch: 152 | Iteration: 5 | Classification loss: 0.00005 | Regression loss: 0.01074 | Running loss: 0.01361\n",
            "Epoch: 152 | Iteration: 6 | Classification loss: 0.00005 | Regression loss: 0.01232 | Running loss: 0.01361\n",
            "Epoch: 152 | Iteration: 7 | Classification loss: 0.00002 | Regression loss: 0.03283 | Running loss: 0.01365\n",
            "Epoch: 152 | Iteration: 8 | Classification loss: 0.00005 | Regression loss: 0.01458 | Running loss: 0.01367\n",
            "Epoch: 152 | Iteration: 9 | Classification loss: 0.00005 | Regression loss: 0.01276 | Running loss: 0.01368\n",
            "Epoch: 152 | Iteration: 10 | Classification loss: 0.00002 | Regression loss: 0.00722 | Running loss: 0.01366\n",
            "Epoch: 152 | Iteration: 11 | Classification loss: 0.00005 | Regression loss: 0.00825 | Running loss: 0.01365\n",
            "Epoch: 152 | Iteration: 12 | Classification loss: 0.00003 | Regression loss: 0.01479 | Running loss: 0.01367\n",
            "Epoch: 152 | Iteration: 13 | Classification loss: 0.00017 | Regression loss: 0.04883 | Running loss: 0.01372\n",
            "Epoch: 152 | Iteration: 14 | Classification loss: 0.00018 | Regression loss: 0.01532 | Running loss: 0.01372\n",
            "Epoch: 152 | Iteration: 15 | Classification loss: 0.00001 | Regression loss: 0.00549 | Running loss: 0.01372\n",
            "Epoch: 152 | Iteration: 16 | Classification loss: 0.00004 | Regression loss: 0.00635 | Running loss: 0.01370\n",
            "Epoch: 152 | Iteration: 17 | Classification loss: 0.00006 | Regression loss: 0.01207 | Running loss: 0.01369\n",
            "Epoch: 152 | Iteration: 18 | Classification loss: 0.00012 | Regression loss: 0.01528 | Running loss: 0.01369\n",
            "Epoch: 152 | Iteration: 19 | Classification loss: 0.00003 | Regression loss: 0.00757 | Running loss: 0.01368\n",
            "Epoch: 152 | Iteration: 20 | Classification loss: 0.00002 | Regression loss: 0.00527 | Running loss: 0.01366\n",
            "Epoch: 152 | Iteration: 21 | Classification loss: 0.00002 | Regression loss: 0.00554 | Running loss: 0.01364\n",
            "Epoch: 152 | Iteration: 22 | Classification loss: 0.00003 | Regression loss: 0.02031 | Running loss: 0.01367\n",
            "Epoch: 152 | Iteration: 23 | Classification loss: 0.00001 | Regression loss: 0.00795 | Running loss: 0.01366\n",
            "Epoch: 152 | Iteration: 24 | Classification loss: 0.00010 | Regression loss: 0.01429 | Running loss: 0.01368\n",
            "Epoch: 152 | Iteration: 25 | Classification loss: 0.00008 | Regression loss: 0.02223 | Running loss: 0.01369\n",
            "Epoch: 152 | Iteration: 26 | Classification loss: 0.00005 | Regression loss: 0.01844 | Running loss: 0.01369\n",
            "Epoch: 152 | Iteration: 27 | Classification loss: 0.00003 | Regression loss: 0.01163 | Running loss: 0.01368\n",
            "Epoch: 152 | Iteration: 28 | Classification loss: 0.00009 | Regression loss: 0.00471 | Running loss: 0.01367\n",
            "Epoch: 152 | Iteration: 29 | Classification loss: 0.00002 | Regression loss: 0.00493 | Running loss: 0.01367\n",
            "Epoch: 152 | Iteration: 30 | Classification loss: 0.00002 | Regression loss: 0.00674 | Running loss: 0.01363\n",
            "Epoch: 152 | Iteration: 31 | Classification loss: 0.00001 | Regression loss: 0.00998 | Running loss: 0.01362\n",
            "Epoch: 152 | Iteration: 32 | Classification loss: 0.00002 | Regression loss: 0.03095 | Running loss: 0.01366\n",
            "Epoch: 152 | Iteration: 33 | Classification loss: 0.00007 | Regression loss: 0.01321 | Running loss: 0.01366\n",
            "Epoch: 152 | Iteration: 34 | Classification loss: 0.00003 | Regression loss: 0.00780 | Running loss: 0.01361\n",
            "Epoch: 152 | Iteration: 35 | Classification loss: 0.00005 | Regression loss: 0.01621 | Running loss: 0.01363\n",
            "Epoch: 152 | Iteration: 36 | Classification loss: 0.00004 | Regression loss: 0.01518 | Running loss: 0.01363\n",
            "Epoch: 152 | Iteration: 37 | Classification loss: 0.00003 | Regression loss: 0.01304 | Running loss: 0.01355\n",
            "Epoch: 152 | Iteration: 38 | Classification loss: 0.00008 | Regression loss: 0.01859 | Running loss: 0.01357\n",
            "Epoch: 152 | Iteration: 39 | Classification loss: 0.00005 | Regression loss: 0.02719 | Running loss: 0.01361\n",
            "Epoch: 152 | Iteration: 40 | Classification loss: 0.00006 | Regression loss: 0.00385 | Running loss: 0.01360\n",
            "Epoch: 152 | Iteration: 41 | Classification loss: 0.00012 | Regression loss: 0.01325 | Running loss: 0.01360\n",
            "Epoch: 152 | Iteration: 42 | Classification loss: 0.00002 | Regression loss: 0.02548 | Running loss: 0.01363\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 188.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 152 | Iteration: 44 | Classification loss: 0.00003 | Regression loss: 0.00600 | Running loss: 0.01363\n",
            "Epoch: 152 | Iteration: 45 | Classification loss: 0.00003 | Regression loss: 0.01396 | Running loss: 0.01363\n",
            "Epoch: 152 | Iteration: 46 | Classification loss: 0.00006 | Regression loss: 0.01945 | Running loss: 0.01365\n",
            "Epoch: 152 | Iteration: 47 | Classification loss: 0.00002 | Regression loss: 0.00434 | Running loss: 0.01363\n",
            "Epoch: 152 | Iteration: 48 | Classification loss: 0.00005 | Regression loss: 0.01074 | Running loss: 0.01362\n",
            "Epoch: 152 | Iteration: 49 | Classification loss: 0.00004 | Regression loss: 0.01362 | Running loss: 0.01364\n",
            "Epoch: 152 | Iteration: 50 | Classification loss: 0.00009 | Regression loss: 0.01198 | Running loss: 0.01362\n",
            "Epoch: 152 | Iteration: 51 | Classification loss: 0.00023 | Regression loss: 0.01046 | Running loss: 0.01361\n",
            "Epoch: 152 | Iteration: 52 | Classification loss: 0.00002 | Regression loss: 0.01317 | Running loss: 0.01360\n",
            "Epoch: 152 | Iteration: 53 | Classification loss: 0.00008 | Regression loss: 0.01969 | Running loss: 0.01361\n",
            "Epoch: 152 | Iteration: 54 | Classification loss: 0.00003 | Regression loss: 0.01755 | Running loss: 0.01359\n",
            "Epoch: 152 | Iteration: 55 | Classification loss: 0.00006 | Regression loss: 0.01085 | Running loss: 0.01357\n",
            "Epoch: 152 | Iteration: 56 | Classification loss: 0.00005 | Regression loss: 0.01573 | Running loss: 0.01357\n",
            "Epoch: 152 | Iteration: 57 | Classification loss: 0.00021 | Regression loss: 0.02189 | Running loss: 0.01360\n",
            "Epoch: 152 | Iteration: 58 | Classification loss: 0.00002 | Regression loss: 0.01961 | Running loss: 0.01362\n",
            "Epoch: 152 | Iteration: 59 | Classification loss: 0.00004 | Regression loss: 0.01160 | Running loss: 0.01361\n",
            "Epoch: 152 | Iteration: 60 | Classification loss: 0.00006 | Regression loss: 0.00520 | Running loss: 0.01361\n",
            "Epoch: 152 | Iteration: 61 | Classification loss: 0.00003 | Regression loss: 0.01342 | Running loss: 0.01361\n",
            "Epoch: 152 | Iteration: 62 | Classification loss: 0.00004 | Regression loss: 0.01327 | Running loss: 0.01359\n",
            "Epoch: 152 | Iteration: 63 | Classification loss: 0.00004 | Regression loss: 0.02353 | Running loss: 0.01362\n",
            "Epoch: 152 | Iteration: 64 | Classification loss: 0.00001 | Regression loss: 0.00412 | Running loss: 0.01361\n",
            "Epoch: 152 | Iteration: 65 | Classification loss: 0.00001 | Regression loss: 0.00416 | Running loss: 0.01356\n",
            "Epoch: 152 | Iteration: 66 | Classification loss: 0.00004 | Regression loss: 0.00726 | Running loss: 0.01356\n",
            "Epoch: 152 | Iteration: 67 | Classification loss: 0.00004 | Regression loss: 0.01459 | Running loss: 0.01357\n",
            "Epoch: 152 | Iteration: 68 | Classification loss: 0.00005 | Regression loss: 0.01471 | Running loss: 0.01359\n",
            "Epoch: 152 | Iteration: 69 | Classification loss: 0.00004 | Regression loss: 0.01581 | Running loss: 0.01356\n",
            "Epoch: 152 | Iteration: 70 | Classification loss: 0.00009 | Regression loss: 0.01670 | Running loss: 0.01356\n",
            "Epoch: 152 | Iteration: 71 | Classification loss: 0.00005 | Regression loss: 0.01702 | Running loss: 0.01358\n",
            "Epoch: 152 | Iteration: 72 | Classification loss: 0.00002 | Regression loss: 0.00680 | Running loss: 0.01357\n",
            "Epoch: 152 | Iteration: 73 | Classification loss: 0.00003 | Regression loss: 0.01367 | Running loss: 0.01356\n",
            "Epoch: 152 | Iteration: 74 | Classification loss: 0.00003 | Regression loss: 0.00692 | Running loss: 0.01355\n",
            "Epoch: 152 | Iteration: 75 | Classification loss: 0.00005 | Regression loss: 0.01311 | Running loss: 0.01355\n",
            "Epoch: 152 | Iteration: 76 | Classification loss: 0.00002 | Regression loss: 0.00820 | Running loss: 0.01354\n",
            "Epoch: 152 | Iteration: 77 | Classification loss: 0.00002 | Regression loss: 0.00489 | Running loss: 0.01351\n",
            "Epoch: 152 | Iteration: 78 | Classification loss: 0.00003 | Regression loss: 0.01351 | Running loss: 0.01350\n",
            "Epoch: 152 | Iteration: 79 | Classification loss: 0.00002 | Regression loss: 0.01126 | Running loss: 0.01350\n",
            "Epoch: 152 | Iteration: 80 | Classification loss: 0.00002 | Regression loss: 0.01446 | Running loss: 0.01351\n",
            "Epoch: 152 | Iteration: 81 | Classification loss: 0.00004 | Regression loss: 0.01019 | Running loss: 0.01349\n",
            "Epoch: 152 | Iteration: 82 | Classification loss: 0.00002 | Regression loss: 0.00771 | Running loss: 0.01348\n",
            "Epoch: 152 | Iteration: 83 | Classification loss: 0.00005 | Regression loss: 0.01573 | Running loss: 0.01347\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.733713296012671\n",
            "Precision:  0.5227272727272727\n",
            "Recall:  0.8181818181818182\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}]\n",
            "Epoch: 153 | Iteration: 0 | Classification loss: 0.00003 | Regression loss: 0.00594 | Running loss: 0.01345\n",
            "Epoch: 153 | Iteration: 1 | Classification loss: 0.00007 | Regression loss: 0.01149 | Running loss: 0.01346\n",
            "Epoch: 153 | Iteration: 2 | Classification loss: 0.00005 | Regression loss: 0.01825 | Running loss: 0.01347\n",
            "Epoch: 153 | Iteration: 3 | Classification loss: 0.00006 | Regression loss: 0.01232 | Running loss: 0.01348\n",
            "Epoch: 153 | Iteration: 4 | Classification loss: 0.00007 | Regression loss: 0.02271 | Running loss: 0.01350\n",
            "Epoch: 153 | Iteration: 5 | Classification loss: 0.00004 | Regression loss: 0.01373 | Running loss: 0.01351\n",
            "Epoch: 153 | Iteration: 6 | Classification loss: 0.00002 | Regression loss: 0.00671 | Running loss: 0.01349\n",
            "Epoch: 153 | Iteration: 7 | Classification loss: 0.00004 | Regression loss: 0.01500 | Running loss: 0.01349\n",
            "Epoch: 153 | Iteration: 8 | Classification loss: 0.00007 | Regression loss: 0.01520 | Running loss: 0.01351\n",
            "Epoch: 153 | Iteration: 9 | Classification loss: 0.00002 | Regression loss: 0.01173 | Running loss: 0.01350\n",
            "Epoch: 153 | Iteration: 10 | Classification loss: 0.00005 | Regression loss: 0.01280 | Running loss: 0.01350\n",
            "Epoch: 153 | Iteration: 11 | Classification loss: 0.00006 | Regression loss: 0.01280 | Running loss: 0.01351\n",
            "Epoch: 153 | Iteration: 12 | Classification loss: 0.00007 | Regression loss: 0.01273 | Running loss: 0.01352\n",
            "Epoch: 153 | Iteration: 13 | Classification loss: 0.00008 | Regression loss: 0.01470 | Running loss: 0.01353\n",
            "Epoch: 153 | Iteration: 14 | Classification loss: 0.00002 | Regression loss: 0.00751 | Running loss: 0.01348\n",
            "Epoch: 153 | Iteration: 15 | Classification loss: 0.00003 | Regression loss: 0.01686 | Running loss: 0.01349\n",
            "Epoch: 153 | Iteration: 16 | Classification loss: 0.00008 | Regression loss: 0.01738 | Running loss: 0.01351\n",
            "Epoch: 153 | Iteration: 17 | Classification loss: 0.00003 | Regression loss: 0.01996 | Running loss: 0.01353\n",
            "Epoch: 153 | Iteration: 18 | Classification loss: 0.00003 | Regression loss: 0.01256 | Running loss: 0.01351\n",
            "Epoch: 153 | Iteration: 19 | Classification loss: 0.00002 | Regression loss: 0.00729 | Running loss: 0.01350\n",
            "Epoch: 153 | Iteration: 20 | Classification loss: 0.00002 | Regression loss: 0.00769 | Running loss: 0.01351\n",
            "Epoch: 153 | Iteration: 21 | Classification loss: 0.00001 | Regression loss: 0.00385 | Running loss: 0.01349\n",
            "Epoch: 153 | Iteration: 22 | Classification loss: 0.00004 | Regression loss: 0.02696 | Running loss: 0.01351\n",
            "Epoch: 153 | Iteration: 23 | Classification loss: 0.00002 | Regression loss: 0.01256 | Running loss: 0.01350\n",
            "Epoch: 153 | Iteration: 24 | Classification loss: 0.00002 | Regression loss: 0.00445 | Running loss: 0.01347\n",
            "Epoch: 153 | Iteration: 25 | Classification loss: 0.00005 | Regression loss: 0.00826 | Running loss: 0.01346\n",
            "Epoch: 153 | Iteration: 26 | Classification loss: 0.00001 | Regression loss: 0.01106 | Running loss: 0.01347\n",
            "Epoch: 153 | Iteration: 27 | Classification loss: 0.00004 | Regression loss: 0.01770 | Running loss: 0.01349\n",
            "Epoch: 153 | Iteration: 28 | Classification loss: 0.00003 | Regression loss: 0.01306 | Running loss: 0.01351\n",
            "Epoch: 153 | Iteration: 29 | Classification loss: 0.00004 | Regression loss: 0.01437 | Running loss: 0.01353\n",
            "Epoch: 153 | Iteration: 30 | Classification loss: 0.00003 | Regression loss: 0.01684 | Running loss: 0.01355\n",
            "Epoch: 153 | Iteration: 31 | Classification loss: 0.00006 | Regression loss: 0.01670 | Running loss: 0.01357\n",
            "Epoch: 153 | Iteration: 32 | Classification loss: 0.00014 | Regression loss: 0.01621 | Running loss: 0.01357\n",
            "Epoch: 153 | Iteration: 33 | Classification loss: 0.00008 | Regression loss: 0.01928 | Running loss: 0.01358\n",
            "Epoch: 153 | Iteration: 34 | Classification loss: 0.00002 | Regression loss: 0.00527 | Running loss: 0.01355\n",
            "Epoch: 153 | Iteration: 35 | Classification loss: 0.00009 | Regression loss: 0.01141 | Running loss: 0.01355\n",
            "Epoch: 153 | Iteration: 36 | Classification loss: 0.00005 | Regression loss: 0.00500 | Running loss: 0.01352\n",
            "Epoch: 153 | Iteration: 37 | Classification loss: 0.00003 | Regression loss: 0.00732 | Running loss: 0.01351\n",
            "Epoch: 153 | Iteration: 38 | Classification loss: 0.00002 | Regression loss: 0.01404 | Running loss: 0.01352\n",
            "Epoch: 153 | Iteration: 39 | Classification loss: 0.00006 | Regression loss: 0.01675 | Running loss: 0.01353\n",
            "Epoch: 153 | Iteration: 40 | Classification loss: 0.00003 | Regression loss: 0.00509 | Running loss: 0.01349\n",
            "Epoch: 153 | Iteration: 41 | Classification loss: 0.00005 | Regression loss: 0.02029 | Running loss: 0.01349\n",
            "Epoch: 153 | Iteration: 42 | Classification loss: 0.00011 | Regression loss: 0.01222 | Running loss: 0.01349\n",
            "Epoch: 153 | Iteration: 43 | Classification loss: 0.00005 | Regression loss: 0.01452 | Running loss: 0.01346\n",
            "Epoch: 153 | Iteration: 44 | Classification loss: 0.00003 | Regression loss: 0.00641 | Running loss: 0.01344\n",
            "Epoch: 153 | Iteration: 45 | Classification loss: 0.00002 | Regression loss: 0.03160 | Running loss: 0.01347\n",
            "Epoch: 153 | Iteration: 46 | Classification loss: 0.00024 | Regression loss: 0.00911 | Running loss: 0.01345\n",
            "Epoch: 153 | Iteration: 47 | Classification loss: 0.00003 | Regression loss: 0.01342 | Running loss: 0.01347\n",
            "Epoch: 153 | Iteration: 48 | Classification loss: 0.00003 | Regression loss: 0.01345 | Running loss: 0.01349\n",
            "Epoch: 153 | Iteration: 49 | Classification loss: 0.00003 | Regression loss: 0.01188 | Running loss: 0.01347\n",
            "Epoch: 153 | Iteration: 50 | Classification loss: 0.00005 | Regression loss: 0.01146 | Running loss: 0.01348\n",
            "Epoch: 153 | Iteration: 51 | Classification loss: 0.00004 | Regression loss: 0.02671 | Running loss: 0.01351\n",
            "Epoch: 153 | Iteration: 52 | Classification loss: 0.00002 | Regression loss: 0.01219 | Running loss: 0.01350\n",
            "Epoch: 153 | Iteration: 53 | Classification loss: 0.00007 | Regression loss: 0.00719 | Running loss: 0.01349\n",
            "Epoch: 153 | Iteration: 54 | Classification loss: 0.00002 | Regression loss: 0.01376 | Running loss: 0.01348\n",
            "Epoch: 153 | Iteration: 55 | Classification loss: 0.00004 | Regression loss: 0.00851 | Running loss: 0.01349\n",
            "Epoch: 153 | Iteration: 56 | Classification loss: 0.00016 | Regression loss: 0.04894 | Running loss: 0.01356\n",
            "Epoch: 153 | Iteration: 57 | Classification loss: 0.00001 | Regression loss: 0.00789 | Running loss: 0.01353\n",
            "Epoch: 153 | Iteration: 58 | Classification loss: 0.00001 | Regression loss: 0.00540 | Running loss: 0.01351\n",
            "Epoch: 153 | Iteration: 59 | Classification loss: 0.00003 | Regression loss: 0.01188 | Running loss: 0.01351\n",
            "Epoch: 153 | Iteration: 60 | Classification loss: 0.00001 | Regression loss: 0.00592 | Running loss: 0.01348\n",
            "Epoch: 153 | Iteration: 61 | Classification loss: 0.00004 | Regression loss: 0.01761 | Running loss: 0.01350\n",
            "Epoch: 153 | Iteration: 62 | Classification loss: 0.00005 | Regression loss: 0.00398 | Running loss: 0.01346\n",
            "Epoch: 153 | Iteration: 63 | Classification loss: 0.00003 | Regression loss: 0.01252 | Running loss: 0.01347\n",
            "Epoch: 153 | Iteration: 64 | Classification loss: 0.00002 | Regression loss: 0.00533 | Running loss: 0.01346\n",
            "Epoch: 153 | Iteration: 65 | Classification loss: 0.00004 | Regression loss: 0.01590 | Running loss: 0.01346\n",
            "Epoch: 153 | Iteration: 66 | Classification loss: 0.00010 | Regression loss: 0.00471 | Running loss: 0.01344\n",
            "Epoch: 153 | Iteration: 67 | Classification loss: 0.00002 | Regression loss: 0.02049 | Running loss: 0.01347\n",
            "Epoch: 153 | Iteration: 68 | Classification loss: 0.00004 | Regression loss: 0.01306 | Running loss: 0.01348\n",
            "Epoch: 153 | Iteration: 69 | Classification loss: 0.00002 | Regression loss: 0.00404 | Running loss: 0.01346\n",
            "Epoch: 153 | Iteration: 70 | Classification loss: 0.00003 | Regression loss: 0.00761 | Running loss: 0.01345\n",
            "Epoch: 153 | Iteration: 71 | Classification loss: 0.00008 | Regression loss: 0.01845 | Running loss: 0.01347\n",
            "Epoch: 153 | Iteration: 72 | Classification loss: 0.00018 | Regression loss: 0.02020 | Running loss: 0.01350\n",
            "Epoch: 153 | Iteration: 73 | Classification loss: 0.00003 | Regression loss: 0.01921 | Running loss: 0.01351\n",
            "Epoch: 153 | Iteration: 74 | Classification loss: 0.00001 | Regression loss: 0.00974 | Running loss: 0.01342\n",
            "Epoch: 153 | Iteration: 75 | Classification loss: 0.00004 | Regression loss: 0.00782 | Running loss: 0.01342\n",
            "Epoch: 153 | Iteration: 76 | Classification loss: 0.00013 | Regression loss: 0.01550 | Running loss: 0.01343\n",
            "Epoch: 153 | Iteration: 77 | Classification loss: 0.00003 | Regression loss: 0.03273 | Running loss: 0.01348\n",
            "Epoch: 153 | Iteration: 78 | Classification loss: 0.00003 | Regression loss: 0.00686 | Running loss: 0.01347\n",
            "Epoch: 153 | Iteration: 79 | Classification loss: 0.00001 | Regression loss: 0.00575 | Running loss: 0.01345\n",
            "Epoch: 153 | Iteration: 80 | Classification loss: 0.00017 | Regression loss: 0.01659 | Running loss: 0.01344\n",
            "Epoch: 153 | Iteration: 81 | Classification loss: 0.00002 | Regression loss: 0.00436 | Running loss: 0.01344\n",
            "Epoch: 153 | Iteration: 82 | Classification loss: 0.00006 | Regression loss: 0.01918 | Running loss: 0.01345\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 190.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.727926315626886\n",
            "Precision:  0.5175\n",
            "Recall:  0.8181818181818182\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}]\n",
            "Epoch: 154 | Iteration: 0 | Classification loss: 0.00004 | Regression loss: 0.01212 | Running loss: 0.01341\n",
            "Epoch: 154 | Iteration: 1 | Classification loss: 0.00008 | Regression loss: 0.01818 | Running loss: 0.01342\n",
            "Epoch: 154 | Iteration: 2 | Classification loss: 0.00001 | Regression loss: 0.00610 | Running loss: 0.01336\n",
            "Epoch: 154 | Iteration: 3 | Classification loss: 0.00006 | Regression loss: 0.01560 | Running loss: 0.01337\n",
            "Epoch: 154 | Iteration: 4 | Classification loss: 0.00005 | Regression loss: 0.01009 | Running loss: 0.01336\n",
            "Epoch: 154 | Iteration: 5 | Classification loss: 0.00002 | Regression loss: 0.01536 | Running loss: 0.01336\n",
            "Epoch: 154 | Iteration: 6 | Classification loss: 0.00003 | Regression loss: 0.01133 | Running loss: 0.01336\n",
            "Epoch: 154 | Iteration: 7 | Classification loss: 0.00009 | Regression loss: 0.01407 | Running loss: 0.01333\n",
            "Epoch: 154 | Iteration: 8 | Classification loss: 0.00002 | Regression loss: 0.01409 | Running loss: 0.01335\n",
            "Epoch: 154 | Iteration: 9 | Classification loss: 0.00015 | Regression loss: 0.01555 | Running loss: 0.01334\n",
            "Epoch: 154 | Iteration: 10 | Classification loss: 0.00006 | Regression loss: 0.02052 | Running loss: 0.01334\n",
            "Epoch: 154 | Iteration: 11 | Classification loss: 0.00004 | Regression loss: 0.01852 | Running loss: 0.01337\n",
            "Epoch: 154 | Iteration: 12 | Classification loss: 0.00007 | Regression loss: 0.00390 | Running loss: 0.01334\n",
            "Epoch: 154 | Iteration: 13 | Classification loss: 0.00002 | Regression loss: 0.00463 | Running loss: 0.01331\n",
            "Epoch: 154 | Iteration: 14 | Classification loss: 0.00004 | Regression loss: 0.02000 | Running loss: 0.01332\n",
            "Epoch: 154 | Iteration: 15 | Classification loss: 0.00004 | Regression loss: 0.01895 | Running loss: 0.01333\n",
            "Epoch: 154 | Iteration: 16 | Classification loss: 0.00001 | Regression loss: 0.00510 | Running loss: 0.01331\n",
            "Epoch: 154 | Iteration: 17 | Classification loss: 0.00005 | Regression loss: 0.01445 | Running loss: 0.01331\n",
            "Epoch: 154 | Iteration: 18 | Classification loss: 0.00002 | Regression loss: 0.03282 | Running loss: 0.01335\n",
            "Epoch: 154 | Iteration: 19 | Classification loss: 0.00008 | Regression loss: 0.01329 | Running loss: 0.01336\n",
            "Epoch: 154 | Iteration: 20 | Classification loss: 0.00006 | Regression loss: 0.01120 | Running loss: 0.01335\n",
            "Epoch: 154 | Iteration: 21 | Classification loss: 0.00004 | Regression loss: 0.02961 | Running loss: 0.01340\n",
            "Epoch: 154 | Iteration: 22 | Classification loss: 0.00005 | Regression loss: 0.01222 | Running loss: 0.01341\n",
            "Epoch: 154 | Iteration: 23 | Classification loss: 0.00002 | Regression loss: 0.00727 | Running loss: 0.01341\n",
            "Epoch: 154 | Iteration: 24 | Classification loss: 0.00004 | Regression loss: 0.00770 | Running loss: 0.01339\n",
            "Epoch: 154 | Iteration: 25 | Classification loss: 0.00005 | Regression loss: 0.00910 | Running loss: 0.01338\n",
            "Epoch: 154 | Iteration: 26 | Classification loss: 0.00002 | Regression loss: 0.01249 | Running loss: 0.01340\n",
            "Epoch: 154 | Iteration: 27 | Classification loss: 0.00004 | Regression loss: 0.01406 | Running loss: 0.01340\n",
            "Epoch: 154 | Iteration: 28 | Classification loss: 0.00003 | Regression loss: 0.01709 | Running loss: 0.01342\n",
            "Epoch: 154 | Iteration: 29 | Classification loss: 0.00003 | Regression loss: 0.01181 | Running loss: 0.01340\n",
            "Epoch: 154 | Iteration: 30 | Classification loss: 0.00003 | Regression loss: 0.00566 | Running loss: 0.01339\n",
            "Epoch: 154 | Iteration: 31 | Classification loss: 0.00002 | Regression loss: 0.00561 | Running loss: 0.01338\n",
            "Epoch: 154 | Iteration: 32 | Classification loss: 0.00003 | Regression loss: 0.01305 | Running loss: 0.01337\n",
            "Epoch: 154 | Iteration: 33 | Classification loss: 0.00004 | Regression loss: 0.01589 | Running loss: 0.01339\n",
            "Epoch: 154 | Iteration: 34 | Classification loss: 0.00002 | Regression loss: 0.00507 | Running loss: 0.01338\n",
            "Epoch: 154 | Iteration: 35 | Classification loss: 0.00016 | Regression loss: 0.01464 | Running loss: 0.01337\n",
            "Epoch: 154 | Iteration: 36 | Classification loss: 0.00006 | Regression loss: 0.00692 | Running loss: 0.01336\n",
            "Epoch: 154 | Iteration: 37 | Classification loss: 0.00002 | Regression loss: 0.02153 | Running loss: 0.01337\n",
            "Epoch: 154 | Iteration: 38 | Classification loss: 0.00001 | Regression loss: 0.00954 | Running loss: 0.01336\n",
            "Epoch: 154 | Iteration: 39 | Classification loss: 0.00001 | Regression loss: 0.00439 | Running loss: 0.01335\n",
            "Epoch: 154 | Iteration: 40 | Classification loss: 0.00005 | Regression loss: 0.01527 | Running loss: 0.01337\n",
            "Epoch: 154 | Iteration: 41 | Classification loss: 0.00001 | Regression loss: 0.00974 | Running loss: 0.01338\n",
            "Epoch: 154 | Iteration: 42 | Classification loss: 0.00001 | Regression loss: 0.00780 | Running loss: 0.01337\n",
            "Epoch: 154 | Iteration: 43 | Classification loss: 0.00004 | Regression loss: 0.00746 | Running loss: 0.01335\n",
            "Epoch: 154 | Iteration: 44 | Classification loss: 0.00018 | Regression loss: 0.02048 | Running loss: 0.01338\n",
            "Epoch: 154 | Iteration: 45 | Classification loss: 0.00004 | Regression loss: 0.00773 | Running loss: 0.01339\n",
            "Epoch: 154 | Iteration: 46 | Classification loss: 0.00002 | Regression loss: 0.01092 | Running loss: 0.01338\n",
            "Epoch: 154 | Iteration: 47 | Classification loss: 0.00002 | Regression loss: 0.00465 | Running loss: 0.01334\n",
            "Epoch: 154 | Iteration: 48 | Classification loss: 0.00010 | Regression loss: 0.01132 | Running loss: 0.01335\n",
            "Epoch: 154 | Iteration: 49 | Classification loss: 0.00001 | Regression loss: 0.00683 | Running loss: 0.01334\n",
            "Epoch: 154 | Iteration: 50 | Classification loss: 0.00005 | Regression loss: 0.01125 | Running loss: 0.01330\n",
            "Epoch: 154 | Iteration: 51 | Classification loss: 0.00005 | Regression loss: 0.01741 | Running loss: 0.01332\n",
            "Epoch: 154 | Iteration: 52 | Classification loss: 0.00002 | Regression loss: 0.00610 | Running loss: 0.01330\n",
            "Epoch: 154 | Iteration: 53 | Classification loss: 0.00002 | Regression loss: 0.00770 | Running loss: 0.01328\n",
            "Epoch: 154 | Iteration: 54 | Classification loss: 0.00010 | Regression loss: 0.01545 | Running loss: 0.01321\n",
            "Epoch: 154 | Iteration: 55 | Classification loss: 0.00023 | Regression loss: 0.00973 | Running loss: 0.01321\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 190.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 154 | Iteration: 57 | Classification loss: 0.00012 | Regression loss: 0.04757 | Running loss: 0.01329\n",
            "Epoch: 154 | Iteration: 58 | Classification loss: 0.00002 | Regression loss: 0.01742 | Running loss: 0.01329\n",
            "Epoch: 154 | Iteration: 59 | Classification loss: 0.00005 | Regression loss: 0.00824 | Running loss: 0.01327\n",
            "Epoch: 154 | Iteration: 60 | Classification loss: 0.00003 | Regression loss: 0.02023 | Running loss: 0.01328\n",
            "Epoch: 154 | Iteration: 61 | Classification loss: 0.00003 | Regression loss: 0.01313 | Running loss: 0.01330\n",
            "Epoch: 154 | Iteration: 62 | Classification loss: 0.00002 | Regression loss: 0.00678 | Running loss: 0.01330\n",
            "Epoch: 154 | Iteration: 63 | Classification loss: 0.00006 | Regression loss: 0.01220 | Running loss: 0.01331\n",
            "Epoch: 154 | Iteration: 64 | Classification loss: 0.00002 | Regression loss: 0.02498 | Running loss: 0.01334\n",
            "Epoch: 154 | Iteration: 65 | Classification loss: 0.00003 | Regression loss: 0.00992 | Running loss: 0.01333\n",
            "Epoch: 154 | Iteration: 66 | Classification loss: 0.00003 | Regression loss: 0.00602 | Running loss: 0.01332\n",
            "Epoch: 154 | Iteration: 67 | Classification loss: 0.00008 | Regression loss: 0.02245 | Running loss: 0.01335\n",
            "Epoch: 154 | Iteration: 68 | Classification loss: 0.00002 | Regression loss: 0.00433 | Running loss: 0.01333\n",
            "Epoch: 154 | Iteration: 69 | Classification loss: 0.00002 | Regression loss: 0.03033 | Running loss: 0.01335\n",
            "Epoch: 154 | Iteration: 70 | Classification loss: 0.00003 | Regression loss: 0.01353 | Running loss: 0.01337\n",
            "Epoch: 154 | Iteration: 71 | Classification loss: 0.00001 | Regression loss: 0.00388 | Running loss: 0.01334\n",
            "Epoch: 154 | Iteration: 72 | Classification loss: 0.00008 | Regression loss: 0.01868 | Running loss: 0.01335\n",
            "Epoch: 154 | Iteration: 73 | Classification loss: 0.00008 | Regression loss: 0.01908 | Running loss: 0.01338\n",
            "Epoch: 154 | Iteration: 74 | Classification loss: 0.00005 | Regression loss: 0.01423 | Running loss: 0.01339\n",
            "Epoch: 154 | Iteration: 75 | Classification loss: 0.00007 | Regression loss: 0.01428 | Running loss: 0.01338\n",
            "Epoch: 154 | Iteration: 76 | Classification loss: 0.00003 | Regression loss: 0.01342 | Running loss: 0.01337\n",
            "Epoch: 154 | Iteration: 77 | Classification loss: 0.00008 | Regression loss: 0.00668 | Running loss: 0.01336\n",
            "Epoch: 154 | Iteration: 78 | Classification loss: 0.00004 | Regression loss: 0.00484 | Running loss: 0.01335\n",
            "Epoch: 154 | Iteration: 79 | Classification loss: 0.00004 | Regression loss: 0.01560 | Running loss: 0.01336\n",
            "Epoch: 154 | Iteration: 80 | Classification loss: 0.00002 | Regression loss: 0.01255 | Running loss: 0.01333\n",
            "Epoch: 154 | Iteration: 81 | Classification loss: 0.00005 | Regression loss: 0.01699 | Running loss: 0.01333\n",
            "Epoch: 154 | Iteration: 82 | Classification loss: 0.00002 | Regression loss: 0.00761 | Running loss: 0.01331\n",
            "Epoch: 154 | Iteration: 83 | Classification loss: 0.00005 | Regression loss: 0.01830 | Running loss: 0.01333\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.7314384456550078\n",
            "Precision:  0.5227272727272727\n",
            "Recall:  0.8181818181818182\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}, {0: (0.7314384456550078, 253.0)}]\n",
            "Epoch: 155 | Iteration: 0 | Classification loss: 0.00010 | Regression loss: 0.01649 | Running loss: 0.01335\n",
            "Epoch: 155 | Iteration: 1 | Classification loss: 0.00001 | Regression loss: 0.00555 | Running loss: 0.01333\n",
            "Epoch: 155 | Iteration: 2 | Classification loss: 0.00005 | Regression loss: 0.01833 | Running loss: 0.01335\n",
            "Epoch: 155 | Iteration: 3 | Classification loss: 0.00015 | Regression loss: 0.01614 | Running loss: 0.01335\n",
            "Epoch: 155 | Iteration: 4 | Classification loss: 0.00012 | Regression loss: 0.01276 | Running loss: 0.01334\n",
            "Epoch: 155 | Iteration: 5 | Classification loss: 0.00003 | Regression loss: 0.00658 | Running loss: 0.01334\n",
            "Epoch: 155 | Iteration: 6 | Classification loss: 0.00004 | Regression loss: 0.01479 | Running loss: 0.01334\n",
            "Epoch: 155 | Iteration: 7 | Classification loss: 0.00002 | Regression loss: 0.00484 | Running loss: 0.01332\n",
            "Epoch: 155 | Iteration: 8 | Classification loss: 0.00004 | Regression loss: 0.01250 | Running loss: 0.01331\n",
            "Epoch: 155 | Iteration: 9 | Classification loss: 0.00001 | Regression loss: 0.00376 | Running loss: 0.01328\n",
            "Epoch: 155 | Iteration: 10 | Classification loss: 0.00009 | Regression loss: 0.01782 | Running loss: 0.01329\n",
            "Epoch: 155 | Iteration: 11 | Classification loss: 0.00002 | Regression loss: 0.00563 | Running loss: 0.01324\n",
            "Epoch: 155 | Iteration: 12 | Classification loss: 0.00002 | Regression loss: 0.00699 | Running loss: 0.01324\n",
            "Epoch: 155 | Iteration: 13 | Classification loss: 0.00003 | Regression loss: 0.00602 | Running loss: 0.01324\n",
            "Epoch: 155 | Iteration: 14 | Classification loss: 0.00006 | Regression loss: 0.01130 | Running loss: 0.01325\n",
            "Epoch: 155 | Iteration: 15 | Classification loss: 0.00002 | Regression loss: 0.01147 | Running loss: 0.01326\n",
            "Epoch: 155 | Iteration: 16 | Classification loss: 0.00002 | Regression loss: 0.00694 | Running loss: 0.01326\n",
            "Epoch: 155 | Iteration: 17 | Classification loss: 0.00001 | Regression loss: 0.00532 | Running loss: 0.01323\n",
            "Epoch: 155 | Iteration: 18 | Classification loss: 0.00004 | Regression loss: 0.01330 | Running loss: 0.01324\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 190.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 155 | Iteration: 20 | Classification loss: 0.00006 | Regression loss: 0.01465 | Running loss: 0.01324\n",
            "Epoch: 155 | Iteration: 21 | Classification loss: 0.00005 | Regression loss: 0.00770 | Running loss: 0.01319\n",
            "Epoch: 155 | Iteration: 22 | Classification loss: 0.00007 | Regression loss: 0.01715 | Running loss: 0.01320\n",
            "Epoch: 155 | Iteration: 23 | Classification loss: 0.00010 | Regression loss: 0.00509 | Running loss: 0.01320\n",
            "Epoch: 155 | Iteration: 24 | Classification loss: 0.00003 | Regression loss: 0.00977 | Running loss: 0.01318\n",
            "Epoch: 155 | Iteration: 25 | Classification loss: 0.00023 | Regression loss: 0.00931 | Running loss: 0.01318\n",
            "Epoch: 155 | Iteration: 26 | Classification loss: 0.00002 | Regression loss: 0.01275 | Running loss: 0.01319\n",
            "Epoch: 155 | Iteration: 27 | Classification loss: 0.00003 | Regression loss: 0.01180 | Running loss: 0.01320\n",
            "Epoch: 155 | Iteration: 28 | Classification loss: 0.00004 | Regression loss: 0.01427 | Running loss: 0.01321\n",
            "Epoch: 155 | Iteration: 29 | Classification loss: 0.00004 | Regression loss: 0.01113 | Running loss: 0.01320\n",
            "Epoch: 155 | Iteration: 30 | Classification loss: 0.00004 | Regression loss: 0.01408 | Running loss: 0.01317\n",
            "Epoch: 155 | Iteration: 31 | Classification loss: 0.00001 | Regression loss: 0.00770 | Running loss: 0.01316\n",
            "Epoch: 155 | Iteration: 32 | Classification loss: 0.00002 | Regression loss: 0.00474 | Running loss: 0.01314\n",
            "Epoch: 155 | Iteration: 33 | Classification loss: 0.00003 | Regression loss: 0.01143 | Running loss: 0.01314\n",
            "Epoch: 155 | Iteration: 34 | Classification loss: 0.00001 | Regression loss: 0.00634 | Running loss: 0.01311\n",
            "Epoch: 155 | Iteration: 35 | Classification loss: 0.00006 | Regression loss: 0.01242 | Running loss: 0.01313\n",
            "Epoch: 155 | Iteration: 36 | Classification loss: 0.00004 | Regression loss: 0.02441 | Running loss: 0.01315\n",
            "Epoch: 155 | Iteration: 37 | Classification loss: 0.00002 | Regression loss: 0.00788 | Running loss: 0.01316\n",
            "Epoch: 155 | Iteration: 38 | Classification loss: 0.00015 | Regression loss: 0.01872 | Running loss: 0.01318\n",
            "Epoch: 155 | Iteration: 39 | Classification loss: 0.00010 | Regression loss: 0.01457 | Running loss: 0.01319\n",
            "Epoch: 155 | Iteration: 40 | Classification loss: 0.00002 | Regression loss: 0.01097 | Running loss: 0.01318\n",
            "Epoch: 155 | Iteration: 41 | Classification loss: 0.00006 | Regression loss: 0.01196 | Running loss: 0.01316\n",
            "Epoch: 155 | Iteration: 42 | Classification loss: 0.00002 | Regression loss: 0.02991 | Running loss: 0.01319\n",
            "Epoch: 155 | Iteration: 43 | Classification loss: 0.00004 | Regression loss: 0.02091 | Running loss: 0.01319\n",
            "Epoch: 155 | Iteration: 44 | Classification loss: 0.00007 | Regression loss: 0.02205 | Running loss: 0.01321\n",
            "Epoch: 155 | Iteration: 45 | Classification loss: 0.00002 | Regression loss: 0.00698 | Running loss: 0.01319\n",
            "Epoch: 155 | Iteration: 46 | Classification loss: 0.00003 | Regression loss: 0.01335 | Running loss: 0.01320\n",
            "Epoch: 155 | Iteration: 47 | Classification loss: 0.00002 | Regression loss: 0.01961 | Running loss: 0.01323\n",
            "Epoch: 155 | Iteration: 48 | Classification loss: 0.00003 | Regression loss: 0.01502 | Running loss: 0.01323\n",
            "Epoch: 155 | Iteration: 49 | Classification loss: 0.00008 | Regression loss: 0.01428 | Running loss: 0.01325\n",
            "Epoch: 155 | Iteration: 50 | Classification loss: 0.00007 | Regression loss: 0.01935 | Running loss: 0.01326\n",
            "Epoch: 155 | Iteration: 51 | Classification loss: 0.00016 | Regression loss: 0.04867 | Running loss: 0.01333\n",
            "Epoch: 155 | Iteration: 52 | Classification loss: 0.00001 | Regression loss: 0.00973 | Running loss: 0.01332\n",
            "Epoch: 155 | Iteration: 53 | Classification loss: 0.00002 | Regression loss: 0.00410 | Running loss: 0.01330\n",
            "Epoch: 155 | Iteration: 54 | Classification loss: 0.00005 | Regression loss: 0.00542 | Running loss: 0.01329\n",
            "Epoch: 155 | Iteration: 55 | Classification loss: 0.00003 | Regression loss: 0.00563 | Running loss: 0.01328\n",
            "Epoch: 155 | Iteration: 56 | Classification loss: 0.00004 | Regression loss: 0.01541 | Running loss: 0.01328\n",
            "Epoch: 155 | Iteration: 57 | Classification loss: 0.00003 | Regression loss: 0.01996 | Running loss: 0.01329\n",
            "Epoch: 155 | Iteration: 58 | Classification loss: 0.00001 | Regression loss: 0.01287 | Running loss: 0.01328\n",
            "Epoch: 155 | Iteration: 59 | Classification loss: 0.00005 | Regression loss: 0.01487 | Running loss: 0.01330\n",
            "Epoch: 155 | Iteration: 60 | Classification loss: 0.00005 | Regression loss: 0.01991 | Running loss: 0.01332\n",
            "Epoch: 155 | Iteration: 61 | Classification loss: 0.00006 | Regression loss: 0.01436 | Running loss: 0.01332\n",
            "Epoch: 155 | Iteration: 62 | Classification loss: 0.00002 | Regression loss: 0.01493 | Running loss: 0.01333\n",
            "Epoch: 155 | Iteration: 63 | Classification loss: 0.00005 | Regression loss: 0.01011 | Running loss: 0.01333\n",
            "Epoch: 155 | Iteration: 64 | Classification loss: 0.00004 | Regression loss: 0.01424 | Running loss: 0.01333\n",
            "Epoch: 155 | Iteration: 65 | Classification loss: 0.00003 | Regression loss: 0.00724 | Running loss: 0.01333\n",
            "Epoch: 155 | Iteration: 66 | Classification loss: 0.00003 | Regression loss: 0.03210 | Running loss: 0.01338\n",
            "Epoch: 155 | Iteration: 67 | Classification loss: 0.00004 | Regression loss: 0.00912 | Running loss: 0.01338\n",
            "Epoch: 155 | Iteration: 68 | Classification loss: 0.00002 | Regression loss: 0.01268 | Running loss: 0.01338\n",
            "Epoch: 155 | Iteration: 69 | Classification loss: 0.00003 | Regression loss: 0.01647 | Running loss: 0.01338\n",
            "Epoch: 155 | Iteration: 70 | Classification loss: 0.00007 | Regression loss: 0.01790 | Running loss: 0.01340\n",
            "Epoch: 155 | Iteration: 71 | Classification loss: 0.00002 | Regression loss: 0.01779 | Running loss: 0.01342\n",
            "Epoch: 155 | Iteration: 72 | Classification loss: 0.00002 | Regression loss: 0.01346 | Running loss: 0.01342\n",
            "Epoch: 155 | Iteration: 73 | Classification loss: 0.00002 | Regression loss: 0.01282 | Running loss: 0.01342\n",
            "Epoch: 155 | Iteration: 74 | Classification loss: 0.00003 | Regression loss: 0.02460 | Running loss: 0.01344\n",
            "Epoch: 155 | Iteration: 75 | Classification loss: 0.00001 | Regression loss: 0.00381 | Running loss: 0.01342\n",
            "Epoch: 155 | Iteration: 76 | Classification loss: 0.00006 | Regression loss: 0.00403 | Running loss: 0.01339\n",
            "Epoch: 155 | Iteration: 77 | Classification loss: 0.00002 | Regression loss: 0.01276 | Running loss: 0.01337\n",
            "Epoch: 155 | Iteration: 78 | Classification loss: 0.00002 | Regression loss: 0.00454 | Running loss: 0.01336\n",
            "Epoch: 155 | Iteration: 79 | Classification loss: 0.00004 | Regression loss: 0.01836 | Running loss: 0.01338\n",
            "Epoch: 155 | Iteration: 80 | Classification loss: 0.00002 | Regression loss: 0.00768 | Running loss: 0.01334\n",
            "Epoch: 155 | Iteration: 81 | Classification loss: 0.00004 | Regression loss: 0.01206 | Running loss: 0.01332\n",
            "Epoch: 155 | Iteration: 82 | Classification loss: 0.00008 | Regression loss: 0.01208 | Running loss: 0.01331\n",
            "Epoch: 155 | Iteration: 83 | Classification loss: 0.00002 | Regression loss: 0.00562 | Running loss: 0.01329\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.7334466242447818\n",
            "Precision:  0.5227272727272727\n",
            "Recall:  0.8181818181818182\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}, {0: (0.7314384456550078, 253.0)}, {0: (0.7334466242447818, 253.0)}]\n",
            "Epoch: 156 | Iteration: 0 | Classification loss: 0.00011 | Regression loss: 0.01505 | Running loss: 0.01322\n",
            "Epoch: 156 | Iteration: 1 | Classification loss: 0.00004 | Regression loss: 0.01484 | Running loss: 0.01324\n",
            "Epoch: 156 | Iteration: 2 | Classification loss: 0.00002 | Regression loss: 0.01277 | Running loss: 0.01324\n",
            "Epoch: 156 | Iteration: 3 | Classification loss: 0.00003 | Regression loss: 0.01330 | Running loss: 0.01325\n",
            "Epoch: 156 | Iteration: 4 | Classification loss: 0.00004 | Regression loss: 0.00779 | Running loss: 0.01325\n",
            "Epoch: 156 | Iteration: 5 | Classification loss: 0.00003 | Regression loss: 0.01650 | Running loss: 0.01326\n",
            "Epoch: 156 | Iteration: 6 | Classification loss: 0.00005 | Regression loss: 0.01850 | Running loss: 0.01328\n",
            "Epoch: 156 | Iteration: 7 | Classification loss: 0.00002 | Regression loss: 0.00790 | Running loss: 0.01329\n",
            "Epoch: 156 | Iteration: 8 | Classification loss: 0.00003 | Regression loss: 0.00644 | Running loss: 0.01327\n",
            "Epoch: 156 | Iteration: 9 | Classification loss: 0.00007 | Regression loss: 0.01136 | Running loss: 0.01326\n",
            "Epoch: 156 | Iteration: 10 | Classification loss: 0.00001 | Regression loss: 0.00415 | Running loss: 0.01325\n",
            "Epoch: 156 | Iteration: 11 | Classification loss: 0.00002 | Regression loss: 0.01411 | Running loss: 0.01326\n",
            "Epoch: 156 | Iteration: 12 | Classification loss: 0.00001 | Regression loss: 0.00765 | Running loss: 0.01325\n",
            "Epoch: 156 | Iteration: 13 | Classification loss: 0.00010 | Regression loss: 0.01499 | Running loss: 0.01325\n",
            "Epoch: 156 | Iteration: 14 | Classification loss: 0.00002 | Regression loss: 0.00493 | Running loss: 0.01323\n",
            "Epoch: 156 | Iteration: 15 | Classification loss: 0.00004 | Regression loss: 0.01131 | Running loss: 0.01323\n",
            "Epoch: 156 | Iteration: 16 | Classification loss: 0.00002 | Regression loss: 0.00542 | Running loss: 0.01323\n",
            "Epoch: 156 | Iteration: 17 | Classification loss: 0.00005 | Regression loss: 0.01914 | Running loss: 0.01325\n",
            "Epoch: 156 | Iteration: 18 | Classification loss: 0.00003 | Regression loss: 0.01208 | Running loss: 0.01324\n",
            "Epoch: 156 | Iteration: 19 | Classification loss: 0.00003 | Regression loss: 0.01212 | Running loss: 0.01325\n",
            "Epoch: 156 | Iteration: 20 | Classification loss: 0.00014 | Regression loss: 0.01907 | Running loss: 0.01326\n",
            "Epoch: 156 | Iteration: 21 | Classification loss: 0.00004 | Regression loss: 0.01389 | Running loss: 0.01326\n",
            "Epoch: 156 | Iteration: 22 | Classification loss: 0.00004 | Regression loss: 0.01672 | Running loss: 0.01326\n",
            "Epoch: 156 | Iteration: 23 | Classification loss: 0.00006 | Regression loss: 0.01277 | Running loss: 0.01325\n",
            "Epoch: 156 | Iteration: 24 | Classification loss: 0.00004 | Regression loss: 0.00809 | Running loss: 0.01325\n",
            "Epoch: 156 | Iteration: 25 | Classification loss: 0.00001 | Regression loss: 0.00941 | Running loss: 0.01324\n",
            "Epoch: 156 | Iteration: 26 | Classification loss: 0.00001 | Regression loss: 0.01208 | Running loss: 0.01324\n",
            "Epoch: 156 | Iteration: 27 | Classification loss: 0.00003 | Regression loss: 0.02050 | Running loss: 0.01326\n",
            "Epoch: 156 | Iteration: 28 | Classification loss: 0.00004 | Regression loss: 0.01568 | Running loss: 0.01326\n",
            "Epoch: 156 | Iteration: 29 | Classification loss: 0.00005 | Regression loss: 0.01427 | Running loss: 0.01328\n",
            "Epoch: 156 | Iteration: 30 | Classification loss: 0.00004 | Regression loss: 0.00719 | Running loss: 0.01325\n",
            "Epoch: 156 | Iteration: 31 | Classification loss: 0.00003 | Regression loss: 0.00752 | Running loss: 0.01320\n",
            "Epoch: 156 | Iteration: 32 | Classification loss: 0.00005 | Regression loss: 0.01059 | Running loss: 0.01320\n",
            "Epoch: 156 | Iteration: 33 | Classification loss: 0.00008 | Regression loss: 0.00501 | Running loss: 0.01319\n",
            "Epoch: 156 | Iteration: 34 | Classification loss: 0.00005 | Regression loss: 0.01661 | Running loss: 0.01321\n",
            "Epoch: 156 | Iteration: 35 | Classification loss: 0.00003 | Regression loss: 0.01499 | Running loss: 0.01322\n",
            "Epoch: 156 | Iteration: 36 | Classification loss: 0.00004 | Regression loss: 0.01182 | Running loss: 0.01323\n",
            "Epoch: 156 | Iteration: 37 | Classification loss: 0.00002 | Regression loss: 0.01040 | Running loss: 0.01324\n",
            "Epoch: 156 | Iteration: 38 | Classification loss: 0.00002 | Regression loss: 0.02417 | Running loss: 0.01324\n",
            "Epoch: 156 | Iteration: 39 | Classification loss: 0.00002 | Regression loss: 0.00688 | Running loss: 0.01323\n",
            "Epoch: 156 | Iteration: 40 | Classification loss: 0.00005 | Regression loss: 0.01074 | Running loss: 0.01324\n",
            "Epoch: 156 | Iteration: 41 | Classification loss: 0.00003 | Regression loss: 0.01682 | Running loss: 0.01325\n",
            "Epoch: 156 | Iteration: 42 | Classification loss: 0.00011 | Regression loss: 0.01967 | Running loss: 0.01327\n",
            "Epoch: 156 | Iteration: 43 | Classification loss: 0.00008 | Regression loss: 0.01900 | Running loss: 0.01328\n",
            "Epoch: 156 | Iteration: 44 | Classification loss: 0.00002 | Regression loss: 0.03053 | Running loss: 0.01332\n",
            "Epoch: 156 | Iteration: 45 | Classification loss: 0.00014 | Regression loss: 0.04798 | Running loss: 0.01339\n",
            "Epoch: 156 | Iteration: 46 | Classification loss: 0.00001 | Regression loss: 0.00371 | Running loss: 0.01337\n",
            "Epoch: 156 | Iteration: 47 | Classification loss: 0.00001 | Regression loss: 0.00689 | Running loss: 0.01335\n",
            "Epoch: 156 | Iteration: 48 | Classification loss: 0.00002 | Regression loss: 0.03277 | Running loss: 0.01341\n",
            "Epoch: 156 | Iteration: 49 | Classification loss: 0.00001 | Regression loss: 0.00535 | Running loss: 0.01339\n",
            "Epoch: 156 | Iteration: 50 | Classification loss: 0.00002 | Regression loss: 0.01155 | Running loss: 0.01339\n",
            "Epoch: 156 | Iteration: 51 | Classification loss: 0.00004 | Regression loss: 0.00826 | Running loss: 0.01339\n",
            "Epoch: 156 | Iteration: 52 | Classification loss: 0.00022 | Regression loss: 0.01048 | Running loss: 0.01338\n",
            "Epoch: 156 | Iteration: 53 | Classification loss: 0.00005 | Regression loss: 0.00381 | Running loss: 0.01337\n",
            "Epoch: 156 | Iteration: 54 | Classification loss: 0.00002 | Regression loss: 0.00720 | Running loss: 0.01336\n",
            "Epoch: 156 | Iteration: 55 | Classification loss: 0.00005 | Regression loss: 0.01179 | Running loss: 0.01333\n",
            "Epoch: 156 | Iteration: 56 | Classification loss: 0.00014 | Regression loss: 0.01536 | Running loss: 0.01333\n",
            "Epoch: 156 | Iteration: 57 | Classification loss: 0.00009 | Regression loss: 0.02227 | Running loss: 0.01336\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 188.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 156 | Iteration: 59 | Classification loss: 0.00004 | Regression loss: 0.01149 | Running loss: 0.01335\n",
            "Epoch: 156 | Iteration: 60 | Classification loss: 0.00002 | Regression loss: 0.00419 | Running loss: 0.01334\n",
            "Epoch: 156 | Iteration: 61 | Classification loss: 0.00002 | Regression loss: 0.01255 | Running loss: 0.01335\n",
            "Epoch: 156 | Iteration: 62 | Classification loss: 0.00003 | Regression loss: 0.00772 | Running loss: 0.01333\n",
            "Epoch: 156 | Iteration: 63 | Classification loss: 0.00005 | Regression loss: 0.01357 | Running loss: 0.01332\n",
            "Epoch: 156 | Iteration: 64 | Classification loss: 0.00004 | Regression loss: 0.01621 | Running loss: 0.01334\n",
            "Epoch: 156 | Iteration: 65 | Classification loss: 0.00002 | Regression loss: 0.01967 | Running loss: 0.01334\n",
            "Epoch: 156 | Iteration: 66 | Classification loss: 0.00003 | Regression loss: 0.00631 | Running loss: 0.01333\n",
            "Epoch: 156 | Iteration: 67 | Classification loss: 0.00002 | Regression loss: 0.01399 | Running loss: 0.01333\n",
            "Epoch: 156 | Iteration: 68 | Classification loss: 0.00003 | Regression loss: 0.01121 | Running loss: 0.01326\n",
            "Epoch: 156 | Iteration: 69 | Classification loss: 0.00012 | Regression loss: 0.01215 | Running loss: 0.01325\n",
            "Epoch: 156 | Iteration: 70 | Classification loss: 0.00003 | Regression loss: 0.01203 | Running loss: 0.01325\n",
            "Epoch: 156 | Iteration: 71 | Classification loss: 0.00003 | Regression loss: 0.00958 | Running loss: 0.01325\n",
            "Epoch: 156 | Iteration: 72 | Classification loss: 0.00002 | Regression loss: 0.01246 | Running loss: 0.01323\n",
            "Epoch: 156 | Iteration: 73 | Classification loss: 0.00003 | Regression loss: 0.02766 | Running loss: 0.01326\n",
            "Epoch: 156 | Iteration: 74 | Classification loss: 0.00001 | Regression loss: 0.00527 | Running loss: 0.01322\n",
            "Epoch: 156 | Iteration: 75 | Classification loss: 0.00002 | Regression loss: 0.01989 | Running loss: 0.01323\n",
            "Epoch: 156 | Iteration: 76 | Classification loss: 0.00002 | Regression loss: 0.00515 | Running loss: 0.01321\n",
            "Epoch: 156 | Iteration: 77 | Classification loss: 0.00004 | Regression loss: 0.01407 | Running loss: 0.01322\n",
            "Epoch: 156 | Iteration: 78 | Classification loss: 0.00003 | Regression loss: 0.01323 | Running loss: 0.01322\n",
            "Epoch: 156 | Iteration: 79 | Classification loss: 0.00001 | Regression loss: 0.00538 | Running loss: 0.01322\n",
            "Epoch: 156 | Iteration: 80 | Classification loss: 0.00002 | Regression loss: 0.00459 | Running loss: 0.01317\n",
            "Epoch: 156 | Iteration: 81 | Classification loss: 0.00007 | Regression loss: 0.01961 | Running loss: 0.01318\n",
            "Epoch: 156 | Iteration: 82 | Classification loss: 0.00002 | Regression loss: 0.01507 | Running loss: 0.01319\n",
            "Epoch: 156 | Iteration: 83 | Classification loss: 0.00004 | Regression loss: 0.00479 | Running loss: 0.01318\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.7357024222917343\n",
            "Precision:  0.5267175572519084\n",
            "Recall:  0.8181818181818182\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}, {0: (0.7314384456550078, 253.0)}, {0: (0.7334466242447818, 253.0)}, {0: (0.7357024222917343, 253.0)}]\n",
            "Epoch: 157 | Iteration: 0 | Classification loss: 0.00007 | Regression loss: 0.02208 | Running loss: 0.01319\n",
            "Epoch: 157 | Iteration: 1 | Classification loss: 0.00001 | Regression loss: 0.00612 | Running loss: 0.01317\n",
            "Epoch: 157 | Iteration: 2 | Classification loss: 0.00005 | Regression loss: 0.00383 | Running loss: 0.01313\n",
            "Epoch: 157 | Iteration: 3 | Classification loss: 0.00002 | Regression loss: 0.00456 | Running loss: 0.01312\n",
            "Epoch: 157 | Iteration: 4 | Classification loss: 0.00005 | Regression loss: 0.01223 | Running loss: 0.01313\n",
            "Epoch: 157 | Iteration: 5 | Classification loss: 0.00006 | Regression loss: 0.01897 | Running loss: 0.01313\n",
            "Epoch: 157 | Iteration: 6 | Classification loss: 0.00003 | Regression loss: 0.01147 | Running loss: 0.01312\n",
            "Epoch: 157 | Iteration: 7 | Classification loss: 0.00001 | Regression loss: 0.00954 | Running loss: 0.01312\n",
            "Epoch: 157 | Iteration: 8 | Classification loss: 0.00002 | Regression loss: 0.01291 | Running loss: 0.01309\n",
            "Epoch: 157 | Iteration: 9 | Classification loss: 0.00017 | Regression loss: 0.02120 | Running loss: 0.01312\n",
            "Epoch: 157 | Iteration: 10 | Classification loss: 0.00020 | Regression loss: 0.00914 | Running loss: 0.01310\n",
            "Epoch: 157 | Iteration: 11 | Classification loss: 0.00002 | Regression loss: 0.01137 | Running loss: 0.01311\n",
            "Epoch: 157 | Iteration: 12 | Classification loss: 0.00003 | Regression loss: 0.00545 | Running loss: 0.01309\n",
            "Epoch: 157 | Iteration: 13 | Classification loss: 0.00001 | Regression loss: 0.00771 | Running loss: 0.01307\n",
            "Epoch: 157 | Iteration: 14 | Classification loss: 0.00001 | Regression loss: 0.00357 | Running loss: 0.01305\n",
            "Epoch: 157 | Iteration: 15 | Classification loss: 0.00005 | Regression loss: 0.01842 | Running loss: 0.01306\n",
            "Epoch: 157 | Iteration: 16 | Classification loss: 0.00002 | Regression loss: 0.01136 | Running loss: 0.01306\n",
            "Epoch: 157 | Iteration: 17 | Classification loss: 0.00003 | Regression loss: 0.01804 | Running loss: 0.01308\n",
            "Epoch: 157 | Iteration: 18 | Classification loss: 0.00007 | Regression loss: 0.01824 | Running loss: 0.01311\n",
            "Epoch: 157 | Iteration: 19 | Classification loss: 0.00004 | Regression loss: 0.00784 | Running loss: 0.01309\n",
            "Epoch: 157 | Iteration: 20 | Classification loss: 0.00005 | Regression loss: 0.01850 | Running loss: 0.01311\n",
            "Epoch: 157 | Iteration: 21 | Classification loss: 0.00002 | Regression loss: 0.00771 | Running loss: 0.01309\n",
            "Epoch: 157 | Iteration: 22 | Classification loss: 0.00007 | Regression loss: 0.01104 | Running loss: 0.01310\n",
            "Epoch: 157 | Iteration: 23 | Classification loss: 0.00001 | Regression loss: 0.00514 | Running loss: 0.01308\n",
            "Epoch: 157 | Iteration: 24 | Classification loss: 0.00005 | Regression loss: 0.01553 | Running loss: 0.01309\n",
            "Epoch: 157 | Iteration: 25 | Classification loss: 0.00008 | Regression loss: 0.01786 | Running loss: 0.01310\n",
            "Epoch: 157 | Iteration: 26 | Classification loss: 0.00003 | Regression loss: 0.00707 | Running loss: 0.01309\n",
            "Epoch: 157 | Iteration: 27 | Classification loss: 0.00002 | Regression loss: 0.01493 | Running loss: 0.01311\n",
            "Epoch: 157 | Iteration: 28 | Classification loss: 0.00001 | Regression loss: 0.00394 | Running loss: 0.01308\n",
            "Epoch: 157 | Iteration: 29 | Classification loss: 0.00003 | Regression loss: 0.02721 | Running loss: 0.01309\n",
            "Epoch: 157 | Iteration: 30 | Classification loss: 0.00006 | Regression loss: 0.01399 | Running loss: 0.01309\n",
            "Epoch: 157 | Iteration: 31 | Classification loss: 0.00002 | Regression loss: 0.00453 | Running loss: 0.01306\n",
            "Epoch: 157 | Iteration: 32 | Classification loss: 0.00009 | Regression loss: 0.00429 | Running loss: 0.01306\n",
            "Epoch: 157 | Iteration: 33 | Classification loss: 0.00004 | Regression loss: 0.01487 | Running loss: 0.01308\n",
            "Epoch: 157 | Iteration: 34 | Classification loss: 0.00005 | Regression loss: 0.01396 | Running loss: 0.01307\n",
            "Epoch: 157 | Iteration: 35 | Classification loss: 0.00004 | Regression loss: 0.01408 | Running loss: 0.01309\n",
            "Epoch: 157 | Iteration: 36 | Classification loss: 0.00008 | Regression loss: 0.01714 | Running loss: 0.01311\n",
            "Epoch: 157 | Iteration: 37 | Classification loss: 0.00002 | Regression loss: 0.01966 | Running loss: 0.01313\n",
            "Epoch: 157 | Iteration: 38 | Classification loss: 0.00002 | Regression loss: 0.01962 | Running loss: 0.01313\n",
            "Epoch: 157 | Iteration: 39 | Classification loss: 0.00001 | Regression loss: 0.00562 | Running loss: 0.01312\n",
            "Epoch: 157 | Iteration: 40 | Classification loss: 0.00003 | Regression loss: 0.01123 | Running loss: 0.01312\n",
            "Epoch: 157 | Iteration: 41 | Classification loss: 0.00003 | Regression loss: 0.00687 | Running loss: 0.01310\n",
            "Epoch: 157 | Iteration: 42 | Classification loss: 0.00009 | Regression loss: 0.01426 | Running loss: 0.01310\n",
            "Epoch: 157 | Iteration: 43 | Classification loss: 0.00002 | Regression loss: 0.00450 | Running loss: 0.01305\n",
            "Epoch: 157 | Iteration: 44 | Classification loss: 0.00014 | Regression loss: 0.01361 | Running loss: 0.01307\n",
            "Epoch: 157 | Iteration: 45 | Classification loss: 0.00002 | Regression loss: 0.03017 | Running loss: 0.01310\n",
            "Epoch: 157 | Iteration: 46 | Classification loss: 0.00004 | Regression loss: 0.01150 | Running loss: 0.01310\n",
            "Epoch: 157 | Iteration: 47 | Classification loss: 0.00002 | Regression loss: 0.01191 | Running loss: 0.01312\n",
            "Epoch: 157 | Iteration: 48 | Classification loss: 0.00002 | Regression loss: 0.01122 | Running loss: 0.01313\n",
            "Epoch: 157 | Iteration: 49 | Classification loss: 0.00002 | Regression loss: 0.02453 | Running loss: 0.01315\n",
            "Epoch: 157 | Iteration: 50 | Classification loss: 0.00004 | Regression loss: 0.01452 | Running loss: 0.01315\n",
            "Epoch: 157 | Iteration: 51 | Classification loss: 0.00003 | Regression loss: 0.01377 | Running loss: 0.01316\n",
            "Epoch: 157 | Iteration: 52 | Classification loss: 0.00002 | Regression loss: 0.00718 | Running loss: 0.01316\n",
            "Epoch: 157 | Iteration: 53 | Classification loss: 0.00004 | Regression loss: 0.01438 | Running loss: 0.01315\n",
            "Epoch: 157 | Iteration: 54 | Classification loss: 0.00001 | Regression loss: 0.00566 | Running loss: 0.01313\n",
            "Epoch: 157 | Iteration: 55 | Classification loss: 0.00004 | Regression loss: 0.00470 | Running loss: 0.01311\n",
            "Epoch: 157 | Iteration: 56 | Classification loss: 0.00005 | Regression loss: 0.01008 | Running loss: 0.01310\n",
            "Epoch: 157 | Iteration: 57 | Classification loss: 0.00003 | Regression loss: 0.01146 | Running loss: 0.01309\n",
            "Epoch: 157 | Iteration: 58 | Classification loss: 0.00011 | Regression loss: 0.01520 | Running loss: 0.01309\n",
            "Epoch: 157 | Iteration: 59 | Classification loss: 0.00005 | Regression loss: 0.01013 | Running loss: 0.01308\n",
            "Epoch: 157 | Iteration: 60 | Classification loss: 0.00003 | Regression loss: 0.01546 | Running loss: 0.01301\n",
            "Epoch: 157 | Iteration: 61 | Classification loss: 0.00002 | Regression loss: 0.03137 | Running loss: 0.01304\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 188.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 157 | Iteration: 63 | Classification loss: 0.00002 | Regression loss: 0.00686 | Running loss: 0.01304\n",
            "Epoch: 157 | Iteration: 64 | Classification loss: 0.00001 | Regression loss: 0.01256 | Running loss: 0.01304\n",
            "Epoch: 157 | Iteration: 65 | Classification loss: 0.00003 | Regression loss: 0.01269 | Running loss: 0.01302\n",
            "Epoch: 157 | Iteration: 66 | Classification loss: 0.00007 | Regression loss: 0.01076 | Running loss: 0.01299\n",
            "Epoch: 157 | Iteration: 67 | Classification loss: 0.00003 | Regression loss: 0.00997 | Running loss: 0.01299\n",
            "Epoch: 157 | Iteration: 68 | Classification loss: 0.00004 | Regression loss: 0.01348 | Running loss: 0.01299\n",
            "Epoch: 157 | Iteration: 69 | Classification loss: 0.00006 | Regression loss: 0.01855 | Running loss: 0.01299\n",
            "Epoch: 157 | Iteration: 70 | Classification loss: 0.00003 | Regression loss: 0.00632 | Running loss: 0.01299\n",
            "Epoch: 157 | Iteration: 71 | Classification loss: 0.00005 | Regression loss: 0.01556 | Running loss: 0.01301\n",
            "Epoch: 157 | Iteration: 72 | Classification loss: 0.00002 | Regression loss: 0.01262 | Running loss: 0.01302\n",
            "Epoch: 157 | Iteration: 73 | Classification loss: 0.00004 | Regression loss: 0.01096 | Running loss: 0.01302\n",
            "Epoch: 157 | Iteration: 74 | Classification loss: 0.00011 | Regression loss: 0.01193 | Running loss: 0.01302\n",
            "Epoch: 157 | Iteration: 75 | Classification loss: 0.00001 | Regression loss: 0.00553 | Running loss: 0.01302\n",
            "Epoch: 157 | Iteration: 76 | Classification loss: 0.00004 | Regression loss: 0.01725 | Running loss: 0.01304\n",
            "Epoch: 157 | Iteration: 77 | Classification loss: 0.00001 | Regression loss: 0.00598 | Running loss: 0.01303\n",
            "Epoch: 157 | Iteration: 78 | Classification loss: 0.00002 | Regression loss: 0.00735 | Running loss: 0.01302\n",
            "Epoch: 157 | Iteration: 79 | Classification loss: 0.00002 | Regression loss: 0.00620 | Running loss: 0.01303\n",
            "Epoch: 157 | Iteration: 80 | Classification loss: 0.00002 | Regression loss: 0.01213 | Running loss: 0.01301\n",
            "Epoch: 157 | Iteration: 81 | Classification loss: 0.00002 | Regression loss: 0.01079 | Running loss: 0.01300\n",
            "Epoch: 157 | Iteration: 82 | Classification loss: 0.00018 | Regression loss: 0.04970 | Running loss: 0.01304\n",
            "Epoch: 157 | Iteration: 83 | Classification loss: 0.00002 | Regression loss: 0.00488 | Running loss: 0.01302\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.7381550871546381\n",
            "Precision:  0.5280612244897959\n",
            "Recall:  0.8181818181818182\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}, {0: (0.7314384456550078, 253.0)}, {0: (0.7334466242447818, 253.0)}, {0: (0.7357024222917343, 253.0)}, {0: (0.7381550871546381, 253.0)}]\n",
            "Epoch: 158 | Iteration: 0 | Classification loss: 0.00005 | Regression loss: 0.00353 | Running loss: 0.01300\n",
            "Epoch: 158 | Iteration: 1 | Classification loss: 0.00004 | Regression loss: 0.01983 | Running loss: 0.01303\n",
            "Epoch: 158 | Iteration: 2 | Classification loss: 0.00002 | Regression loss: 0.00631 | Running loss: 0.01302\n",
            "Epoch: 158 | Iteration: 3 | Classification loss: 0.00005 | Regression loss: 0.01081 | Running loss: 0.01303\n",
            "Epoch: 158 | Iteration: 4 | Classification loss: 0.00004 | Regression loss: 0.01636 | Running loss: 0.01305\n",
            "Epoch: 158 | Iteration: 5 | Classification loss: 0.00015 | Regression loss: 0.01492 | Running loss: 0.01306\n",
            "Epoch: 158 | Iteration: 6 | Classification loss: 0.00002 | Regression loss: 0.00946 | Running loss: 0.01304\n",
            "Epoch: 158 | Iteration: 7 | Classification loss: 0.00004 | Regression loss: 0.00748 | Running loss: 0.01303\n",
            "Epoch: 158 | Iteration: 8 | Classification loss: 0.00002 | Regression loss: 0.00696 | Running loss: 0.01302\n",
            "Epoch: 158 | Iteration: 9 | Classification loss: 0.00004 | Regression loss: 0.01419 | Running loss: 0.01298\n",
            "Epoch: 158 | Iteration: 10 | Classification loss: 0.00004 | Regression loss: 0.02108 | Running loss: 0.01300\n",
            "Epoch: 158 | Iteration: 11 | Classification loss: 0.00008 | Regression loss: 0.01769 | Running loss: 0.01301\n",
            "Epoch: 158 | Iteration: 12 | Classification loss: 0.00010 | Regression loss: 0.01480 | Running loss: 0.01302\n",
            "Epoch: 158 | Iteration: 13 | Classification loss: 0.00002 | Regression loss: 0.03173 | Running loss: 0.01307\n",
            "Epoch: 158 | Iteration: 14 | Classification loss: 0.00001 | Regression loss: 0.00604 | Running loss: 0.01305\n",
            "Epoch: 158 | Iteration: 15 | Classification loss: 0.00004 | Regression loss: 0.01466 | Running loss: 0.01298\n",
            "Epoch: 158 | Iteration: 16 | Classification loss: 0.00018 | Regression loss: 0.00861 | Running loss: 0.01297\n",
            "Epoch: 158 | Iteration: 17 | Classification loss: 0.00006 | Regression loss: 0.02158 | Running loss: 0.01300\n",
            "Epoch: 158 | Iteration: 18 | Classification loss: 0.00002 | Regression loss: 0.00764 | Running loss: 0.01300\n",
            "Epoch: 158 | Iteration: 19 | Classification loss: 0.00002 | Regression loss: 0.01324 | Running loss: 0.01301\n",
            "Epoch: 158 | Iteration: 20 | Classification loss: 0.00019 | Regression loss: 0.02037 | Running loss: 0.01302\n",
            "Epoch: 158 | Iteration: 21 | Classification loss: 0.00005 | Regression loss: 0.01415 | Running loss: 0.01303\n",
            "Epoch: 158 | Iteration: 22 | Classification loss: 0.00005 | Regression loss: 0.01054 | Running loss: 0.01304\n",
            "Epoch: 158 | Iteration: 23 | Classification loss: 0.00002 | Regression loss: 0.02009 | Running loss: 0.01307\n",
            "Epoch: 158 | Iteration: 24 | Classification loss: 0.00003 | Regression loss: 0.01389 | Running loss: 0.01306\n",
            "Epoch: 158 | Iteration: 25 | Classification loss: 0.00007 | Regression loss: 0.01882 | Running loss: 0.01308\n",
            "Epoch: 158 | Iteration: 26 | Classification loss: 0.00004 | Regression loss: 0.00695 | Running loss: 0.01306\n",
            "Epoch: 158 | Iteration: 27 | Classification loss: 0.00005 | Regression loss: 0.01926 | Running loss: 0.01306\n",
            "Epoch: 158 | Iteration: 28 | Classification loss: 0.00006 | Regression loss: 0.01689 | Running loss: 0.01306\n",
            "Epoch: 158 | Iteration: 29 | Classification loss: 0.00004 | Regression loss: 0.01803 | Running loss: 0.01307\n",
            "Epoch: 158 | Iteration: 30 | Classification loss: 0.00002 | Regression loss: 0.01417 | Running loss: 0.01309\n",
            "Epoch: 158 | Iteration: 31 | Classification loss: 0.00003 | Regression loss: 0.01294 | Running loss: 0.01310\n",
            "Epoch: 158 | Iteration: 32 | Classification loss: 0.00002 | Regression loss: 0.00525 | Running loss: 0.01310\n",
            "Epoch: 158 | Iteration: 33 | Classification loss: 0.00002 | Regression loss: 0.01040 | Running loss: 0.01310\n",
            "Epoch: 158 | Iteration: 34 | Classification loss: 0.00011 | Regression loss: 0.01189 | Running loss: 0.01306\n",
            "Epoch: 158 | Iteration: 35 | Classification loss: 0.00003 | Regression loss: 0.00770 | Running loss: 0.01305\n",
            "Epoch: 158 | Iteration: 36 | Classification loss: 0.00004 | Regression loss: 0.02620 | Running loss: 0.01309\n",
            "Epoch: 158 | Iteration: 37 | Classification loss: 0.00006 | Regression loss: 0.01211 | Running loss: 0.01308\n",
            "Epoch: 158 | Iteration: 38 | Classification loss: 0.00002 | Regression loss: 0.00431 | Running loss: 0.01306\n",
            "Epoch: 158 | Iteration: 39 | Classification loss: 0.00001 | Regression loss: 0.00492 | Running loss: 0.01304\n",
            "Epoch: 158 | Iteration: 40 | Classification loss: 0.00001 | Regression loss: 0.00348 | Running loss: 0.01301\n",
            "Epoch: 158 | Iteration: 41 | Classification loss: 0.00004 | Regression loss: 0.00855 | Running loss: 0.01297\n",
            "Epoch: 158 | Iteration: 42 | Classification loss: 0.00002 | Regression loss: 0.00637 | Running loss: 0.01298\n",
            "Epoch: 158 | Iteration: 43 | Classification loss: 0.00006 | Regression loss: 0.01215 | Running loss: 0.01298\n",
            "Epoch: 158 | Iteration: 44 | Classification loss: 0.00006 | Regression loss: 0.01359 | Running loss: 0.01295\n",
            "Epoch: 158 | Iteration: 45 | Classification loss: 0.00001 | Regression loss: 0.00740 | Running loss: 0.01296\n",
            "Epoch: 158 | Iteration: 46 | Classification loss: 0.00002 | Regression loss: 0.00519 | Running loss: 0.01294\n",
            "Epoch: 158 | Iteration: 47 | Classification loss: 0.00003 | Regression loss: 0.01501 | Running loss: 0.01293\n",
            "Epoch: 158 | Iteration: 48 | Classification loss: 0.00001 | Regression loss: 0.00570 | Running loss: 0.01293\n",
            "Epoch: 158 | Iteration: 49 | Classification loss: 0.00008 | Regression loss: 0.00591 | Running loss: 0.01292\n",
            "Epoch: 158 | Iteration: 50 | Classification loss: 0.00005 | Regression loss: 0.01526 | Running loss: 0.01293\n",
            "Epoch: 158 | Iteration: 51 | Classification loss: 0.00004 | Regression loss: 0.00788 | Running loss: 0.01292\n",
            "Epoch: 158 | Iteration: 52 | Classification loss: 0.00003 | Regression loss: 0.01320 | Running loss: 0.01292\n",
            "Epoch: 158 | Iteration: 53 | Classification loss: 0.00003 | Regression loss: 0.01147 | Running loss: 0.01292\n",
            "Epoch: 158 | Iteration: 54 | Classification loss: 0.00002 | Regression loss: 0.01166 | Running loss: 0.01290\n",
            "Epoch: 158 | Iteration: 55 | Classification loss: 0.00005 | Regression loss: 0.01206 | Running loss: 0.01289\n",
            "Epoch: 158 | Iteration: 56 | Classification loss: 0.00001 | Regression loss: 0.00993 | Running loss: 0.01289\n",
            "Epoch: 158 | Iteration: 57 | Classification loss: 0.00014 | Regression loss: 0.04762 | Running loss: 0.01295\n",
            "Epoch: 158 | Iteration: 58 | Classification loss: 0.00003 | Regression loss: 0.01142 | Running loss: 0.01293\n",
            "Epoch: 158 | Iteration: 59 | Classification loss: 0.00007 | Regression loss: 0.01123 | Running loss: 0.01292\n",
            "Epoch: 158 | Iteration: 60 | Classification loss: 0.00003 | Regression loss: 0.01379 | Running loss: 0.01292\n",
            "Epoch: 158 | Iteration: 61 | Classification loss: 0.00001 | Regression loss: 0.01085 | Running loss: 0.01293\n",
            "Epoch: 158 | Iteration: 62 | Classification loss: 0.00003 | Regression loss: 0.01283 | Running loss: 0.01293\n",
            "Epoch: 158 | Iteration: 63 | Classification loss: 0.00003 | Regression loss: 0.01229 | Running loss: 0.01293\n",
            "Epoch: 158 | Iteration: 64 | Classification loss: 0.00011 | Regression loss: 0.01528 | Running loss: 0.01291\n",
            "Epoch: 158 | Iteration: 65 | Classification loss: 0.00005 | Regression loss: 0.00669 | Running loss: 0.01292\n",
            "Epoch: 158 | Iteration: 66 | Classification loss: 0.00003 | Regression loss: 0.01784 | Running loss: 0.01295\n",
            "Epoch: 158 | Iteration: 67 | Classification loss: 0.00006 | Regression loss: 0.01631 | Running loss: 0.01296\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 188.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 158 | Iteration: 69 | Classification loss: 0.00001 | Regression loss: 0.00387 | Running loss: 0.01294\n",
            "Epoch: 158 | Iteration: 70 | Classification loss: 0.00004 | Regression loss: 0.00553 | Running loss: 0.01292\n",
            "Epoch: 158 | Iteration: 71 | Classification loss: 0.00004 | Regression loss: 0.01318 | Running loss: 0.01292\n",
            "Epoch: 158 | Iteration: 72 | Classification loss: 0.00002 | Regression loss: 0.01813 | Running loss: 0.01292\n",
            "Epoch: 158 | Iteration: 73 | Classification loss: 0.00002 | Regression loss: 0.01358 | Running loss: 0.01291\n",
            "Epoch: 158 | Iteration: 74 | Classification loss: 0.00003 | Regression loss: 0.00565 | Running loss: 0.01291\n",
            "Epoch: 158 | Iteration: 75 | Classification loss: 0.00004 | Regression loss: 0.01778 | Running loss: 0.01292\n",
            "Epoch: 158 | Iteration: 76 | Classification loss: 0.00009 | Regression loss: 0.01728 | Running loss: 0.01294\n",
            "Epoch: 158 | Iteration: 77 | Classification loss: 0.00002 | Regression loss: 0.03073 | Running loss: 0.01298\n",
            "Epoch: 158 | Iteration: 78 | Classification loss: 0.00003 | Regression loss: 0.01305 | Running loss: 0.01299\n",
            "Epoch: 158 | Iteration: 79 | Classification loss: 0.00001 | Regression loss: 0.00811 | Running loss: 0.01299\n",
            "Epoch: 158 | Iteration: 80 | Classification loss: 0.00002 | Regression loss: 0.00694 | Running loss: 0.01298\n",
            "Epoch: 158 | Iteration: 81 | Classification loss: 0.00004 | Regression loss: 0.02610 | Running loss: 0.01301\n",
            "Epoch: 158 | Iteration: 82 | Classification loss: 0.00002 | Regression loss: 0.00427 | Running loss: 0.01299\n",
            "Epoch: 158 | Iteration: 83 | Classification loss: 0.00002 | Regression loss: 0.00437 | Running loss: 0.01298\n",
            "Evaluating dataset\n",
            "\n",
            "mAP:\n",
            "person: 0.739092434551302\n",
            "Precision:  0.5227272727272727\n",
            "Recall:  0.8181818181818182\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}, {0: (0.7314384456550078, 253.0)}, {0: (0.7334466242447818, 253.0)}, {0: (0.7357024222917343, 253.0)}, {0: (0.7381550871546381, 253.0)}, {0: (0.739092434551302, 253.0)}]\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 188.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 159 | Iteration: 1 | Classification loss: 0.00004 | Regression loss: 0.01033 | Running loss: 0.01298\n",
            "Epoch: 159 | Iteration: 2 | Classification loss: 0.00003 | Regression loss: 0.00554 | Running loss: 0.01296\n",
            "Epoch: 159 | Iteration: 3 | Classification loss: 0.00002 | Regression loss: 0.02056 | Running loss: 0.01299\n",
            "Epoch: 159 | Iteration: 4 | Classification loss: 0.00005 | Regression loss: 0.01133 | Running loss: 0.01299\n",
            "Epoch: 159 | Iteration: 5 | Classification loss: 0.00003 | Regression loss: 0.00756 | Running loss: 0.01297\n",
            "Epoch: 159 | Iteration: 6 | Classification loss: 0.00001 | Regression loss: 0.00756 | Running loss: 0.01296\n",
            "Epoch: 159 | Iteration: 7 | Classification loss: 0.00001 | Regression loss: 0.00578 | Running loss: 0.01293\n",
            "Epoch: 159 | Iteration: 8 | Classification loss: 0.00002 | Regression loss: 0.01690 | Running loss: 0.01293\n",
            "Epoch: 159 | Iteration: 9 | Classification loss: 0.00005 | Regression loss: 0.02808 | Running loss: 0.01297\n",
            "Epoch: 159 | Iteration: 10 | Classification loss: 0.00002 | Regression loss: 0.01697 | Running loss: 0.01298\n",
            "Epoch: 159 | Iteration: 11 | Classification loss: 0.00005 | Regression loss: 0.01492 | Running loss: 0.01298\n",
            "Epoch: 159 | Iteration: 12 | Classification loss: 0.00002 | Regression loss: 0.00719 | Running loss: 0.01297\n",
            "Epoch: 159 | Iteration: 13 | Classification loss: 0.00007 | Regression loss: 0.02174 | Running loss: 0.01299\n",
            "Epoch: 159 | Iteration: 14 | Classification loss: 0.00015 | Regression loss: 0.04756 | Running loss: 0.01306\n",
            "Epoch: 159 | Iteration: 15 | Classification loss: 0.00007 | Regression loss: 0.01762 | Running loss: 0.01307\n",
            "Epoch: 159 | Iteration: 16 | Classification loss: 0.00001 | Regression loss: 0.03035 | Running loss: 0.01310\n",
            "Epoch: 159 | Iteration: 17 | Classification loss: 0.00006 | Regression loss: 0.01601 | Running loss: 0.01311\n",
            "Epoch: 159 | Iteration: 18 | Classification loss: 0.00002 | Regression loss: 0.00613 | Running loss: 0.01309\n",
            "Epoch: 159 | Iteration: 19 | Classification loss: 0.00009 | Regression loss: 0.01515 | Running loss: 0.01309\n",
            "Epoch: 159 | Iteration: 20 | Classification loss: 0.00002 | Regression loss: 0.01539 | Running loss: 0.01308\n",
            "Epoch: 159 | Iteration: 21 | Classification loss: 0.00004 | Regression loss: 0.00729 | Running loss: 0.01307\n",
            "Epoch: 159 | Iteration: 22 | Classification loss: 0.00001 | Regression loss: 0.00920 | Running loss: 0.01307\n",
            "Epoch: 159 | Iteration: 23 | Classification loss: 0.00005 | Regression loss: 0.01744 | Running loss: 0.01309\n",
            "Epoch: 159 | Iteration: 24 | Classification loss: 0.00001 | Regression loss: 0.00930 | Running loss: 0.01310\n",
            "Epoch: 159 | Iteration: 25 | Classification loss: 0.00002 | Regression loss: 0.01955 | Running loss: 0.01309\n",
            "Epoch: 159 | Iteration: 26 | Classification loss: 0.00001 | Regression loss: 0.00503 | Running loss: 0.01307\n",
            "Epoch: 159 | Iteration: 27 | Classification loss: 0.00002 | Regression loss: 0.00429 | Running loss: 0.01307\n",
            "Epoch: 159 | Iteration: 28 | Classification loss: 0.00004 | Regression loss: 0.01354 | Running loss: 0.01308\n",
            "Epoch: 159 | Iteration: 29 | Classification loss: 0.00004 | Regression loss: 0.01804 | Running loss: 0.01310\n",
            "Epoch: 159 | Iteration: 30 | Classification loss: 0.00004 | Regression loss: 0.00935 | Running loss: 0.01308\n",
            "Epoch: 159 | Iteration: 31 | Classification loss: 0.00004 | Regression loss: 0.00645 | Running loss: 0.01307\n",
            "Epoch: 159 | Iteration: 32 | Classification loss: 0.00006 | Regression loss: 0.01458 | Running loss: 0.01307\n",
            "Epoch: 159 | Iteration: 33 | Classification loss: 0.00009 | Regression loss: 0.01383 | Running loss: 0.01306\n",
            "Epoch: 159 | Iteration: 34 | Classification loss: 0.00002 | Regression loss: 0.00415 | Running loss: 0.01304\n",
            "Epoch: 159 | Iteration: 35 | Classification loss: 0.00006 | Regression loss: 0.01037 | Running loss: 0.01303\n",
            "Epoch: 159 | Iteration: 36 | Classification loss: 0.00022 | Regression loss: 0.00956 | Running loss: 0.01301\n",
            "Epoch: 159 | Iteration: 37 | Classification loss: 0.00004 | Regression loss: 0.00529 | Running loss: 0.01301\n",
            "Epoch: 159 | Iteration: 38 | Classification loss: 0.00004 | Regression loss: 0.00339 | Running loss: 0.01299\n",
            "Epoch: 159 | Iteration: 39 | Classification loss: 0.00002 | Regression loss: 0.03208 | Running loss: 0.01304\n",
            "Epoch: 159 | Iteration: 40 | Classification loss: 0.00010 | Regression loss: 0.01058 | Running loss: 0.01305\n",
            "Epoch: 159 | Iteration: 41 | Classification loss: 0.00006 | Regression loss: 0.01766 | Running loss: 0.01306\n",
            "Epoch: 159 | Iteration: 42 | Classification loss: 0.00005 | Regression loss: 0.01811 | Running loss: 0.01306\n",
            "Epoch: 159 | Iteration: 43 | Classification loss: 0.00004 | Regression loss: 0.00779 | Running loss: 0.01307\n",
            "Epoch: 159 | Iteration: 44 | Classification loss: 0.00014 | Regression loss: 0.02029 | Running loss: 0.01307\n",
            "Epoch: 159 | Iteration: 45 | Classification loss: 0.00004 | Regression loss: 0.01034 | Running loss: 0.01306\n",
            "Epoch: 159 | Iteration: 46 | Classification loss: 0.00002 | Regression loss: 0.01322 | Running loss: 0.01306\n",
            "Epoch: 159 | Iteration: 47 | Classification loss: 0.00007 | Regression loss: 0.01102 | Running loss: 0.01307\n",
            "Epoch: 159 | Iteration: 48 | Classification loss: 0.00003 | Regression loss: 0.01368 | Running loss: 0.01303\n",
            "Epoch: 159 | Iteration: 49 | Classification loss: 0.00001 | Regression loss: 0.00579 | Running loss: 0.01303\n",
            "Epoch: 159 | Iteration: 50 | Classification loss: 0.00003 | Regression loss: 0.01172 | Running loss: 0.01302\n",
            "Epoch: 159 | Iteration: 51 | Classification loss: 0.00001 | Regression loss: 0.01075 | Running loss: 0.01302\n",
            "Epoch: 159 | Iteration: 52 | Classification loss: 0.00002 | Regression loss: 0.00738 | Running loss: 0.01301\n",
            "Epoch: 159 | Iteration: 53 | Classification loss: 0.00004 | Regression loss: 0.01523 | Running loss: 0.01302\n",
            "Epoch: 159 | Iteration: 54 | Classification loss: 0.00004 | Regression loss: 0.02125 | Running loss: 0.01300\n",
            "Epoch: 159 | Iteration: 55 | Classification loss: 0.00003 | Regression loss: 0.01320 | Running loss: 0.01301\n",
            "Epoch: 159 | Iteration: 56 | Classification loss: 0.00008 | Regression loss: 0.00450 | Running loss: 0.01300\n",
            "Epoch: 159 | Iteration: 57 | Classification loss: 0.00002 | Regression loss: 0.00699 | Running loss: 0.01299\n",
            "Epoch: 159 | Iteration: 58 | Classification loss: 0.00001 | Regression loss: 0.00779 | Running loss: 0.01299\n",
            "Epoch: 159 | Iteration: 59 | Classification loss: 0.00009 | Regression loss: 0.01466 | Running loss: 0.01292\n",
            "Epoch: 159 | Iteration: 60 | Classification loss: 0.00001 | Regression loss: 0.00387 | Running loss: 0.01291\n",
            "Epoch: 159 | Iteration: 61 | Classification loss: 0.00004 | Regression loss: 0.01707 | Running loss: 0.01293\n",
            "Epoch: 159 | Iteration: 62 | Classification loss: 0.00002 | Regression loss: 0.01248 | Running loss: 0.01293\n",
            "Epoch: 159 | Iteration: 63 | Classification loss: 0.00004 | Regression loss: 0.01494 | Running loss: 0.01295\n",
            "Epoch: 159 | Iteration: 64 | Classification loss: 0.00003 | Regression loss: 0.01238 | Running loss: 0.01294\n",
            "Epoch: 159 | Iteration: 65 | Classification loss: 0.00002 | Regression loss: 0.00417 | Running loss: 0.01294\n",
            "Epoch: 159 | Iteration: 66 | Classification loss: 0.00003 | Regression loss: 0.01552 | Running loss: 0.01295\n",
            "Epoch: 159 | Iteration: 67 | Classification loss: 0.00003 | Regression loss: 0.01419 | Running loss: 0.01297\n",
            "Epoch: 159 | Iteration: 68 | Classification loss: 0.00003 | Regression loss: 0.01177 | Running loss: 0.01296\n",
            "Epoch: 159 | Iteration: 69 | Classification loss: 0.00007 | Regression loss: 0.01882 | Running loss: 0.01299\n",
            "Epoch: 159 | Iteration: 70 | Classification loss: 0.00004 | Regression loss: 0.01900 | Running loss: 0.01298\n",
            "Epoch: 159 | Iteration: 71 | Classification loss: 0.00015 | Regression loss: 0.01533 | Running loss: 0.01299\n",
            "Epoch: 159 | Iteration: 72 | Classification loss: 0.00002 | Regression loss: 0.01131 | Running loss: 0.01300\n",
            "Epoch: 159 | Iteration: 73 | Classification loss: 0.00002 | Regression loss: 0.00733 | Running loss: 0.01300\n",
            "Epoch: 159 | Iteration: 74 | Classification loss: 0.00004 | Regression loss: 0.01382 | Running loss: 0.01299\n",
            "Epoch: 159 | Iteration: 75 | Classification loss: 0.00002 | Regression loss: 0.01116 | Running loss: 0.01297\n",
            "Epoch: 159 | Iteration: 76 | Classification loss: 0.00003 | Regression loss: 0.01610 | Running loss: 0.01297\n",
            "Epoch: 159 | Iteration: 77 | Classification loss: 0.00001 | Regression loss: 0.00428 | Running loss: 0.01296\n",
            "Epoch: 159 | Iteration: 78 | Classification loss: 0.00001 | Regression loss: 0.00567 | Running loss: 0.01295\n",
            "Epoch: 159 | Iteration: 79 | Classification loss: 0.00003 | Regression loss: 0.01091 | Running loss: 0.01294\n",
            "Epoch: 159 | Iteration: 80 | Classification loss: 0.00001 | Regression loss: 0.00487 | Running loss: 0.01289\n",
            "Epoch: 159 | Iteration: 81 | Classification loss: 0.00002 | Regression loss: 0.00483 | Running loss: 0.01288\n",
            "Epoch: 159 | Iteration: 82 | Classification loss: 0.00002 | Regression loss: 0.01210 | Running loss: 0.01290\n",
            "Epoch: 159 | Iteration: 83 | Classification loss: 0.00002 | Regression loss: 0.01226 | Running loss: 0.01289\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.72091448182607\n",
            "Precision:  0.5282051282051282\n",
            "Recall:  0.8142292490118577\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}, {0: (0.7314384456550078, 253.0)}, {0: (0.7334466242447818, 253.0)}, {0: (0.7357024222917343, 253.0)}, {0: (0.7381550871546381, 253.0)}, {0: (0.739092434551302, 253.0)}, {0: (0.72091448182607, 253.0)}]\n",
            "Epoch: 160 | Iteration: 0 | Classification loss: 0.00002 | Regression loss: 0.00383 | Running loss: 0.01289\n",
            "Epoch: 160 | Iteration: 1 | Classification loss: 0.00002 | Regression loss: 0.00679 | Running loss: 0.01286\n",
            "Epoch: 160 | Iteration: 2 | Classification loss: 0.00001 | Regression loss: 0.00350 | Running loss: 0.01284\n",
            "Epoch: 160 | Iteration: 3 | Classification loss: 0.00003 | Regression loss: 0.01338 | Running loss: 0.01283\n",
            "Epoch: 160 | Iteration: 4 | Classification loss: 0.00002 | Regression loss: 0.01111 | Running loss: 0.01284\n",
            "Epoch: 160 | Iteration: 5 | Classification loss: 0.00002 | Regression loss: 0.01971 | Running loss: 0.01285\n",
            "Epoch: 160 | Iteration: 6 | Classification loss: 0.00001 | Regression loss: 0.00501 | Running loss: 0.01284\n",
            "Epoch: 160 | Iteration: 7 | Classification loss: 0.00004 | Regression loss: 0.01330 | Running loss: 0.01284\n",
            "Epoch: 160 | Iteration: 8 | Classification loss: 0.00002 | Regression loss: 0.01156 | Running loss: 0.01284\n",
            "Epoch: 160 | Iteration: 9 | Classification loss: 0.00003 | Regression loss: 0.00307 | Running loss: 0.01282\n",
            "Epoch: 160 | Iteration: 10 | Classification loss: 0.00001 | Regression loss: 0.00943 | Running loss: 0.01281\n",
            "Epoch: 160 | Iteration: 11 | Classification loss: 0.00004 | Regression loss: 0.01479 | Running loss: 0.01281\n",
            "Epoch: 160 | Iteration: 12 | Classification loss: 0.00003 | Regression loss: 0.01863 | Running loss: 0.01280\n",
            "Epoch: 160 | Iteration: 13 | Classification loss: 0.00002 | Regression loss: 0.01151 | Running loss: 0.01279\n",
            "Epoch: 160 | Iteration: 14 | Classification loss: 0.00003 | Regression loss: 0.01375 | Running loss: 0.01281\n",
            "Epoch: 160 | Iteration: 15 | Classification loss: 0.00003 | Regression loss: 0.00777 | Running loss: 0.01281\n",
            "Epoch: 160 | Iteration: 16 | Classification loss: 0.00015 | Regression loss: 0.01529 | Running loss: 0.01280\n",
            "Epoch: 160 | Iteration: 17 | Classification loss: 0.00019 | Regression loss: 0.00926 | Running loss: 0.01278\n",
            "Epoch: 160 | Iteration: 18 | Classification loss: 0.00007 | Regression loss: 0.01133 | Running loss: 0.01280\n",
            "Epoch: 160 | Iteration: 19 | Classification loss: 0.00001 | Regression loss: 0.00493 | Running loss: 0.01278\n",
            "Epoch: 160 | Iteration: 20 | Classification loss: 0.00001 | Regression loss: 0.00515 | Running loss: 0.01272\n",
            "Epoch: 160 | Iteration: 21 | Classification loss: 0.00002 | Regression loss: 0.03001 | Running loss: 0.01276\n",
            "Epoch: 160 | Iteration: 22 | Classification loss: 0.00002 | Regression loss: 0.01179 | Running loss: 0.01276\n",
            "Epoch: 160 | Iteration: 23 | Classification loss: 0.00002 | Regression loss: 0.00670 | Running loss: 0.01271\n",
            "Epoch: 160 | Iteration: 24 | Classification loss: 0.00005 | Regression loss: 0.01122 | Running loss: 0.01271\n",
            "Epoch: 160 | Iteration: 25 | Classification loss: 0.00003 | Regression loss: 0.02730 | Running loss: 0.01275\n",
            "Epoch: 160 | Iteration: 26 | Classification loss: 0.00011 | Regression loss: 0.01579 | Running loss: 0.01277\n",
            "Epoch: 160 | Iteration: 27 | Classification loss: 0.00013 | Regression loss: 0.04730 | Running loss: 0.01284\n",
            "Epoch: 160 | Iteration: 28 | Classification loss: 0.00003 | Regression loss: 0.00766 | Running loss: 0.01283\n",
            "Epoch: 160 | Iteration: 29 | Classification loss: 0.00004 | Regression loss: 0.01197 | Running loss: 0.01283\n",
            "Epoch: 160 | Iteration: 30 | Classification loss: 0.00010 | Regression loss: 0.01457 | Running loss: 0.01282\n",
            "Epoch: 160 | Iteration: 31 | Classification loss: 0.00006 | Regression loss: 0.01757 | Running loss: 0.01284\n",
            "Epoch: 160 | Iteration: 32 | Classification loss: 0.00004 | Regression loss: 0.01133 | Running loss: 0.01285\n",
            "Epoch: 160 | Iteration: 33 | Classification loss: 0.00007 | Regression loss: 0.01844 | Running loss: 0.01287\n",
            "Epoch: 160 | Iteration: 34 | Classification loss: 0.00002 | Regression loss: 0.03121 | Running loss: 0.01291\n",
            "Epoch: 160 | Iteration: 35 | Classification loss: 0.00002 | Regression loss: 0.01458 | Running loss: 0.01291\n",
            "Epoch: 160 | Iteration: 36 | Classification loss: 0.00002 | Regression loss: 0.01482 | Running loss: 0.01293\n",
            "Epoch: 160 | Iteration: 37 | Classification loss: 0.00005 | Regression loss: 0.01554 | Running loss: 0.01293\n",
            "Epoch: 160 | Iteration: 38 | Classification loss: 0.00004 | Regression loss: 0.01386 | Running loss: 0.01294\n",
            "Epoch: 160 | Iteration: 39 | Classification loss: 0.00005 | Regression loss: 0.01918 | Running loss: 0.01294\n",
            "Epoch: 160 | Iteration: 40 | Classification loss: 0.00018 | Regression loss: 0.02165 | Running loss: 0.01296\n",
            "Epoch: 160 | Iteration: 41 | Classification loss: 0.00003 | Regression loss: 0.01231 | Running loss: 0.01298\n",
            "Epoch: 160 | Iteration: 42 | Classification loss: 0.00003 | Regression loss: 0.01659 | Running loss: 0.01298\n",
            "Epoch: 160 | Iteration: 43 | Classification loss: 0.00002 | Regression loss: 0.01824 | Running loss: 0.01300\n",
            "Epoch: 160 | Iteration: 44 | Classification loss: 0.00002 | Regression loss: 0.00753 | Running loss: 0.01300\n",
            "Epoch: 160 | Iteration: 45 | Classification loss: 0.00001 | Regression loss: 0.00915 | Running loss: 0.01300\n",
            "Epoch: 160 | Iteration: 46 | Classification loss: 0.00002 | Regression loss: 0.01532 | Running loss: 0.01299\n",
            "Epoch: 160 | Iteration: 47 | Classification loss: 0.00002 | Regression loss: 0.00574 | Running loss: 0.01298\n",
            "Epoch: 160 | Iteration: 48 | Classification loss: 0.00009 | Regression loss: 0.01075 | Running loss: 0.01298\n",
            "Epoch: 160 | Iteration: 49 | Classification loss: 0.00002 | Regression loss: 0.02418 | Running loss: 0.01302\n",
            "Epoch: 160 | Iteration: 50 | Classification loss: 0.00008 | Regression loss: 0.01456 | Running loss: 0.01303\n",
            "Epoch: 160 | Iteration: 51 | Classification loss: 0.00002 | Regression loss: 0.00490 | Running loss: 0.01303\n",
            "Epoch: 160 | Iteration: 52 | Classification loss: 0.00002 | Regression loss: 0.00821 | Running loss: 0.01302\n",
            "Epoch: 160 | Iteration: 53 | Classification loss: 0.00008 | Regression loss: 0.00478 | Running loss: 0.01299\n",
            "Epoch: 160 | Iteration: 54 | Classification loss: 0.00003 | Regression loss: 0.01821 | Running loss: 0.01302\n",
            "Epoch: 160 | Iteration: 55 | Classification loss: 0.00003 | Regression loss: 0.00933 | Running loss: 0.01302\n",
            "Epoch: 160 | Iteration: 56 | Classification loss: 0.00002 | Regression loss: 0.00419 | Running loss: 0.01300\n",
            "Epoch: 160 | Iteration: 57 | Classification loss: 0.00002 | Regression loss: 0.01289 | Running loss: 0.01301\n",
            "Epoch: 160 | Iteration: 58 | Classification loss: 0.00006 | Regression loss: 0.01055 | Running loss: 0.01293\n",
            "Epoch: 160 | Iteration: 59 | Classification loss: 0.00001 | Regression loss: 0.00410 | Running loss: 0.01290\n",
            "Epoch: 160 | Iteration: 60 | Classification loss: 0.00006 | Regression loss: 0.01903 | Running loss: 0.01293\n",
            "Epoch: 160 | Iteration: 61 | Classification loss: 0.00001 | Regression loss: 0.00439 | Running loss: 0.01289\n",
            "Epoch: 160 | Iteration: 62 | Classification loss: 0.00004 | Regression loss: 0.00810 | Running loss: 0.01288\n",
            "Epoch: 160 | Iteration: 63 | Classification loss: 0.00003 | Regression loss: 0.00537 | Running loss: 0.01288\n",
            "Epoch: 160 | Iteration: 64 | Classification loss: 0.00004 | Regression loss: 0.01546 | Running loss: 0.01289\n",
            "Epoch: 160 | Iteration: 65 | Classification loss: 0.00006 | Regression loss: 0.01548 | Running loss: 0.01287\n",
            "Epoch: 160 | Iteration: 66 | Classification loss: 0.00002 | Regression loss: 0.01314 | Running loss: 0.01288\n",
            "Epoch: 160 | Iteration: 67 | Classification loss: 0.00004 | Regression loss: 0.01608 | Running loss: 0.01290\n",
            "Epoch: 160 | Iteration: 68 | Classification loss: 0.00001 | Regression loss: 0.00784 | Running loss: 0.01287\n",
            "Epoch: 160 | Iteration: 69 | Classification loss: 0.00003 | Regression loss: 0.01133 | Running loss: 0.01288\n",
            "Epoch: 160 | Iteration: 70 | Classification loss: 0.00003 | Regression loss: 0.01518 | Running loss: 0.01285\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 188.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 160 | Iteration: 72 | Classification loss: 0.00004 | Regression loss: 0.00728 | Running loss: 0.01284\n",
            "Epoch: 160 | Iteration: 73 | Classification loss: 0.00001 | Regression loss: 0.00476 | Running loss: 0.01284\n",
            "Epoch: 160 | Iteration: 74 | Classification loss: 0.00003 | Regression loss: 0.00595 | Running loss: 0.01281\n",
            "Epoch: 160 | Iteration: 75 | Classification loss: 0.00004 | Regression loss: 0.01802 | Running loss: 0.01281\n",
            "Epoch: 160 | Iteration: 76 | Classification loss: 0.00007 | Regression loss: 0.02265 | Running loss: 0.01283\n",
            "Epoch: 160 | Iteration: 77 | Classification loss: 0.00003 | Regression loss: 0.01093 | Running loss: 0.01282\n",
            "Epoch: 160 | Iteration: 78 | Classification loss: 0.00002 | Regression loss: 0.00753 | Running loss: 0.01281\n",
            "Epoch: 160 | Iteration: 79 | Classification loss: 0.00005 | Regression loss: 0.01210 | Running loss: 0.01282\n",
            "Epoch: 160 | Iteration: 80 | Classification loss: 0.00001 | Regression loss: 0.01043 | Running loss: 0.01283\n",
            "Epoch: 160 | Iteration: 81 | Classification loss: 0.00003 | Regression loss: 0.00465 | Running loss: 0.01281\n",
            "Epoch: 160 | Iteration: 82 | Classification loss: 0.00003 | Regression loss: 0.00811 | Running loss: 0.01280\n",
            "Epoch: 160 | Iteration: 83 | Classification loss: 0.00003 | Regression loss: 0.01963 | Running loss: 0.01281\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.7389512050165785\n",
            "Precision:  0.5253807106598984\n",
            "Recall:  0.8181818181818182\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}, {0: (0.7314384456550078, 253.0)}, {0: (0.7334466242447818, 253.0)}, {0: (0.7357024222917343, 253.0)}, {0: (0.7381550871546381, 253.0)}, {0: (0.739092434551302, 253.0)}, {0: (0.72091448182607, 253.0)}, {0: (0.7389512050165785, 253.0)}]\n",
            "Epoch: 161 | Iteration: 0 | Classification loss: 0.00001 | Regression loss: 0.02980 | Running loss: 0.01285\n",
            "Epoch: 161 | Iteration: 1 | Classification loss: 0.00018 | Regression loss: 0.00823 | Running loss: 0.01283\n",
            "Epoch: 161 | Iteration: 2 | Classification loss: 0.00002 | Regression loss: 0.01237 | Running loss: 0.01282\n",
            "Epoch: 161 | Iteration: 3 | Classification loss: 0.00003 | Regression loss: 0.00930 | Running loss: 0.01283\n",
            "Epoch: 161 | Iteration: 4 | Classification loss: 0.00002 | Regression loss: 0.01602 | Running loss: 0.01283\n",
            "Epoch: 161 | Iteration: 5 | Classification loss: 0.00002 | Regression loss: 0.01347 | Running loss: 0.01282\n",
            "Epoch: 161 | Iteration: 6 | Classification loss: 0.00004 | Regression loss: 0.01716 | Running loss: 0.01283\n",
            "Epoch: 161 | Iteration: 7 | Classification loss: 0.00005 | Regression loss: 0.02793 | Running loss: 0.01287\n",
            "Epoch: 161 | Iteration: 8 | Classification loss: 0.00003 | Regression loss: 0.00583 | Running loss: 0.01285\n",
            "Epoch: 161 | Iteration: 9 | Classification loss: 0.00001 | Regression loss: 0.00589 | Running loss: 0.01286\n",
            "Epoch: 161 | Iteration: 10 | Classification loss: 0.00014 | Regression loss: 0.01387 | Running loss: 0.01286\n",
            "Epoch: 161 | Iteration: 11 | Classification loss: 0.00007 | Regression loss: 0.01712 | Running loss: 0.01289\n",
            "Epoch: 161 | Iteration: 12 | Classification loss: 0.00008 | Regression loss: 0.00460 | Running loss: 0.01286\n",
            "Epoch: 161 | Iteration: 13 | Classification loss: 0.00003 | Regression loss: 0.00548 | Running loss: 0.01286\n",
            "Epoch: 161 | Iteration: 14 | Classification loss: 0.00002 | Regression loss: 0.01907 | Running loss: 0.01288\n",
            "Epoch: 161 | Iteration: 15 | Classification loss: 0.00010 | Regression loss: 0.01549 | Running loss: 0.01290\n",
            "Epoch: 161 | Iteration: 16 | Classification loss: 0.00003 | Regression loss: 0.01349 | Running loss: 0.01291\n",
            "Epoch: 161 | Iteration: 17 | Classification loss: 0.00001 | Regression loss: 0.00418 | Running loss: 0.01289\n",
            "Epoch: 161 | Iteration: 18 | Classification loss: 0.00002 | Regression loss: 0.00779 | Running loss: 0.01289\n",
            "Epoch: 161 | Iteration: 19 | Classification loss: 0.00001 | Regression loss: 0.00766 | Running loss: 0.01290\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 188.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 161 | Iteration: 21 | Classification loss: 0.00001 | Regression loss: 0.00486 | Running loss: 0.01288\n",
            "Epoch: 161 | Iteration: 22 | Classification loss: 0.00007 | Regression loss: 0.01785 | Running loss: 0.01289\n",
            "Epoch: 161 | Iteration: 23 | Classification loss: 0.00008 | Regression loss: 0.01175 | Running loss: 0.01290\n",
            "Epoch: 161 | Iteration: 24 | Classification loss: 0.00010 | Regression loss: 0.01414 | Running loss: 0.01289\n",
            "Epoch: 161 | Iteration: 25 | Classification loss: 0.00007 | Regression loss: 0.02216 | Running loss: 0.01292\n",
            "Epoch: 161 | Iteration: 26 | Classification loss: 0.00003 | Regression loss: 0.01799 | Running loss: 0.01294\n",
            "Epoch: 161 | Iteration: 27 | Classification loss: 0.00002 | Regression loss: 0.00523 | Running loss: 0.01293\n",
            "Epoch: 161 | Iteration: 28 | Classification loss: 0.00001 | Regression loss: 0.00359 | Running loss: 0.01291\n",
            "Epoch: 161 | Iteration: 29 | Classification loss: 0.00004 | Regression loss: 0.01160 | Running loss: 0.01291\n",
            "Epoch: 161 | Iteration: 30 | Classification loss: 0.00004 | Regression loss: 0.01838 | Running loss: 0.01292\n",
            "Epoch: 161 | Iteration: 31 | Classification loss: 0.00001 | Regression loss: 0.01118 | Running loss: 0.01292\n",
            "Epoch: 161 | Iteration: 32 | Classification loss: 0.00005 | Regression loss: 0.02766 | Running loss: 0.01295\n",
            "Epoch: 161 | Iteration: 33 | Classification loss: 0.00007 | Regression loss: 0.01714 | Running loss: 0.01297\n",
            "Epoch: 161 | Iteration: 34 | Classification loss: 0.00002 | Regression loss: 0.01356 | Running loss: 0.01298\n",
            "Epoch: 161 | Iteration: 35 | Classification loss: 0.00003 | Regression loss: 0.01447 | Running loss: 0.01299\n",
            "Epoch: 161 | Iteration: 36 | Classification loss: 0.00005 | Regression loss: 0.00753 | Running loss: 0.01299\n",
            "Epoch: 161 | Iteration: 37 | Classification loss: 0.00004 | Regression loss: 0.00735 | Running loss: 0.01298\n",
            "Epoch: 161 | Iteration: 38 | Classification loss: 0.00003 | Regression loss: 0.00761 | Running loss: 0.01295\n",
            "Epoch: 161 | Iteration: 39 | Classification loss: 0.00017 | Regression loss: 0.04805 | Running loss: 0.01303\n",
            "Epoch: 161 | Iteration: 40 | Classification loss: 0.00005 | Regression loss: 0.01103 | Running loss: 0.01301\n",
            "Epoch: 161 | Iteration: 41 | Classification loss: 0.00004 | Regression loss: 0.01112 | Running loss: 0.01301\n",
            "Epoch: 161 | Iteration: 42 | Classification loss: 0.00003 | Regression loss: 0.01259 | Running loss: 0.01301\n",
            "Epoch: 161 | Iteration: 43 | Classification loss: 0.00002 | Regression loss: 0.01057 | Running loss: 0.01301\n",
            "Epoch: 161 | Iteration: 44 | Classification loss: 0.00003 | Regression loss: 0.01788 | Running loss: 0.01298\n",
            "Epoch: 161 | Iteration: 45 | Classification loss: 0.00004 | Regression loss: 0.00307 | Running loss: 0.01295\n",
            "Epoch: 161 | Iteration: 46 | Classification loss: 0.00004 | Regression loss: 0.01303 | Running loss: 0.01293\n",
            "Epoch: 161 | Iteration: 47 | Classification loss: 0.00004 | Regression loss: 0.00976 | Running loss: 0.01294\n",
            "Epoch: 161 | Iteration: 48 | Classification loss: 0.00003 | Regression loss: 0.01230 | Running loss: 0.01293\n",
            "Epoch: 161 | Iteration: 49 | Classification loss: 0.00001 | Regression loss: 0.01038 | Running loss: 0.01292\n",
            "Epoch: 161 | Iteration: 50 | Classification loss: 0.00004 | Regression loss: 0.00534 | Running loss: 0.01290\n",
            "Epoch: 161 | Iteration: 51 | Classification loss: 0.00001 | Regression loss: 0.00532 | Running loss: 0.01288\n",
            "Epoch: 161 | Iteration: 52 | Classification loss: 0.00009 | Regression loss: 0.01121 | Running loss: 0.01286\n",
            "Epoch: 161 | Iteration: 53 | Classification loss: 0.00002 | Regression loss: 0.01199 | Running loss: 0.01279\n",
            "Epoch: 161 | Iteration: 54 | Classification loss: 0.00002 | Regression loss: 0.00655 | Running loss: 0.01278\n",
            "Epoch: 161 | Iteration: 55 | Classification loss: 0.00004 | Regression loss: 0.01516 | Running loss: 0.01280\n",
            "Epoch: 161 | Iteration: 56 | Classification loss: 0.00002 | Regression loss: 0.00533 | Running loss: 0.01280\n",
            "Epoch: 161 | Iteration: 57 | Classification loss: 0.00005 | Regression loss: 0.01492 | Running loss: 0.01282\n",
            "Epoch: 161 | Iteration: 58 | Classification loss: 0.00004 | Regression loss: 0.01084 | Running loss: 0.01281\n",
            "Epoch: 161 | Iteration: 59 | Classification loss: 0.00002 | Regression loss: 0.00644 | Running loss: 0.01279\n",
            "Epoch: 161 | Iteration: 60 | Classification loss: 0.00002 | Regression loss: 0.01371 | Running loss: 0.01279\n",
            "Epoch: 161 | Iteration: 61 | Classification loss: 0.00001 | Regression loss: 0.00409 | Running loss: 0.01277\n",
            "Epoch: 161 | Iteration: 62 | Classification loss: 0.00003 | Regression loss: 0.01615 | Running loss: 0.01276\n",
            "Epoch: 161 | Iteration: 63 | Classification loss: 0.00012 | Regression loss: 0.01864 | Running loss: 0.01277\n",
            "Epoch: 161 | Iteration: 64 | Classification loss: 0.00001 | Regression loss: 0.00907 | Running loss: 0.01276\n",
            "Epoch: 161 | Iteration: 65 | Classification loss: 0.00003 | Regression loss: 0.01605 | Running loss: 0.01277\n",
            "Epoch: 161 | Iteration: 66 | Classification loss: 0.00002 | Regression loss: 0.00461 | Running loss: 0.01275\n",
            "Epoch: 161 | Iteration: 67 | Classification loss: 0.00002 | Regression loss: 0.00433 | Running loss: 0.01274\n",
            "Epoch: 161 | Iteration: 68 | Classification loss: 0.00003 | Regression loss: 0.01464 | Running loss: 0.01271\n",
            "Epoch: 161 | Iteration: 69 | Classification loss: 0.00002 | Regression loss: 0.00724 | Running loss: 0.01270\n",
            "Epoch: 161 | Iteration: 70 | Classification loss: 0.00004 | Regression loss: 0.01658 | Running loss: 0.01271\n",
            "Epoch: 161 | Iteration: 71 | Classification loss: 0.00002 | Regression loss: 0.03145 | Running loss: 0.01274\n",
            "Epoch: 161 | Iteration: 72 | Classification loss: 0.00002 | Regression loss: 0.00688 | Running loss: 0.01272\n",
            "Epoch: 161 | Iteration: 73 | Classification loss: 0.00005 | Regression loss: 0.01196 | Running loss: 0.01271\n",
            "Epoch: 161 | Iteration: 74 | Classification loss: 0.00004 | Regression loss: 0.01499 | Running loss: 0.01271\n",
            "Epoch: 161 | Iteration: 75 | Classification loss: 0.00002 | Regression loss: 0.00634 | Running loss: 0.01270\n",
            "Epoch: 161 | Iteration: 76 | Classification loss: 0.00002 | Regression loss: 0.01353 | Running loss: 0.01268\n",
            "Epoch: 161 | Iteration: 77 | Classification loss: 0.00002 | Regression loss: 0.01171 | Running loss: 0.01269\n",
            "Epoch: 161 | Iteration: 78 | Classification loss: 0.00002 | Regression loss: 0.01961 | Running loss: 0.01272\n",
            "Epoch: 161 | Iteration: 79 | Classification loss: 0.00006 | Regression loss: 0.01707 | Running loss: 0.01273\n",
            "Epoch: 161 | Iteration: 80 | Classification loss: 0.00001 | Regression loss: 0.00421 | Running loss: 0.01273\n",
            "Epoch: 161 | Iteration: 81 | Classification loss: 0.00002 | Regression loss: 0.01313 | Running loss: 0.01272\n",
            "Epoch: 161 | Iteration: 82 | Classification loss: 0.00003 | Regression loss: 0.01121 | Running loss: 0.01273\n",
            "Epoch: 161 | Iteration: 83 | Classification loss: 0.00005 | Regression loss: 0.01721 | Running loss: 0.01274\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.73050932626622\n",
            "Precision:  0.5227272727272727\n",
            "Recall:  0.8181818181818182\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}, {0: (0.7314384456550078, 253.0)}, {0: (0.7334466242447818, 253.0)}, {0: (0.7357024222917343, 253.0)}, {0: (0.7381550871546381, 253.0)}, {0: (0.739092434551302, 253.0)}, {0: (0.72091448182607, 253.0)}, {0: (0.7389512050165785, 253.0)}, {0: (0.73050932626622, 253.0)}]\n",
            "Epoch: 162 | Iteration: 0 | Classification loss: 0.00001 | Regression loss: 0.01043 | Running loss: 0.01273\n",
            "Epoch: 162 | Iteration: 1 | Classification loss: 0.00005 | Regression loss: 0.01285 | Running loss: 0.01275\n",
            "Epoch: 162 | Iteration: 2 | Classification loss: 0.00002 | Regression loss: 0.00595 | Running loss: 0.01273\n",
            "Epoch: 162 | Iteration: 3 | Classification loss: 0.00005 | Regression loss: 0.01643 | Running loss: 0.01273\n",
            "Epoch: 162 | Iteration: 4 | Classification loss: 0.00002 | Regression loss: 0.01378 | Running loss: 0.01273\n",
            "Epoch: 162 | Iteration: 5 | Classification loss: 0.00002 | Regression loss: 0.00901 | Running loss: 0.01273\n",
            "Epoch: 162 | Iteration: 6 | Classification loss: 0.00002 | Regression loss: 0.01309 | Running loss: 0.01274\n",
            "Epoch: 162 | Iteration: 7 | Classification loss: 0.00010 | Regression loss: 0.01497 | Running loss: 0.01273\n",
            "Epoch: 162 | Iteration: 8 | Classification loss: 0.00002 | Regression loss: 0.01343 | Running loss: 0.01272\n",
            "Epoch: 162 | Iteration: 9 | Classification loss: 0.00006 | Regression loss: 0.01652 | Running loss: 0.01274\n",
            "Epoch: 162 | Iteration: 10 | Classification loss: 0.00001 | Regression loss: 0.01768 | Running loss: 0.01276\n",
            "Epoch: 162 | Iteration: 11 | Classification loss: 0.00002 | Regression loss: 0.01594 | Running loss: 0.01277\n",
            "Epoch: 162 | Iteration: 12 | Classification loss: 0.00003 | Regression loss: 0.02663 | Running loss: 0.01282\n",
            "Epoch: 162 | Iteration: 13 | Classification loss: 0.00003 | Regression loss: 0.01159 | Running loss: 0.01281\n",
            "Epoch: 162 | Iteration: 14 | Classification loss: 0.00002 | Regression loss: 0.01160 | Running loss: 0.01282\n",
            "Epoch: 162 | Iteration: 15 | Classification loss: 0.00018 | Regression loss: 0.00827 | Running loss: 0.01281\n",
            "Epoch: 162 | Iteration: 16 | Classification loss: 0.00003 | Regression loss: 0.01199 | Running loss: 0.01282\n",
            "Epoch: 162 | Iteration: 17 | Classification loss: 0.00007 | Regression loss: 0.00416 | Running loss: 0.01281\n",
            "Epoch: 162 | Iteration: 18 | Classification loss: 0.00002 | Regression loss: 0.01705 | Running loss: 0.01283\n",
            "Epoch: 162 | Iteration: 19 | Classification loss: 0.00007 | Regression loss: 0.01176 | Running loss: 0.01282\n",
            "Epoch: 162 | Iteration: 20 | Classification loss: 0.00014 | Regression loss: 0.01579 | Running loss: 0.01282\n",
            "Epoch: 162 | Iteration: 21 | Classification loss: 0.00004 | Regression loss: 0.01410 | Running loss: 0.01283\n",
            "Epoch: 162 | Iteration: 22 | Classification loss: 0.00014 | Regression loss: 0.04739 | Running loss: 0.01288\n",
            "Epoch: 162 | Iteration: 23 | Classification loss: 0.00003 | Regression loss: 0.02521 | Running loss: 0.01291\n",
            "Epoch: 162 | Iteration: 24 | Classification loss: 0.00002 | Regression loss: 0.00682 | Running loss: 0.01289\n",
            "Epoch: 162 | Iteration: 25 | Classification loss: 0.00003 | Regression loss: 0.00916 | Running loss: 0.01288\n",
            "Epoch: 162 | Iteration: 26 | Classification loss: 0.00004 | Regression loss: 0.01388 | Running loss: 0.01289\n",
            "Epoch: 162 | Iteration: 27 | Classification loss: 0.00001 | Regression loss: 0.00472 | Running loss: 0.01288\n",
            "Epoch: 162 | Iteration: 28 | Classification loss: 0.00001 | Regression loss: 0.01012 | Running loss: 0.01288\n",
            "Epoch: 162 | Iteration: 29 | Classification loss: 0.00002 | Regression loss: 0.01402 | Running loss: 0.01286\n",
            "Epoch: 162 | Iteration: 30 | Classification loss: 0.00001 | Regression loss: 0.00572 | Running loss: 0.01284\n",
            "Epoch: 162 | Iteration: 31 | Classification loss: 0.00005 | Regression loss: 0.01080 | Running loss: 0.01284\n",
            "Epoch: 162 | Iteration: 32 | Classification loss: 0.00009 | Regression loss: 0.01144 | Running loss: 0.01285\n",
            "Epoch: 162 | Iteration: 33 | Classification loss: 0.00004 | Regression loss: 0.01381 | Running loss: 0.01286\n",
            "Epoch: 162 | Iteration: 34 | Classification loss: 0.00001 | Regression loss: 0.00367 | Running loss: 0.01285\n",
            "Epoch: 162 | Iteration: 35 | Classification loss: 0.00003 | Regression loss: 0.01356 | Running loss: 0.01286\n",
            "Epoch: 162 | Iteration: 36 | Classification loss: 0.00002 | Regression loss: 0.02915 | Running loss: 0.01289\n",
            "Epoch: 162 | Iteration: 37 | Classification loss: 0.00001 | Regression loss: 0.00482 | Running loss: 0.01287\n",
            "Epoch: 162 | Iteration: 38 | Classification loss: 0.00003 | Regression loss: 0.00476 | Running loss: 0.01285\n",
            "Epoch: 162 | Iteration: 39 | Classification loss: 0.00002 | Regression loss: 0.00625 | Running loss: 0.01284\n",
            "Epoch: 162 | Iteration: 40 | Classification loss: 0.00005 | Regression loss: 0.01209 | Running loss: 0.01282\n",
            "Epoch: 162 | Iteration: 41 | Classification loss: 0.00004 | Regression loss: 0.01701 | Running loss: 0.01284\n",
            "Epoch: 162 | Iteration: 42 | Classification loss: 0.00003 | Regression loss: 0.00745 | Running loss: 0.01283\n",
            "Epoch: 162 | Iteration: 43 | Classification loss: 0.00003 | Regression loss: 0.01974 | Running loss: 0.01284\n",
            "Epoch: 162 | Iteration: 44 | Classification loss: 0.00003 | Regression loss: 0.00738 | Running loss: 0.01281\n",
            "Epoch: 162 | Iteration: 45 | Classification loss: 0.00002 | Regression loss: 0.00433 | Running loss: 0.01279\n",
            "Epoch: 162 | Iteration: 46 | Classification loss: 0.00002 | Regression loss: 0.01945 | Running loss: 0.01276\n",
            "Epoch: 162 | Iteration: 47 | Classification loss: 0.00003 | Regression loss: 0.01610 | Running loss: 0.01270\n",
            "Epoch: 162 | Iteration: 48 | Classification loss: 0.00004 | Regression loss: 0.01364 | Running loss: 0.01272\n",
            "Epoch: 162 | Iteration: 49 | Classification loss: 0.00002 | Regression loss: 0.00457 | Running loss: 0.01271\n",
            "Epoch: 162 | Iteration: 50 | Classification loss: 0.00004 | Regression loss: 0.01162 | Running loss: 0.01267\n",
            "Epoch: 162 | Iteration: 51 | Classification loss: 0.00008 | Regression loss: 0.01676 | Running loss: 0.01270\n",
            "Epoch: 162 | Iteration: 52 | Classification loss: 0.00003 | Regression loss: 0.00309 | Running loss: 0.01268\n",
            "Epoch: 162 | Iteration: 53 | Classification loss: 0.00006 | Regression loss: 0.01773 | Running loss: 0.01270\n",
            "Epoch: 162 | Iteration: 54 | Classification loss: 0.00003 | Regression loss: 0.01223 | Running loss: 0.01270\n",
            "Epoch: 162 | Iteration: 55 | Classification loss: 0.00004 | Regression loss: 0.00628 | Running loss: 0.01271\n",
            "Epoch: 162 | Iteration: 56 | Classification loss: 0.00002 | Regression loss: 0.00566 | Running loss: 0.01270\n",
            "Epoch: 162 | Iteration: 57 | Classification loss: 0.00002 | Regression loss: 0.00663 | Running loss: 0.01269\n",
            "Epoch: 162 | Iteration: 58 | Classification loss: 0.00002 | Regression loss: 0.01209 | Running loss: 0.01269\n",
            "Epoch: 162 | Iteration: 59 | Classification loss: 0.00001 | Regression loss: 0.00757 | Running loss: 0.01266\n",
            "Epoch: 162 | Iteration: 60 | Classification loss: 0.00005 | Regression loss: 0.01364 | Running loss: 0.01266\n",
            "Epoch: 162 | Iteration: 61 | Classification loss: 0.00001 | Regression loss: 0.00383 | Running loss: 0.01266\n",
            "Epoch: 162 | Iteration: 62 | Classification loss: 0.00002 | Regression loss: 0.03079 | Running loss: 0.01270\n",
            "Epoch: 162 | Iteration: 63 | Classification loss: 0.00001 | Regression loss: 0.00492 | Running loss: 0.01269\n",
            "Epoch: 162 | Iteration: 64 | Classification loss: 0.00004 | Regression loss: 0.01360 | Running loss: 0.01269\n",
            "Epoch: 162 | Iteration: 65 | Classification loss: 0.00014 | Regression loss: 0.02088 | Running loss: 0.01270\n",
            "Epoch: 162 | Iteration: 66 | Classification loss: 0.00009 | Regression loss: 0.01372 | Running loss: 0.01269\n",
            "Epoch: 162 | Iteration: 67 | Classification loss: 0.00005 | Regression loss: 0.02083 | Running loss: 0.01272\n",
            "Epoch: 162 | Iteration: 68 | Classification loss: 0.00003 | Regression loss: 0.01913 | Running loss: 0.01273\n",
            "Epoch: 162 | Iteration: 69 | Classification loss: 0.00001 | Regression loss: 0.00512 | Running loss: 0.01272\n",
            "Epoch: 162 | Iteration: 70 | Classification loss: 0.00004 | Regression loss: 0.01459 | Running loss: 0.01272\n",
            "Epoch: 162 | Iteration: 71 | Classification loss: 0.00005 | Regression loss: 0.01910 | Running loss: 0.01273\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 190.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 162 | Iteration: 73 | Classification loss: 0.00002 | Regression loss: 0.01195 | Running loss: 0.01274\n",
            "Epoch: 162 | Iteration: 74 | Classification loss: 0.00002 | Regression loss: 0.01247 | Running loss: 0.01274\n",
            "Epoch: 162 | Iteration: 75 | Classification loss: 0.00001 | Regression loss: 0.00945 | Running loss: 0.01270\n",
            "Epoch: 162 | Iteration: 76 | Classification loss: 0.00001 | Regression loss: 0.01191 | Running loss: 0.01272\n",
            "Epoch: 162 | Iteration: 77 | Classification loss: 0.00002 | Regression loss: 0.00715 | Running loss: 0.01269\n",
            "Epoch: 162 | Iteration: 78 | Classification loss: 0.00003 | Regression loss: 0.01492 | Running loss: 0.01271\n",
            "Epoch: 162 | Iteration: 79 | Classification loss: 0.00002 | Regression loss: 0.00741 | Running loss: 0.01270\n",
            "Epoch: 162 | Iteration: 80 | Classification loss: 0.00003 | Regression loss: 0.00654 | Running loss: 0.01268\n",
            "Epoch: 162 | Iteration: 81 | Classification loss: 0.00003 | Regression loss: 0.00696 | Running loss: 0.01269\n",
            "Epoch: 162 | Iteration: 82 | Classification loss: 0.00005 | Regression loss: 0.01176 | Running loss: 0.01270\n",
            "Epoch: 162 | Iteration: 83 | Classification loss: 0.00002 | Regression loss: 0.00491 | Running loss: 0.01267\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.7263226120133723\n",
            "Precision:  0.5267175572519084\n",
            "Recall:  0.8181818181818182\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}, {0: (0.7314384456550078, 253.0)}, {0: (0.7334466242447818, 253.0)}, {0: (0.7357024222917343, 253.0)}, {0: (0.7381550871546381, 253.0)}, {0: (0.739092434551302, 253.0)}, {0: (0.72091448182607, 253.0)}, {0: (0.7389512050165785, 253.0)}, {0: (0.73050932626622, 253.0)}, {0: (0.7263226120133723, 253.0)}]\n",
            "Epoch: 163 | Iteration: 0 | Classification loss: 0.00001 | Regression loss: 0.00739 | Running loss: 0.01266\n",
            "Epoch: 163 | Iteration: 1 | Classification loss: 0.00003 | Regression loss: 0.01300 | Running loss: 0.01267\n",
            "Epoch: 163 | Iteration: 2 | Classification loss: 0.00001 | Regression loss: 0.00387 | Running loss: 0.01264\n",
            "Epoch: 163 | Iteration: 3 | Classification loss: 0.00002 | Regression loss: 0.00429 | Running loss: 0.01263\n",
            "Epoch: 163 | Iteration: 4 | Classification loss: 0.00006 | Regression loss: 0.01919 | Running loss: 0.01266\n",
            "Epoch: 163 | Iteration: 5 | Classification loss: 0.00001 | Regression loss: 0.00899 | Running loss: 0.01267\n",
            "Epoch: 163 | Iteration: 6 | Classification loss: 0.00001 | Regression loss: 0.00461 | Running loss: 0.01266\n",
            "Epoch: 163 | Iteration: 7 | Classification loss: 0.00002 | Regression loss: 0.00598 | Running loss: 0.01263\n",
            "Epoch: 163 | Iteration: 8 | Classification loss: 0.00014 | Regression loss: 0.01514 | Running loss: 0.01264\n",
            "Epoch: 163 | Iteration: 9 | Classification loss: 0.00002 | Regression loss: 0.01409 | Running loss: 0.01265\n",
            "Epoch: 163 | Iteration: 10 | Classification loss: 0.00002 | Regression loss: 0.01230 | Running loss: 0.01265\n",
            "Epoch: 163 | Iteration: 11 | Classification loss: 0.00002 | Regression loss: 0.00469 | Running loss: 0.01261\n",
            "Epoch: 163 | Iteration: 12 | Classification loss: 0.00005 | Regression loss: 0.01482 | Running loss: 0.01262\n",
            "Epoch: 163 | Iteration: 13 | Classification loss: 0.00002 | Regression loss: 0.03047 | Running loss: 0.01266\n",
            "Epoch: 163 | Iteration: 14 | Classification loss: 0.00004 | Regression loss: 0.00650 | Running loss: 0.01266\n",
            "Epoch: 163 | Iteration: 15 | Classification loss: 0.00003 | Regression loss: 0.00584 | Running loss: 0.01266\n",
            "Epoch: 163 | Iteration: 16 | Classification loss: 0.00002 | Regression loss: 0.00733 | Running loss: 0.01267\n",
            "Epoch: 163 | Iteration: 17 | Classification loss: 0.00004 | Regression loss: 0.01598 | Running loss: 0.01266\n",
            "Epoch: 163 | Iteration: 18 | Classification loss: 0.00003 | Regression loss: 0.02606 | Running loss: 0.01269\n",
            "Epoch: 163 | Iteration: 19 | Classification loss: 0.00005 | Regression loss: 0.01650 | Running loss: 0.01269\n",
            "Epoch: 163 | Iteration: 20 | Classification loss: 0.00009 | Regression loss: 0.01405 | Running loss: 0.01268\n",
            "Epoch: 163 | Iteration: 21 | Classification loss: 0.00004 | Regression loss: 0.01504 | Running loss: 0.01269\n",
            "Epoch: 163 | Iteration: 22 | Classification loss: 0.00004 | Regression loss: 0.01288 | Running loss: 0.01268\n",
            "Epoch: 163 | Iteration: 23 | Classification loss: 0.00001 | Regression loss: 0.01620 | Running loss: 0.01270\n",
            "Epoch: 163 | Iteration: 24 | Classification loss: 0.00002 | Regression loss: 0.01206 | Running loss: 0.01270\n",
            "Epoch: 163 | Iteration: 25 | Classification loss: 0.00003 | Regression loss: 0.01100 | Running loss: 0.01271\n",
            "Epoch: 163 | Iteration: 26 | Classification loss: 0.00006 | Regression loss: 0.02141 | Running loss: 0.01273\n",
            "Epoch: 163 | Iteration: 27 | Classification loss: 0.00005 | Regression loss: 0.01775 | Running loss: 0.01273\n",
            "Epoch: 163 | Iteration: 28 | Classification loss: 0.00017 | Regression loss: 0.00829 | Running loss: 0.01273\n",
            "Epoch: 163 | Iteration: 29 | Classification loss: 0.00001 | Regression loss: 0.00425 | Running loss: 0.01271\n",
            "Epoch: 163 | Iteration: 30 | Classification loss: 0.00002 | Regression loss: 0.00752 | Running loss: 0.01271\n",
            "Epoch: 163 | Iteration: 31 | Classification loss: 0.00007 | Regression loss: 0.01487 | Running loss: 0.01269\n",
            "Epoch: 163 | Iteration: 32 | Classification loss: 0.00004 | Regression loss: 0.01118 | Running loss: 0.01268\n",
            "Epoch: 163 | Iteration: 33 | Classification loss: 0.00002 | Regression loss: 0.01473 | Running loss: 0.01270\n",
            "Epoch: 163 | Iteration: 34 | Classification loss: 0.00005 | Regression loss: 0.01324 | Running loss: 0.01272\n",
            "Epoch: 163 | Iteration: 35 | Classification loss: 0.00003 | Regression loss: 0.01358 | Running loss: 0.01272\n",
            "Epoch: 163 | Iteration: 36 | Classification loss: 0.00004 | Regression loss: 0.01058 | Running loss: 0.01271\n",
            "Epoch: 163 | Iteration: 37 | Classification loss: 0.00007 | Regression loss: 0.01853 | Running loss: 0.01272\n",
            "Epoch: 163 | Iteration: 38 | Classification loss: 0.00015 | Regression loss: 0.02021 | Running loss: 0.01273\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 190.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 163 | Iteration: 40 | Classification loss: 0.00003 | Regression loss: 0.01596 | Running loss: 0.01272\n",
            "Epoch: 163 | Iteration: 41 | Classification loss: 0.00001 | Regression loss: 0.02967 | Running loss: 0.01274\n",
            "Epoch: 163 | Iteration: 42 | Classification loss: 0.00002 | Regression loss: 0.01131 | Running loss: 0.01275\n",
            "Epoch: 163 | Iteration: 43 | Classification loss: 0.00002 | Regression loss: 0.02237 | Running loss: 0.01277\n",
            "Epoch: 163 | Iteration: 44 | Classification loss: 0.00001 | Regression loss: 0.00579 | Running loss: 0.01277\n",
            "Epoch: 163 | Iteration: 45 | Classification loss: 0.00004 | Regression loss: 0.01608 | Running loss: 0.01278\n",
            "Epoch: 163 | Iteration: 46 | Classification loss: 0.00001 | Regression loss: 0.01035 | Running loss: 0.01279\n",
            "Epoch: 163 | Iteration: 47 | Classification loss: 0.00002 | Regression loss: 0.01082 | Running loss: 0.01278\n",
            "Epoch: 163 | Iteration: 48 | Classification loss: 0.00003 | Regression loss: 0.01635 | Running loss: 0.01275\n",
            "Epoch: 163 | Iteration: 49 | Classification loss: 0.00007 | Regression loss: 0.00435 | Running loss: 0.01274\n",
            "Epoch: 163 | Iteration: 50 | Classification loss: 0.00003 | Regression loss: 0.00745 | Running loss: 0.01273\n",
            "Epoch: 163 | Iteration: 51 | Classification loss: 0.00004 | Regression loss: 0.00751 | Running loss: 0.01272\n",
            "Epoch: 163 | Iteration: 52 | Classification loss: 0.00002 | Regression loss: 0.01674 | Running loss: 0.01271\n",
            "Epoch: 163 | Iteration: 53 | Classification loss: 0.00009 | Regression loss: 0.01427 | Running loss: 0.01271\n",
            "Epoch: 163 | Iteration: 54 | Classification loss: 0.00006 | Regression loss: 0.01731 | Running loss: 0.01271\n",
            "Epoch: 163 | Iteration: 55 | Classification loss: 0.00001 | Regression loss: 0.00823 | Running loss: 0.01272\n",
            "Epoch: 163 | Iteration: 56 | Classification loss: 0.00003 | Regression loss: 0.01011 | Running loss: 0.01271\n",
            "Epoch: 163 | Iteration: 57 | Classification loss: 0.00002 | Regression loss: 0.00491 | Running loss: 0.01271\n",
            "Epoch: 163 | Iteration: 58 | Classification loss: 0.00002 | Regression loss: 0.01881 | Running loss: 0.01274\n",
            "Epoch: 163 | Iteration: 59 | Classification loss: 0.00005 | Regression loss: 0.01472 | Running loss: 0.01274\n",
            "Epoch: 163 | Iteration: 60 | Classification loss: 0.00005 | Regression loss: 0.01107 | Running loss: 0.01274\n",
            "Epoch: 163 | Iteration: 61 | Classification loss: 0.00002 | Regression loss: 0.01255 | Running loss: 0.01274\n",
            "Epoch: 163 | Iteration: 62 | Classification loss: 0.00004 | Regression loss: 0.00488 | Running loss: 0.01273\n",
            "Epoch: 163 | Iteration: 63 | Classification loss: 0.00003 | Regression loss: 0.01098 | Running loss: 0.01272\n",
            "Epoch: 163 | Iteration: 64 | Classification loss: 0.00004 | Regression loss: 0.01307 | Running loss: 0.01268\n",
            "Epoch: 163 | Iteration: 65 | Classification loss: 0.00005 | Regression loss: 0.00402 | Running loss: 0.01268\n",
            "Epoch: 163 | Iteration: 66 | Classification loss: 0.00014 | Regression loss: 0.04770 | Running loss: 0.01275\n",
            "Epoch: 163 | Iteration: 67 | Classification loss: 0.00003 | Regression loss: 0.01823 | Running loss: 0.01276\n",
            "Epoch: 163 | Iteration: 68 | Classification loss: 0.00001 | Regression loss: 0.01225 | Running loss: 0.01276\n",
            "Epoch: 163 | Iteration: 69 | Classification loss: 0.00007 | Regression loss: 0.01162 | Running loss: 0.01276\n",
            "Epoch: 163 | Iteration: 70 | Classification loss: 0.00001 | Regression loss: 0.00396 | Running loss: 0.01275\n",
            "Epoch: 163 | Iteration: 71 | Classification loss: 0.00003 | Regression loss: 0.00707 | Running loss: 0.01272\n",
            "Epoch: 163 | Iteration: 72 | Classification loss: 0.00009 | Regression loss: 0.00992 | Running loss: 0.01273\n",
            "Epoch: 163 | Iteration: 73 | Classification loss: 0.00002 | Regression loss: 0.01188 | Running loss: 0.01272\n",
            "Epoch: 163 | Iteration: 74 | Classification loss: 0.00003 | Regression loss: 0.00551 | Running loss: 0.01271\n",
            "Epoch: 163 | Iteration: 75 | Classification loss: 0.00001 | Regression loss: 0.00372 | Running loss: 0.01269\n",
            "Epoch: 163 | Iteration: 76 | Classification loss: 0.00003 | Regression loss: 0.01861 | Running loss: 0.01271\n",
            "Epoch: 163 | Iteration: 77 | Classification loss: 0.00003 | Regression loss: 0.01079 | Running loss: 0.01272\n",
            "Epoch: 163 | Iteration: 78 | Classification loss: 0.00002 | Regression loss: 0.01216 | Running loss: 0.01271\n",
            "Epoch: 163 | Iteration: 79 | Classification loss: 0.00005 | Regression loss: 0.01168 | Running loss: 0.01272\n",
            "Epoch: 163 | Iteration: 80 | Classification loss: 0.00001 | Regression loss: 0.00519 | Running loss: 0.01271\n",
            "Epoch: 163 | Iteration: 81 | Classification loss: 0.00004 | Regression loss: 0.01153 | Running loss: 0.01272\n",
            "Epoch: 163 | Iteration: 82 | Classification loss: 0.00003 | Regression loss: 0.00843 | Running loss: 0.01272\n",
            "Epoch: 163 | Iteration: 83 | Classification loss: 0.00001 | Regression loss: 0.00487 | Running loss: 0.01271\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.7283463725823673\n",
            "Precision:  0.532133676092545\n",
            "Recall:  0.8181818181818182\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}, {0: (0.7314384456550078, 253.0)}, {0: (0.7334466242447818, 253.0)}, {0: (0.7357024222917343, 253.0)}, {0: (0.7381550871546381, 253.0)}, {0: (0.739092434551302, 253.0)}, {0: (0.72091448182607, 253.0)}, {0: (0.7389512050165785, 253.0)}, {0: (0.73050932626622, 253.0)}, {0: (0.7263226120133723, 253.0)}, {0: (0.7283463725823673, 253.0)}]\n",
            "Epoch: 164 | Iteration: 0 | Classification loss: 0.00004 | Regression loss: 0.01630 | Running loss: 0.01264\n",
            "Epoch: 164 | Iteration: 1 | Classification loss: 0.00006 | Regression loss: 0.01676 | Running loss: 0.01266\n",
            "Epoch: 164 | Iteration: 2 | Classification loss: 0.00004 | Regression loss: 0.02693 | Running loss: 0.01271\n",
            "Epoch: 164 | Iteration: 3 | Classification loss: 0.00001 | Regression loss: 0.00405 | Running loss: 0.01268\n",
            "Epoch: 164 | Iteration: 4 | Classification loss: 0.00004 | Regression loss: 0.01517 | Running loss: 0.01270\n",
            "Epoch: 164 | Iteration: 5 | Classification loss: 0.00001 | Regression loss: 0.00793 | Running loss: 0.01269\n",
            "Epoch: 164 | Iteration: 6 | Classification loss: 0.00004 | Regression loss: 0.00327 | Running loss: 0.01266\n",
            "Epoch: 164 | Iteration: 7 | Classification loss: 0.00004 | Regression loss: 0.01402 | Running loss: 0.01266\n",
            "Epoch: 164 | Iteration: 8 | Classification loss: 0.00002 | Regression loss: 0.00603 | Running loss: 0.01265\n",
            "Epoch: 164 | Iteration: 9 | Classification loss: 0.00003 | Regression loss: 0.00521 | Running loss: 0.01265\n",
            "Epoch: 164 | Iteration: 10 | Classification loss: 0.00001 | Regression loss: 0.01041 | Running loss: 0.01266\n",
            "Epoch: 164 | Iteration: 11 | Classification loss: 0.00008 | Regression loss: 0.01433 | Running loss: 0.01266\n",
            "Epoch: 164 | Iteration: 12 | Classification loss: 0.00004 | Regression loss: 0.00635 | Running loss: 0.01263\n",
            "Epoch: 164 | Iteration: 13 | Classification loss: 0.00006 | Regression loss: 0.01350 | Running loss: 0.01262\n",
            "Epoch: 164 | Iteration: 14 | Classification loss: 0.00005 | Regression loss: 0.01365 | Running loss: 0.01262\n",
            "Epoch: 164 | Iteration: 15 | Classification loss: 0.00003 | Regression loss: 0.00776 | Running loss: 0.01257\n",
            "Epoch: 164 | Iteration: 16 | Classification loss: 0.00014 | Regression loss: 0.01869 | Running loss: 0.01259\n",
            "Epoch: 164 | Iteration: 17 | Classification loss: 0.00002 | Regression loss: 0.00712 | Running loss: 0.01258\n",
            "Epoch: 164 | Iteration: 18 | Classification loss: 0.00003 | Regression loss: 0.01379 | Running loss: 0.01259\n",
            "Epoch: 164 | Iteration: 19 | Classification loss: 0.00007 | Regression loss: 0.00435 | Running loss: 0.01255\n",
            "Epoch: 164 | Iteration: 20 | Classification loss: 0.00003 | Regression loss: 0.01173 | Running loss: 0.01256\n",
            "Epoch: 164 | Iteration: 21 | Classification loss: 0.00001 | Regression loss: 0.00422 | Running loss: 0.01254\n",
            "Epoch: 164 | Iteration: 22 | Classification loss: 0.00002 | Regression loss: 0.01770 | Running loss: 0.01254\n",
            "Epoch: 164 | Iteration: 23 | Classification loss: 0.00003 | Regression loss: 0.02106 | Running loss: 0.01255\n",
            "Epoch: 164 | Iteration: 24 | Classification loss: 0.00004 | Regression loss: 0.01576 | Running loss: 0.01256\n",
            "Epoch: 164 | Iteration: 25 | Classification loss: 0.00002 | Regression loss: 0.01638 | Running loss: 0.01256\n",
            "Epoch: 164 | Iteration: 26 | Classification loss: 0.00004 | Regression loss: 0.00897 | Running loss: 0.01255\n",
            "Epoch: 164 | Iteration: 27 | Classification loss: 0.00002 | Regression loss: 0.01424 | Running loss: 0.01254\n",
            "Epoch: 164 | Iteration: 28 | Classification loss: 0.00001 | Regression loss: 0.00931 | Running loss: 0.01254\n",
            "Epoch: 164 | Iteration: 29 | Classification loss: 0.00002 | Regression loss: 0.01456 | Running loss: 0.01253\n",
            "Epoch: 164 | Iteration: 30 | Classification loss: 0.00003 | Regression loss: 0.01154 | Running loss: 0.01252\n",
            "Epoch: 164 | Iteration: 31 | Classification loss: 0.00001 | Regression loss: 0.00468 | Running loss: 0.01249\n",
            "Epoch: 164 | Iteration: 32 | Classification loss: 0.00002 | Regression loss: 0.00872 | Running loss: 0.01248\n",
            "Epoch: 164 | Iteration: 33 | Classification loss: 0.00003 | Regression loss: 0.01573 | Running loss: 0.01249\n",
            "Epoch: 164 | Iteration: 34 | Classification loss: 0.00001 | Regression loss: 0.01038 | Running loss: 0.01250\n",
            "Epoch: 164 | Iteration: 35 | Classification loss: 0.00013 | Regression loss: 0.04661 | Running loss: 0.01257\n",
            "Epoch: 164 | Iteration: 36 | Classification loss: 0.00003 | Regression loss: 0.01625 | Running loss: 0.01258\n",
            "Epoch: 164 | Iteration: 37 | Classification loss: 0.00002 | Regression loss: 0.00420 | Running loss: 0.01257\n",
            "Epoch: 164 | Iteration: 38 | Classification loss: 0.00012 | Regression loss: 0.01439 | Running loss: 0.01255\n",
            "Epoch: 164 | Iteration: 39 | Classification loss: 0.00018 | Regression loss: 0.00845 | Running loss: 0.01254\n",
            "Epoch: 164 | Iteration: 40 | Classification loss: 0.00004 | Regression loss: 0.01478 | Running loss: 0.01256\n",
            "Epoch: 164 | Iteration: 41 | Classification loss: 0.00001 | Regression loss: 0.00367 | Running loss: 0.01256\n",
            "Epoch: 164 | Iteration: 42 | Classification loss: 0.00008 | Regression loss: 0.01174 | Running loss: 0.01258\n",
            "Epoch: 164 | Iteration: 43 | Classification loss: 0.00001 | Regression loss: 0.00528 | Running loss: 0.01257\n",
            "Epoch: 164 | Iteration: 44 | Classification loss: 0.00005 | Regression loss: 0.00973 | Running loss: 0.01258\n",
            "Epoch: 164 | Iteration: 45 | Classification loss: 0.00002 | Regression loss: 0.00676 | Running loss: 0.01257\n",
            "Epoch: 164 | Iteration: 46 | Classification loss: 0.00001 | Regression loss: 0.00540 | Running loss: 0.01255\n",
            "Epoch: 164 | Iteration: 47 | Classification loss: 0.00002 | Regression loss: 0.01699 | Running loss: 0.01257\n",
            "Epoch: 164 | Iteration: 48 | Classification loss: 0.00003 | Regression loss: 0.01169 | Running loss: 0.01258\n",
            "Epoch: 164 | Iteration: 49 | Classification loss: 0.00004 | Regression loss: 0.01848 | Running loss: 0.01259\n",
            "Epoch: 164 | Iteration: 50 | Classification loss: 0.00004 | Regression loss: 0.00515 | Running loss: 0.01259\n",
            "Epoch: 164 | Iteration: 51 | Classification loss: 0.00010 | Regression loss: 0.01430 | Running loss: 0.01261\n",
            "Epoch: 164 | Iteration: 52 | Classification loss: 0.00002 | Regression loss: 0.01264 | Running loss: 0.01260\n",
            "Epoch: 164 | Iteration: 53 | Classification loss: 0.00003 | Regression loss: 0.01050 | Running loss: 0.01261\n",
            "Epoch: 164 | Iteration: 54 | Classification loss: 0.00005 | Regression loss: 0.01150 | Running loss: 0.01260\n",
            "Epoch: 164 | Iteration: 55 | Classification loss: 0.00002 | Regression loss: 0.01163 | Running loss: 0.01260\n",
            "Epoch: 164 | Iteration: 56 | Classification loss: 0.00004 | Regression loss: 0.01081 | Running loss: 0.01260\n",
            "Epoch: 164 | Iteration: 57 | Classification loss: 0.00001 | Regression loss: 0.00771 | Running loss: 0.01259\n",
            "Epoch: 164 | Iteration: 58 | Classification loss: 0.00003 | Regression loss: 0.01820 | Running loss: 0.01261\n",
            "Epoch: 164 | Iteration: 59 | Classification loss: 0.00007 | Regression loss: 0.01147 | Running loss: 0.01254\n",
            "Epoch: 164 | Iteration: 60 | Classification loss: 0.00002 | Regression loss: 0.00557 | Running loss: 0.01253\n",
            "Epoch: 164 | Iteration: 61 | Classification loss: 0.00006 | Regression loss: 0.02153 | Running loss: 0.01255\n",
            "Epoch: 164 | Iteration: 62 | Classification loss: 0.00002 | Regression loss: 0.00703 | Running loss: 0.01253\n",
            "Epoch: 164 | Iteration: 63 | Classification loss: 0.00002 | Regression loss: 0.01270 | Running loss: 0.01254\n",
            "Epoch: 164 | Iteration: 64 | Classification loss: 0.00002 | Regression loss: 0.00719 | Running loss: 0.01252\n",
            "Epoch: 164 | Iteration: 65 | Classification loss: 0.00001 | Regression loss: 0.02955 | Running loss: 0.01256\n",
            "Epoch: 164 | Iteration: 66 | Classification loss: 0.00001 | Regression loss: 0.00652 | Running loss: 0.01254\n",
            "Epoch: 164 | Iteration: 67 | Classification loss: 0.00005 | Regression loss: 0.01615 | Running loss: 0.01256\n",
            "Epoch: 164 | Iteration: 68 | Classification loss: 0.00001 | Regression loss: 0.00566 | Running loss: 0.01254\n",
            "Epoch: 164 | Iteration: 69 | Classification loss: 0.00004 | Regression loss: 0.01756 | Running loss: 0.01254\n",
            "Epoch: 164 | Iteration: 70 | Classification loss: 0.00003 | Regression loss: 0.01164 | Running loss: 0.01255\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 188.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 164 | Iteration: 72 | Classification loss: 0.00003 | Regression loss: 0.02549 | Running loss: 0.01259\n",
            "Epoch: 164 | Iteration: 73 | Classification loss: 0.00001 | Regression loss: 0.00481 | Running loss: 0.01258\n",
            "Epoch: 164 | Iteration: 74 | Classification loss: 0.00002 | Regression loss: 0.01219 | Running loss: 0.01257\n",
            "Epoch: 164 | Iteration: 75 | Classification loss: 0.00004 | Regression loss: 0.01334 | Running loss: 0.01256\n",
            "Epoch: 164 | Iteration: 76 | Classification loss: 0.00002 | Regression loss: 0.01822 | Running loss: 0.01259\n",
            "Epoch: 164 | Iteration: 77 | Classification loss: 0.00002 | Regression loss: 0.01084 | Running loss: 0.01258\n",
            "Epoch: 164 | Iteration: 78 | Classification loss: 0.00002 | Regression loss: 0.03222 | Running loss: 0.01261\n",
            "Epoch: 164 | Iteration: 79 | Classification loss: 0.00003 | Regression loss: 0.00676 | Running loss: 0.01256\n",
            "Epoch: 164 | Iteration: 80 | Classification loss: 0.00001 | Regression loss: 0.01026 | Running loss: 0.01255\n",
            "Epoch: 164 | Iteration: 81 | Classification loss: 0.00002 | Regression loss: 0.01184 | Running loss: 0.01256\n",
            "Epoch: 164 | Iteration: 82 | Classification loss: 0.00002 | Regression loss: 0.01231 | Running loss: 0.01257\n",
            "Epoch: 164 | Iteration: 83 | Classification loss: 0.00001 | Regression loss: 0.00380 | Running loss: 0.01253\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.7254913772539314\n",
            "Precision:  0.532133676092545\n",
            "Recall:  0.8181818181818182\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}, {0: (0.7314384456550078, 253.0)}, {0: (0.7334466242447818, 253.0)}, {0: (0.7357024222917343, 253.0)}, {0: (0.7381550871546381, 253.0)}, {0: (0.739092434551302, 253.0)}, {0: (0.72091448182607, 253.0)}, {0: (0.7389512050165785, 253.0)}, {0: (0.73050932626622, 253.0)}, {0: (0.7263226120133723, 253.0)}, {0: (0.7283463725823673, 253.0)}, {0: (0.7254913772539314, 253.0)}]\n",
            "Epoch: 165 | Iteration: 0 | Classification loss: 0.00005 | Regression loss: 0.00355 | Running loss: 0.01252\n",
            "Epoch: 165 | Iteration: 1 | Classification loss: 0.00002 | Regression loss: 0.01537 | Running loss: 0.01255\n",
            "Epoch: 165 | Iteration: 2 | Classification loss: 0.00001 | Regression loss: 0.00426 | Running loss: 0.01253\n",
            "Epoch: 165 | Iteration: 3 | Classification loss: 0.00001 | Regression loss: 0.01210 | Running loss: 0.01255\n",
            "Epoch: 165 | Iteration: 4 | Classification loss: 0.00003 | Regression loss: 0.01138 | Running loss: 0.01253\n",
            "Epoch: 165 | Iteration: 5 | Classification loss: 0.00004 | Regression loss: 0.01940 | Running loss: 0.01254\n",
            "Epoch: 165 | Iteration: 6 | Classification loss: 0.00001 | Regression loss: 0.01069 | Running loss: 0.01255\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 190.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 165 | Iteration: 8 | Classification loss: 0.00004 | Regression loss: 0.01151 | Running loss: 0.01256\n",
            "Epoch: 165 | Iteration: 9 | Classification loss: 0.00002 | Regression loss: 0.01328 | Running loss: 0.01257\n",
            "Epoch: 165 | Iteration: 10 | Classification loss: 0.00004 | Regression loss: 0.01303 | Running loss: 0.01257\n",
            "Epoch: 165 | Iteration: 11 | Classification loss: 0.00004 | Regression loss: 0.01715 | Running loss: 0.01254\n",
            "Epoch: 165 | Iteration: 12 | Classification loss: 0.00003 | Regression loss: 0.01247 | Running loss: 0.01254\n",
            "Epoch: 165 | Iteration: 13 | Classification loss: 0.00003 | Regression loss: 0.01616 | Running loss: 0.01254\n",
            "Epoch: 165 | Iteration: 14 | Classification loss: 0.00006 | Regression loss: 0.01787 | Running loss: 0.01256\n",
            "Epoch: 165 | Iteration: 15 | Classification loss: 0.00001 | Regression loss: 0.00340 | Running loss: 0.01252\n",
            "Epoch: 165 | Iteration: 16 | Classification loss: 0.00001 | Regression loss: 0.00569 | Running loss: 0.01244\n",
            "Epoch: 165 | Iteration: 17 | Classification loss: 0.00003 | Regression loss: 0.00593 | Running loss: 0.01242\n",
            "Epoch: 165 | Iteration: 18 | Classification loss: 0.00005 | Regression loss: 0.01068 | Running loss: 0.01238\n",
            "Epoch: 165 | Iteration: 19 | Classification loss: 0.00003 | Regression loss: 0.00483 | Running loss: 0.01235\n",
            "Epoch: 165 | Iteration: 20 | Classification loss: 0.00003 | Regression loss: 0.00514 | Running loss: 0.01235\n",
            "Epoch: 165 | Iteration: 21 | Classification loss: 0.00002 | Regression loss: 0.00516 | Running loss: 0.01233\n",
            "Epoch: 165 | Iteration: 22 | Classification loss: 0.00003 | Regression loss: 0.01344 | Running loss: 0.01233\n",
            "Epoch: 165 | Iteration: 23 | Classification loss: 0.00006 | Regression loss: 0.01742 | Running loss: 0.01235\n",
            "Epoch: 165 | Iteration: 24 | Classification loss: 0.00001 | Regression loss: 0.00697 | Running loss: 0.01234\n",
            "Epoch: 165 | Iteration: 25 | Classification loss: 0.00011 | Regression loss: 0.04641 | Running loss: 0.01240\n",
            "Epoch: 165 | Iteration: 26 | Classification loss: 0.00005 | Regression loss: 0.02121 | Running loss: 0.01243\n",
            "Epoch: 165 | Iteration: 27 | Classification loss: 0.00001 | Regression loss: 0.00369 | Running loss: 0.01239\n",
            "Epoch: 165 | Iteration: 28 | Classification loss: 0.00003 | Regression loss: 0.00742 | Running loss: 0.01240\n",
            "Epoch: 165 | Iteration: 29 | Classification loss: 0.00002 | Regression loss: 0.01154 | Running loss: 0.01241\n",
            "Epoch: 165 | Iteration: 30 | Classification loss: 0.00003 | Regression loss: 0.01396 | Running loss: 0.01241\n",
            "Epoch: 165 | Iteration: 31 | Classification loss: 0.00001 | Regression loss: 0.00503 | Running loss: 0.01239\n",
            "Epoch: 165 | Iteration: 32 | Classification loss: 0.00006 | Regression loss: 0.00402 | Running loss: 0.01238\n",
            "Epoch: 165 | Iteration: 33 | Classification loss: 0.00003 | Regression loss: 0.01490 | Running loss: 0.01239\n",
            "Epoch: 165 | Iteration: 34 | Classification loss: 0.00010 | Regression loss: 0.01507 | Running loss: 0.01239\n",
            "Epoch: 165 | Iteration: 35 | Classification loss: 0.00003 | Regression loss: 0.01578 | Running loss: 0.01240\n",
            "Epoch: 165 | Iteration: 36 | Classification loss: 0.00001 | Regression loss: 0.00473 | Running loss: 0.01240\n",
            "Epoch: 165 | Iteration: 37 | Classification loss: 0.00002 | Regression loss: 0.00552 | Running loss: 0.01239\n",
            "Epoch: 165 | Iteration: 38 | Classification loss: 0.00001 | Regression loss: 0.00775 | Running loss: 0.01239\n",
            "Epoch: 165 | Iteration: 39 | Classification loss: 0.00003 | Regression loss: 0.00659 | Running loss: 0.01239\n",
            "Epoch: 165 | Iteration: 40 | Classification loss: 0.00002 | Regression loss: 0.00705 | Running loss: 0.01240\n",
            "Epoch: 165 | Iteration: 41 | Classification loss: 0.00004 | Regression loss: 0.01374 | Running loss: 0.01236\n",
            "Epoch: 165 | Iteration: 42 | Classification loss: 0.00001 | Regression loss: 0.01899 | Running loss: 0.01238\n",
            "Epoch: 165 | Iteration: 43 | Classification loss: 0.00002 | Regression loss: 0.00683 | Running loss: 0.01235\n",
            "Epoch: 165 | Iteration: 44 | Classification loss: 0.00004 | Regression loss: 0.00966 | Running loss: 0.01234\n",
            "Epoch: 165 | Iteration: 45 | Classification loss: 0.00004 | Regression loss: 0.00723 | Running loss: 0.01234\n",
            "Epoch: 165 | Iteration: 46 | Classification loss: 0.00004 | Regression loss: 0.01052 | Running loss: 0.01232\n",
            "Epoch: 165 | Iteration: 47 | Classification loss: 0.00003 | Regression loss: 0.01045 | Running loss: 0.01232\n",
            "Epoch: 165 | Iteration: 48 | Classification loss: 0.00002 | Regression loss: 0.01166 | Running loss: 0.01231\n",
            "Epoch: 165 | Iteration: 49 | Classification loss: 0.00003 | Regression loss: 0.01124 | Running loss: 0.01231\n",
            "Epoch: 165 | Iteration: 50 | Classification loss: 0.00008 | Regression loss: 0.00983 | Running loss: 0.01231\n",
            "Epoch: 165 | Iteration: 51 | Classification loss: 0.00001 | Regression loss: 0.00612 | Running loss: 0.01231\n",
            "Epoch: 165 | Iteration: 52 | Classification loss: 0.00002 | Regression loss: 0.01368 | Running loss: 0.01231\n",
            "Epoch: 165 | Iteration: 53 | Classification loss: 0.00013 | Regression loss: 0.01507 | Running loss: 0.01232\n",
            "Epoch: 165 | Iteration: 54 | Classification loss: 0.00001 | Regression loss: 0.00490 | Running loss: 0.01231\n",
            "Epoch: 165 | Iteration: 55 | Classification loss: 0.00004 | Regression loss: 0.01777 | Running loss: 0.01232\n",
            "Epoch: 165 | Iteration: 56 | Classification loss: 0.00002 | Regression loss: 0.01937 | Running loss: 0.01232\n",
            "Epoch: 165 | Iteration: 57 | Classification loss: 0.00001 | Regression loss: 0.00870 | Running loss: 0.01231\n",
            "Epoch: 165 | Iteration: 58 | Classification loss: 0.00001 | Regression loss: 0.01045 | Running loss: 0.01232\n",
            "Epoch: 165 | Iteration: 59 | Classification loss: 0.00002 | Regression loss: 0.00493 | Running loss: 0.01231\n",
            "Epoch: 165 | Iteration: 60 | Classification loss: 0.00002 | Regression loss: 0.01395 | Running loss: 0.01233\n",
            "Epoch: 165 | Iteration: 61 | Classification loss: 0.00017 | Regression loss: 0.02119 | Running loss: 0.01234\n",
            "Epoch: 165 | Iteration: 62 | Classification loss: 0.00003 | Regression loss: 0.00735 | Running loss: 0.01235\n",
            "Epoch: 165 | Iteration: 63 | Classification loss: 0.00004 | Regression loss: 0.01441 | Running loss: 0.01234\n",
            "Epoch: 165 | Iteration: 64 | Classification loss: 0.00002 | Regression loss: 0.01254 | Running loss: 0.01234\n",
            "Epoch: 165 | Iteration: 65 | Classification loss: 0.00003 | Regression loss: 0.01566 | Running loss: 0.01234\n",
            "Epoch: 165 | Iteration: 66 | Classification loss: 0.00001 | Regression loss: 0.02970 | Running loss: 0.01238\n",
            "Epoch: 165 | Iteration: 67 | Classification loss: 0.00002 | Regression loss: 0.01076 | Running loss: 0.01239\n",
            "Epoch: 165 | Iteration: 68 | Classification loss: 0.00001 | Regression loss: 0.02970 | Running loss: 0.01242\n",
            "Epoch: 165 | Iteration: 69 | Classification loss: 0.00008 | Regression loss: 0.01349 | Running loss: 0.01242\n",
            "Epoch: 165 | Iteration: 70 | Classification loss: 0.00002 | Regression loss: 0.02522 | Running loss: 0.01245\n",
            "Epoch: 165 | Iteration: 71 | Classification loss: 0.00004 | Regression loss: 0.01788 | Running loss: 0.01244\n",
            "Epoch: 165 | Iteration: 72 | Classification loss: 0.00002 | Regression loss: 0.01195 | Running loss: 0.01243\n",
            "Epoch: 165 | Iteration: 73 | Classification loss: 0.00005 | Regression loss: 0.01426 | Running loss: 0.01243\n",
            "Epoch: 165 | Iteration: 74 | Classification loss: 0.00001 | Regression loss: 0.00434 | Running loss: 0.01241\n",
            "Epoch: 165 | Iteration: 75 | Classification loss: 0.00004 | Regression loss: 0.01268 | Running loss: 0.01242\n",
            "Epoch: 165 | Iteration: 76 | Classification loss: 0.00005 | Regression loss: 0.01123 | Running loss: 0.01242\n",
            "Epoch: 165 | Iteration: 77 | Classification loss: 0.00018 | Regression loss: 0.00885 | Running loss: 0.01241\n",
            "Epoch: 165 | Iteration: 78 | Classification loss: 0.00002 | Regression loss: 0.00505 | Running loss: 0.01239\n",
            "Epoch: 165 | Iteration: 79 | Classification loss: 0.00002 | Regression loss: 0.01309 | Running loss: 0.01241\n",
            "Epoch: 165 | Iteration: 80 | Classification loss: 0.00007 | Regression loss: 0.01646 | Running loss: 0.01243\n",
            "Epoch: 165 | Iteration: 81 | Classification loss: 0.00003 | Regression loss: 0.01110 | Running loss: 0.01243\n",
            "Epoch: 165 | Iteration: 82 | Classification loss: 0.00004 | Regression loss: 0.01245 | Running loss: 0.01245\n",
            "Epoch: 165 | Iteration: 83 | Classification loss: 0.00005 | Regression loss: 0.02456 | Running loss: 0.01249\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.7281179290861395\n",
            "Precision:  0.5335051546391752\n",
            "Recall:  0.8181818181818182\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}, {0: (0.7314384456550078, 253.0)}, {0: (0.7334466242447818, 253.0)}, {0: (0.7357024222917343, 253.0)}, {0: (0.7381550871546381, 253.0)}, {0: (0.739092434551302, 253.0)}, {0: (0.72091448182607, 253.0)}, {0: (0.7389512050165785, 253.0)}, {0: (0.73050932626622, 253.0)}, {0: (0.7263226120133723, 253.0)}, {0: (0.7283463725823673, 253.0)}, {0: (0.7254913772539314, 253.0)}, {0: (0.7281179290861395, 253.0)}]\n",
            "Epoch: 166 | Iteration: 0 | Classification loss: 0.00003 | Regression loss: 0.01790 | Running loss: 0.01250\n",
            "Epoch: 166 | Iteration: 1 | Classification loss: 0.00003 | Regression loss: 0.01210 | Running loss: 0.01250\n",
            "Epoch: 166 | Iteration: 2 | Classification loss: 0.00004 | Regression loss: 0.01373 | Running loss: 0.01252\n",
            "Epoch: 166 | Iteration: 3 | Classification loss: 0.00003 | Regression loss: 0.01237 | Running loss: 0.01253\n",
            "Epoch: 166 | Iteration: 4 | Classification loss: 0.00005 | Regression loss: 0.01663 | Running loss: 0.01255\n",
            "Epoch: 166 | Iteration: 5 | Classification loss: 0.00002 | Regression loss: 0.01364 | Running loss: 0.01256\n",
            "Epoch: 166 | Iteration: 6 | Classification loss: 0.00001 | Regression loss: 0.00407 | Running loss: 0.01254\n",
            "Epoch: 166 | Iteration: 7 | Classification loss: 0.00009 | Regression loss: 0.04511 | Running loss: 0.01259\n",
            "Epoch: 166 | Iteration: 8 | Classification loss: 0.00002 | Regression loss: 0.01230 | Running loss: 0.01261\n",
            "Epoch: 166 | Iteration: 9 | Classification loss: 0.00014 | Regression loss: 0.01986 | Running loss: 0.01262\n",
            "Epoch: 166 | Iteration: 10 | Classification loss: 0.00004 | Regression loss: 0.01368 | Running loss: 0.01262\n",
            "Epoch: 166 | Iteration: 11 | Classification loss: 0.00002 | Regression loss: 0.02170 | Running loss: 0.01266\n",
            "Epoch: 166 | Iteration: 12 | Classification loss: 0.00002 | Regression loss: 0.02988 | Running loss: 0.01270\n",
            "Epoch: 166 | Iteration: 13 | Classification loss: 0.00004 | Regression loss: 0.01544 | Running loss: 0.01270\n",
            "Epoch: 166 | Iteration: 14 | Classification loss: 0.00004 | Regression loss: 0.01859 | Running loss: 0.01270\n",
            "Epoch: 166 | Iteration: 15 | Classification loss: 0.00007 | Regression loss: 0.01345 | Running loss: 0.01271\n",
            "Epoch: 166 | Iteration: 16 | Classification loss: 0.00001 | Regression loss: 0.00473 | Running loss: 0.01269\n",
            "Epoch: 166 | Iteration: 17 | Classification loss: 0.00001 | Regression loss: 0.00696 | Running loss: 0.01269\n",
            "Epoch: 166 | Iteration: 18 | Classification loss: 0.00002 | Regression loss: 0.00913 | Running loss: 0.01268\n",
            "Epoch: 166 | Iteration: 19 | Classification loss: 0.00005 | Regression loss: 0.00311 | Running loss: 0.01266\n",
            "Epoch: 166 | Iteration: 20 | Classification loss: 0.00002 | Regression loss: 0.01749 | Running loss: 0.01268\n",
            "Epoch: 166 | Iteration: 21 | Classification loss: 0.00001 | Regression loss: 0.01208 | Running loss: 0.01269\n",
            "Epoch: 166 | Iteration: 22 | Classification loss: 0.00001 | Regression loss: 0.00987 | Running loss: 0.01270\n",
            "Epoch: 166 | Iteration: 23 | Classification loss: 0.00002 | Regression loss: 0.01157 | Running loss: 0.01266\n",
            "Epoch: 166 | Iteration: 24 | Classification loss: 0.00001 | Regression loss: 0.00403 | Running loss: 0.01265\n",
            "Epoch: 166 | Iteration: 25 | Classification loss: 0.00001 | Regression loss: 0.00768 | Running loss: 0.01265\n",
            "Epoch: 166 | Iteration: 26 | Classification loss: 0.00001 | Regression loss: 0.00881 | Running loss: 0.01264\n",
            "Epoch: 166 | Iteration: 27 | Classification loss: 0.00002 | Regression loss: 0.01171 | Running loss: 0.01261\n",
            "Epoch: 166 | Iteration: 28 | Classification loss: 0.00003 | Regression loss: 0.00591 | Running loss: 0.01259\n",
            "Epoch: 166 | Iteration: 29 | Classification loss: 0.00001 | Regression loss: 0.00423 | Running loss: 0.01251\n",
            "Epoch: 166 | Iteration: 30 | Classification loss: 0.00001 | Regression loss: 0.00503 | Running loss: 0.01250\n",
            "Epoch: 166 | Iteration: 31 | Classification loss: 0.00001 | Regression loss: 0.00368 | Running loss: 0.01248\n",
            "Epoch: 166 | Iteration: 32 | Classification loss: 0.00006 | Regression loss: 0.01759 | Running loss: 0.01249\n",
            "Epoch: 166 | Iteration: 33 | Classification loss: 0.00001 | Regression loss: 0.00462 | Running loss: 0.01246\n",
            "Epoch: 166 | Iteration: 34 | Classification loss: 0.00002 | Regression loss: 0.00717 | Running loss: 0.01246\n",
            "Epoch: 166 | Iteration: 35 | Classification loss: 0.00002 | Regression loss: 0.00662 | Running loss: 0.01243\n",
            "Epoch: 166 | Iteration: 36 | Classification loss: 0.00004 | Regression loss: 0.01071 | Running loss: 0.01239\n",
            "Epoch: 166 | Iteration: 37 | Classification loss: 0.00004 | Regression loss: 0.01322 | Running loss: 0.01239\n",
            "Epoch: 166 | Iteration: 38 | Classification loss: 0.00016 | Regression loss: 0.00815 | Running loss: 0.01238\n",
            "Epoch: 166 | Iteration: 39 | Classification loss: 0.00005 | Regression loss: 0.01293 | Running loss: 0.01237\n",
            "Epoch: 166 | Iteration: 40 | Classification loss: 0.00002 | Regression loss: 0.02371 | Running loss: 0.01239\n",
            "Epoch: 166 | Iteration: 41 | Classification loss: 0.00006 | Regression loss: 0.01808 | Running loss: 0.01239\n",
            "Epoch: 166 | Iteration: 42 | Classification loss: 0.00003 | Regression loss: 0.01090 | Running loss: 0.01237\n",
            "Epoch: 166 | Iteration: 43 | Classification loss: 0.00002 | Regression loss: 0.01170 | Running loss: 0.01236\n",
            "Epoch: 166 | Iteration: 44 | Classification loss: 0.00002 | Regression loss: 0.01120 | Running loss: 0.01235\n",
            "Epoch: 166 | Iteration: 45 | Classification loss: 0.00005 | Regression loss: 0.01032 | Running loss: 0.01234\n",
            "Epoch: 166 | Iteration: 46 | Classification loss: 0.00003 | Regression loss: 0.01264 | Running loss: 0.01235\n",
            "Epoch: 166 | Iteration: 47 | Classification loss: 0.00002 | Regression loss: 0.01383 | Running loss: 0.01236\n",
            "Epoch: 166 | Iteration: 48 | Classification loss: 0.00003 | Regression loss: 0.01626 | Running loss: 0.01236\n",
            "Epoch: 166 | Iteration: 49 | Classification loss: 0.00001 | Regression loss: 0.00770 | Running loss: 0.01236\n",
            "Epoch: 166 | Iteration: 50 | Classification loss: 0.00006 | Regression loss: 0.00409 | Running loss: 0.01235\n",
            "Epoch: 166 | Iteration: 51 | Classification loss: 0.00004 | Regression loss: 0.01627 | Running loss: 0.01233\n",
            "Epoch: 166 | Iteration: 52 | Classification loss: 0.00001 | Regression loss: 0.00363 | Running loss: 0.01231\n",
            "Epoch: 166 | Iteration: 53 | Classification loss: 0.00003 | Regression loss: 0.00549 | Running loss: 0.01231\n",
            "Epoch: 166 | Iteration: 54 | Classification loss: 0.00002 | Regression loss: 0.01618 | Running loss: 0.01233\n",
            "Epoch: 166 | Iteration: 55 | Classification loss: 0.00003 | Regression loss: 0.01015 | Running loss: 0.01234\n",
            "Epoch: 166 | Iteration: 56 | Classification loss: 0.00007 | Regression loss: 0.01676 | Running loss: 0.01234\n",
            "Epoch: 166 | Iteration: 57 | Classification loss: 0.00012 | Regression loss: 0.01455 | Running loss: 0.01235\n",
            "Epoch: 166 | Iteration: 58 | Classification loss: 0.00003 | Regression loss: 0.00814 | Running loss: 0.01236\n",
            "Epoch: 166 | Iteration: 59 | Classification loss: 0.00008 | Regression loss: 0.00998 | Running loss: 0.01235\n",
            "Epoch: 166 | Iteration: 60 | Classification loss: 0.00009 | Regression loss: 0.01359 | Running loss: 0.01236\n",
            "Epoch: 166 | Iteration: 61 | Classification loss: 0.00002 | Regression loss: 0.00672 | Running loss: 0.01236\n",
            "Epoch: 166 | Iteration: 62 | Classification loss: 0.00004 | Regression loss: 0.01020 | Running loss: 0.01234\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 190.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 166 | Iteration: 64 | Classification loss: 0.00003 | Regression loss: 0.01780 | Running loss: 0.01237\n",
            "Epoch: 166 | Iteration: 65 | Classification loss: 0.00002 | Regression loss: 0.00574 | Running loss: 0.01237\n",
            "Epoch: 166 | Iteration: 66 | Classification loss: 0.00001 | Regression loss: 0.01067 | Running loss: 0.01238\n",
            "Epoch: 166 | Iteration: 67 | Classification loss: 0.00003 | Regression loss: 0.00479 | Running loss: 0.01235\n",
            "Epoch: 166 | Iteration: 68 | Classification loss: 0.00006 | Regression loss: 0.01164 | Running loss: 0.01235\n",
            "Epoch: 166 | Iteration: 69 | Classification loss: 0.00003 | Regression loss: 0.01489 | Running loss: 0.01235\n",
            "Epoch: 166 | Iteration: 70 | Classification loss: 0.00002 | Regression loss: 0.01163 | Running loss: 0.01234\n",
            "Epoch: 166 | Iteration: 71 | Classification loss: 0.00001 | Regression loss: 0.00515 | Running loss: 0.01234\n",
            "Epoch: 166 | Iteration: 72 | Classification loss: 0.00003 | Regression loss: 0.01117 | Running loss: 0.01234\n",
            "Epoch: 166 | Iteration: 73 | Classification loss: 0.00002 | Regression loss: 0.00661 | Running loss: 0.01232\n",
            "Epoch: 166 | Iteration: 74 | Classification loss: 0.00005 | Regression loss: 0.02039 | Running loss: 0.01235\n",
            "Epoch: 166 | Iteration: 75 | Classification loss: 0.00002 | Regression loss: 0.01574 | Running loss: 0.01237\n",
            "Epoch: 166 | Iteration: 76 | Classification loss: 0.00003 | Regression loss: 0.00691 | Running loss: 0.01237\n",
            "Epoch: 166 | Iteration: 77 | Classification loss: 0.00002 | Regression loss: 0.00398 | Running loss: 0.01234\n",
            "Epoch: 166 | Iteration: 78 | Classification loss: 0.00002 | Regression loss: 0.00501 | Running loss: 0.01231\n",
            "Epoch: 166 | Iteration: 79 | Classification loss: 0.00001 | Regression loss: 0.02916 | Running loss: 0.01234\n",
            "Epoch: 166 | Iteration: 80 | Classification loss: 0.00002 | Regression loss: 0.01831 | Running loss: 0.01236\n",
            "Epoch: 166 | Iteration: 81 | Classification loss: 0.00005 | Regression loss: 0.01099 | Running loss: 0.01236\n",
            "Epoch: 166 | Iteration: 82 | Classification loss: 0.00003 | Regression loss: 0.01439 | Running loss: 0.01237\n",
            "Epoch: 166 | Iteration: 83 | Classification loss: 0.00002 | Regression loss: 0.01036 | Running loss: 0.01238\n",
            "Evaluating dataset\n",
            "\n",
            "mAP:\n",
            "person: 0.7260020498565122\n",
            "Precision:  0.5362694300518135\n",
            "Recall:  0.8181818181818182\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}, {0: (0.7314384456550078, 253.0)}, {0: (0.7334466242447818, 253.0)}, {0: (0.7357024222917343, 253.0)}, {0: (0.7381550871546381, 253.0)}, {0: (0.739092434551302, 253.0)}, {0: (0.72091448182607, 253.0)}, {0: (0.7389512050165785, 253.0)}, {0: (0.73050932626622, 253.0)}, {0: (0.7263226120133723, 253.0)}, {0: (0.7283463725823673, 253.0)}, {0: (0.7254913772539314, 253.0)}, {0: (0.7281179290861395, 253.0)}, {0: (0.7260020498565122, 253.0)}]\n",
            "Epoch: 167 | Iteration: 0 | Classification loss: 0.00006 | Regression loss: 0.01739 | Running loss: 0.01240\n",
            "Epoch: 167 | Iteration: 1 | Classification loss: 0.00003 | Regression loss: 0.01039 | Running loss: 0.01238\n",
            "Epoch: 167 | Iteration: 2 | Classification loss: 0.00001 | Regression loss: 0.02901 | Running loss: 0.01238\n",
            "Epoch: 167 | Iteration: 3 | Classification loss: 0.00001 | Regression loss: 0.00518 | Running loss: 0.01237\n",
            "Epoch: 167 | Iteration: 4 | Classification loss: 0.00003 | Regression loss: 0.00428 | Running loss: 0.01236\n",
            "Epoch: 167 | Iteration: 5 | Classification loss: 0.00001 | Regression loss: 0.01245 | Running loss: 0.01236\n",
            "Epoch: 167 | Iteration: 6 | Classification loss: 0.00002 | Regression loss: 0.03039 | Running loss: 0.01239\n",
            "Epoch: 167 | Iteration: 7 | Classification loss: 0.00003 | Regression loss: 0.01521 | Running loss: 0.01240\n",
            "Epoch: 167 | Iteration: 8 | Classification loss: 0.00001 | Regression loss: 0.00964 | Running loss: 0.01238\n",
            "Epoch: 167 | Iteration: 9 | Classification loss: 0.00002 | Regression loss: 0.01047 | Running loss: 0.01234\n",
            "Epoch: 167 | Iteration: 10 | Classification loss: 0.00004 | Regression loss: 0.00739 | Running loss: 0.01235\n",
            "Epoch: 167 | Iteration: 11 | Classification loss: 0.00001 | Regression loss: 0.00613 | Running loss: 0.01235\n",
            "Epoch: 167 | Iteration: 12 | Classification loss: 0.00002 | Regression loss: 0.00887 | Running loss: 0.01234\n",
            "Epoch: 167 | Iteration: 13 | Classification loss: 0.00001 | Regression loss: 0.00762 | Running loss: 0.01232\n",
            "Epoch: 167 | Iteration: 14 | Classification loss: 0.00001 | Regression loss: 0.00370 | Running loss: 0.01232\n",
            "Epoch: 167 | Iteration: 15 | Classification loss: 0.00003 | Regression loss: 0.01710 | Running loss: 0.01234\n",
            "Epoch: 167 | Iteration: 16 | Classification loss: 0.00004 | Regression loss: 0.02785 | Running loss: 0.01236\n",
            "Epoch: 167 | Iteration: 17 | Classification loss: 0.00004 | Regression loss: 0.01770 | Running loss: 0.01236\n",
            "Epoch: 167 | Iteration: 18 | Classification loss: 0.00003 | Regression loss: 0.01313 | Running loss: 0.01236\n",
            "Epoch: 167 | Iteration: 19 | Classification loss: 0.00002 | Regression loss: 0.00675 | Running loss: 0.01237\n",
            "Epoch: 167 | Iteration: 20 | Classification loss: 0.00003 | Regression loss: 0.00798 | Running loss: 0.01237\n",
            "Epoch: 167 | Iteration: 21 | Classification loss: 0.00003 | Regression loss: 0.02520 | Running loss: 0.01240\n",
            "Epoch: 167 | Iteration: 22 | Classification loss: 0.00003 | Regression loss: 0.00489 | Running loss: 0.01240\n",
            "Epoch: 167 | Iteration: 23 | Classification loss: 0.00001 | Regression loss: 0.00379 | Running loss: 0.01237\n",
            "Epoch: 167 | Iteration: 24 | Classification loss: 0.00009 | Regression loss: 0.01213 | Running loss: 0.01238\n",
            "Epoch: 167 | Iteration: 25 | Classification loss: 0.00011 | Regression loss: 0.04570 | Running loss: 0.01244\n",
            "Epoch: 167 | Iteration: 26 | Classification loss: 0.00005 | Regression loss: 0.01481 | Running loss: 0.01242\n",
            "Epoch: 167 | Iteration: 27 | Classification loss: 0.00002 | Regression loss: 0.01169 | Running loss: 0.01241\n",
            "Epoch: 167 | Iteration: 28 | Classification loss: 0.00003 | Regression loss: 0.01288 | Running loss: 0.01243\n",
            "Epoch: 167 | Iteration: 29 | Classification loss: 0.00004 | Regression loss: 0.01129 | Running loss: 0.01244\n",
            "Epoch: 167 | Iteration: 30 | Classification loss: 0.00002 | Regression loss: 0.01317 | Running loss: 0.01244\n",
            "Epoch: 167 | Iteration: 31 | Classification loss: 0.00004 | Regression loss: 0.01263 | Running loss: 0.01243\n",
            "Epoch: 167 | Iteration: 32 | Classification loss: 0.00003 | Regression loss: 0.01490 | Running loss: 0.01244\n",
            "Epoch: 167 | Iteration: 33 | Classification loss: 0.00003 | Regression loss: 0.00602 | Running loss: 0.01240\n",
            "Epoch: 167 | Iteration: 34 | Classification loss: 0.00007 | Regression loss: 0.01393 | Running loss: 0.01239\n",
            "Epoch: 167 | Iteration: 35 | Classification loss: 0.00002 | Regression loss: 0.01091 | Running loss: 0.01239\n",
            "Epoch: 167 | Iteration: 36 | Classification loss: 0.00002 | Regression loss: 0.01302 | Running loss: 0.01238\n",
            "Epoch: 167 | Iteration: 37 | Classification loss: 0.00001 | Regression loss: 0.00358 | Running loss: 0.01237\n",
            "Epoch: 167 | Iteration: 38 | Classification loss: 0.00002 | Regression loss: 0.01731 | Running loss: 0.01239\n",
            "Epoch: 167 | Iteration: 39 | Classification loss: 0.00003 | Regression loss: 0.01305 | Running loss: 0.01241\n",
            "Epoch: 167 | Iteration: 40 | Classification loss: 0.00003 | Regression loss: 0.01459 | Running loss: 0.01234\n",
            "Epoch: 167 | Iteration: 41 | Classification loss: 0.00013 | Regression loss: 0.01425 | Running loss: 0.01234\n",
            "Epoch: 167 | Iteration: 42 | Classification loss: 0.00002 | Regression loss: 0.01295 | Running loss: 0.01235\n",
            "Epoch: 167 | Iteration: 43 | Classification loss: 0.00001 | Regression loss: 0.01092 | Running loss: 0.01235\n",
            "Epoch: 167 | Iteration: 44 | Classification loss: 0.00001 | Regression loss: 0.00525 | Running loss: 0.01233\n",
            "Epoch: 167 | Iteration: 45 | Classification loss: 0.00004 | Regression loss: 0.00877 | Running loss: 0.01232\n",
            "Epoch: 167 | Iteration: 46 | Classification loss: 0.00002 | Regression loss: 0.00538 | Running loss: 0.01232\n",
            "Epoch: 167 | Iteration: 47 | Classification loss: 0.00006 | Regression loss: 0.01007 | Running loss: 0.01232\n",
            "Epoch: 167 | Iteration: 48 | Classification loss: 0.00005 | Regression loss: 0.01530 | Running loss: 0.01233\n",
            "Epoch: 167 | Iteration: 49 | Classification loss: 0.00004 | Regression loss: 0.01519 | Running loss: 0.01233\n",
            "Epoch: 167 | Iteration: 50 | Classification loss: 0.00002 | Regression loss: 0.01736 | Running loss: 0.01235\n",
            "Epoch: 167 | Iteration: 51 | Classification loss: 0.00003 | Regression loss: 0.01705 | Running loss: 0.01237\n",
            "Epoch: 167 | Iteration: 52 | Classification loss: 0.00002 | Regression loss: 0.01542 | Running loss: 0.01239\n",
            "Epoch: 167 | Iteration: 53 | Classification loss: 0.00001 | Regression loss: 0.00510 | Running loss: 0.01238\n",
            "Epoch: 167 | Iteration: 54 | Classification loss: 0.00001 | Regression loss: 0.01023 | Running loss: 0.01237\n",
            "Epoch: 167 | Iteration: 55 | Classification loss: 0.00001 | Regression loss: 0.00424 | Running loss: 0.01237\n",
            "Epoch: 167 | Iteration: 56 | Classification loss: 0.00006 | Regression loss: 0.00404 | Running loss: 0.01235\n",
            "Epoch: 167 | Iteration: 57 | Classification loss: 0.00001 | Regression loss: 0.00644 | Running loss: 0.01235\n",
            "Epoch: 167 | Iteration: 58 | Classification loss: 0.00001 | Regression loss: 0.00463 | Running loss: 0.01233\n",
            "Epoch: 167 | Iteration: 59 | Classification loss: 0.00002 | Regression loss: 0.01234 | Running loss: 0.01233\n",
            "Epoch: 167 | Iteration: 60 | Classification loss: 0.00001 | Regression loss: 0.01022 | Running loss: 0.01234\n",
            "Epoch: 167 | Iteration: 61 | Classification loss: 0.00004 | Regression loss: 0.01046 | Running loss: 0.01233\n",
            "Epoch: 167 | Iteration: 62 | Classification loss: 0.00006 | Regression loss: 0.01719 | Running loss: 0.01236\n",
            "Epoch: 167 | Iteration: 63 | Classification loss: 0.00004 | Regression loss: 0.00765 | Running loss: 0.01234\n",
            "Epoch: 167 | Iteration: 64 | Classification loss: 0.00001 | Regression loss: 0.00576 | Running loss: 0.01232\n",
            "Epoch: 167 | Iteration: 65 | Classification loss: 0.00002 | Regression loss: 0.01408 | Running loss: 0.01233\n",
            "Epoch: 167 | Iteration: 66 | Classification loss: 0.00001 | Regression loss: 0.00362 | Running loss: 0.01230\n",
            "Epoch: 167 | Iteration: 67 | Classification loss: 0.00002 | Regression loss: 0.00545 | Running loss: 0.01230\n",
            "Epoch: 167 | Iteration: 68 | Classification loss: 0.00001 | Regression loss: 0.00471 | Running loss: 0.01230\n",
            "Epoch: 167 | Iteration: 69 | Classification loss: 0.00008 | Regression loss: 0.01416 | Running loss: 0.01230\n",
            "Epoch: 167 | Iteration: 70 | Classification loss: 0.00002 | Regression loss: 0.01068 | Running loss: 0.01231\n",
            "Epoch: 167 | Iteration: 71 | Classification loss: 0.00002 | Regression loss: 0.00663 | Running loss: 0.01229\n",
            "Epoch: 167 | Iteration: 72 | Classification loss: 0.00015 | Regression loss: 0.00804 | Running loss: 0.01224\n",
            "Epoch: 167 | Iteration: 73 | Classification loss: 0.00004 | Regression loss: 0.00356 | Running loss: 0.01224\n",
            "Epoch: 167 | Iteration: 74 | Classification loss: 0.00004 | Regression loss: 0.01440 | Running loss: 0.01224\n",
            "Epoch: 167 | Iteration: 75 | Classification loss: 0.00008 | Regression loss: 0.01478 | Running loss: 0.01224\n",
            "Epoch: 167 | Iteration: 76 | Classification loss: 0.00005 | Regression loss: 0.01021 | Running loss: 0.01225\n",
            "Epoch: 167 | Iteration: 77 | Classification loss: 0.00012 | Regression loss: 0.01988 | Running loss: 0.01226\n",
            "Epoch: 167 | Iteration: 78 | Classification loss: 0.00003 | Regression loss: 0.01716 | Running loss: 0.01227\n",
            "Epoch: 167 | Iteration: 79 | Classification loss: 0.00002 | Regression loss: 0.00701 | Running loss: 0.01225\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 188.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 167 | Iteration: 81 | Classification loss: 0.00003 | Regression loss: 0.01286 | Running loss: 0.01224\n",
            "Epoch: 167 | Iteration: 82 | Classification loss: 0.00005 | Regression loss: 0.02083 | Running loss: 0.01227\n",
            "Epoch: 167 | Iteration: 83 | Classification loss: 0.00002 | Regression loss: 0.01233 | Running loss: 0.01227\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.7217236108381175\n",
            "Precision:  0.5280612244897959\n",
            "Recall:  0.8181818181818182\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}, {0: (0.7314384456550078, 253.0)}, {0: (0.7334466242447818, 253.0)}, {0: (0.7357024222917343, 253.0)}, {0: (0.7381550871546381, 253.0)}, {0: (0.739092434551302, 253.0)}, {0: (0.72091448182607, 253.0)}, {0: (0.7389512050165785, 253.0)}, {0: (0.73050932626622, 253.0)}, {0: (0.7263226120133723, 253.0)}, {0: (0.7283463725823673, 253.0)}, {0: (0.7254913772539314, 253.0)}, {0: (0.7281179290861395, 253.0)}, {0: (0.7260020498565122, 253.0)}, {0: (0.7217236108381175, 253.0)}]\n",
            "Epoch: 168 | Iteration: 0 | Classification loss: 0.00003 | Regression loss: 0.01430 | Running loss: 0.01228\n",
            "Epoch: 168 | Iteration: 1 | Classification loss: 0.00004 | Regression loss: 0.01483 | Running loss: 0.01227\n",
            "Epoch: 168 | Iteration: 2 | Classification loss: 0.00001 | Regression loss: 0.00715 | Running loss: 0.01227\n",
            "Epoch: 168 | Iteration: 3 | Classification loss: 0.00001 | Regression loss: 0.02853 | Running loss: 0.01230\n",
            "Epoch: 168 | Iteration: 4 | Classification loss: 0.00003 | Regression loss: 0.00309 | Running loss: 0.01229\n",
            "Epoch: 168 | Iteration: 5 | Classification loss: 0.00001 | Regression loss: 0.01211 | Running loss: 0.01228\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 188.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 168 | Iteration: 7 | Classification loss: 0.00004 | Regression loss: 0.00956 | Running loss: 0.01227\n",
            "Epoch: 168 | Iteration: 8 | Classification loss: 0.00001 | Regression loss: 0.00735 | Running loss: 0.01227\n",
            "Epoch: 168 | Iteration: 9 | Classification loss: 0.00001 | Regression loss: 0.01076 | Running loss: 0.01227\n",
            "Epoch: 168 | Iteration: 10 | Classification loss: 0.00001 | Regression loss: 0.00888 | Running loss: 0.01225\n",
            "Epoch: 168 | Iteration: 11 | Classification loss: 0.00006 | Regression loss: 0.01688 | Running loss: 0.01226\n",
            "Epoch: 168 | Iteration: 12 | Classification loss: 0.00005 | Regression loss: 0.01481 | Running loss: 0.01226\n",
            "Epoch: 168 | Iteration: 13 | Classification loss: 0.00006 | Regression loss: 0.02146 | Running loss: 0.01226\n",
            "Epoch: 168 | Iteration: 14 | Classification loss: 0.00003 | Regression loss: 0.01201 | Running loss: 0.01226\n",
            "Epoch: 168 | Iteration: 15 | Classification loss: 0.00006 | Regression loss: 0.00399 | Running loss: 0.01221\n",
            "Epoch: 168 | Iteration: 16 | Classification loss: 0.00005 | Regression loss: 0.01270 | Running loss: 0.01221\n",
            "Epoch: 168 | Iteration: 17 | Classification loss: 0.00002 | Regression loss: 0.01624 | Running loss: 0.01222\n",
            "Epoch: 168 | Iteration: 18 | Classification loss: 0.00004 | Regression loss: 0.01746 | Running loss: 0.01224\n",
            "Epoch: 168 | Iteration: 19 | Classification loss: 0.00003 | Regression loss: 0.00732 | Running loss: 0.01223\n",
            "Epoch: 168 | Iteration: 20 | Classification loss: 0.00001 | Regression loss: 0.02256 | Running loss: 0.01227\n",
            "Epoch: 168 | Iteration: 21 | Classification loss: 0.00001 | Regression loss: 0.00501 | Running loss: 0.01224\n",
            "Epoch: 168 | Iteration: 22 | Classification loss: 0.00002 | Regression loss: 0.01192 | Running loss: 0.01224\n",
            "Epoch: 168 | Iteration: 23 | Classification loss: 0.00001 | Regression loss: 0.00464 | Running loss: 0.01222\n",
            "Epoch: 168 | Iteration: 24 | Classification loss: 0.00001 | Regression loss: 0.00574 | Running loss: 0.01221\n",
            "Epoch: 168 | Iteration: 25 | Classification loss: 0.00003 | Regression loss: 0.01668 | Running loss: 0.01214\n",
            "Epoch: 168 | Iteration: 26 | Classification loss: 0.00001 | Regression loss: 0.00551 | Running loss: 0.01210\n",
            "Epoch: 168 | Iteration: 27 | Classification loss: 0.00002 | Regression loss: 0.00628 | Running loss: 0.01210\n",
            "Epoch: 168 | Iteration: 28 | Classification loss: 0.00002 | Regression loss: 0.01991 | Running loss: 0.01212\n",
            "Epoch: 168 | Iteration: 29 | Classification loss: 0.00001 | Regression loss: 0.00510 | Running loss: 0.01211\n",
            "Epoch: 168 | Iteration: 30 | Classification loss: 0.00003 | Regression loss: 0.01141 | Running loss: 0.01212\n",
            "Epoch: 168 | Iteration: 31 | Classification loss: 0.00011 | Regression loss: 0.01359 | Running loss: 0.01213\n",
            "Epoch: 168 | Iteration: 32 | Classification loss: 0.00002 | Regression loss: 0.00623 | Running loss: 0.01211\n",
            "Epoch: 168 | Iteration: 33 | Classification loss: 0.00003 | Regression loss: 0.02283 | Running loss: 0.01215\n",
            "Epoch: 168 | Iteration: 34 | Classification loss: 0.00002 | Regression loss: 0.00700 | Running loss: 0.01214\n",
            "Epoch: 168 | Iteration: 35 | Classification loss: 0.00002 | Regression loss: 0.01830 | Running loss: 0.01215\n",
            "Epoch: 168 | Iteration: 36 | Classification loss: 0.00009 | Regression loss: 0.01238 | Running loss: 0.01215\n",
            "Epoch: 168 | Iteration: 37 | Classification loss: 0.00007 | Regression loss: 0.01481 | Running loss: 0.01217\n",
            "Epoch: 168 | Iteration: 38 | Classification loss: 0.00003 | Regression loss: 0.00615 | Running loss: 0.01216\n",
            "Epoch: 168 | Iteration: 39 | Classification loss: 0.00003 | Regression loss: 0.00693 | Running loss: 0.01211\n",
            "Epoch: 168 | Iteration: 40 | Classification loss: 0.00002 | Regression loss: 0.01208 | Running loss: 0.01213\n",
            "Epoch: 168 | Iteration: 41 | Classification loss: 0.00005 | Regression loss: 0.01691 | Running loss: 0.01215\n",
            "Epoch: 168 | Iteration: 42 | Classification loss: 0.00001 | Regression loss: 0.00444 | Running loss: 0.01215\n",
            "Epoch: 168 | Iteration: 43 | Classification loss: 0.00005 | Regression loss: 0.01386 | Running loss: 0.01215\n",
            "Epoch: 168 | Iteration: 44 | Classification loss: 0.00003 | Regression loss: 0.00461 | Running loss: 0.01213\n",
            "Epoch: 168 | Iteration: 45 | Classification loss: 0.00002 | Regression loss: 0.01156 | Running loss: 0.01213\n",
            "Epoch: 168 | Iteration: 46 | Classification loss: 0.00002 | Regression loss: 0.01096 | Running loss: 0.01212\n",
            "Epoch: 168 | Iteration: 47 | Classification loss: 0.00001 | Regression loss: 0.03103 | Running loss: 0.01216\n",
            "Epoch: 168 | Iteration: 48 | Classification loss: 0.00002 | Regression loss: 0.01563 | Running loss: 0.01219\n",
            "Epoch: 168 | Iteration: 49 | Classification loss: 0.00003 | Regression loss: 0.00951 | Running loss: 0.01217\n",
            "Epoch: 168 | Iteration: 50 | Classification loss: 0.00008 | Regression loss: 0.01383 | Running loss: 0.01216\n",
            "Epoch: 168 | Iteration: 51 | Classification loss: 0.00003 | Regression loss: 0.01584 | Running loss: 0.01217\n",
            "Epoch: 168 | Iteration: 52 | Classification loss: 0.00002 | Regression loss: 0.01125 | Running loss: 0.01218\n",
            "Epoch: 168 | Iteration: 53 | Classification loss: 0.00015 | Regression loss: 0.00737 | Running loss: 0.01217\n",
            "Epoch: 168 | Iteration: 54 | Classification loss: 0.00001 | Regression loss: 0.00641 | Running loss: 0.01215\n",
            "Epoch: 168 | Iteration: 55 | Classification loss: 0.00001 | Regression loss: 0.00338 | Running loss: 0.01215\n",
            "Epoch: 168 | Iteration: 56 | Classification loss: 0.00003 | Regression loss: 0.01291 | Running loss: 0.01214\n",
            "Epoch: 168 | Iteration: 57 | Classification loss: 0.00003 | Regression loss: 0.01415 | Running loss: 0.01215\n",
            "Epoch: 168 | Iteration: 58 | Classification loss: 0.00005 | Regression loss: 0.01629 | Running loss: 0.01217\n",
            "Epoch: 168 | Iteration: 59 | Classification loss: 0.00002 | Regression loss: 0.00425 | Running loss: 0.01216\n",
            "Epoch: 168 | Iteration: 60 | Classification loss: 0.00002 | Regression loss: 0.01121 | Running loss: 0.01217\n",
            "Epoch: 168 | Iteration: 61 | Classification loss: 0.00004 | Regression loss: 0.01565 | Running loss: 0.01218\n",
            "Epoch: 168 | Iteration: 62 | Classification loss: 0.00004 | Regression loss: 0.01080 | Running loss: 0.01219\n",
            "Epoch: 168 | Iteration: 63 | Classification loss: 0.00007 | Regression loss: 0.01393 | Running loss: 0.01219\n",
            "Epoch: 168 | Iteration: 64 | Classification loss: 0.00001 | Regression loss: 0.01139 | Running loss: 0.01220\n",
            "Epoch: 168 | Iteration: 65 | Classification loss: 0.00002 | Regression loss: 0.01157 | Running loss: 0.01216\n",
            "Epoch: 168 | Iteration: 66 | Classification loss: 0.00003 | Regression loss: 0.01785 | Running loss: 0.01219\n",
            "Epoch: 168 | Iteration: 67 | Classification loss: 0.00001 | Regression loss: 0.00444 | Running loss: 0.01217\n",
            "Epoch: 168 | Iteration: 68 | Classification loss: 0.00001 | Regression loss: 0.00367 | Running loss: 0.01214\n",
            "Epoch: 168 | Iteration: 69 | Classification loss: 0.00002 | Regression loss: 0.01203 | Running loss: 0.01213\n",
            "Epoch: 168 | Iteration: 70 | Classification loss: 0.00001 | Regression loss: 0.00365 | Running loss: 0.01210\n",
            "Epoch: 168 | Iteration: 71 | Classification loss: 0.00012 | Regression loss: 0.04588 | Running loss: 0.01215\n",
            "Epoch: 168 | Iteration: 72 | Classification loss: 0.00002 | Regression loss: 0.00626 | Running loss: 0.01215\n",
            "Epoch: 168 | Iteration: 73 | Classification loss: 0.00005 | Regression loss: 0.01003 | Running loss: 0.01214\n",
            "Epoch: 168 | Iteration: 74 | Classification loss: 0.00005 | Regression loss: 0.01745 | Running loss: 0.01214\n",
            "Epoch: 168 | Iteration: 75 | Classification loss: 0.00003 | Regression loss: 0.01039 | Running loss: 0.01214\n",
            "Epoch: 168 | Iteration: 76 | Classification loss: 0.00001 | Regression loss: 0.00986 | Running loss: 0.01213\n",
            "Epoch: 168 | Iteration: 77 | Classification loss: 0.00003 | Regression loss: 0.00752 | Running loss: 0.01213\n",
            "Epoch: 168 | Iteration: 78 | Classification loss: 0.00002 | Regression loss: 0.01211 | Running loss: 0.01213\n",
            "Epoch: 168 | Iteration: 79 | Classification loss: 0.00001 | Regression loss: 0.01511 | Running loss: 0.01215\n",
            "Epoch: 168 | Iteration: 80 | Classification loss: 0.00001 | Regression loss: 0.01254 | Running loss: 0.01214\n",
            "Epoch: 168 | Iteration: 81 | Classification loss: 0.00010 | Regression loss: 0.01769 | Running loss: 0.01216\n",
            "Epoch: 168 | Iteration: 82 | Classification loss: 0.00002 | Regression loss: 0.00604 | Running loss: 0.01216\n",
            "Epoch: 168 | Iteration: 83 | Classification loss: 0.00006 | Regression loss: 0.01093 | Running loss: 0.01217\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.7267514829312511\n",
            "Precision:  0.5267175572519084\n",
            "Recall:  0.8181818181818182\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}, {0: (0.7314384456550078, 253.0)}, {0: (0.7334466242447818, 253.0)}, {0: (0.7357024222917343, 253.0)}, {0: (0.7381550871546381, 253.0)}, {0: (0.739092434551302, 253.0)}, {0: (0.72091448182607, 253.0)}, {0: (0.7389512050165785, 253.0)}, {0: (0.73050932626622, 253.0)}, {0: (0.7263226120133723, 253.0)}, {0: (0.7283463725823673, 253.0)}, {0: (0.7254913772539314, 253.0)}, {0: (0.7281179290861395, 253.0)}, {0: (0.7260020498565122, 253.0)}, {0: (0.7217236108381175, 253.0)}, {0: (0.7267514829312511, 253.0)}]\n",
            "Epoch: 169 | Iteration: 0 | Classification loss: 0.00001 | Regression loss: 0.00991 | Running loss: 0.01216\n",
            "Epoch: 169 | Iteration: 1 | Classification loss: 0.00002 | Regression loss: 0.01316 | Running loss: 0.01218\n",
            "Epoch: 169 | Iteration: 2 | Classification loss: 0.00001 | Regression loss: 0.00536 | Running loss: 0.01218\n",
            "Epoch: 169 | Iteration: 3 | Classification loss: 0.00003 | Regression loss: 0.01502 | Running loss: 0.01218\n",
            "Epoch: 169 | Iteration: 4 | Classification loss: 0.00007 | Regression loss: 0.01404 | Running loss: 0.01220\n",
            "Epoch: 169 | Iteration: 5 | Classification loss: 0.00016 | Regression loss: 0.00839 | Running loss: 0.01221\n",
            "Epoch: 169 | Iteration: 6 | Classification loss: 0.00002 | Regression loss: 0.01204 | Running loss: 0.01220\n",
            "Epoch: 169 | Iteration: 7 | Classification loss: 0.00003 | Regression loss: 0.01256 | Running loss: 0.01220\n",
            "Epoch: 169 | Iteration: 8 | Classification loss: 0.00003 | Regression loss: 0.01352 | Running loss: 0.01222\n",
            "Epoch: 169 | Iteration: 9 | Classification loss: 0.00003 | Regression loss: 0.01291 | Running loss: 0.01223\n",
            "Epoch: 169 | Iteration: 10 | Classification loss: 0.00003 | Regression loss: 0.00299 | Running loss: 0.01221\n",
            "Epoch: 169 | Iteration: 11 | Classification loss: 0.00001 | Regression loss: 0.00422 | Running loss: 0.01219\n",
            "Epoch: 169 | Iteration: 12 | Classification loss: 0.00012 | Regression loss: 0.01458 | Running loss: 0.01220\n",
            "Epoch: 169 | Iteration: 13 | Classification loss: 0.00006 | Regression loss: 0.01057 | Running loss: 0.01221\n",
            "Epoch: 169 | Iteration: 14 | Classification loss: 0.00001 | Regression loss: 0.01147 | Running loss: 0.01220\n",
            "Epoch: 169 | Iteration: 15 | Classification loss: 0.00001 | Regression loss: 0.00411 | Running loss: 0.01215\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 188.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 169 | Iteration: 17 | Classification loss: 0.00014 | Regression loss: 0.02003 | Running loss: 0.01217\n",
            "Epoch: 169 | Iteration: 18 | Classification loss: 0.00001 | Regression loss: 0.00639 | Running loss: 0.01218\n",
            "Epoch: 169 | Iteration: 19 | Classification loss: 0.00003 | Regression loss: 0.01300 | Running loss: 0.01219\n",
            "Epoch: 169 | Iteration: 20 | Classification loss: 0.00002 | Regression loss: 0.01622 | Running loss: 0.01219\n",
            "Epoch: 169 | Iteration: 21 | Classification loss: 0.00002 | Regression loss: 0.00682 | Running loss: 0.01215\n",
            "Epoch: 169 | Iteration: 22 | Classification loss: 0.00002 | Regression loss: 0.00732 | Running loss: 0.01213\n",
            "Epoch: 169 | Iteration: 23 | Classification loss: 0.00004 | Regression loss: 0.01839 | Running loss: 0.01214\n",
            "Epoch: 169 | Iteration: 24 | Classification loss: 0.00001 | Regression loss: 0.01220 | Running loss: 0.01213\n",
            "Epoch: 169 | Iteration: 25 | Classification loss: 0.00002 | Regression loss: 0.02495 | Running loss: 0.01216\n",
            "Epoch: 169 | Iteration: 26 | Classification loss: 0.00002 | Regression loss: 0.01570 | Running loss: 0.01216\n",
            "Epoch: 169 | Iteration: 27 | Classification loss: 0.00006 | Regression loss: 0.02109 | Running loss: 0.01217\n",
            "Epoch: 169 | Iteration: 28 | Classification loss: 0.00002 | Regression loss: 0.01909 | Running loss: 0.01219\n",
            "Epoch: 169 | Iteration: 29 | Classification loss: 0.00001 | Regression loss: 0.02939 | Running loss: 0.01221\n",
            "Epoch: 169 | Iteration: 30 | Classification loss: 0.00002 | Regression loss: 0.01241 | Running loss: 0.01220\n",
            "Epoch: 169 | Iteration: 31 | Classification loss: 0.00004 | Regression loss: 0.01078 | Running loss: 0.01220\n",
            "Epoch: 169 | Iteration: 32 | Classification loss: 0.00002 | Regression loss: 0.01459 | Running loss: 0.01222\n",
            "Epoch: 169 | Iteration: 33 | Classification loss: 0.00012 | Regression loss: 0.04601 | Running loss: 0.01230\n",
            "Epoch: 169 | Iteration: 34 | Classification loss: 0.00002 | Regression loss: 0.00591 | Running loss: 0.01228\n",
            "Epoch: 169 | Iteration: 35 | Classification loss: 0.00002 | Regression loss: 0.00746 | Running loss: 0.01227\n",
            "Epoch: 169 | Iteration: 36 | Classification loss: 0.00001 | Regression loss: 0.00322 | Running loss: 0.01225\n",
            "Epoch: 169 | Iteration: 37 | Classification loss: 0.00001 | Regression loss: 0.02887 | Running loss: 0.01228\n",
            "Epoch: 169 | Iteration: 38 | Classification loss: 0.00004 | Regression loss: 0.01643 | Running loss: 0.01229\n",
            "Epoch: 169 | Iteration: 39 | Classification loss: 0.00002 | Regression loss: 0.01374 | Running loss: 0.01229\n",
            "Epoch: 169 | Iteration: 40 | Classification loss: 0.00004 | Regression loss: 0.00970 | Running loss: 0.01228\n",
            "Epoch: 169 | Iteration: 41 | Classification loss: 0.00001 | Regression loss: 0.00728 | Running loss: 0.01225\n",
            "Epoch: 169 | Iteration: 42 | Classification loss: 0.00002 | Regression loss: 0.00692 | Running loss: 0.01223\n",
            "Epoch: 169 | Iteration: 43 | Classification loss: 0.00009 | Regression loss: 0.01379 | Running loss: 0.01220\n",
            "Epoch: 169 | Iteration: 44 | Classification loss: 0.00002 | Regression loss: 0.00536 | Running loss: 0.01219\n",
            "Epoch: 169 | Iteration: 45 | Classification loss: 0.00001 | Regression loss: 0.00383 | Running loss: 0.01215\n",
            "Epoch: 169 | Iteration: 46 | Classification loss: 0.00001 | Regression loss: 0.00714 | Running loss: 0.01215\n",
            "Epoch: 169 | Iteration: 47 | Classification loss: 0.00003 | Regression loss: 0.01636 | Running loss: 0.01215\n",
            "Epoch: 169 | Iteration: 48 | Classification loss: 0.00001 | Regression loss: 0.00892 | Running loss: 0.01215\n",
            "Epoch: 169 | Iteration: 49 | Classification loss: 0.00001 | Regression loss: 0.01223 | Running loss: 0.01215\n",
            "Epoch: 169 | Iteration: 50 | Classification loss: 0.00001 | Regression loss: 0.00486 | Running loss: 0.01213\n",
            "Epoch: 169 | Iteration: 51 | Classification loss: 0.00003 | Regression loss: 0.00745 | Running loss: 0.01214\n",
            "Epoch: 169 | Iteration: 52 | Classification loss: 0.00002 | Regression loss: 0.00874 | Running loss: 0.01214\n",
            "Epoch: 169 | Iteration: 53 | Classification loss: 0.00002 | Regression loss: 0.00390 | Running loss: 0.01213\n",
            "Epoch: 169 | Iteration: 54 | Classification loss: 0.00003 | Regression loss: 0.00619 | Running loss: 0.01211\n",
            "Epoch: 169 | Iteration: 55 | Classification loss: 0.00004 | Regression loss: 0.02639 | Running loss: 0.01214\n",
            "Epoch: 169 | Iteration: 56 | Classification loss: 0.00002 | Regression loss: 0.00439 | Running loss: 0.01211\n",
            "Epoch: 169 | Iteration: 57 | Classification loss: 0.00002 | Regression loss: 0.01074 | Running loss: 0.01211\n",
            "Epoch: 169 | Iteration: 58 | Classification loss: 0.00003 | Regression loss: 0.00501 | Running loss: 0.01210\n",
            "Epoch: 169 | Iteration: 59 | Classification loss: 0.00004 | Regression loss: 0.00982 | Running loss: 0.01211\n",
            "Epoch: 169 | Iteration: 60 | Classification loss: 0.00003 | Regression loss: 0.01353 | Running loss: 0.01210\n",
            "Epoch: 169 | Iteration: 61 | Classification loss: 0.00006 | Regression loss: 0.00463 | Running loss: 0.01208\n",
            "Epoch: 169 | Iteration: 62 | Classification loss: 0.00001 | Regression loss: 0.00735 | Running loss: 0.01208\n",
            "Epoch: 169 | Iteration: 63 | Classification loss: 0.00002 | Regression loss: 0.01060 | Running loss: 0.01207\n",
            "Epoch: 169 | Iteration: 64 | Classification loss: 0.00001 | Regression loss: 0.00460 | Running loss: 0.01207\n",
            "Epoch: 169 | Iteration: 65 | Classification loss: 0.00002 | Regression loss: 0.01027 | Running loss: 0.01207\n",
            "Epoch: 169 | Iteration: 66 | Classification loss: 0.00005 | Regression loss: 0.01593 | Running loss: 0.01208\n",
            "Epoch: 169 | Iteration: 67 | Classification loss: 0.00006 | Regression loss: 0.01696 | Running loss: 0.01210\n",
            "Epoch: 169 | Iteration: 68 | Classification loss: 0.00002 | Regression loss: 0.00582 | Running loss: 0.01202\n",
            "Epoch: 169 | Iteration: 69 | Classification loss: 0.00003 | Regression loss: 0.01166 | Running loss: 0.01200\n",
            "Epoch: 169 | Iteration: 70 | Classification loss: 0.00001 | Regression loss: 0.00460 | Running loss: 0.01199\n",
            "Epoch: 169 | Iteration: 71 | Classification loss: 0.00002 | Regression loss: 0.00758 | Running loss: 0.01198\n",
            "Epoch: 169 | Iteration: 72 | Classification loss: 0.00005 | Regression loss: 0.01658 | Running loss: 0.01201\n",
            "Epoch: 169 | Iteration: 73 | Classification loss: 0.00002 | Regression loss: 0.01235 | Running loss: 0.01202\n",
            "Epoch: 169 | Iteration: 74 | Classification loss: 0.00006 | Regression loss: 0.01596 | Running loss: 0.01203\n",
            "Epoch: 169 | Iteration: 75 | Classification loss: 0.00007 | Regression loss: 0.00966 | Running loss: 0.01202\n",
            "Epoch: 169 | Iteration: 76 | Classification loss: 0.00003 | Regression loss: 0.01449 | Running loss: 0.01204\n",
            "Epoch: 169 | Iteration: 77 | Classification loss: 0.00001 | Regression loss: 0.01153 | Running loss: 0.01206\n",
            "Epoch: 169 | Iteration: 78 | Classification loss: 0.00003 | Regression loss: 0.01359 | Running loss: 0.01205\n",
            "Epoch: 169 | Iteration: 79 | Classification loss: 0.00004 | Regression loss: 0.01331 | Running loss: 0.01205\n",
            "Epoch: 169 | Iteration: 80 | Classification loss: 0.00004 | Regression loss: 0.01366 | Running loss: 0.01206\n",
            "Epoch: 169 | Iteration: 81 | Classification loss: 0.00002 | Regression loss: 0.01147 | Running loss: 0.01206\n",
            "Epoch: 169 | Iteration: 82 | Classification loss: 0.00004 | Regression loss: 0.01050 | Running loss: 0.01207\n",
            "Epoch: 169 | Iteration: 83 | Classification loss: 0.00002 | Regression loss: 0.01876 | Running loss: 0.01208\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.7227483652714889\n",
            "Precision:  0.5362694300518135\n",
            "Recall:  0.8181818181818182\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}, {0: (0.7314384456550078, 253.0)}, {0: (0.7334466242447818, 253.0)}, {0: (0.7357024222917343, 253.0)}, {0: (0.7381550871546381, 253.0)}, {0: (0.739092434551302, 253.0)}, {0: (0.72091448182607, 253.0)}, {0: (0.7389512050165785, 253.0)}, {0: (0.73050932626622, 253.0)}, {0: (0.7263226120133723, 253.0)}, {0: (0.7283463725823673, 253.0)}, {0: (0.7254913772539314, 253.0)}, {0: (0.7281179290861395, 253.0)}, {0: (0.7260020498565122, 253.0)}, {0: (0.7217236108381175, 253.0)}, {0: (0.7267514829312511, 253.0)}, {0: (0.7227483652714889, 253.0)}]\n",
            "Epoch: 170 | Iteration: 0 | Classification loss: 0.00001 | Regression loss: 0.00387 | Running loss: 0.01207\n",
            "Epoch: 170 | Iteration: 1 | Classification loss: 0.00004 | Regression loss: 0.01919 | Running loss: 0.01210\n",
            "Epoch: 170 | Iteration: 2 | Classification loss: 0.00003 | Regression loss: 0.01728 | Running loss: 0.01210\n",
            "Epoch: 170 | Iteration: 3 | Classification loss: 0.00001 | Regression loss: 0.00639 | Running loss: 0.01208\n",
            "Epoch: 170 | Iteration: 4 | Classification loss: 0.00004 | Regression loss: 0.00913 | Running loss: 0.01205\n",
            "Epoch: 170 | Iteration: 5 | Classification loss: 0.00002 | Regression loss: 0.01128 | Running loss: 0.01206\n",
            "Epoch: 170 | Iteration: 6 | Classification loss: 0.00002 | Regression loss: 0.03115 | Running loss: 0.01209\n",
            "Epoch: 170 | Iteration: 7 | Classification loss: 0.00002 | Regression loss: 0.01114 | Running loss: 0.01210\n",
            "Epoch: 170 | Iteration: 8 | Classification loss: 0.00003 | Regression loss: 0.01496 | Running loss: 0.01212\n",
            "Epoch: 170 | Iteration: 9 | Classification loss: 0.00003 | Regression loss: 0.02233 | Running loss: 0.01214\n",
            "Epoch: 170 | Iteration: 10 | Classification loss: 0.00015 | Regression loss: 0.00741 | Running loss: 0.01214\n",
            "Epoch: 170 | Iteration: 11 | Classification loss: 0.00003 | Regression loss: 0.01007 | Running loss: 0.01215\n",
            "Epoch: 170 | Iteration: 12 | Classification loss: 0.00002 | Regression loss: 0.00564 | Running loss: 0.01214\n",
            "Epoch: 170 | Iteration: 13 | Classification loss: 0.00004 | Regression loss: 0.01115 | Running loss: 0.01214\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 188.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 170 | Iteration: 15 | Classification loss: 0.00005 | Regression loss: 0.01768 | Running loss: 0.01216\n",
            "Epoch: 170 | Iteration: 16 | Classification loss: 0.00005 | Regression loss: 0.00364 | Running loss: 0.01214\n",
            "Epoch: 170 | Iteration: 17 | Classification loss: 0.00001 | Regression loss: 0.00312 | Running loss: 0.01212\n",
            "Epoch: 170 | Iteration: 18 | Classification loss: 0.00003 | Regression loss: 0.01215 | Running loss: 0.01213\n",
            "Epoch: 170 | Iteration: 19 | Classification loss: 0.00001 | Regression loss: 0.00668 | Running loss: 0.01210\n",
            "Epoch: 170 | Iteration: 20 | Classification loss: 0.00001 | Regression loss: 0.01303 | Running loss: 0.01211\n",
            "Epoch: 170 | Iteration: 21 | Classification loss: 0.00002 | Regression loss: 0.01006 | Running loss: 0.01211\n",
            "Epoch: 170 | Iteration: 22 | Classification loss: 0.00003 | Regression loss: 0.01513 | Running loss: 0.01213\n",
            "Epoch: 170 | Iteration: 23 | Classification loss: 0.00003 | Regression loss: 0.00753 | Running loss: 0.01212\n",
            "Epoch: 170 | Iteration: 24 | Classification loss: 0.00001 | Regression loss: 0.00986 | Running loss: 0.01213\n",
            "Epoch: 170 | Iteration: 25 | Classification loss: 0.00004 | Regression loss: 0.00313 | Running loss: 0.01210\n",
            "Epoch: 170 | Iteration: 26 | Classification loss: 0.00001 | Regression loss: 0.00858 | Running loss: 0.01208\n",
            "Epoch: 170 | Iteration: 27 | Classification loss: 0.00007 | Regression loss: 0.01186 | Running loss: 0.01207\n",
            "Epoch: 170 | Iteration: 28 | Classification loss: 0.00002 | Regression loss: 0.01802 | Running loss: 0.01207\n",
            "Epoch: 170 | Iteration: 29 | Classification loss: 0.00002 | Regression loss: 0.02028 | Running loss: 0.01209\n",
            "Epoch: 170 | Iteration: 30 | Classification loss: 0.00005 | Regression loss: 0.00997 | Running loss: 0.01209\n",
            "Epoch: 170 | Iteration: 31 | Classification loss: 0.00002 | Regression loss: 0.00496 | Running loss: 0.01208\n",
            "Epoch: 170 | Iteration: 32 | Classification loss: 0.00003 | Regression loss: 0.01536 | Running loss: 0.01208\n",
            "Epoch: 170 | Iteration: 33 | Classification loss: 0.00002 | Regression loss: 0.00442 | Running loss: 0.01206\n",
            "Epoch: 170 | Iteration: 34 | Classification loss: 0.00005 | Regression loss: 0.01758 | Running loss: 0.01209\n",
            "Epoch: 170 | Iteration: 35 | Classification loss: 0.00003 | Regression loss: 0.01366 | Running loss: 0.01210\n",
            "Epoch: 170 | Iteration: 36 | Classification loss: 0.00001 | Regression loss: 0.00485 | Running loss: 0.01208\n",
            "Epoch: 170 | Iteration: 37 | Classification loss: 0.00001 | Regression loss: 0.00548 | Running loss: 0.01207\n",
            "Epoch: 170 | Iteration: 38 | Classification loss: 0.00003 | Regression loss: 0.01334 | Running loss: 0.01200\n",
            "Epoch: 170 | Iteration: 39 | Classification loss: 0.00001 | Regression loss: 0.00736 | Running loss: 0.01198\n",
            "Epoch: 170 | Iteration: 40 | Classification loss: 0.00001 | Regression loss: 0.01194 | Running loss: 0.01200\n",
            "Epoch: 170 | Iteration: 41 | Classification loss: 0.00001 | Regression loss: 0.01215 | Running loss: 0.01199\n",
            "Epoch: 170 | Iteration: 42 | Classification loss: 0.00002 | Regression loss: 0.00840 | Running loss: 0.01199\n",
            "Epoch: 170 | Iteration: 43 | Classification loss: 0.00005 | Regression loss: 0.01579 | Running loss: 0.01200\n",
            "Epoch: 170 | Iteration: 44 | Classification loss: 0.00001 | Regression loss: 0.02849 | Running loss: 0.01205\n",
            "Epoch: 170 | Iteration: 45 | Classification loss: 0.00002 | Regression loss: 0.01867 | Running loss: 0.01206\n",
            "Epoch: 170 | Iteration: 46 | Classification loss: 0.00007 | Regression loss: 0.01326 | Running loss: 0.01208\n",
            "Epoch: 170 | Iteration: 47 | Classification loss: 0.00001 | Regression loss: 0.00812 | Running loss: 0.01207\n",
            "Epoch: 170 | Iteration: 48 | Classification loss: 0.00006 | Regression loss: 0.01532 | Running loss: 0.01209\n",
            "Epoch: 170 | Iteration: 49 | Classification loss: 0.00003 | Regression loss: 0.01581 | Running loss: 0.01211\n",
            "Epoch: 170 | Iteration: 50 | Classification loss: 0.00001 | Regression loss: 0.00320 | Running loss: 0.01208\n",
            "Epoch: 170 | Iteration: 51 | Classification loss: 0.00002 | Regression loss: 0.01254 | Running loss: 0.01208\n",
            "Epoch: 170 | Iteration: 52 | Classification loss: 0.00002 | Regression loss: 0.01195 | Running loss: 0.01207\n",
            "Epoch: 170 | Iteration: 53 | Classification loss: 0.00004 | Regression loss: 0.01131 | Running loss: 0.01208\n",
            "Epoch: 170 | Iteration: 54 | Classification loss: 0.00004 | Regression loss: 0.01324 | Running loss: 0.01208\n",
            "Epoch: 170 | Iteration: 55 | Classification loss: 0.00010 | Regression loss: 0.01376 | Running loss: 0.01208\n",
            "Epoch: 170 | Iteration: 56 | Classification loss: 0.00007 | Regression loss: 0.01462 | Running loss: 0.01209\n",
            "Epoch: 170 | Iteration: 57 | Classification loss: 0.00005 | Regression loss: 0.02081 | Running loss: 0.01211\n",
            "Epoch: 170 | Iteration: 58 | Classification loss: 0.00003 | Regression loss: 0.00670 | Running loss: 0.01210\n",
            "Epoch: 170 | Iteration: 59 | Classification loss: 0.00004 | Regression loss: 0.01149 | Running loss: 0.01210\n",
            "Epoch: 170 | Iteration: 60 | Classification loss: 0.00001 | Regression loss: 0.00950 | Running loss: 0.01211\n",
            "Epoch: 170 | Iteration: 61 | Classification loss: 0.00003 | Regression loss: 0.01473 | Running loss: 0.01210\n",
            "Epoch: 170 | Iteration: 62 | Classification loss: 0.00003 | Regression loss: 0.00981 | Running loss: 0.01210\n",
            "Epoch: 170 | Iteration: 63 | Classification loss: 0.00002 | Regression loss: 0.01106 | Running loss: 0.01211\n",
            "Epoch: 170 | Iteration: 64 | Classification loss: 0.00001 | Regression loss: 0.00674 | Running loss: 0.01208\n",
            "Epoch: 170 | Iteration: 65 | Classification loss: 0.00002 | Regression loss: 0.00435 | Running loss: 0.01207\n",
            "Epoch: 170 | Iteration: 66 | Classification loss: 0.00001 | Regression loss: 0.00574 | Running loss: 0.01206\n",
            "Epoch: 170 | Iteration: 67 | Classification loss: 0.00001 | Regression loss: 0.00635 | Running loss: 0.01206\n",
            "Epoch: 170 | Iteration: 68 | Classification loss: 0.00010 | Regression loss: 0.01868 | Running loss: 0.01203\n",
            "Epoch: 170 | Iteration: 69 | Classification loss: 0.00004 | Regression loss: 0.01251 | Running loss: 0.01205\n",
            "Epoch: 170 | Iteration: 70 | Classification loss: 0.00002 | Regression loss: 0.01140 | Running loss: 0.01204\n",
            "Epoch: 170 | Iteration: 71 | Classification loss: 0.00011 | Regression loss: 0.04452 | Running loss: 0.01212\n",
            "Epoch: 170 | Iteration: 72 | Classification loss: 0.00003 | Regression loss: 0.01455 | Running loss: 0.01211\n",
            "Epoch: 170 | Iteration: 73 | Classification loss: 0.00002 | Regression loss: 0.01236 | Running loss: 0.01211\n",
            "Epoch: 170 | Iteration: 74 | Classification loss: 0.00004 | Regression loss: 0.01350 | Running loss: 0.01209\n",
            "Epoch: 170 | Iteration: 75 | Classification loss: 0.00001 | Regression loss: 0.02227 | Running loss: 0.01212\n",
            "Epoch: 170 | Iteration: 76 | Classification loss: 0.00003 | Regression loss: 0.00594 | Running loss: 0.01211\n",
            "Epoch: 170 | Iteration: 77 | Classification loss: 0.00001 | Regression loss: 0.00472 | Running loss: 0.01209\n",
            "Epoch: 170 | Iteration: 78 | Classification loss: 0.00001 | Regression loss: 0.00425 | Running loss: 0.01206\n",
            "Epoch: 170 | Iteration: 79 | Classification loss: 0.00002 | Regression loss: 0.01172 | Running loss: 0.01207\n",
            "Epoch: 170 | Iteration: 80 | Classification loss: 0.00002 | Regression loss: 0.00668 | Running loss: 0.01201\n",
            "Epoch: 170 | Iteration: 81 | Classification loss: 0.00004 | Regression loss: 0.01734 | Running loss: 0.01204\n",
            "Epoch: 170 | Iteration: 82 | Classification loss: 0.00002 | Regression loss: 0.00675 | Running loss: 0.01203\n",
            "Epoch: 170 | Iteration: 83 | Classification loss: 0.00001 | Regression loss: 0.00391 | Running loss: 0.01201\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.7241298168882813\n",
            "Precision:  0.5348837209302325\n",
            "Recall:  0.8181818181818182\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}, {0: (0.7314384456550078, 253.0)}, {0: (0.7334466242447818, 253.0)}, {0: (0.7357024222917343, 253.0)}, {0: (0.7381550871546381, 253.0)}, {0: (0.739092434551302, 253.0)}, {0: (0.72091448182607, 253.0)}, {0: (0.7389512050165785, 253.0)}, {0: (0.73050932626622, 253.0)}, {0: (0.7263226120133723, 253.0)}, {0: (0.7283463725823673, 253.0)}, {0: (0.7254913772539314, 253.0)}, {0: (0.7281179290861395, 253.0)}, {0: (0.7260020498565122, 253.0)}, {0: (0.7217236108381175, 253.0)}, {0: (0.7267514829312511, 253.0)}, {0: (0.7227483652714889, 253.0)}, {0: (0.7241298168882813, 253.0)}]\n",
            "Epoch: 171 | Iteration: 0 | Classification loss: 0.00001 | Regression loss: 0.00404 | Running loss: 0.01200\n",
            "Epoch: 171 | Iteration: 1 | Classification loss: 0.00002 | Regression loss: 0.01457 | Running loss: 0.01202\n",
            "Epoch: 171 | Iteration: 2 | Classification loss: 0.00003 | Regression loss: 0.01017 | Running loss: 0.01203\n",
            "Epoch: 171 | Iteration: 3 | Classification loss: 0.00001 | Regression loss: 0.00424 | Running loss: 0.01201\n",
            "Epoch: 171 | Iteration: 4 | Classification loss: 0.00002 | Regression loss: 0.02413 | Running loss: 0.01205\n",
            "Epoch: 171 | Iteration: 5 | Classification loss: 0.00004 | Regression loss: 0.01023 | Running loss: 0.01204\n",
            "Epoch: 171 | Iteration: 6 | Classification loss: 0.00003 | Regression loss: 0.01693 | Running loss: 0.01206\n",
            "Epoch: 171 | Iteration: 7 | Classification loss: 0.00004 | Regression loss: 0.01592 | Running loss: 0.01205\n",
            "Epoch: 171 | Iteration: 8 | Classification loss: 0.00001 | Regression loss: 0.00506 | Running loss: 0.01204\n",
            "Epoch: 171 | Iteration: 9 | Classification loss: 0.00001 | Regression loss: 0.00727 | Running loss: 0.01203\n",
            "Epoch: 171 | Iteration: 10 | Classification loss: 0.00005 | Regression loss: 0.01656 | Running loss: 0.01204\n",
            "Epoch: 171 | Iteration: 11 | Classification loss: 0.00003 | Regression loss: 0.01161 | Running loss: 0.01203\n",
            "Epoch: 171 | Iteration: 12 | Classification loss: 0.00002 | Regression loss: 0.01937 | Running loss: 0.01204\n",
            "Epoch: 171 | Iteration: 13 | Classification loss: 0.00004 | Regression loss: 0.01786 | Running loss: 0.01205\n",
            "Epoch: 171 | Iteration: 14 | Classification loss: 0.00001 | Regression loss: 0.02890 | Running loss: 0.01207\n",
            "Epoch: 171 | Iteration: 15 | Classification loss: 0.00003 | Regression loss: 0.00681 | Running loss: 0.01205\n",
            "Epoch: 171 | Iteration: 16 | Classification loss: 0.00003 | Regression loss: 0.01270 | Running loss: 0.01207\n",
            "Epoch: 171 | Iteration: 17 | Classification loss: 0.00014 | Regression loss: 0.00674 | Running loss: 0.01207\n",
            "Epoch: 171 | Iteration: 18 | Classification loss: 0.00011 | Regression loss: 0.01446 | Running loss: 0.01209\n",
            "Epoch: 171 | Iteration: 19 | Classification loss: 0.00001 | Regression loss: 0.00981 | Running loss: 0.01209\n",
            "Epoch: 171 | Iteration: 20 | Classification loss: 0.00001 | Regression loss: 0.00897 | Running loss: 0.01210\n",
            "Epoch: 171 | Iteration: 21 | Classification loss: 0.00002 | Regression loss: 0.00721 | Running loss: 0.01210\n",
            "Epoch: 171 | Iteration: 22 | Classification loss: 0.00001 | Regression loss: 0.01131 | Running loss: 0.01211\n",
            "Epoch: 171 | Iteration: 23 | Classification loss: 0.00001 | Regression loss: 0.01066 | Running loss: 0.01211\n",
            "Epoch: 171 | Iteration: 24 | Classification loss: 0.00003 | Regression loss: 0.01610 | Running loss: 0.01210\n",
            "Epoch: 171 | Iteration: 25 | Classification loss: 0.00002 | Regression loss: 0.00555 | Running loss: 0.01210\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 188.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 171 | Iteration: 27 | Classification loss: 0.00001 | Regression loss: 0.00691 | Running loss: 0.01202\n",
            "Epoch: 171 | Iteration: 28 | Classification loss: 0.00002 | Regression loss: 0.00656 | Running loss: 0.01199\n",
            "Epoch: 171 | Iteration: 29 | Classification loss: 0.00001 | Regression loss: 0.01181 | Running loss: 0.01201\n",
            "Epoch: 171 | Iteration: 30 | Classification loss: 0.00001 | Regression loss: 0.01265 | Running loss: 0.01202\n",
            "Epoch: 171 | Iteration: 31 | Classification loss: 0.00002 | Regression loss: 0.00432 | Running loss: 0.01200\n",
            "Epoch: 171 | Iteration: 32 | Classification loss: 0.00006 | Regression loss: 0.00415 | Running loss: 0.01199\n",
            "Epoch: 171 | Iteration: 33 | Classification loss: 0.00001 | Regression loss: 0.00737 | Running loss: 0.01199\n",
            "Epoch: 171 | Iteration: 34 | Classification loss: 0.00003 | Regression loss: 0.00618 | Running loss: 0.01199\n",
            "Epoch: 171 | Iteration: 35 | Classification loss: 0.00001 | Regression loss: 0.00399 | Running loss: 0.01197\n",
            "Epoch: 171 | Iteration: 36 | Classification loss: 0.00002 | Regression loss: 0.01802 | Running loss: 0.01198\n",
            "Epoch: 171 | Iteration: 37 | Classification loss: 0.00003 | Regression loss: 0.01631 | Running loss: 0.01198\n",
            "Epoch: 171 | Iteration: 38 | Classification loss: 0.00004 | Regression loss: 0.00971 | Running loss: 0.01199\n",
            "Epoch: 171 | Iteration: 39 | Classification loss: 0.00001 | Regression loss: 0.01082 | Running loss: 0.01200\n",
            "Epoch: 171 | Iteration: 40 | Classification loss: 0.00002 | Regression loss: 0.00660 | Running loss: 0.01200\n",
            "Epoch: 171 | Iteration: 41 | Classification loss: 0.00004 | Regression loss: 0.01300 | Running loss: 0.01201\n",
            "Epoch: 171 | Iteration: 42 | Classification loss: 0.00002 | Regression loss: 0.01075 | Running loss: 0.01202\n",
            "Epoch: 171 | Iteration: 43 | Classification loss: 0.00001 | Regression loss: 0.01717 | Running loss: 0.01202\n",
            "Epoch: 171 | Iteration: 44 | Classification loss: 0.00001 | Regression loss: 0.01305 | Running loss: 0.01201\n",
            "Epoch: 171 | Iteration: 45 | Classification loss: 0.00004 | Regression loss: 0.01462 | Running loss: 0.01203\n",
            "Epoch: 171 | Iteration: 46 | Classification loss: 0.00001 | Regression loss: 0.00440 | Running loss: 0.01202\n",
            "Epoch: 171 | Iteration: 47 | Classification loss: 0.00010 | Regression loss: 0.04479 | Running loss: 0.01209\n",
            "Epoch: 171 | Iteration: 48 | Classification loss: 0.00001 | Regression loss: 0.00343 | Running loss: 0.01208\n",
            "Epoch: 171 | Iteration: 49 | Classification loss: 0.00013 | Regression loss: 0.01979 | Running loss: 0.01210\n",
            "Epoch: 171 | Iteration: 50 | Classification loss: 0.00003 | Regression loss: 0.00967 | Running loss: 0.01209\n",
            "Epoch: 171 | Iteration: 51 | Classification loss: 0.00003 | Regression loss: 0.01319 | Running loss: 0.01210\n",
            "Epoch: 171 | Iteration: 52 | Classification loss: 0.00002 | Regression loss: 0.01581 | Running loss: 0.01211\n",
            "Epoch: 171 | Iteration: 53 | Classification loss: 0.00002 | Regression loss: 0.00485 | Running loss: 0.01211\n",
            "Epoch: 171 | Iteration: 54 | Classification loss: 0.00001 | Regression loss: 0.00394 | Running loss: 0.01209\n",
            "Epoch: 171 | Iteration: 55 | Classification loss: 0.00004 | Regression loss: 0.01621 | Running loss: 0.01209\n",
            "Epoch: 171 | Iteration: 56 | Classification loss: 0.00001 | Regression loss: 0.00475 | Running loss: 0.01209\n",
            "Epoch: 171 | Iteration: 57 | Classification loss: 0.00005 | Regression loss: 0.01685 | Running loss: 0.01209\n",
            "Epoch: 171 | Iteration: 58 | Classification loss: 0.00003 | Regression loss: 0.00315 | Running loss: 0.01205\n",
            "Epoch: 171 | Iteration: 59 | Classification loss: 0.00006 | Regression loss: 0.01330 | Running loss: 0.01206\n",
            "Epoch: 171 | Iteration: 60 | Classification loss: 0.00002 | Regression loss: 0.03071 | Running loss: 0.01210\n",
            "Epoch: 171 | Iteration: 61 | Classification loss: 0.00002 | Regression loss: 0.01420 | Running loss: 0.01212\n",
            "Epoch: 171 | Iteration: 62 | Classification loss: 0.00003 | Regression loss: 0.01263 | Running loss: 0.01212\n",
            "Epoch: 171 | Iteration: 63 | Classification loss: 0.00004 | Regression loss: 0.01110 | Running loss: 0.01210\n",
            "Epoch: 171 | Iteration: 64 | Classification loss: 0.00002 | Regression loss: 0.00462 | Running loss: 0.01209\n",
            "Epoch: 171 | Iteration: 65 | Classification loss: 0.00003 | Regression loss: 0.01177 | Running loss: 0.01209\n",
            "Epoch: 171 | Iteration: 66 | Classification loss: 0.00002 | Regression loss: 0.01282 | Running loss: 0.01209\n",
            "Epoch: 171 | Iteration: 67 | Classification loss: 0.00006 | Regression loss: 0.02036 | Running loss: 0.01210\n",
            "Epoch: 171 | Iteration: 68 | Classification loss: 0.00002 | Regression loss: 0.00641 | Running loss: 0.01205\n",
            "Epoch: 171 | Iteration: 69 | Classification loss: 0.00002 | Regression loss: 0.01209 | Running loss: 0.01206\n",
            "Epoch: 171 | Iteration: 70 | Classification loss: 0.00002 | Regression loss: 0.01324 | Running loss: 0.01202\n",
            "Epoch: 171 | Iteration: 71 | Classification loss: 0.00007 | Regression loss: 0.01406 | Running loss: 0.01202\n",
            "Epoch: 171 | Iteration: 72 | Classification loss: 0.00002 | Regression loss: 0.00850 | Running loss: 0.01199\n",
            "Epoch: 171 | Iteration: 73 | Classification loss: 0.00004 | Regression loss: 0.01390 | Running loss: 0.01198\n",
            "Epoch: 171 | Iteration: 74 | Classification loss: 0.00005 | Regression loss: 0.01054 | Running loss: 0.01198\n",
            "Epoch: 171 | Iteration: 75 | Classification loss: 0.00004 | Regression loss: 0.01359 | Running loss: 0.01198\n",
            "Epoch: 171 | Iteration: 76 | Classification loss: 0.00002 | Regression loss: 0.01087 | Running loss: 0.01199\n",
            "Epoch: 171 | Iteration: 77 | Classification loss: 0.00001 | Regression loss: 0.00358 | Running loss: 0.01197\n",
            "Epoch: 171 | Iteration: 78 | Classification loss: 0.00008 | Regression loss: 0.01035 | Running loss: 0.01197\n",
            "Epoch: 171 | Iteration: 79 | Classification loss: 0.00003 | Regression loss: 0.01510 | Running loss: 0.01198\n",
            "Epoch: 171 | Iteration: 80 | Classification loss: 0.00002 | Regression loss: 0.01042 | Running loss: 0.01199\n",
            "Epoch: 171 | Iteration: 81 | Classification loss: 0.00001 | Regression loss: 0.00657 | Running loss: 0.01198\n",
            "Epoch: 171 | Iteration: 82 | Classification loss: 0.00002 | Regression loss: 0.00662 | Running loss: 0.01196\n",
            "Epoch: 171 | Iteration: 83 | Classification loss: 0.00002 | Regression loss: 0.02372 | Running loss: 0.01199\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.7261051666397325\n",
            "Precision:  0.5362694300518135\n",
            "Recall:  0.8181818181818182\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}, {0: (0.7314384456550078, 253.0)}, {0: (0.7334466242447818, 253.0)}, {0: (0.7357024222917343, 253.0)}, {0: (0.7381550871546381, 253.0)}, {0: (0.739092434551302, 253.0)}, {0: (0.72091448182607, 253.0)}, {0: (0.7389512050165785, 253.0)}, {0: (0.73050932626622, 253.0)}, {0: (0.7263226120133723, 253.0)}, {0: (0.7283463725823673, 253.0)}, {0: (0.7254913772539314, 253.0)}, {0: (0.7281179290861395, 253.0)}, {0: (0.7260020498565122, 253.0)}, {0: (0.7217236108381175, 253.0)}, {0: (0.7267514829312511, 253.0)}, {0: (0.7227483652714889, 253.0)}, {0: (0.7241298168882813, 253.0)}, {0: (0.7261051666397325, 253.0)}]\n",
            "Epoch: 172 | Iteration: 0 | Classification loss: 0.00003 | Regression loss: 0.01499 | Running loss: 0.01199\n",
            "Epoch: 172 | Iteration: 1 | Classification loss: 0.00004 | Regression loss: 0.01055 | Running loss: 0.01196\n",
            "Epoch: 172 | Iteration: 2 | Classification loss: 0.00001 | Regression loss: 0.01463 | Running loss: 0.01196\n",
            "Epoch: 172 | Iteration: 3 | Classification loss: 0.00001 | Regression loss: 0.00949 | Running loss: 0.01195\n",
            "Epoch: 172 | Iteration: 4 | Classification loss: 0.00002 | Regression loss: 0.01036 | Running loss: 0.01194\n",
            "Epoch: 172 | Iteration: 5 | Classification loss: 0.00001 | Regression loss: 0.01181 | Running loss: 0.01194\n",
            "Epoch: 172 | Iteration: 6 | Classification loss: 0.00009 | Regression loss: 0.01870 | Running loss: 0.01195\n",
            "Epoch: 172 | Iteration: 7 | Classification loss: 0.00007 | Regression loss: 0.01401 | Running loss: 0.01195\n",
            "Epoch: 172 | Iteration: 8 | Classification loss: 0.00001 | Regression loss: 0.02941 | Running loss: 0.01200\n",
            "Epoch: 172 | Iteration: 9 | Classification loss: 0.00003 | Regression loss: 0.01356 | Running loss: 0.01194\n",
            "Epoch: 172 | Iteration: 10 | Classification loss: 0.00006 | Regression loss: 0.01696 | Running loss: 0.01195\n",
            "Epoch: 172 | Iteration: 11 | Classification loss: 0.00003 | Regression loss: 0.01656 | Running loss: 0.01194\n",
            "Epoch: 172 | Iteration: 12 | Classification loss: 0.00002 | Regression loss: 0.00696 | Running loss: 0.01193\n",
            "Epoch: 172 | Iteration: 13 | Classification loss: 0.00001 | Regression loss: 0.00486 | Running loss: 0.01189\n",
            "Epoch: 172 | Iteration: 14 | Classification loss: 0.00005 | Regression loss: 0.01180 | Running loss: 0.01186\n",
            "Epoch: 172 | Iteration: 15 | Classification loss: 0.00002 | Regression loss: 0.00606 | Running loss: 0.01184\n",
            "Epoch: 172 | Iteration: 16 | Classification loss: 0.00001 | Regression loss: 0.00354 | Running loss: 0.01181\n",
            "Epoch: 172 | Iteration: 17 | Classification loss: 0.00003 | Regression loss: 0.01168 | Running loss: 0.01180\n",
            "Epoch: 172 | Iteration: 18 | Classification loss: 0.00002 | Regression loss: 0.01140 | Running loss: 0.01182\n",
            "Epoch: 172 | Iteration: 19 | Classification loss: 0.00003 | Regression loss: 0.02483 | Running loss: 0.01185\n",
            "Epoch: 172 | Iteration: 20 | Classification loss: 0.00003 | Regression loss: 0.01252 | Running loss: 0.01186\n",
            "Epoch: 172 | Iteration: 21 | Classification loss: 0.00002 | Regression loss: 0.00413 | Running loss: 0.01186\n",
            "Epoch: 172 | Iteration: 22 | Classification loss: 0.00001 | Regression loss: 0.00336 | Running loss: 0.01183\n",
            "Epoch: 172 | Iteration: 23 | Classification loss: 0.00003 | Regression loss: 0.01844 | Running loss: 0.01185\n",
            "Epoch: 172 | Iteration: 24 | Classification loss: 0.00003 | Regression loss: 0.01347 | Running loss: 0.01185\n",
            "Epoch: 172 | Iteration: 25 | Classification loss: 0.00003 | Regression loss: 0.01754 | Running loss: 0.01186\n",
            "Epoch: 172 | Iteration: 26 | Classification loss: 0.00003 | Regression loss: 0.01212 | Running loss: 0.01188\n",
            "Epoch: 172 | Iteration: 27 | Classification loss: 0.00002 | Regression loss: 0.01444 | Running loss: 0.01189\n",
            "Epoch: 172 | Iteration: 28 | Classification loss: 0.00001 | Regression loss: 0.00392 | Running loss: 0.01188\n",
            "Epoch: 172 | Iteration: 29 | Classification loss: 0.00003 | Regression loss: 0.01277 | Running loss: 0.01189\n",
            "Epoch: 172 | Iteration: 30 | Classification loss: 0.00001 | Regression loss: 0.00712 | Running loss: 0.01189\n",
            "Epoch: 172 | Iteration: 31 | Classification loss: 0.00004 | Regression loss: 0.01063 | Running loss: 0.01190\n",
            "Epoch: 172 | Iteration: 32 | Classification loss: 0.00001 | Regression loss: 0.00707 | Running loss: 0.01191\n",
            "Epoch: 172 | Iteration: 33 | Classification loss: 0.00001 | Regression loss: 0.01197 | Running loss: 0.01192\n",
            "Epoch: 172 | Iteration: 34 | Classification loss: 0.00003 | Regression loss: 0.01338 | Running loss: 0.01191\n",
            "Epoch: 172 | Iteration: 35 | Classification loss: 0.00001 | Regression loss: 0.01068 | Running loss: 0.01193\n",
            "Epoch: 172 | Iteration: 36 | Classification loss: 0.00006 | Regression loss: 0.01572 | Running loss: 0.01194\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 188.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 172 | Iteration: 38 | Classification loss: 0.00003 | Regression loss: 0.01251 | Running loss: 0.01196\n",
            "Epoch: 172 | Iteration: 39 | Classification loss: 0.00005 | Regression loss: 0.01654 | Running loss: 0.01197\n",
            "Epoch: 172 | Iteration: 40 | Classification loss: 0.00002 | Regression loss: 0.01026 | Running loss: 0.01196\n",
            "Epoch: 172 | Iteration: 41 | Classification loss: 0.00006 | Regression loss: 0.00341 | Running loss: 0.01195\n",
            "Epoch: 172 | Iteration: 42 | Classification loss: 0.00004 | Regression loss: 0.01029 | Running loss: 0.01195\n",
            "Epoch: 172 | Iteration: 43 | Classification loss: 0.00002 | Regression loss: 0.00694 | Running loss: 0.01191\n",
            "Epoch: 172 | Iteration: 44 | Classification loss: 0.00001 | Regression loss: 0.00488 | Running loss: 0.01189\n",
            "Epoch: 172 | Iteration: 45 | Classification loss: 0.00002 | Regression loss: 0.00992 | Running loss: 0.01188\n",
            "Epoch: 172 | Iteration: 46 | Classification loss: 0.00001 | Regression loss: 0.00949 | Running loss: 0.01188\n",
            "Epoch: 172 | Iteration: 47 | Classification loss: 0.00001 | Regression loss: 0.00611 | Running loss: 0.01187\n",
            "Epoch: 172 | Iteration: 48 | Classification loss: 0.00001 | Regression loss: 0.02856 | Running loss: 0.01191\n",
            "Epoch: 172 | Iteration: 49 | Classification loss: 0.00001 | Regression loss: 0.00521 | Running loss: 0.01189\n",
            "Epoch: 172 | Iteration: 50 | Classification loss: 0.00003 | Regression loss: 0.00297 | Running loss: 0.01187\n",
            "Epoch: 172 | Iteration: 51 | Classification loss: 0.00002 | Regression loss: 0.00686 | Running loss: 0.01185\n",
            "Epoch: 172 | Iteration: 52 | Classification loss: 0.00001 | Regression loss: 0.01157 | Running loss: 0.01186\n",
            "Epoch: 172 | Iteration: 53 | Classification loss: 0.00002 | Regression loss: 0.01843 | Running loss: 0.01189\n",
            "Epoch: 172 | Iteration: 54 | Classification loss: 0.00002 | Regression loss: 0.00521 | Running loss: 0.01186\n",
            "Epoch: 172 | Iteration: 55 | Classification loss: 0.00001 | Regression loss: 0.00428 | Running loss: 0.01187\n",
            "Epoch: 172 | Iteration: 56 | Classification loss: 0.00002 | Regression loss: 0.00615 | Running loss: 0.01187\n",
            "Epoch: 172 | Iteration: 57 | Classification loss: 0.00007 | Regression loss: 0.01315 | Running loss: 0.01186\n",
            "Epoch: 172 | Iteration: 58 | Classification loss: 0.00001 | Regression loss: 0.00599 | Running loss: 0.01185\n",
            "Epoch: 172 | Iteration: 59 | Classification loss: 0.00002 | Regression loss: 0.00518 | Running loss: 0.01183\n",
            "Epoch: 172 | Iteration: 60 | Classification loss: 0.00004 | Regression loss: 0.00607 | Running loss: 0.01181\n",
            "Epoch: 172 | Iteration: 61 | Classification loss: 0.00002 | Regression loss: 0.00447 | Running loss: 0.01181\n",
            "Epoch: 172 | Iteration: 62 | Classification loss: 0.00001 | Regression loss: 0.00393 | Running loss: 0.01179\n",
            "Epoch: 172 | Iteration: 63 | Classification loss: 0.00007 | Regression loss: 0.01007 | Running loss: 0.01179\n",
            "Epoch: 172 | Iteration: 64 | Classification loss: 0.00003 | Regression loss: 0.01070 | Running loss: 0.01179\n",
            "Epoch: 172 | Iteration: 65 | Classification loss: 0.00001 | Regression loss: 0.00885 | Running loss: 0.01179\n",
            "Epoch: 172 | Iteration: 66 | Classification loss: 0.00002 | Regression loss: 0.01106 | Running loss: 0.01178\n",
            "Epoch: 172 | Iteration: 67 | Classification loss: 0.00003 | Regression loss: 0.01453 | Running loss: 0.01180\n",
            "Epoch: 172 | Iteration: 68 | Classification loss: 0.00013 | Regression loss: 0.00747 | Running loss: 0.01179\n",
            "Epoch: 172 | Iteration: 69 | Classification loss: 0.00002 | Regression loss: 0.01524 | Running loss: 0.01181\n",
            "Epoch: 172 | Iteration: 70 | Classification loss: 0.00011 | Regression loss: 0.01331 | Running loss: 0.01181\n",
            "Epoch: 172 | Iteration: 71 | Classification loss: 0.00002 | Regression loss: 0.01225 | Running loss: 0.01181\n",
            "Epoch: 172 | Iteration: 72 | Classification loss: 0.00004 | Regression loss: 0.01293 | Running loss: 0.01181\n",
            "Epoch: 172 | Iteration: 73 | Classification loss: 0.00002 | Regression loss: 0.01031 | Running loss: 0.01182\n",
            "Epoch: 172 | Iteration: 74 | Classification loss: 0.00001 | Regression loss: 0.01755 | Running loss: 0.01183\n",
            "Epoch: 172 | Iteration: 75 | Classification loss: 0.00001 | Regression loss: 0.00739 | Running loss: 0.01184\n",
            "Epoch: 172 | Iteration: 76 | Classification loss: 0.00001 | Regression loss: 0.01054 | Running loss: 0.01182\n",
            "Epoch: 172 | Iteration: 77 | Classification loss: 0.00003 | Regression loss: 0.01156 | Running loss: 0.01181\n",
            "Epoch: 172 | Iteration: 78 | Classification loss: 0.00005 | Regression loss: 0.01765 | Running loss: 0.01183\n",
            "Epoch: 172 | Iteration: 79 | Classification loss: 0.00003 | Regression loss: 0.01835 | Running loss: 0.01186\n",
            "Epoch: 172 | Iteration: 80 | Classification loss: 0.00010 | Regression loss: 0.04505 | Running loss: 0.01194\n",
            "Epoch: 172 | Iteration: 81 | Classification loss: 0.00003 | Regression loss: 0.02448 | Running loss: 0.01193\n",
            "Epoch: 172 | Iteration: 82 | Classification loss: 0.00004 | Regression loss: 0.01969 | Running loss: 0.01193\n",
            "Epoch: 172 | Iteration: 83 | Classification loss: 0.00001 | Regression loss: 0.00576 | Running loss: 0.01192\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.7277246704086413\n",
            "Precision:  0.5348837209302325\n",
            "Recall:  0.8181818181818182\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}, {0: (0.7314384456550078, 253.0)}, {0: (0.7334466242447818, 253.0)}, {0: (0.7357024222917343, 253.0)}, {0: (0.7381550871546381, 253.0)}, {0: (0.739092434551302, 253.0)}, {0: (0.72091448182607, 253.0)}, {0: (0.7389512050165785, 253.0)}, {0: (0.73050932626622, 253.0)}, {0: (0.7263226120133723, 253.0)}, {0: (0.7283463725823673, 253.0)}, {0: (0.7254913772539314, 253.0)}, {0: (0.7281179290861395, 253.0)}, {0: (0.7260020498565122, 253.0)}, {0: (0.7217236108381175, 253.0)}, {0: (0.7267514829312511, 253.0)}, {0: (0.7227483652714889, 253.0)}, {0: (0.7241298168882813, 253.0)}, {0: (0.7261051666397325, 253.0)}, {0: (0.7277246704086413, 253.0)}]\n",
            "Epoch: 173 | Iteration: 0 | Classification loss: 0.00001 | Regression loss: 0.01142 | Running loss: 0.01191\n",
            "Epoch: 173 | Iteration: 1 | Classification loss: 0.00004 | Regression loss: 0.01356 | Running loss: 0.01192\n",
            "Epoch: 173 | Iteration: 2 | Classification loss: 0.00002 | Regression loss: 0.00513 | Running loss: 0.01190\n",
            "Epoch: 173 | Iteration: 3 | Classification loss: 0.00005 | Regression loss: 0.01603 | Running loss: 0.01191\n",
            "Epoch: 173 | Iteration: 4 | Classification loss: 0.00005 | Regression loss: 0.01459 | Running loss: 0.01188\n",
            "Epoch: 173 | Iteration: 5 | Classification loss: 0.00004 | Regression loss: 0.01631 | Running loss: 0.01190\n",
            "Epoch: 173 | Iteration: 6 | Classification loss: 0.00007 | Regression loss: 0.01355 | Running loss: 0.01192\n",
            "Epoch: 173 | Iteration: 7 | Classification loss: 0.00001 | Regression loss: 0.00532 | Running loss: 0.01191\n",
            "Epoch: 173 | Iteration: 8 | Classification loss: 0.00003 | Regression loss: 0.01455 | Running loss: 0.01187\n",
            "Epoch: 173 | Iteration: 9 | Classification loss: 0.00004 | Regression loss: 0.01096 | Running loss: 0.01187\n",
            "Epoch: 173 | Iteration: 10 | Classification loss: 0.00001 | Regression loss: 0.00451 | Running loss: 0.01186\n",
            "Epoch: 173 | Iteration: 11 | Classification loss: 0.00004 | Regression loss: 0.01636 | Running loss: 0.01187\n",
            "Epoch: 173 | Iteration: 12 | Classification loss: 0.00001 | Regression loss: 0.02780 | Running loss: 0.01191\n",
            "Epoch: 173 | Iteration: 13 | Classification loss: 0.00001 | Regression loss: 0.01435 | Running loss: 0.01192\n",
            "Epoch: 173 | Iteration: 14 | Classification loss: 0.00004 | Regression loss: 0.01384 | Running loss: 0.01193\n",
            "Epoch: 173 | Iteration: 15 | Classification loss: 0.00001 | Regression loss: 0.00737 | Running loss: 0.01193\n",
            "Epoch: 173 | Iteration: 16 | Classification loss: 0.00002 | Regression loss: 0.01065 | Running loss: 0.01195\n",
            "Epoch: 173 | Iteration: 17 | Classification loss: 0.00009 | Regression loss: 0.04377 | Running loss: 0.01200\n",
            "Epoch: 173 | Iteration: 18 | Classification loss: 0.00001 | Regression loss: 0.00666 | Running loss: 0.01196\n",
            "Epoch: 173 | Iteration: 19 | Classification loss: 0.00004 | Regression loss: 0.01015 | Running loss: 0.01194\n",
            "Epoch: 173 | Iteration: 20 | Classification loss: 0.00002 | Regression loss: 0.00546 | Running loss: 0.01193\n",
            "Epoch: 173 | Iteration: 21 | Classification loss: 0.00001 | Regression loss: 0.01076 | Running loss: 0.01194\n",
            "Epoch: 173 | Iteration: 22 | Classification loss: 0.00003 | Regression loss: 0.00649 | Running loss: 0.01193\n",
            "Epoch: 173 | Iteration: 23 | Classification loss: 0.00002 | Regression loss: 0.01121 | Running loss: 0.01191\n",
            "Epoch: 173 | Iteration: 24 | Classification loss: 0.00001 | Regression loss: 0.00318 | Running loss: 0.01190\n",
            "Epoch: 173 | Iteration: 25 | Classification loss: 0.00001 | Regression loss: 0.00431 | Running loss: 0.01190\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 188.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 173 | Iteration: 27 | Classification loss: 0.00001 | Regression loss: 0.00435 | Running loss: 0.01189\n",
            "Epoch: 173 | Iteration: 28 | Classification loss: 0.00002 | Regression loss: 0.01543 | Running loss: 0.01183\n",
            "Epoch: 173 | Iteration: 29 | Classification loss: 0.00004 | Regression loss: 0.01865 | Running loss: 0.01183\n",
            "Epoch: 173 | Iteration: 30 | Classification loss: 0.00005 | Regression loss: 0.01039 | Running loss: 0.01183\n",
            "Epoch: 173 | Iteration: 31 | Classification loss: 0.00002 | Regression loss: 0.00609 | Running loss: 0.01182\n",
            "Epoch: 173 | Iteration: 32 | Classification loss: 0.00001 | Regression loss: 0.00656 | Running loss: 0.01181\n",
            "Epoch: 173 | Iteration: 33 | Classification loss: 0.00001 | Regression loss: 0.00293 | Running loss: 0.01179\n",
            "Epoch: 173 | Iteration: 34 | Classification loss: 0.00001 | Regression loss: 0.00485 | Running loss: 0.01177\n",
            "Epoch: 173 | Iteration: 35 | Classification loss: 0.00001 | Regression loss: 0.00404 | Running loss: 0.01175\n",
            "Epoch: 173 | Iteration: 36 | Classification loss: 0.00003 | Regression loss: 0.00398 | Running loss: 0.01175\n",
            "Epoch: 173 | Iteration: 37 | Classification loss: 0.00002 | Regression loss: 0.00470 | Running loss: 0.01173\n",
            "Epoch: 173 | Iteration: 38 | Classification loss: 0.00002 | Regression loss: 0.01243 | Running loss: 0.01173\n",
            "Epoch: 173 | Iteration: 39 | Classification loss: 0.00003 | Regression loss: 0.00692 | Running loss: 0.01172\n",
            "Epoch: 173 | Iteration: 40 | Classification loss: 0.00003 | Regression loss: 0.01351 | Running loss: 0.01174\n",
            "Epoch: 173 | Iteration: 41 | Classification loss: 0.00001 | Regression loss: 0.00388 | Running loss: 0.01171\n",
            "Epoch: 173 | Iteration: 42 | Classification loss: 0.00002 | Regression loss: 0.00259 | Running loss: 0.01169\n",
            "Epoch: 173 | Iteration: 43 | Classification loss: 0.00006 | Regression loss: 0.00435 | Running loss: 0.01167\n",
            "Epoch: 173 | Iteration: 44 | Classification loss: 0.00002 | Regression loss: 0.01286 | Running loss: 0.01167\n",
            "Epoch: 173 | Iteration: 45 | Classification loss: 0.00001 | Regression loss: 0.00563 | Running loss: 0.01165\n",
            "Epoch: 173 | Iteration: 46 | Classification loss: 0.00002 | Regression loss: 0.01243 | Running loss: 0.01166\n",
            "Epoch: 173 | Iteration: 47 | Classification loss: 0.00003 | Regression loss: 0.01687 | Running loss: 0.01168\n",
            "Epoch: 173 | Iteration: 48 | Classification loss: 0.00003 | Regression loss: 0.01128 | Running loss: 0.01168\n",
            "Epoch: 173 | Iteration: 49 | Classification loss: 0.00002 | Regression loss: 0.02946 | Running loss: 0.01173\n",
            "Epoch: 173 | Iteration: 50 | Classification loss: 0.00004 | Regression loss: 0.01264 | Running loss: 0.01174\n",
            "Epoch: 173 | Iteration: 51 | Classification loss: 0.00005 | Regression loss: 0.02127 | Running loss: 0.01175\n",
            "Epoch: 173 | Iteration: 52 | Classification loss: 0.00003 | Regression loss: 0.01698 | Running loss: 0.01175\n",
            "Epoch: 173 | Iteration: 53 | Classification loss: 0.00002 | Regression loss: 0.00665 | Running loss: 0.01173\n",
            "Epoch: 173 | Iteration: 54 | Classification loss: 0.00003 | Regression loss: 0.01360 | Running loss: 0.01172\n",
            "Epoch: 173 | Iteration: 55 | Classification loss: 0.00002 | Regression loss: 0.00817 | Running loss: 0.01171\n",
            "Epoch: 173 | Iteration: 56 | Classification loss: 0.00002 | Regression loss: 0.01008 | Running loss: 0.01172\n",
            "Epoch: 173 | Iteration: 57 | Classification loss: 0.00002 | Regression loss: 0.01157 | Running loss: 0.01172\n",
            "Epoch: 173 | Iteration: 58 | Classification loss: 0.00010 | Regression loss: 0.01284 | Running loss: 0.01174\n",
            "Epoch: 173 | Iteration: 59 | Classification loss: 0.00008 | Regression loss: 0.01388 | Running loss: 0.01176\n",
            "Epoch: 173 | Iteration: 60 | Classification loss: 0.00003 | Regression loss: 0.01370 | Running loss: 0.01177\n",
            "Epoch: 173 | Iteration: 61 | Classification loss: 0.00002 | Regression loss: 0.01325 | Running loss: 0.01179\n",
            "Epoch: 173 | Iteration: 62 | Classification loss: 0.00002 | Regression loss: 0.01064 | Running loss: 0.01179\n",
            "Epoch: 173 | Iteration: 63 | Classification loss: 0.00006 | Regression loss: 0.00950 | Running loss: 0.01179\n",
            "Epoch: 173 | Iteration: 64 | Classification loss: 0.00002 | Regression loss: 0.01168 | Running loss: 0.01179\n",
            "Epoch: 173 | Iteration: 65 | Classification loss: 0.00001 | Regression loss: 0.01778 | Running loss: 0.01179\n",
            "Epoch: 173 | Iteration: 66 | Classification loss: 0.00002 | Regression loss: 0.01279 | Running loss: 0.01180\n",
            "Epoch: 173 | Iteration: 67 | Classification loss: 0.00002 | Regression loss: 0.01426 | Running loss: 0.01182\n",
            "Epoch: 173 | Iteration: 68 | Classification loss: 0.00001 | Regression loss: 0.01115 | Running loss: 0.01181\n",
            "Epoch: 173 | Iteration: 69 | Classification loss: 0.00002 | Regression loss: 0.00620 | Running loss: 0.01182\n",
            "Epoch: 173 | Iteration: 70 | Classification loss: 0.00001 | Regression loss: 0.00401 | Running loss: 0.01181\n",
            "Epoch: 173 | Iteration: 71 | Classification loss: 0.00014 | Regression loss: 0.00659 | Running loss: 0.01182\n",
            "Epoch: 173 | Iteration: 72 | Classification loss: 0.00004 | Regression loss: 0.02356 | Running loss: 0.01184\n",
            "Epoch: 173 | Iteration: 73 | Classification loss: 0.00004 | Regression loss: 0.01569 | Running loss: 0.01185\n",
            "Epoch: 173 | Iteration: 74 | Classification loss: 0.00004 | Regression loss: 0.01007 | Running loss: 0.01185\n",
            "Epoch: 173 | Iteration: 75 | Classification loss: 0.00001 | Regression loss: 0.01669 | Running loss: 0.01187\n",
            "Epoch: 173 | Iteration: 76 | Classification loss: 0.00001 | Regression loss: 0.00922 | Running loss: 0.01188\n",
            "Epoch: 173 | Iteration: 77 | Classification loss: 0.00003 | Regression loss: 0.02536 | Running loss: 0.01190\n",
            "Epoch: 173 | Iteration: 78 | Classification loss: 0.00001 | Regression loss: 0.00951 | Running loss: 0.01189\n",
            "Epoch: 173 | Iteration: 79 | Classification loss: 0.00001 | Regression loss: 0.00435 | Running loss: 0.01188\n",
            "Epoch: 173 | Iteration: 80 | Classification loss: 0.00009 | Regression loss: 0.01737 | Running loss: 0.01188\n",
            "Epoch: 173 | Iteration: 81 | Classification loss: 0.00004 | Regression loss: 0.00947 | Running loss: 0.01186\n",
            "Epoch: 173 | Iteration: 82 | Classification loss: 0.00001 | Regression loss: 0.00865 | Running loss: 0.01186\n",
            "Epoch: 173 | Iteration: 83 | Classification loss: 0.00003 | Regression loss: 0.01296 | Running loss: 0.01186\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.7259496886680885\n",
            "Precision:  0.5433070866141733\n",
            "Recall:  0.8181818181818182\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}, {0: (0.7314384456550078, 253.0)}, {0: (0.7334466242447818, 253.0)}, {0: (0.7357024222917343, 253.0)}, {0: (0.7381550871546381, 253.0)}, {0: (0.739092434551302, 253.0)}, {0: (0.72091448182607, 253.0)}, {0: (0.7389512050165785, 253.0)}, {0: (0.73050932626622, 253.0)}, {0: (0.7263226120133723, 253.0)}, {0: (0.7283463725823673, 253.0)}, {0: (0.7254913772539314, 253.0)}, {0: (0.7281179290861395, 253.0)}, {0: (0.7260020498565122, 253.0)}, {0: (0.7217236108381175, 253.0)}, {0: (0.7267514829312511, 253.0)}, {0: (0.7227483652714889, 253.0)}, {0: (0.7241298168882813, 253.0)}, {0: (0.7261051666397325, 253.0)}, {0: (0.7277246704086413, 253.0)}, {0: (0.7259496886680885, 253.0)}]\n",
            "Epoch: 174 | Iteration: 0 | Classification loss: 0.00002 | Regression loss: 0.00660 | Running loss: 0.01184\n",
            "Epoch: 174 | Iteration: 1 | Classification loss: 0.00002 | Regression loss: 0.00982 | Running loss: 0.01183\n",
            "Epoch: 174 | Iteration: 2 | Classification loss: 0.00011 | Regression loss: 0.04427 | Running loss: 0.01189\n",
            "Epoch: 174 | Iteration: 3 | Classification loss: 0.00006 | Regression loss: 0.01301 | Running loss: 0.01189\n",
            "Epoch: 174 | Iteration: 4 | Classification loss: 0.00003 | Regression loss: 0.01112 | Running loss: 0.01190\n",
            "Epoch: 174 | Iteration: 5 | Classification loss: 0.00002 | Regression loss: 0.01299 | Running loss: 0.01186\n",
            "Epoch: 174 | Iteration: 6 | Classification loss: 0.00001 | Regression loss: 0.00949 | Running loss: 0.01188\n",
            "Epoch: 174 | Iteration: 7 | Classification loss: 0.00003 | Regression loss: 0.00903 | Running loss: 0.01187\n",
            "Epoch: 174 | Iteration: 8 | Classification loss: 0.00004 | Regression loss: 0.01377 | Running loss: 0.01188\n",
            "Epoch: 174 | Iteration: 9 | Classification loss: 0.00001 | Regression loss: 0.02176 | Running loss: 0.01191\n",
            "Epoch: 174 | Iteration: 10 | Classification loss: 0.00004 | Regression loss: 0.01166 | Running loss: 0.01191\n",
            "Epoch: 174 | Iteration: 11 | Classification loss: 0.00002 | Regression loss: 0.00684 | Running loss: 0.01191\n",
            "Epoch: 174 | Iteration: 12 | Classification loss: 0.00004 | Regression loss: 0.01268 | Running loss: 0.01190\n",
            "Epoch: 174 | Iteration: 13 | Classification loss: 0.00002 | Regression loss: 0.01775 | Running loss: 0.01190\n",
            "Epoch: 174 | Iteration: 14 | Classification loss: 0.00002 | Regression loss: 0.01074 | Running loss: 0.01188\n",
            "Epoch: 174 | Iteration: 15 | Classification loss: 0.00002 | Regression loss: 0.01058 | Running loss: 0.01188\n",
            "Epoch: 174 | Iteration: 16 | Classification loss: 0.00001 | Regression loss: 0.00960 | Running loss: 0.01189\n",
            "Epoch: 174 | Iteration: 17 | Classification loss: 0.00001 | Regression loss: 0.00401 | Running loss: 0.01187\n",
            "Epoch: 174 | Iteration: 18 | Classification loss: 0.00002 | Regression loss: 0.01114 | Running loss: 0.01186\n",
            "Epoch: 174 | Iteration: 19 | Classification loss: 0.00003 | Regression loss: 0.01183 | Running loss: 0.01185\n",
            "Epoch: 174 | Iteration: 20 | Classification loss: 0.00003 | Regression loss: 0.01115 | Running loss: 0.01186\n",
            "Epoch: 174 | Iteration: 21 | Classification loss: 0.00001 | Regression loss: 0.00491 | Running loss: 0.01182\n",
            "Epoch: 174 | Iteration: 22 | Classification loss: 0.00001 | Regression loss: 0.00436 | Running loss: 0.01182\n",
            "Epoch: 174 | Iteration: 23 | Classification loss: 0.00005 | Regression loss: 0.01704 | Running loss: 0.01183\n",
            "Epoch: 174 | Iteration: 24 | Classification loss: 0.00002 | Regression loss: 0.01138 | Running loss: 0.01185\n",
            "Epoch: 174 | Iteration: 25 | Classification loss: 0.00002 | Regression loss: 0.00525 | Running loss: 0.01184\n",
            "Epoch: 174 | Iteration: 26 | Classification loss: 0.00001 | Regression loss: 0.00447 | Running loss: 0.01182\n",
            "Epoch: 174 | Iteration: 27 | Classification loss: 0.00004 | Regression loss: 0.01657 | Running loss: 0.01184\n",
            "Epoch: 174 | Iteration: 28 | Classification loss: 0.00003 | Regression loss: 0.00313 | Running loss: 0.01184\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 188.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 174 | Iteration: 30 | Classification loss: 0.00004 | Regression loss: 0.02080 | Running loss: 0.01184\n",
            "Epoch: 174 | Iteration: 31 | Classification loss: 0.00001 | Regression loss: 0.00619 | Running loss: 0.01184\n",
            "Epoch: 174 | Iteration: 32 | Classification loss: 0.00003 | Regression loss: 0.00433 | Running loss: 0.01183\n",
            "Epoch: 174 | Iteration: 33 | Classification loss: 0.00002 | Regression loss: 0.00858 | Running loss: 0.01182\n",
            "Epoch: 174 | Iteration: 34 | Classification loss: 0.00001 | Regression loss: 0.00721 | Running loss: 0.01182\n",
            "Epoch: 174 | Iteration: 35 | Classification loss: 0.00003 | Regression loss: 0.00897 | Running loss: 0.01179\n",
            "Epoch: 174 | Iteration: 36 | Classification loss: 0.00004 | Regression loss: 0.01638 | Running loss: 0.01181\n",
            "Epoch: 174 | Iteration: 37 | Classification loss: 0.00006 | Regression loss: 0.01486 | Running loss: 0.01180\n",
            "Epoch: 174 | Iteration: 38 | Classification loss: 0.00001 | Regression loss: 0.00425 | Running loss: 0.01179\n",
            "Epoch: 174 | Iteration: 39 | Classification loss: 0.00004 | Regression loss: 0.01468 | Running loss: 0.01179\n",
            "Epoch: 174 | Iteration: 40 | Classification loss: 0.00005 | Regression loss: 0.01608 | Running loss: 0.01181\n",
            "Epoch: 174 | Iteration: 41 | Classification loss: 0.00007 | Regression loss: 0.01008 | Running loss: 0.01181\n",
            "Epoch: 174 | Iteration: 42 | Classification loss: 0.00010 | Regression loss: 0.01345 | Running loss: 0.01181\n",
            "Epoch: 174 | Iteration: 43 | Classification loss: 0.00001 | Regression loss: 0.01395 | Running loss: 0.01181\n",
            "Epoch: 174 | Iteration: 44 | Classification loss: 0.00007 | Regression loss: 0.01381 | Running loss: 0.01183\n",
            "Epoch: 174 | Iteration: 45 | Classification loss: 0.00002 | Regression loss: 0.00666 | Running loss: 0.01181\n",
            "Epoch: 174 | Iteration: 46 | Classification loss: 0.00011 | Regression loss: 0.01815 | Running loss: 0.01184\n",
            "Epoch: 174 | Iteration: 47 | Classification loss: 0.00001 | Regression loss: 0.00879 | Running loss: 0.01183\n",
            "Epoch: 174 | Iteration: 48 | Classification loss: 0.00003 | Regression loss: 0.00631 | Running loss: 0.01183\n",
            "Epoch: 174 | Iteration: 49 | Classification loss: 0.00002 | Regression loss: 0.01356 | Running loss: 0.01179\n",
            "Epoch: 174 | Iteration: 50 | Classification loss: 0.00001 | Regression loss: 0.00564 | Running loss: 0.01177\n",
            "Epoch: 174 | Iteration: 51 | Classification loss: 0.00001 | Regression loss: 0.00610 | Running loss: 0.01176\n",
            "Epoch: 174 | Iteration: 52 | Classification loss: 0.00001 | Regression loss: 0.01138 | Running loss: 0.01176\n",
            "Epoch: 174 | Iteration: 53 | Classification loss: 0.00004 | Regression loss: 0.01013 | Running loss: 0.01175\n",
            "Epoch: 174 | Iteration: 54 | Classification loss: 0.00002 | Regression loss: 0.01604 | Running loss: 0.01176\n",
            "Epoch: 174 | Iteration: 55 | Classification loss: 0.00002 | Regression loss: 0.00774 | Running loss: 0.01176\n",
            "Epoch: 174 | Iteration: 56 | Classification loss: 0.00002 | Regression loss: 0.01824 | Running loss: 0.01178\n",
            "Epoch: 174 | Iteration: 57 | Classification loss: 0.00002 | Regression loss: 0.01402 | Running loss: 0.01180\n",
            "Epoch: 174 | Iteration: 58 | Classification loss: 0.00002 | Regression loss: 0.01099 | Running loss: 0.01180\n",
            "Epoch: 174 | Iteration: 59 | Classification loss: 0.00001 | Regression loss: 0.01153 | Running loss: 0.01179\n",
            "Epoch: 174 | Iteration: 60 | Classification loss: 0.00002 | Regression loss: 0.01421 | Running loss: 0.01179\n",
            "Epoch: 174 | Iteration: 61 | Classification loss: 0.00002 | Regression loss: 0.00588 | Running loss: 0.01179\n",
            "Epoch: 174 | Iteration: 62 | Classification loss: 0.00015 | Regression loss: 0.00740 | Running loss: 0.01178\n",
            "Epoch: 174 | Iteration: 63 | Classification loss: 0.00001 | Regression loss: 0.03082 | Running loss: 0.01182\n",
            "Epoch: 174 | Iteration: 64 | Classification loss: 0.00003 | Regression loss: 0.01415 | Running loss: 0.01182\n",
            "Epoch: 174 | Iteration: 65 | Classification loss: 0.00002 | Regression loss: 0.00962 | Running loss: 0.01181\n",
            "Epoch: 174 | Iteration: 66 | Classification loss: 0.00001 | Regression loss: 0.00629 | Running loss: 0.01180\n",
            "Epoch: 174 | Iteration: 67 | Classification loss: 0.00001 | Regression loss: 0.00354 | Running loss: 0.01179\n",
            "Epoch: 174 | Iteration: 68 | Classification loss: 0.00002 | Regression loss: 0.01118 | Running loss: 0.01177\n",
            "Epoch: 174 | Iteration: 69 | Classification loss: 0.00003 | Regression loss: 0.02332 | Running loss: 0.01181\n",
            "Epoch: 174 | Iteration: 70 | Classification loss: 0.00001 | Regression loss: 0.00527 | Running loss: 0.01181\n",
            "Epoch: 174 | Iteration: 71 | Classification loss: 0.00002 | Regression loss: 0.01818 | Running loss: 0.01183\n",
            "Epoch: 174 | Iteration: 72 | Classification loss: 0.00001 | Regression loss: 0.00377 | Running loss: 0.01183\n",
            "Epoch: 174 | Iteration: 73 | Classification loss: 0.00002 | Regression loss: 0.01457 | Running loss: 0.01176\n",
            "Epoch: 174 | Iteration: 74 | Classification loss: 0.00001 | Regression loss: 0.00403 | Running loss: 0.01176\n",
            "Epoch: 174 | Iteration: 75 | Classification loss: 0.00001 | Regression loss: 0.00322 | Running loss: 0.01175\n",
            "Epoch: 174 | Iteration: 76 | Classification loss: 0.00001 | Regression loss: 0.00579 | Running loss: 0.01172\n",
            "Epoch: 174 | Iteration: 77 | Classification loss: 0.00002 | Regression loss: 0.01504 | Running loss: 0.01173\n",
            "Epoch: 174 | Iteration: 78 | Classification loss: 0.00003 | Regression loss: 0.01251 | Running loss: 0.01174\n",
            "Epoch: 174 | Iteration: 79 | Classification loss: 0.00002 | Regression loss: 0.01233 | Running loss: 0.01175\n",
            "Epoch: 174 | Iteration: 80 | Classification loss: 0.00003 | Regression loss: 0.00954 | Running loss: 0.01174\n",
            "Epoch: 174 | Iteration: 81 | Classification loss: 0.00004 | Regression loss: 0.01061 | Running loss: 0.01173\n",
            "Epoch: 174 | Iteration: 82 | Classification loss: 0.00001 | Regression loss: 0.02766 | Running loss: 0.01176\n",
            "Epoch: 174 | Iteration: 83 | Classification loss: 0.00005 | Regression loss: 0.00469 | Running loss: 0.01174\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.7290251963867072\n",
            "Precision:  0.5404699738903395\n",
            "Recall:  0.8181818181818182\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}, {0: (0.7314384456550078, 253.0)}, {0: (0.7334466242447818, 253.0)}, {0: (0.7357024222917343, 253.0)}, {0: (0.7381550871546381, 253.0)}, {0: (0.739092434551302, 253.0)}, {0: (0.72091448182607, 253.0)}, {0: (0.7389512050165785, 253.0)}, {0: (0.73050932626622, 253.0)}, {0: (0.7263226120133723, 253.0)}, {0: (0.7283463725823673, 253.0)}, {0: (0.7254913772539314, 253.0)}, {0: (0.7281179290861395, 253.0)}, {0: (0.7260020498565122, 253.0)}, {0: (0.7217236108381175, 253.0)}, {0: (0.7267514829312511, 253.0)}, {0: (0.7227483652714889, 253.0)}, {0: (0.7241298168882813, 253.0)}, {0: (0.7261051666397325, 253.0)}, {0: (0.7277246704086413, 253.0)}, {0: (0.7259496886680885, 253.0)}, {0: (0.7290251963867072, 253.0)}]\n",
            "Epoch: 175 | Iteration: 0 | Classification loss: 0.00003 | Regression loss: 0.00602 | Running loss: 0.01174\n",
            "Epoch: 175 | Iteration: 1 | Classification loss: 0.00002 | Regression loss: 0.01032 | Running loss: 0.01174\n",
            "Epoch: 175 | Iteration: 2 | Classification loss: 0.00001 | Regression loss: 0.00410 | Running loss: 0.01172\n",
            "Epoch: 175 | Iteration: 3 | Classification loss: 0.00011 | Regression loss: 0.04382 | Running loss: 0.01179\n",
            "Epoch: 175 | Iteration: 4 | Classification loss: 0.00001 | Regression loss: 0.02866 | Running loss: 0.01183\n",
            "Epoch: 175 | Iteration: 5 | Classification loss: 0.00002 | Regression loss: 0.01283 | Running loss: 0.01183\n",
            "Epoch: 175 | Iteration: 6 | Classification loss: 0.00001 | Regression loss: 0.00432 | Running loss: 0.01181\n",
            "Epoch: 175 | Iteration: 7 | Classification loss: 0.00005 | Regression loss: 0.01078 | Running loss: 0.01181\n",
            "Epoch: 175 | Iteration: 8 | Classification loss: 0.00002 | Regression loss: 0.02057 | Running loss: 0.01183\n",
            "Epoch: 175 | Iteration: 9 | Classification loss: 0.00002 | Regression loss: 0.01069 | Running loss: 0.01183\n",
            "Epoch: 175 | Iteration: 10 | Classification loss: 0.00001 | Regression loss: 0.01111 | Running loss: 0.01182\n",
            "Epoch: 175 | Iteration: 11 | Classification loss: 0.00003 | Regression loss: 0.01610 | Running loss: 0.01183\n",
            "Epoch: 175 | Iteration: 12 | Classification loss: 0.00014 | Regression loss: 0.00780 | Running loss: 0.01184\n",
            "Epoch: 175 | Iteration: 13 | Classification loss: 0.00002 | Regression loss: 0.01340 | Running loss: 0.01186\n",
            "Epoch: 175 | Iteration: 14 | Classification loss: 0.00002 | Regression loss: 0.01073 | Running loss: 0.01185\n",
            "Epoch: 175 | Iteration: 15 | Classification loss: 0.00002 | Regression loss: 0.00665 | Running loss: 0.01184\n",
            "Epoch: 175 | Iteration: 16 | Classification loss: 0.00002 | Regression loss: 0.00531 | Running loss: 0.01183\n",
            "Epoch: 175 | Iteration: 17 | Classification loss: 0.00002 | Regression loss: 0.00726 | Running loss: 0.01183\n",
            "Epoch: 175 | Iteration: 18 | Classification loss: 0.00001 | Regression loss: 0.01356 | Running loss: 0.01182\n",
            "Epoch: 175 | Iteration: 19 | Classification loss: 0.00002 | Regression loss: 0.00547 | Running loss: 0.01182\n",
            "Epoch: 175 | Iteration: 20 | Classification loss: 0.00004 | Regression loss: 0.01010 | Running loss: 0.01181\n",
            "Epoch: 175 | Iteration: 21 | Classification loss: 0.00001 | Regression loss: 0.00401 | Running loss: 0.01179\n",
            "Epoch: 175 | Iteration: 22 | Classification loss: 0.00001 | Regression loss: 0.00363 | Running loss: 0.01178\n",
            "Epoch: 175 | Iteration: 23 | Classification loss: 0.00004 | Regression loss: 0.00934 | Running loss: 0.01179\n",
            "Epoch: 175 | Iteration: 24 | Classification loss: 0.00003 | Regression loss: 0.01250 | Running loss: 0.01177\n",
            "Epoch: 175 | Iteration: 25 | Classification loss: 0.00004 | Regression loss: 0.01609 | Running loss: 0.01178\n",
            "Epoch: 175 | Iteration: 26 | Classification loss: 0.00001 | Regression loss: 0.01180 | Running loss: 0.01176\n",
            "Epoch: 175 | Iteration: 27 | Classification loss: 0.00007 | Regression loss: 0.01416 | Running loss: 0.01175\n",
            "Epoch: 175 | Iteration: 28 | Classification loss: 0.00000 | Regression loss: 0.00849 | Running loss: 0.01173\n",
            "Epoch: 175 | Iteration: 29 | Classification loss: 0.00002 | Regression loss: 0.01207 | Running loss: 0.01171\n",
            "Epoch: 175 | Iteration: 30 | Classification loss: 0.00001 | Regression loss: 0.00915 | Running loss: 0.01167\n",
            "Epoch: 175 | Iteration: 31 | Classification loss: 0.00003 | Regression loss: 0.01704 | Running loss: 0.01168\n",
            "Epoch: 175 | Iteration: 32 | Classification loss: 0.00002 | Regression loss: 0.00472 | Running loss: 0.01167\n",
            "Epoch: 175 | Iteration: 33 | Classification loss: 0.00001 | Regression loss: 0.01285 | Running loss: 0.01167\n",
            "Epoch: 175 | Iteration: 34 | Classification loss: 0.00001 | Regression loss: 0.01119 | Running loss: 0.01160\n",
            "Epoch: 175 | Iteration: 35 | Classification loss: 0.00001 | Regression loss: 0.01155 | Running loss: 0.01161\n",
            "Epoch: 175 | Iteration: 36 | Classification loss: 0.00009 | Regression loss: 0.01904 | Running loss: 0.01163\n",
            "Epoch: 175 | Iteration: 37 | Classification loss: 0.00004 | Regression loss: 0.01715 | Running loss: 0.01166\n",
            "Epoch: 175 | Iteration: 38 | Classification loss: 0.00003 | Regression loss: 0.00500 | Running loss: 0.01161\n",
            "Epoch: 175 | Iteration: 39 | Classification loss: 0.00001 | Regression loss: 0.00476 | Running loss: 0.01159\n",
            "Epoch: 175 | Iteration: 40 | Classification loss: 0.00003 | Regression loss: 0.00287 | Running loss: 0.01157\n",
            "Epoch: 175 | Iteration: 41 | Classification loss: 0.00002 | Regression loss: 0.01056 | Running loss: 0.01157\n",
            "Epoch: 175 | Iteration: 42 | Classification loss: 0.00001 | Regression loss: 0.01619 | Running loss: 0.01159\n",
            "Epoch: 175 | Iteration: 43 | Classification loss: 0.00001 | Regression loss: 0.00333 | Running loss: 0.01158\n",
            "Epoch: 175 | Iteration: 44 | Classification loss: 0.00007 | Regression loss: 0.01068 | Running loss: 0.01157\n",
            "Epoch: 175 | Iteration: 45 | Classification loss: 0.00001 | Regression loss: 0.01163 | Running loss: 0.01159\n",
            "Epoch: 175 | Iteration: 46 | Classification loss: 0.00003 | Regression loss: 0.00917 | Running loss: 0.01160\n",
            "Epoch: 175 | Iteration: 47 | Classification loss: 0.00003 | Regression loss: 0.01423 | Running loss: 0.01161\n",
            "Epoch: 175 | Iteration: 48 | Classification loss: 0.00002 | Regression loss: 0.00983 | Running loss: 0.01160\n",
            "Epoch: 175 | Iteration: 49 | Classification loss: 0.00001 | Regression loss: 0.00345 | Running loss: 0.01159\n",
            "Epoch: 175 | Iteration: 50 | Classification loss: 0.00002 | Regression loss: 0.00659 | Running loss: 0.01158\n",
            "Epoch: 175 | Iteration: 51 | Classification loss: 0.00002 | Regression loss: 0.01810 | Running loss: 0.01160\n",
            "Epoch: 175 | Iteration: 52 | Classification loss: 0.00001 | Regression loss: 0.00755 | Running loss: 0.01160\n",
            "Epoch: 175 | Iteration: 53 | Classification loss: 0.00001 | Regression loss: 0.02800 | Running loss: 0.01164\n",
            "Epoch: 175 | Iteration: 54 | Classification loss: 0.00003 | Regression loss: 0.01554 | Running loss: 0.01166\n",
            "Epoch: 175 | Iteration: 55 | Classification loss: 0.00003 | Regression loss: 0.00924 | Running loss: 0.01167\n",
            "Epoch: 175 | Iteration: 56 | Classification loss: 0.00006 | Regression loss: 0.01341 | Running loss: 0.01164\n",
            "Epoch: 175 | Iteration: 57 | Classification loss: 0.00002 | Regression loss: 0.01427 | Running loss: 0.01166\n",
            "Epoch: 175 | Iteration: 58 | Classification loss: 0.00005 | Regression loss: 0.00363 | Running loss: 0.01165\n",
            "Epoch: 175 | Iteration: 59 | Classification loss: 0.00005 | Regression loss: 0.01560 | Running loss: 0.01167\n",
            "Epoch: 175 | Iteration: 60 | Classification loss: 0.00001 | Regression loss: 0.00464 | Running loss: 0.01166\n",
            "Epoch: 175 | Iteration: 61 | Classification loss: 0.00001 | Regression loss: 0.00642 | Running loss: 0.01165\n",
            "Epoch: 175 | Iteration: 62 | Classification loss: 0.00001 | Regression loss: 0.00368 | Running loss: 0.01164\n",
            "Epoch: 175 | Iteration: 63 | Classification loss: 0.00001 | Regression loss: 0.00718 | Running loss: 0.01164\n",
            "Epoch: 175 | Iteration: 64 | Classification loss: 0.00003 | Regression loss: 0.01292 | Running loss: 0.01165\n",
            "Epoch: 175 | Iteration: 65 | Classification loss: 0.00002 | Regression loss: 0.01410 | Running loss: 0.01167\n",
            "Epoch: 175 | Iteration: 66 | Classification loss: 0.00001 | Regression loss: 0.00662 | Running loss: 0.01166\n",
            "Epoch: 175 | Iteration: 67 | Classification loss: 0.00001 | Regression loss: 0.01559 | Running loss: 0.01166\n",
            "Epoch: 175 | Iteration: 68 | Classification loss: 0.00002 | Regression loss: 0.00795 | Running loss: 0.01164\n",
            "Epoch: 175 | Iteration: 69 | Classification loss: 0.00011 | Regression loss: 0.01394 | Running loss: 0.01166\n",
            "Epoch: 175 | Iteration: 70 | Classification loss: 0.00003 | Regression loss: 0.02743 | Running loss: 0.01169\n",
            "Epoch: 175 | Iteration: 71 | Classification loss: 0.00001 | Regression loss: 0.00456 | Running loss: 0.01169\n",
            "Epoch: 175 | Iteration: 72 | Classification loss: 0.00004 | Regression loss: 0.01629 | Running loss: 0.01171\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 190.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 175 | Iteration: 74 | Classification loss: 0.00004 | Regression loss: 0.02015 | Running loss: 0.01171\n",
            "Epoch: 175 | Iteration: 75 | Classification loss: 0.00001 | Regression loss: 0.00948 | Running loss: 0.01171\n",
            "Epoch: 175 | Iteration: 76 | Classification loss: 0.00002 | Regression loss: 0.01558 | Running loss: 0.01171\n",
            "Epoch: 175 | Iteration: 77 | Classification loss: 0.00001 | Regression loss: 0.00658 | Running loss: 0.01170\n",
            "Epoch: 175 | Iteration: 78 | Classification loss: 0.00001 | Regression loss: 0.01245 | Running loss: 0.01170\n",
            "Epoch: 175 | Iteration: 79 | Classification loss: 0.00002 | Regression loss: 0.00691 | Running loss: 0.01169\n",
            "Epoch: 175 | Iteration: 80 | Classification loss: 0.00003 | Regression loss: 0.00958 | Running loss: 0.01168\n",
            "Epoch: 175 | Iteration: 81 | Classification loss: 0.00002 | Regression loss: 0.01161 | Running loss: 0.01168\n",
            "Epoch: 175 | Iteration: 82 | Classification loss: 0.00003 | Regression loss: 0.01818 | Running loss: 0.01168\n",
            "Epoch: 175 | Iteration: 83 | Classification loss: 0.00002 | Regression loss: 0.01305 | Running loss: 0.01169\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.7242894157206492\n",
            "Precision:  0.5418848167539267\n",
            "Recall:  0.8181818181818182\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}, {0: (0.7314384456550078, 253.0)}, {0: (0.7334466242447818, 253.0)}, {0: (0.7357024222917343, 253.0)}, {0: (0.7381550871546381, 253.0)}, {0: (0.739092434551302, 253.0)}, {0: (0.72091448182607, 253.0)}, {0: (0.7389512050165785, 253.0)}, {0: (0.73050932626622, 253.0)}, {0: (0.7263226120133723, 253.0)}, {0: (0.7283463725823673, 253.0)}, {0: (0.7254913772539314, 253.0)}, {0: (0.7281179290861395, 253.0)}, {0: (0.7260020498565122, 253.0)}, {0: (0.7217236108381175, 253.0)}, {0: (0.7267514829312511, 253.0)}, {0: (0.7227483652714889, 253.0)}, {0: (0.7241298168882813, 253.0)}, {0: (0.7261051666397325, 253.0)}, {0: (0.7277246704086413, 253.0)}, {0: (0.7259496886680885, 253.0)}, {0: (0.7290251963867072, 253.0)}, {0: (0.7242894157206492, 253.0)}]\n",
            "Epoch: 176 | Iteration: 0 | Classification loss: 0.00002 | Regression loss: 0.01795 | Running loss: 0.01170\n",
            "Epoch: 176 | Iteration: 1 | Classification loss: 0.00001 | Regression loss: 0.00368 | Running loss: 0.01167\n",
            "Epoch: 176 | Iteration: 2 | Classification loss: 0.00001 | Regression loss: 0.02787 | Running loss: 0.01172\n",
            "Epoch: 176 | Iteration: 3 | Classification loss: 0.00003 | Regression loss: 0.01278 | Running loss: 0.01171\n",
            "Epoch: 176 | Iteration: 4 | Classification loss: 0.00006 | Regression loss: 0.01596 | Running loss: 0.01171\n",
            "Epoch: 176 | Iteration: 5 | Classification loss: 0.00003 | Regression loss: 0.00434 | Running loss: 0.01170\n",
            "Epoch: 176 | Iteration: 6 | Classification loss: 0.00002 | Regression loss: 0.00725 | Running loss: 0.01170\n",
            "Epoch: 176 | Iteration: 7 | Classification loss: 0.00002 | Regression loss: 0.01028 | Running loss: 0.01170\n",
            "Epoch: 176 | Iteration: 8 | Classification loss: 0.00003 | Regression loss: 0.01016 | Running loss: 0.01165\n",
            "Epoch: 176 | Iteration: 9 | Classification loss: 0.00002 | Regression loss: 0.01770 | Running loss: 0.01167\n",
            "Epoch: 176 | Iteration: 10 | Classification loss: 0.00001 | Regression loss: 0.00424 | Running loss: 0.01165\n",
            "Epoch: 176 | Iteration: 11 | Classification loss: 0.00005 | Regression loss: 0.01275 | Running loss: 0.01163\n",
            "Epoch: 176 | Iteration: 12 | Classification loss: 0.00001 | Regression loss: 0.01167 | Running loss: 0.01163\n",
            "Epoch: 176 | Iteration: 13 | Classification loss: 0.00002 | Regression loss: 0.02951 | Running loss: 0.01167\n",
            "Epoch: 176 | Iteration: 14 | Classification loss: 0.00003 | Regression loss: 0.01397 | Running loss: 0.01169\n",
            "Epoch: 176 | Iteration: 15 | Classification loss: 0.00002 | Regression loss: 0.00276 | Running loss: 0.01167\n",
            "Epoch: 176 | Iteration: 16 | Classification loss: 0.00003 | Regression loss: 0.01392 | Running loss: 0.01167\n",
            "Epoch: 176 | Iteration: 17 | Classification loss: 0.00002 | Regression loss: 0.01223 | Running loss: 0.01168\n",
            "Epoch: 176 | Iteration: 18 | Classification loss: 0.00001 | Regression loss: 0.00573 | Running loss: 0.01169\n",
            "Epoch: 176 | Iteration: 19 | Classification loss: 0.00006 | Regression loss: 0.00967 | Running loss: 0.01168\n",
            "Epoch: 176 | Iteration: 20 | Classification loss: 0.00002 | Regression loss: 0.01464 | Running loss: 0.01170\n",
            "Epoch: 176 | Iteration: 21 | Classification loss: 0.00010 | Regression loss: 0.01721 | Running loss: 0.01171\n",
            "Epoch: 176 | Iteration: 22 | Classification loss: 0.00009 | Regression loss: 0.01305 | Running loss: 0.01171\n",
            "Epoch: 176 | Iteration: 23 | Classification loss: 0.00001 | Regression loss: 0.00611 | Running loss: 0.01170\n",
            "Epoch: 176 | Iteration: 24 | Classification loss: 0.00003 | Regression loss: 0.01373 | Running loss: 0.01171\n",
            "Epoch: 176 | Iteration: 25 | Classification loss: 0.00002 | Regression loss: 0.01022 | Running loss: 0.01171\n",
            "Epoch: 176 | Iteration: 26 | Classification loss: 0.00002 | Regression loss: 0.01841 | Running loss: 0.01174\n",
            "Epoch: 176 | Iteration: 27 | Classification loss: 0.00001 | Regression loss: 0.00360 | Running loss: 0.01173\n",
            "Epoch: 176 | Iteration: 28 | Classification loss: 0.00009 | Regression loss: 0.04294 | Running loss: 0.01179\n",
            "Epoch: 176 | Iteration: 29 | Classification loss: 0.00003 | Regression loss: 0.00863 | Running loss: 0.01177\n",
            "Epoch: 176 | Iteration: 30 | Classification loss: 0.00003 | Regression loss: 0.00994 | Running loss: 0.01175\n",
            "Epoch: 176 | Iteration: 31 | Classification loss: 0.00001 | Regression loss: 0.00422 | Running loss: 0.01174\n",
            "Epoch: 176 | Iteration: 32 | Classification loss: 0.00003 | Regression loss: 0.01269 | Running loss: 0.01176\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 190.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 176 | Iteration: 34 | Classification loss: 0.00002 | Regression loss: 0.01027 | Running loss: 0.01175\n",
            "Epoch: 176 | Iteration: 35 | Classification loss: 0.00002 | Regression loss: 0.00626 | Running loss: 0.01175\n",
            "Epoch: 176 | Iteration: 36 | Classification loss: 0.00001 | Regression loss: 0.01107 | Running loss: 0.01174\n",
            "Epoch: 176 | Iteration: 37 | Classification loss: 0.00001 | Regression loss: 0.00424 | Running loss: 0.01172\n",
            "Epoch: 176 | Iteration: 38 | Classification loss: 0.00002 | Regression loss: 0.00854 | Running loss: 0.01172\n",
            "Epoch: 176 | Iteration: 39 | Classification loss: 0.00003 | Regression loss: 0.01493 | Running loss: 0.01174\n",
            "Epoch: 176 | Iteration: 40 | Classification loss: 0.00004 | Regression loss: 0.02064 | Running loss: 0.01176\n",
            "Epoch: 176 | Iteration: 41 | Classification loss: 0.00003 | Regression loss: 0.01521 | Running loss: 0.01177\n",
            "Epoch: 176 | Iteration: 42 | Classification loss: 0.00001 | Regression loss: 0.01088 | Running loss: 0.01177\n",
            "Epoch: 176 | Iteration: 43 | Classification loss: 0.00005 | Regression loss: 0.01310 | Running loss: 0.01177\n",
            "Epoch: 176 | Iteration: 44 | Classification loss: 0.00002 | Regression loss: 0.00669 | Running loss: 0.01177\n",
            "Epoch: 176 | Iteration: 45 | Classification loss: 0.00002 | Regression loss: 0.01075 | Running loss: 0.01176\n",
            "Epoch: 176 | Iteration: 46 | Classification loss: 0.00001 | Regression loss: 0.00440 | Running loss: 0.01171\n",
            "Epoch: 176 | Iteration: 47 | Classification loss: 0.00001 | Regression loss: 0.01257 | Running loss: 0.01170\n",
            "Epoch: 176 | Iteration: 48 | Classification loss: 0.00002 | Regression loss: 0.02419 | Running loss: 0.01172\n",
            "Epoch: 176 | Iteration: 49 | Classification loss: 0.00001 | Regression loss: 0.00362 | Running loss: 0.01171\n",
            "Epoch: 176 | Iteration: 50 | Classification loss: 0.00002 | Regression loss: 0.01219 | Running loss: 0.01171\n",
            "Epoch: 176 | Iteration: 51 | Classification loss: 0.00003 | Regression loss: 0.01802 | Running loss: 0.01171\n",
            "Epoch: 176 | Iteration: 52 | Classification loss: 0.00001 | Regression loss: 0.01076 | Running loss: 0.01173\n",
            "Epoch: 176 | Iteration: 53 | Classification loss: 0.00002 | Regression loss: 0.01455 | Running loss: 0.01173\n",
            "Epoch: 176 | Iteration: 54 | Classification loss: 0.00015 | Regression loss: 0.00758 | Running loss: 0.01172\n",
            "Epoch: 176 | Iteration: 55 | Classification loss: 0.00001 | Regression loss: 0.00340 | Running loss: 0.01171\n",
            "Epoch: 176 | Iteration: 56 | Classification loss: 0.00002 | Regression loss: 0.00523 | Running loss: 0.01169\n",
            "Epoch: 176 | Iteration: 57 | Classification loss: 0.00003 | Regression loss: 0.01047 | Running loss: 0.01168\n",
            "Epoch: 176 | Iteration: 58 | Classification loss: 0.00002 | Regression loss: 0.00583 | Running loss: 0.01167\n",
            "Epoch: 176 | Iteration: 59 | Classification loss: 0.00001 | Regression loss: 0.00681 | Running loss: 0.01164\n",
            "Epoch: 176 | Iteration: 60 | Classification loss: 0.00002 | Regression loss: 0.01652 | Running loss: 0.01166\n",
            "Epoch: 176 | Iteration: 61 | Classification loss: 0.00005 | Regression loss: 0.01083 | Running loss: 0.01166\n",
            "Epoch: 176 | Iteration: 62 | Classification loss: 0.00001 | Regression loss: 0.01397 | Running loss: 0.01166\n",
            "Epoch: 176 | Iteration: 63 | Classification loss: 0.00001 | Regression loss: 0.01012 | Running loss: 0.01166\n",
            "Epoch: 176 | Iteration: 64 | Classification loss: 0.00001 | Regression loss: 0.00545 | Running loss: 0.01165\n",
            "Epoch: 176 | Iteration: 65 | Classification loss: 0.00001 | Regression loss: 0.00611 | Running loss: 0.01164\n",
            "Epoch: 176 | Iteration: 66 | Classification loss: 0.00001 | Regression loss: 0.00722 | Running loss: 0.01164\n",
            "Epoch: 176 | Iteration: 67 | Classification loss: 0.00001 | Regression loss: 0.01731 | Running loss: 0.01166\n",
            "Epoch: 176 | Iteration: 68 | Classification loss: 0.00001 | Regression loss: 0.00982 | Running loss: 0.01167\n",
            "Epoch: 176 | Iteration: 69 | Classification loss: 0.00003 | Regression loss: 0.01378 | Running loss: 0.01169\n",
            "Epoch: 176 | Iteration: 70 | Classification loss: 0.00001 | Regression loss: 0.00839 | Running loss: 0.01167\n",
            "Epoch: 176 | Iteration: 71 | Classification loss: 0.00003 | Regression loss: 0.01223 | Running loss: 0.01166\n",
            "Epoch: 176 | Iteration: 72 | Classification loss: 0.00002 | Regression loss: 0.01132 | Running loss: 0.01166\n",
            "Epoch: 176 | Iteration: 73 | Classification loss: 0.00001 | Regression loss: 0.00330 | Running loss: 0.01158\n",
            "Epoch: 176 | Iteration: 74 | Classification loss: 0.00002 | Regression loss: 0.00690 | Running loss: 0.01157\n",
            "Epoch: 176 | Iteration: 75 | Classification loss: 0.00003 | Regression loss: 0.01082 | Running loss: 0.01156\n",
            "Epoch: 176 | Iteration: 76 | Classification loss: 0.00002 | Regression loss: 0.00831 | Running loss: 0.01155\n",
            "Epoch: 176 | Iteration: 77 | Classification loss: 0.00004 | Regression loss: 0.00386 | Running loss: 0.01152\n",
            "Epoch: 176 | Iteration: 78 | Classification loss: 0.00002 | Regression loss: 0.01457 | Running loss: 0.01153\n",
            "Epoch: 176 | Iteration: 79 | Classification loss: 0.00001 | Regression loss: 0.00464 | Running loss: 0.01153\n",
            "Epoch: 176 | Iteration: 80 | Classification loss: 0.00006 | Regression loss: 0.01294 | Running loss: 0.01155\n",
            "Epoch: 176 | Iteration: 81 | Classification loss: 0.00002 | Regression loss: 0.00480 | Running loss: 0.01154\n",
            "Epoch: 176 | Iteration: 82 | Classification loss: 0.00003 | Regression loss: 0.01336 | Running loss: 0.01155\n",
            "Epoch: 176 | Iteration: 83 | Classification loss: 0.00005 | Regression loss: 0.01497 | Running loss: 0.01155\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.7329303843522665\n",
            "Precision:  0.5447368421052632\n",
            "Recall:  0.8181818181818182\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}, {0: (0.7314384456550078, 253.0)}, {0: (0.7334466242447818, 253.0)}, {0: (0.7357024222917343, 253.0)}, {0: (0.7381550871546381, 253.0)}, {0: (0.739092434551302, 253.0)}, {0: (0.72091448182607, 253.0)}, {0: (0.7389512050165785, 253.0)}, {0: (0.73050932626622, 253.0)}, {0: (0.7263226120133723, 253.0)}, {0: (0.7283463725823673, 253.0)}, {0: (0.7254913772539314, 253.0)}, {0: (0.7281179290861395, 253.0)}, {0: (0.7260020498565122, 253.0)}, {0: (0.7217236108381175, 253.0)}, {0: (0.7267514829312511, 253.0)}, {0: (0.7227483652714889, 253.0)}, {0: (0.7241298168882813, 253.0)}, {0: (0.7261051666397325, 253.0)}, {0: (0.7277246704086413, 253.0)}, {0: (0.7259496886680885, 253.0)}, {0: (0.7290251963867072, 253.0)}, {0: (0.7242894157206492, 253.0)}, {0: (0.7329303843522665, 253.0)}]\n",
            "Epoch: 177 | Iteration: 0 | Classification loss: 0.00002 | Regression loss: 0.01037 | Running loss: 0.01155\n",
            "Epoch: 177 | Iteration: 1 | Classification loss: 0.00001 | Regression loss: 0.02995 | Running loss: 0.01161\n",
            "Epoch: 177 | Iteration: 2 | Classification loss: 0.00002 | Regression loss: 0.00641 | Running loss: 0.01161\n",
            "Epoch: 177 | Iteration: 3 | Classification loss: 0.00001 | Regression loss: 0.00711 | Running loss: 0.01160\n",
            "Epoch: 177 | Iteration: 4 | Classification loss: 0.00001 | Regression loss: 0.00409 | Running loss: 0.01158\n",
            "Epoch: 177 | Iteration: 5 | Classification loss: 0.00001 | Regression loss: 0.00366 | Running loss: 0.01158\n",
            "Epoch: 177 | Iteration: 6 | Classification loss: 0.00002 | Regression loss: 0.00940 | Running loss: 0.01155\n",
            "Epoch: 177 | Iteration: 7 | Classification loss: 0.00002 | Regression loss: 0.01126 | Running loss: 0.01155\n",
            "Epoch: 177 | Iteration: 8 | Classification loss: 0.00001 | Regression loss: 0.00279 | Running loss: 0.01153\n",
            "Epoch: 177 | Iteration: 9 | Classification loss: 0.00002 | Regression loss: 0.00993 | Running loss: 0.01151\n",
            "Epoch: 177 | Iteration: 10 | Classification loss: 0.00001 | Regression loss: 0.01688 | Running loss: 0.01154\n",
            "Epoch: 177 | Iteration: 11 | Classification loss: 0.00007 | Regression loss: 0.01468 | Running loss: 0.01155\n",
            "Epoch: 177 | Iteration: 12 | Classification loss: 0.00002 | Regression loss: 0.00757 | Running loss: 0.01153\n",
            "Epoch: 177 | Iteration: 13 | Classification loss: 0.00005 | Regression loss: 0.01304 | Running loss: 0.01154\n",
            "Epoch: 177 | Iteration: 14 | Classification loss: 0.00010 | Regression loss: 0.01390 | Running loss: 0.01153\n",
            "Epoch: 177 | Iteration: 15 | Classification loss: 0.00003 | Regression loss: 0.01367 | Running loss: 0.01152\n",
            "Epoch: 177 | Iteration: 16 | Classification loss: 0.00003 | Regression loss: 0.00298 | Running loss: 0.01147\n",
            "Epoch: 177 | Iteration: 17 | Classification loss: 0.00001 | Regression loss: 0.00459 | Running loss: 0.01146\n",
            "Epoch: 177 | Iteration: 18 | Classification loss: 0.00002 | Regression loss: 0.01221 | Running loss: 0.01146\n",
            "Epoch: 177 | Iteration: 19 | Classification loss: 0.00002 | Regression loss: 0.01211 | Running loss: 0.01147\n",
            "Epoch: 177 | Iteration: 20 | Classification loss: 0.00002 | Regression loss: 0.01366 | Running loss: 0.01147\n",
            "Epoch: 177 | Iteration: 21 | Classification loss: 0.00002 | Regression loss: 0.01021 | Running loss: 0.01147\n",
            "Epoch: 177 | Iteration: 22 | Classification loss: 0.00001 | Regression loss: 0.00395 | Running loss: 0.01146\n",
            "Epoch: 177 | Iteration: 23 | Classification loss: 0.00003 | Regression loss: 0.01626 | Running loss: 0.01148\n",
            "Epoch: 177 | Iteration: 24 | Classification loss: 0.00001 | Regression loss: 0.01312 | Running loss: 0.01148\n",
            "Epoch: 177 | Iteration: 25 | Classification loss: 0.00003 | Regression loss: 0.01315 | Running loss: 0.01149\n",
            "Epoch: 177 | Iteration: 26 | Classification loss: 0.00001 | Regression loss: 0.00919 | Running loss: 0.01147\n",
            "Epoch: 177 | Iteration: 27 | Classification loss: 0.00001 | Regression loss: 0.01658 | Running loss: 0.01150\n",
            "Epoch: 177 | Iteration: 28 | Classification loss: 0.00002 | Regression loss: 0.00627 | Running loss: 0.01149\n",
            "Epoch: 177 | Iteration: 29 | Classification loss: 0.00003 | Regression loss: 0.01371 | Running loss: 0.01151\n",
            "Epoch: 177 | Iteration: 30 | Classification loss: 0.00003 | Regression loss: 0.01491 | Running loss: 0.01151\n",
            "Epoch: 177 | Iteration: 31 | Classification loss: 0.00001 | Regression loss: 0.02163 | Running loss: 0.01153\n",
            "Epoch: 177 | Iteration: 32 | Classification loss: 0.00001 | Regression loss: 0.00327 | Running loss: 0.01153\n",
            "Epoch: 177 | Iteration: 33 | Classification loss: 0.00001 | Regression loss: 0.00391 | Running loss: 0.01153\n",
            "Epoch: 177 | Iteration: 34 | Classification loss: 0.00003 | Regression loss: 0.01684 | Running loss: 0.01155\n",
            "Epoch: 177 | Iteration: 35 | Classification loss: 0.00010 | Regression loss: 0.01856 | Running loss: 0.01157\n",
            "Epoch: 177 | Iteration: 36 | Classification loss: 0.00002 | Regression loss: 0.01364 | Running loss: 0.01159\n",
            "Epoch: 177 | Iteration: 37 | Classification loss: 0.00001 | Regression loss: 0.00702 | Running loss: 0.01157\n",
            "Epoch: 177 | Iteration: 38 | Classification loss: 0.00002 | Regression loss: 0.00980 | Running loss: 0.01156\n",
            "Epoch: 177 | Iteration: 39 | Classification loss: 0.00001 | Regression loss: 0.01106 | Running loss: 0.01156\n",
            "Epoch: 177 | Iteration: 40 | Classification loss: 0.00001 | Regression loss: 0.00525 | Running loss: 0.01155\n",
            "Epoch: 177 | Iteration: 41 | Classification loss: 0.00004 | Regression loss: 0.01118 | Running loss: 0.01156\n",
            "Epoch: 177 | Iteration: 42 | Classification loss: 0.00002 | Regression loss: 0.01218 | Running loss: 0.01156\n",
            "Epoch: 177 | Iteration: 43 | Classification loss: 0.00002 | Regression loss: 0.00418 | Running loss: 0.01154\n",
            "Epoch: 177 | Iteration: 44 | Classification loss: 0.00004 | Regression loss: 0.02068 | Running loss: 0.01155\n",
            "Epoch: 177 | Iteration: 45 | Classification loss: 0.00002 | Regression loss: 0.01109 | Running loss: 0.01155\n",
            "Epoch: 177 | Iteration: 46 | Classification loss: 0.00002 | Regression loss: 0.00935 | Running loss: 0.01154\n",
            "Epoch: 177 | Iteration: 47 | Classification loss: 0.00002 | Regression loss: 0.01410 | Running loss: 0.01156\n",
            "Epoch: 177 | Iteration: 48 | Classification loss: 0.00001 | Regression loss: 0.01039 | Running loss: 0.01149\n",
            "Epoch: 177 | Iteration: 49 | Classification loss: 0.00004 | Regression loss: 0.00389 | Running loss: 0.01149\n",
            "Epoch: 177 | Iteration: 50 | Classification loss: 0.00002 | Regression loss: 0.01428 | Running loss: 0.01148\n",
            "Epoch: 177 | Iteration: 51 | Classification loss: 0.00003 | Regression loss: 0.00605 | Running loss: 0.01147\n",
            "Epoch: 177 | Iteration: 52 | Classification loss: 0.00008 | Regression loss: 0.04267 | Running loss: 0.01153\n",
            "Epoch: 177 | Iteration: 53 | Classification loss: 0.00002 | Regression loss: 0.01599 | Running loss: 0.01153\n",
            "Epoch: 177 | Iteration: 54 | Classification loss: 0.00001 | Regression loss: 0.01133 | Running loss: 0.01154\n",
            "Epoch: 177 | Iteration: 55 | Classification loss: 0.00000 | Regression loss: 0.00864 | Running loss: 0.01155\n",
            "Epoch: 177 | Iteration: 56 | Classification loss: 0.00004 | Regression loss: 0.01531 | Running loss: 0.01155\n",
            "Epoch: 177 | Iteration: 57 | Classification loss: 0.00001 | Regression loss: 0.02763 | Running loss: 0.01160\n",
            "Epoch: 177 | Iteration: 58 | Classification loss: 0.00003 | Regression loss: 0.01304 | Running loss: 0.01159\n",
            "Epoch: 177 | Iteration: 59 | Classification loss: 0.00002 | Regression loss: 0.00454 | Running loss: 0.01159\n",
            "Epoch: 177 | Iteration: 60 | Classification loss: 0.00002 | Regression loss: 0.00744 | Running loss: 0.01158\n",
            "Epoch: 177 | Iteration: 61 | Classification loss: 0.00007 | Regression loss: 0.01156 | Running loss: 0.01154\n",
            "Epoch: 177 | Iteration: 62 | Classification loss: 0.00001 | Regression loss: 0.00481 | Running loss: 0.01152\n",
            "Epoch: 177 | Iteration: 63 | Classification loss: 0.00001 | Regression loss: 0.00570 | Running loss: 0.01151\n",
            "Epoch: 177 | Iteration: 64 | Classification loss: 0.00001 | Regression loss: 0.00964 | Running loss: 0.01150\n",
            "Epoch: 177 | Iteration: 65 | Classification loss: 0.00001 | Regression loss: 0.01218 | Running loss: 0.01152\n",
            "Epoch: 177 | Iteration: 66 | Classification loss: 0.00002 | Regression loss: 0.01793 | Running loss: 0.01153\n",
            "Epoch: 177 | Iteration: 67 | Classification loss: 0.00003 | Regression loss: 0.01190 | Running loss: 0.01153\n",
            "Epoch: 177 | Iteration: 68 | Classification loss: 0.00001 | Regression loss: 0.00440 | Running loss: 0.01150\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 190.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 177 | Iteration: 70 | Classification loss: 0.00001 | Regression loss: 0.00586 | Running loss: 0.01150\n",
            "Epoch: 177 | Iteration: 71 | Classification loss: 0.00001 | Regression loss: 0.00728 | Running loss: 0.01149\n",
            "Epoch: 177 | Iteration: 72 | Classification loss: 0.00001 | Regression loss: 0.00592 | Running loss: 0.01147\n",
            "Epoch: 177 | Iteration: 73 | Classification loss: 0.00003 | Regression loss: 0.01171 | Running loss: 0.01147\n",
            "Epoch: 177 | Iteration: 74 | Classification loss: 0.00003 | Regression loss: 0.01004 | Running loss: 0.01147\n",
            "Epoch: 177 | Iteration: 75 | Classification loss: 0.00002 | Regression loss: 0.00512 | Running loss: 0.01145\n",
            "Epoch: 177 | Iteration: 76 | Classification loss: 0.00005 | Regression loss: 0.01668 | Running loss: 0.01147\n",
            "Epoch: 177 | Iteration: 77 | Classification loss: 0.00003 | Regression loss: 0.01036 | Running loss: 0.01146\n",
            "Epoch: 177 | Iteration: 78 | Classification loss: 0.00003 | Regression loss: 0.01333 | Running loss: 0.01146\n",
            "Epoch: 177 | Iteration: 79 | Classification loss: 0.00003 | Regression loss: 0.00642 | Running loss: 0.01147\n",
            "Epoch: 177 | Iteration: 80 | Classification loss: 0.00003 | Regression loss: 0.00923 | Running loss: 0.01147\n",
            "Epoch: 177 | Iteration: 81 | Classification loss: 0.00006 | Regression loss: 0.01565 | Running loss: 0.01147\n",
            "Epoch: 177 | Iteration: 82 | Classification loss: 0.00014 | Regression loss: 0.00831 | Running loss: 0.01146\n",
            "Epoch: 177 | Iteration: 83 | Classification loss: 0.00003 | Regression loss: 0.00948 | Running loss: 0.01147\n",
            "Evaluating dataset\n",
            "\n",
            "mAP:\n",
            "person: 0.733154886135177\n",
            "Precision:  0.5362694300518135\n",
            "Recall:  0.8181818181818182\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}, {0: (0.7314384456550078, 253.0)}, {0: (0.7334466242447818, 253.0)}, {0: (0.7357024222917343, 253.0)}, {0: (0.7381550871546381, 253.0)}, {0: (0.739092434551302, 253.0)}, {0: (0.72091448182607, 253.0)}, {0: (0.7389512050165785, 253.0)}, {0: (0.73050932626622, 253.0)}, {0: (0.7263226120133723, 253.0)}, {0: (0.7283463725823673, 253.0)}, {0: (0.7254913772539314, 253.0)}, {0: (0.7281179290861395, 253.0)}, {0: (0.7260020498565122, 253.0)}, {0: (0.7217236108381175, 253.0)}, {0: (0.7267514829312511, 253.0)}, {0: (0.7227483652714889, 253.0)}, {0: (0.7241298168882813, 253.0)}, {0: (0.7261051666397325, 253.0)}, {0: (0.7277246704086413, 253.0)}, {0: (0.7259496886680885, 253.0)}, {0: (0.7290251963867072, 253.0)}, {0: (0.7242894157206492, 253.0)}, {0: (0.7329303843522665, 253.0)}, {0: (0.733154886135177, 253.0)}]\n",
            "Epoch: 178 | Iteration: 0 | Classification loss: 0.00003 | Regression loss: 0.01578 | Running loss: 0.01149\n",
            "Epoch: 178 | Iteration: 1 | Classification loss: 0.00001 | Regression loss: 0.01033 | Running loss: 0.01146\n",
            "Epoch: 178 | Iteration: 2 | Classification loss: 0.00003 | Regression loss: 0.01272 | Running loss: 0.01146\n",
            "Epoch: 178 | Iteration: 3 | Classification loss: 0.00001 | Regression loss: 0.00994 | Running loss: 0.01146\n",
            "Epoch: 178 | Iteration: 4 | Classification loss: 0.00002 | Regression loss: 0.00639 | Running loss: 0.01144\n",
            "Epoch: 178 | Iteration: 5 | Classification loss: 0.00002 | Regression loss: 0.01481 | Running loss: 0.01145\n",
            "Epoch: 178 | Iteration: 6 | Classification loss: 0.00003 | Regression loss: 0.00623 | Running loss: 0.01144\n",
            "Epoch: 178 | Iteration: 7 | Classification loss: 0.00001 | Regression loss: 0.01055 | Running loss: 0.01144\n",
            "Epoch: 178 | Iteration: 8 | Classification loss: 0.00001 | Regression loss: 0.00417 | Running loss: 0.01141\n",
            "Epoch: 178 | Iteration: 9 | Classification loss: 0.00001 | Regression loss: 0.00390 | Running loss: 0.01139\n",
            "Epoch: 178 | Iteration: 10 | Classification loss: 0.00005 | Regression loss: 0.02053 | Running loss: 0.01137\n",
            "Epoch: 178 | Iteration: 11 | Classification loss: 0.00002 | Regression loss: 0.01598 | Running loss: 0.01138\n",
            "Epoch: 178 | Iteration: 12 | Classification loss: 0.00002 | Regression loss: 0.00982 | Running loss: 0.01136\n",
            "Epoch: 178 | Iteration: 13 | Classification loss: 0.00003 | Regression loss: 0.01184 | Running loss: 0.01135\n",
            "Epoch: 178 | Iteration: 14 | Classification loss: 0.00003 | Regression loss: 0.01500 | Running loss: 0.01137\n",
            "Epoch: 178 | Iteration: 15 | Classification loss: 0.00007 | Regression loss: 0.01277 | Running loss: 0.01139\n",
            "Epoch: 178 | Iteration: 16 | Classification loss: 0.00001 | Regression loss: 0.00566 | Running loss: 0.01137\n",
            "Epoch: 178 | Iteration: 17 | Classification loss: 0.00002 | Regression loss: 0.00540 | Running loss: 0.01137\n",
            "Epoch: 178 | Iteration: 18 | Classification loss: 0.00002 | Regression loss: 0.00520 | Running loss: 0.01138\n",
            "Epoch: 178 | Iteration: 19 | Classification loss: 0.00001 | Regression loss: 0.00418 | Running loss: 0.01136\n",
            "Epoch: 178 | Iteration: 20 | Classification loss: 0.00001 | Regression loss: 0.00309 | Running loss: 0.01134\n",
            "Epoch: 178 | Iteration: 21 | Classification loss: 0.00001 | Regression loss: 0.00337 | Running loss: 0.01130\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 190.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 178 | Iteration: 23 | Classification loss: 0.00003 | Regression loss: 0.01487 | Running loss: 0.01131\n",
            "Epoch: 178 | Iteration: 24 | Classification loss: 0.00005 | Regression loss: 0.01600 | Running loss: 0.01133\n",
            "Epoch: 178 | Iteration: 25 | Classification loss: 0.00001 | Regression loss: 0.01331 | Running loss: 0.01135\n",
            "Epoch: 178 | Iteration: 26 | Classification loss: 0.00001 | Regression loss: 0.01209 | Running loss: 0.01134\n",
            "Epoch: 178 | Iteration: 27 | Classification loss: 0.00003 | Regression loss: 0.01577 | Running loss: 0.01134\n",
            "Epoch: 178 | Iteration: 28 | Classification loss: 0.00003 | Regression loss: 0.00903 | Running loss: 0.01132\n",
            "Epoch: 178 | Iteration: 29 | Classification loss: 0.00002 | Regression loss: 0.00643 | Running loss: 0.01131\n",
            "Epoch: 178 | Iteration: 30 | Classification loss: 0.00002 | Regression loss: 0.01050 | Running loss: 0.01130\n",
            "Epoch: 178 | Iteration: 31 | Classification loss: 0.00001 | Regression loss: 0.02146 | Running loss: 0.01134\n",
            "Epoch: 178 | Iteration: 32 | Classification loss: 0.00013 | Regression loss: 0.00950 | Running loss: 0.01133\n",
            "Epoch: 178 | Iteration: 33 | Classification loss: 0.00006 | Regression loss: 0.01380 | Running loss: 0.01135\n",
            "Epoch: 178 | Iteration: 34 | Classification loss: 0.00003 | Regression loss: 0.01200 | Running loss: 0.01135\n",
            "Epoch: 178 | Iteration: 35 | Classification loss: 0.00001 | Regression loss: 0.00466 | Running loss: 0.01134\n",
            "Epoch: 178 | Iteration: 36 | Classification loss: 0.00002 | Regression loss: 0.01332 | Running loss: 0.01135\n",
            "Epoch: 178 | Iteration: 37 | Classification loss: 0.00001 | Regression loss: 0.00387 | Running loss: 0.01133\n",
            "Epoch: 178 | Iteration: 38 | Classification loss: 0.00008 | Regression loss: 0.01489 | Running loss: 0.01134\n",
            "Epoch: 178 | Iteration: 39 | Classification loss: 0.00002 | Regression loss: 0.01247 | Running loss: 0.01133\n",
            "Epoch: 178 | Iteration: 40 | Classification loss: 0.00001 | Regression loss: 0.01074 | Running loss: 0.01133\n",
            "Epoch: 178 | Iteration: 41 | Classification loss: 0.00002 | Regression loss: 0.00602 | Running loss: 0.01131\n",
            "Epoch: 178 | Iteration: 42 | Classification loss: 0.00002 | Regression loss: 0.01382 | Running loss: 0.01131\n",
            "Epoch: 178 | Iteration: 43 | Classification loss: 0.00002 | Regression loss: 0.00699 | Running loss: 0.01132\n",
            "Epoch: 178 | Iteration: 44 | Classification loss: 0.00003 | Regression loss: 0.00980 | Running loss: 0.01132\n",
            "Epoch: 178 | Iteration: 45 | Classification loss: 0.00001 | Regression loss: 0.00418 | Running loss: 0.01131\n",
            "Epoch: 178 | Iteration: 46 | Classification loss: 0.00002 | Regression loss: 0.01148 | Running loss: 0.01133\n",
            "Epoch: 178 | Iteration: 47 | Classification loss: 0.00003 | Regression loss: 0.01218 | Running loss: 0.01133\n",
            "Epoch: 178 | Iteration: 48 | Classification loss: 0.00001 | Regression loss: 0.00939 | Running loss: 0.01133\n",
            "Epoch: 178 | Iteration: 49 | Classification loss: 0.00003 | Regression loss: 0.01318 | Running loss: 0.01134\n",
            "Epoch: 178 | Iteration: 50 | Classification loss: 0.00003 | Regression loss: 0.00475 | Running loss: 0.01130\n",
            "Epoch: 178 | Iteration: 51 | Classification loss: 0.00001 | Regression loss: 0.01122 | Running loss: 0.01131\n",
            "Epoch: 178 | Iteration: 52 | Classification loss: 0.00003 | Regression loss: 0.02370 | Running loss: 0.01135\n",
            "Epoch: 178 | Iteration: 53 | Classification loss: 0.00001 | Regression loss: 0.01792 | Running loss: 0.01137\n",
            "Epoch: 178 | Iteration: 54 | Classification loss: 0.00002 | Regression loss: 0.01269 | Running loss: 0.01138\n",
            "Epoch: 178 | Iteration: 55 | Classification loss: 0.00001 | Regression loss: 0.02771 | Running loss: 0.01139\n",
            "Epoch: 178 | Iteration: 56 | Classification loss: 0.00000 | Regression loss: 0.00712 | Running loss: 0.01140\n",
            "Epoch: 178 | Iteration: 57 | Classification loss: 0.00003 | Regression loss: 0.01051 | Running loss: 0.01141\n",
            "Epoch: 178 | Iteration: 58 | Classification loss: 0.00009 | Regression loss: 0.01704 | Running loss: 0.01143\n",
            "Epoch: 178 | Iteration: 59 | Classification loss: 0.00002 | Regression loss: 0.00262 | Running loss: 0.01141\n",
            "Epoch: 178 | Iteration: 60 | Classification loss: 0.00001 | Regression loss: 0.00631 | Running loss: 0.01141\n",
            "Epoch: 178 | Iteration: 61 | Classification loss: 0.00001 | Regression loss: 0.00446 | Running loss: 0.01141\n",
            "Epoch: 178 | Iteration: 62 | Classification loss: 0.00001 | Regression loss: 0.01732 | Running loss: 0.01143\n",
            "Epoch: 178 | Iteration: 63 | Classification loss: 0.00002 | Regression loss: 0.01080 | Running loss: 0.01145\n",
            "Epoch: 178 | Iteration: 64 | Classification loss: 0.00002 | Regression loss: 0.01014 | Running loss: 0.01146\n",
            "Epoch: 178 | Iteration: 65 | Classification loss: 0.00002 | Regression loss: 0.01205 | Running loss: 0.01146\n",
            "Epoch: 178 | Iteration: 66 | Classification loss: 0.00001 | Regression loss: 0.00436 | Running loss: 0.01145\n",
            "Epoch: 178 | Iteration: 67 | Classification loss: 0.00002 | Regression loss: 0.03002 | Running loss: 0.01149\n",
            "Epoch: 178 | Iteration: 68 | Classification loss: 0.00005 | Regression loss: 0.00396 | Running loss: 0.01148\n",
            "Epoch: 178 | Iteration: 69 | Classification loss: 0.00001 | Regression loss: 0.01110 | Running loss: 0.01147\n",
            "Epoch: 178 | Iteration: 70 | Classification loss: 0.00001 | Regression loss: 0.01442 | Running loss: 0.01148\n",
            "Epoch: 178 | Iteration: 71 | Classification loss: 0.00001 | Regression loss: 0.00675 | Running loss: 0.01147\n",
            "Epoch: 178 | Iteration: 72 | Classification loss: 0.00001 | Regression loss: 0.00365 | Running loss: 0.01145\n",
            "Epoch: 178 | Iteration: 73 | Classification loss: 0.00005 | Regression loss: 0.01636 | Running loss: 0.01146\n",
            "Epoch: 178 | Iteration: 74 | Classification loss: 0.00004 | Regression loss: 0.01766 | Running loss: 0.01146\n",
            "Epoch: 178 | Iteration: 75 | Classification loss: 0.00010 | Regression loss: 0.04361 | Running loss: 0.01153\n",
            "Epoch: 178 | Iteration: 76 | Classification loss: 0.00011 | Regression loss: 0.01464 | Running loss: 0.01153\n",
            "Epoch: 178 | Iteration: 77 | Classification loss: 0.00002 | Regression loss: 0.01821 | Running loss: 0.01155\n",
            "Epoch: 178 | Iteration: 78 | Classification loss: 0.00001 | Regression loss: 0.00815 | Running loss: 0.01154\n",
            "Epoch: 178 | Iteration: 79 | Classification loss: 0.00003 | Regression loss: 0.00942 | Running loss: 0.01154\n",
            "Epoch: 178 | Iteration: 80 | Classification loss: 0.00001 | Regression loss: 0.00738 | Running loss: 0.01152\n",
            "Epoch: 178 | Iteration: 81 | Classification loss: 0.00004 | Regression loss: 0.01023 | Running loss: 0.01150\n",
            "Epoch: 178 | Iteration: 82 | Classification loss: 0.00002 | Regression loss: 0.01553 | Running loss: 0.01144\n",
            "Epoch: 178 | Iteration: 83 | Classification loss: 0.00001 | Regression loss: 0.00687 | Running loss: 0.01141\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.7291842105419786\n",
            "Precision:  0.532133676092545\n",
            "Recall:  0.8181818181818182\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}, {0: (0.7314384456550078, 253.0)}, {0: (0.7334466242447818, 253.0)}, {0: (0.7357024222917343, 253.0)}, {0: (0.7381550871546381, 253.0)}, {0: (0.739092434551302, 253.0)}, {0: (0.72091448182607, 253.0)}, {0: (0.7389512050165785, 253.0)}, {0: (0.73050932626622, 253.0)}, {0: (0.7263226120133723, 253.0)}, {0: (0.7283463725823673, 253.0)}, {0: (0.7254913772539314, 253.0)}, {0: (0.7281179290861395, 253.0)}, {0: (0.7260020498565122, 253.0)}, {0: (0.7217236108381175, 253.0)}, {0: (0.7267514829312511, 253.0)}, {0: (0.7227483652714889, 253.0)}, {0: (0.7241298168882813, 253.0)}, {0: (0.7261051666397325, 253.0)}, {0: (0.7277246704086413, 253.0)}, {0: (0.7259496886680885, 253.0)}, {0: (0.7290251963867072, 253.0)}, {0: (0.7242894157206492, 253.0)}, {0: (0.7329303843522665, 253.0)}, {0: (0.733154886135177, 253.0)}, {0: (0.7291842105419786, 253.0)}]\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 188.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 179 | Iteration: 1 | Classification loss: 0.00001 | Regression loss: 0.00414 | Running loss: 0.01138\n",
            "Epoch: 179 | Iteration: 2 | Classification loss: 0.00005 | Regression loss: 0.00517 | Running loss: 0.01138\n",
            "Epoch: 179 | Iteration: 3 | Classification loss: 0.00001 | Regression loss: 0.00652 | Running loss: 0.01137\n",
            "Epoch: 179 | Iteration: 4 | Classification loss: 0.00002 | Regression loss: 0.00638 | Running loss: 0.01135\n",
            "Epoch: 179 | Iteration: 5 | Classification loss: 0.00005 | Regression loss: 0.01446 | Running loss: 0.01137\n",
            "Epoch: 179 | Iteration: 6 | Classification loss: 0.00002 | Regression loss: 0.01490 | Running loss: 0.01137\n",
            "Epoch: 179 | Iteration: 7 | Classification loss: 0.00002 | Regression loss: 0.01171 | Running loss: 0.01136\n",
            "Epoch: 179 | Iteration: 8 | Classification loss: 0.00002 | Regression loss: 0.01207 | Running loss: 0.01135\n",
            "Epoch: 179 | Iteration: 9 | Classification loss: 0.00002 | Regression loss: 0.01196 | Running loss: 0.01135\n",
            "Epoch: 179 | Iteration: 10 | Classification loss: 0.00000 | Regression loss: 0.00820 | Running loss: 0.01136\n",
            "Epoch: 179 | Iteration: 11 | Classification loss: 0.00001 | Regression loss: 0.01013 | Running loss: 0.01135\n",
            "Epoch: 179 | Iteration: 12 | Classification loss: 0.00001 | Regression loss: 0.00393 | Running loss: 0.01133\n",
            "Epoch: 179 | Iteration: 13 | Classification loss: 0.00001 | Regression loss: 0.00723 | Running loss: 0.01134\n",
            "Epoch: 179 | Iteration: 14 | Classification loss: 0.00003 | Regression loss: 0.01493 | Running loss: 0.01134\n",
            "Epoch: 179 | Iteration: 15 | Classification loss: 0.00001 | Regression loss: 0.00977 | Running loss: 0.01130\n",
            "Epoch: 179 | Iteration: 16 | Classification loss: 0.00002 | Regression loss: 0.01215 | Running loss: 0.01129\n",
            "Epoch: 179 | Iteration: 17 | Classification loss: 0.00002 | Regression loss: 0.00281 | Running loss: 0.01127\n",
            "Epoch: 179 | Iteration: 18 | Classification loss: 0.00003 | Regression loss: 0.00594 | Running loss: 0.01127\n",
            "Epoch: 179 | Iteration: 19 | Classification loss: 0.00006 | Regression loss: 0.01024 | Running loss: 0.01127\n",
            "Epoch: 179 | Iteration: 20 | Classification loss: 0.00001 | Regression loss: 0.00715 | Running loss: 0.01120\n",
            "Epoch: 179 | Iteration: 21 | Classification loss: 0.00001 | Regression loss: 0.00946 | Running loss: 0.01120\n",
            "Epoch: 179 | Iteration: 22 | Classification loss: 0.00002 | Regression loss: 0.01116 | Running loss: 0.01120\n",
            "Epoch: 179 | Iteration: 23 | Classification loss: 0.00005 | Regression loss: 0.01051 | Running loss: 0.01121\n",
            "Epoch: 179 | Iteration: 24 | Classification loss: 0.00002 | Regression loss: 0.02390 | Running loss: 0.01124\n",
            "Epoch: 179 | Iteration: 25 | Classification loss: 0.00003 | Regression loss: 0.00667 | Running loss: 0.01124\n",
            "Epoch: 179 | Iteration: 26 | Classification loss: 0.00004 | Regression loss: 0.02036 | Running loss: 0.01126\n",
            "Epoch: 179 | Iteration: 27 | Classification loss: 0.00010 | Regression loss: 0.01401 | Running loss: 0.01128\n",
            "Epoch: 179 | Iteration: 28 | Classification loss: 0.00002 | Regression loss: 0.00984 | Running loss: 0.01129\n",
            "Epoch: 179 | Iteration: 29 | Classification loss: 0.00002 | Regression loss: 0.01339 | Running loss: 0.01131\n",
            "Epoch: 179 | Iteration: 30 | Classification loss: 0.00006 | Regression loss: 0.01371 | Running loss: 0.01131\n",
            "Epoch: 179 | Iteration: 31 | Classification loss: 0.00003 | Regression loss: 0.01315 | Running loss: 0.01129\n",
            "Epoch: 179 | Iteration: 32 | Classification loss: 0.00008 | Regression loss: 0.01898 | Running loss: 0.01131\n",
            "Epoch: 179 | Iteration: 33 | Classification loss: 0.00001 | Regression loss: 0.02943 | Running loss: 0.01136\n",
            "Epoch: 179 | Iteration: 34 | Classification loss: 0.00001 | Regression loss: 0.02797 | Running loss: 0.01140\n",
            "Epoch: 179 | Iteration: 35 | Classification loss: 0.00003 | Regression loss: 0.02580 | Running loss: 0.01145\n",
            "Epoch: 179 | Iteration: 36 | Classification loss: 0.00002 | Regression loss: 0.01114 | Running loss: 0.01146\n",
            "Epoch: 179 | Iteration: 37 | Classification loss: 0.00004 | Regression loss: 0.01555 | Running loss: 0.01148\n",
            "Epoch: 179 | Iteration: 38 | Classification loss: 0.00003 | Regression loss: 0.01439 | Running loss: 0.01150\n",
            "Epoch: 179 | Iteration: 39 | Classification loss: 0.00005 | Regression loss: 0.01318 | Running loss: 0.01152\n",
            "Epoch: 179 | Iteration: 40 | Classification loss: 0.00001 | Regression loss: 0.00595 | Running loss: 0.01151\n",
            "Epoch: 179 | Iteration: 41 | Classification loss: 0.00002 | Regression loss: 0.00415 | Running loss: 0.01150\n",
            "Epoch: 179 | Iteration: 42 | Classification loss: 0.00001 | Regression loss: 0.01758 | Running loss: 0.01151\n",
            "Epoch: 179 | Iteration: 43 | Classification loss: 0.00001 | Regression loss: 0.00478 | Running loss: 0.01151\n",
            "Epoch: 179 | Iteration: 44 | Classification loss: 0.00001 | Regression loss: 0.01228 | Running loss: 0.01153\n",
            "Epoch: 179 | Iteration: 45 | Classification loss: 0.00004 | Regression loss: 0.01666 | Running loss: 0.01156\n",
            "Epoch: 179 | Iteration: 46 | Classification loss: 0.00003 | Regression loss: 0.01043 | Running loss: 0.01155\n",
            "Epoch: 179 | Iteration: 47 | Classification loss: 0.00001 | Regression loss: 0.00394 | Running loss: 0.01155\n",
            "Epoch: 179 | Iteration: 48 | Classification loss: 0.00002 | Regression loss: 0.01085 | Running loss: 0.01154\n",
            "Epoch: 179 | Iteration: 49 | Classification loss: 0.00004 | Regression loss: 0.01524 | Running loss: 0.01154\n",
            "Epoch: 179 | Iteration: 50 | Classification loss: 0.00001 | Regression loss: 0.00314 | Running loss: 0.01153\n",
            "Epoch: 179 | Iteration: 51 | Classification loss: 0.00001 | Regression loss: 0.00448 | Running loss: 0.01148\n",
            "Epoch: 179 | Iteration: 52 | Classification loss: 0.00008 | Regression loss: 0.04229 | Running loss: 0.01153\n",
            "Epoch: 179 | Iteration: 53 | Classification loss: 0.00003 | Regression loss: 0.01178 | Running loss: 0.01152\n",
            "Epoch: 179 | Iteration: 54 | Classification loss: 0.00002 | Regression loss: 0.01181 | Running loss: 0.01151\n",
            "Epoch: 179 | Iteration: 55 | Classification loss: 0.00001 | Regression loss: 0.01323 | Running loss: 0.01152\n",
            "Epoch: 179 | Iteration: 56 | Classification loss: 0.00004 | Regression loss: 0.01399 | Running loss: 0.01152\n",
            "Epoch: 179 | Iteration: 57 | Classification loss: 0.00003 | Regression loss: 0.01554 | Running loss: 0.01153\n",
            "Epoch: 179 | Iteration: 58 | Classification loss: 0.00002 | Regression loss: 0.01228 | Running loss: 0.01154\n",
            "Epoch: 179 | Iteration: 59 | Classification loss: 0.00001 | Regression loss: 0.00371 | Running loss: 0.01152\n",
            "Epoch: 179 | Iteration: 60 | Classification loss: 0.00002 | Regression loss: 0.00532 | Running loss: 0.01151\n",
            "Epoch: 179 | Iteration: 61 | Classification loss: 0.00002 | Regression loss: 0.00662 | Running loss: 0.01149\n",
            "Epoch: 179 | Iteration: 62 | Classification loss: 0.00002 | Regression loss: 0.00649 | Running loss: 0.01148\n",
            "Epoch: 179 | Iteration: 63 | Classification loss: 0.00001 | Regression loss: 0.00410 | Running loss: 0.01146\n",
            "Epoch: 179 | Iteration: 64 | Classification loss: 0.00001 | Regression loss: 0.00916 | Running loss: 0.01146\n",
            "Epoch: 179 | Iteration: 65 | Classification loss: 0.00002 | Regression loss: 0.01190 | Running loss: 0.01146\n",
            "Epoch: 179 | Iteration: 66 | Classification loss: 0.00001 | Regression loss: 0.00392 | Running loss: 0.01145\n",
            "Epoch: 179 | Iteration: 67 | Classification loss: 0.00001 | Regression loss: 0.01079 | Running loss: 0.01143\n",
            "Epoch: 179 | Iteration: 68 | Classification loss: 0.00003 | Regression loss: 0.01036 | Running loss: 0.01143\n",
            "Epoch: 179 | Iteration: 69 | Classification loss: 0.00001 | Regression loss: 0.00691 | Running loss: 0.01141\n",
            "Epoch: 179 | Iteration: 70 | Classification loss: 0.00001 | Regression loss: 0.00824 | Running loss: 0.01141\n",
            "Epoch: 179 | Iteration: 71 | Classification loss: 0.00001 | Regression loss: 0.00659 | Running loss: 0.01141\n",
            "Epoch: 179 | Iteration: 72 | Classification loss: 0.00003 | Regression loss: 0.01202 | Running loss: 0.01142\n",
            "Epoch: 179 | Iteration: 73 | Classification loss: 0.00001 | Regression loss: 0.00338 | Running loss: 0.01142\n",
            "Epoch: 179 | Iteration: 74 | Classification loss: 0.00002 | Regression loss: 0.01467 | Running loss: 0.01140\n",
            "Epoch: 179 | Iteration: 75 | Classification loss: 0.00003 | Regression loss: 0.00892 | Running loss: 0.01139\n",
            "Epoch: 179 | Iteration: 76 | Classification loss: 0.00004 | Regression loss: 0.01659 | Running loss: 0.01140\n",
            "Epoch: 179 | Iteration: 77 | Classification loss: 0.00002 | Regression loss: 0.01141 | Running loss: 0.01139\n",
            "Epoch: 179 | Iteration: 78 | Classification loss: 0.00001 | Regression loss: 0.00480 | Running loss: 0.01138\n",
            "Epoch: 179 | Iteration: 79 | Classification loss: 0.00002 | Regression loss: 0.00971 | Running loss: 0.01135\n",
            "Epoch: 179 | Iteration: 80 | Classification loss: 0.00003 | Regression loss: 0.00862 | Running loss: 0.01135\n",
            "Epoch: 179 | Iteration: 81 | Classification loss: 0.00013 | Regression loss: 0.00680 | Running loss: 0.01135\n",
            "Epoch: 179 | Iteration: 82 | Classification loss: 0.00001 | Regression loss: 0.01741 | Running loss: 0.01135\n",
            "Epoch: 179 | Iteration: 83 | Classification loss: 0.00001 | Regression loss: 0.01560 | Running loss: 0.01136\n",
            "Evaluating dataset\n",
            "\n",
            "mAP:\n",
            "person: 0.7251405627702188\n",
            "Precision:  0.5447368421052632\n",
            "Recall:  0.8181818181818182\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}, {0: (0.7314384456550078, 253.0)}, {0: (0.7334466242447818, 253.0)}, {0: (0.7357024222917343, 253.0)}, {0: (0.7381550871546381, 253.0)}, {0: (0.739092434551302, 253.0)}, {0: (0.72091448182607, 253.0)}, {0: (0.7389512050165785, 253.0)}, {0: (0.73050932626622, 253.0)}, {0: (0.7263226120133723, 253.0)}, {0: (0.7283463725823673, 253.0)}, {0: (0.7254913772539314, 253.0)}, {0: (0.7281179290861395, 253.0)}, {0: (0.7260020498565122, 253.0)}, {0: (0.7217236108381175, 253.0)}, {0: (0.7267514829312511, 253.0)}, {0: (0.7227483652714889, 253.0)}, {0: (0.7241298168882813, 253.0)}, {0: (0.7261051666397325, 253.0)}, {0: (0.7277246704086413, 253.0)}, {0: (0.7259496886680885, 253.0)}, {0: (0.7290251963867072, 253.0)}, {0: (0.7242894157206492, 253.0)}, {0: (0.7329303843522665, 253.0)}, {0: (0.733154886135177, 253.0)}, {0: (0.7291842105419786, 253.0)}, {0: (0.7251405627702188, 253.0)}]\n",
            "Epoch: 180 | Iteration: 0 | Classification loss: 0.00001 | Regression loss: 0.02819 | Running loss: 0.01140\n",
            "Epoch: 180 | Iteration: 1 | Classification loss: 0.00000 | Regression loss: 0.00330 | Running loss: 0.01138\n",
            "Epoch: 180 | Iteration: 2 | Classification loss: 0.00002 | Regression loss: 0.01005 | Running loss: 0.01139\n",
            "Epoch: 180 | Iteration: 3 | Classification loss: 0.00001 | Regression loss: 0.01079 | Running loss: 0.01139\n",
            "Epoch: 180 | Iteration: 4 | Classification loss: 0.00002 | Regression loss: 0.00569 | Running loss: 0.01131\n",
            "Epoch: 180 | Iteration: 5 | Classification loss: 0.00007 | Regression loss: 0.04186 | Running loss: 0.01137\n",
            "Epoch: 180 | Iteration: 6 | Classification loss: 0.00005 | Regression loss: 0.01560 | Running loss: 0.01138\n",
            "Epoch: 180 | Iteration: 7 | Classification loss: 0.00001 | Regression loss: 0.01091 | Running loss: 0.01138\n",
            "Epoch: 180 | Iteration: 8 | Classification loss: 0.00002 | Regression loss: 0.00987 | Running loss: 0.01138\n",
            "Epoch: 180 | Iteration: 9 | Classification loss: 0.00003 | Regression loss: 0.01570 | Running loss: 0.01139\n",
            "Epoch: 180 | Iteration: 10 | Classification loss: 0.00002 | Regression loss: 0.01181 | Running loss: 0.01139\n",
            "Epoch: 180 | Iteration: 11 | Classification loss: 0.00002 | Regression loss: 0.01299 | Running loss: 0.01137\n",
            "Epoch: 180 | Iteration: 12 | Classification loss: 0.00003 | Regression loss: 0.01018 | Running loss: 0.01137\n",
            "Epoch: 180 | Iteration: 13 | Classification loss: 0.00004 | Regression loss: 0.02019 | Running loss: 0.01139\n",
            "Epoch: 180 | Iteration: 14 | Classification loss: 0.00001 | Regression loss: 0.00393 | Running loss: 0.01138\n",
            "Epoch: 180 | Iteration: 15 | Classification loss: 0.00001 | Regression loss: 0.00670 | Running loss: 0.01135\n",
            "Epoch: 180 | Iteration: 16 | Classification loss: 0.00003 | Regression loss: 0.02599 | Running loss: 0.01138\n",
            "Epoch: 180 | Iteration: 17 | Classification loss: 0.00003 | Regression loss: 0.01243 | Running loss: 0.01139\n",
            "Epoch: 180 | Iteration: 18 | Classification loss: 0.00002 | Regression loss: 0.01041 | Running loss: 0.01139\n",
            "Epoch: 180 | Iteration: 19 | Classification loss: 0.00003 | Regression loss: 0.00904 | Running loss: 0.01140\n",
            "Epoch: 180 | Iteration: 20 | Classification loss: 0.00001 | Regression loss: 0.00964 | Running loss: 0.01140\n",
            "Epoch: 180 | Iteration: 21 | Classification loss: 0.00002 | Regression loss: 0.00271 | Running loss: 0.01138\n",
            "Epoch: 180 | Iteration: 22 | Classification loss: 0.00003 | Regression loss: 0.00949 | Running loss: 0.01138\n",
            "Epoch: 180 | Iteration: 23 | Classification loss: 0.00002 | Regression loss: 0.00591 | Running loss: 0.01138\n",
            "Epoch: 180 | Iteration: 24 | Classification loss: 0.00003 | Regression loss: 0.01302 | Running loss: 0.01139\n",
            "Epoch: 180 | Iteration: 25 | Classification loss: 0.00002 | Regression loss: 0.01365 | Running loss: 0.01139\n",
            "Epoch: 180 | Iteration: 26 | Classification loss: 0.00001 | Regression loss: 0.01159 | Running loss: 0.01139\n",
            "Epoch: 180 | Iteration: 27 | Classification loss: 0.00001 | Regression loss: 0.01047 | Running loss: 0.01140\n",
            "Epoch: 180 | Iteration: 28 | Classification loss: 0.00002 | Regression loss: 0.01024 | Running loss: 0.01141\n",
            "Epoch: 180 | Iteration: 29 | Classification loss: 0.00004 | Regression loss: 0.01583 | Running loss: 0.01141\n",
            "Epoch: 180 | Iteration: 30 | Classification loss: 0.00002 | Regression loss: 0.01155 | Running loss: 0.01143\n",
            "Epoch: 180 | Iteration: 31 | Classification loss: 0.00002 | Regression loss: 0.01333 | Running loss: 0.01141\n",
            "Epoch: 180 | Iteration: 32 | Classification loss: 0.00002 | Regression loss: 0.01711 | Running loss: 0.01143\n",
            "Epoch: 180 | Iteration: 33 | Classification loss: 0.00001 | Regression loss: 0.00895 | Running loss: 0.01144\n",
            "Epoch: 180 | Iteration: 34 | Classification loss: 0.00001 | Regression loss: 0.00383 | Running loss: 0.01143\n",
            "Epoch: 180 | Iteration: 35 | Classification loss: 0.00003 | Regression loss: 0.01508 | Running loss: 0.01145\n",
            "Epoch: 180 | Iteration: 36 | Classification loss: 0.00002 | Regression loss: 0.01117 | Running loss: 0.01145\n",
            "Epoch: 180 | Iteration: 37 | Classification loss: 0.00001 | Regression loss: 0.01188 | Running loss: 0.01144\n",
            "Epoch: 180 | Iteration: 38 | Classification loss: 0.00001 | Regression loss: 0.00430 | Running loss: 0.01142\n",
            "Epoch: 180 | Iteration: 39 | Classification loss: 0.00003 | Regression loss: 0.02521 | Running loss: 0.01146\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 190.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 180 | Iteration: 41 | Classification loss: 0.00005 | Regression loss: 0.01559 | Running loss: 0.01147\n",
            "Epoch: 180 | Iteration: 42 | Classification loss: 0.00001 | Regression loss: 0.00398 | Running loss: 0.01144\n",
            "Epoch: 180 | Iteration: 43 | Classification loss: 0.00001 | Regression loss: 0.01492 | Running loss: 0.01145\n",
            "Epoch: 180 | Iteration: 44 | Classification loss: 0.00004 | Regression loss: 0.00943 | Running loss: 0.01144\n",
            "Epoch: 180 | Iteration: 45 | Classification loss: 0.00002 | Regression loss: 0.01278 | Running loss: 0.01144\n",
            "Epoch: 180 | Iteration: 46 | Classification loss: 0.00002 | Regression loss: 0.00609 | Running loss: 0.01142\n",
            "Epoch: 180 | Iteration: 47 | Classification loss: 0.00001 | Regression loss: 0.00929 | Running loss: 0.01143\n",
            "Epoch: 180 | Iteration: 48 | Classification loss: 0.00001 | Regression loss: 0.01094 | Running loss: 0.01142\n",
            "Epoch: 180 | Iteration: 49 | Classification loss: 0.00001 | Regression loss: 0.02716 | Running loss: 0.01145\n",
            "Epoch: 180 | Iteration: 50 | Classification loss: 0.00005 | Regression loss: 0.01310 | Running loss: 0.01147\n",
            "Epoch: 180 | Iteration: 51 | Classification loss: 0.00002 | Regression loss: 0.00580 | Running loss: 0.01145\n",
            "Epoch: 180 | Iteration: 52 | Classification loss: 0.00002 | Regression loss: 0.01226 | Running loss: 0.01146\n",
            "Epoch: 180 | Iteration: 53 | Classification loss: 0.00003 | Regression loss: 0.00923 | Running loss: 0.01147\n",
            "Epoch: 180 | Iteration: 54 | Classification loss: 0.00002 | Regression loss: 0.01779 | Running loss: 0.01148\n",
            "Epoch: 180 | Iteration: 55 | Classification loss: 0.00001 | Regression loss: 0.00722 | Running loss: 0.01148\n",
            "Epoch: 180 | Iteration: 56 | Classification loss: 0.00005 | Regression loss: 0.01247 | Running loss: 0.01147\n",
            "Epoch: 180 | Iteration: 57 | Classification loss: 0.00001 | Regression loss: 0.00679 | Running loss: 0.01147\n",
            "Epoch: 180 | Iteration: 58 | Classification loss: 0.00001 | Regression loss: 0.01307 | Running loss: 0.01146\n",
            "Epoch: 180 | Iteration: 59 | Classification loss: 0.00001 | Regression loss: 0.00810 | Running loss: 0.01145\n",
            "Epoch: 180 | Iteration: 60 | Classification loss: 0.00001 | Regression loss: 0.00352 | Running loss: 0.01143\n",
            "Epoch: 180 | Iteration: 61 | Classification loss: 0.00005 | Regression loss: 0.00417 | Running loss: 0.01142\n",
            "Epoch: 180 | Iteration: 62 | Classification loss: 0.00002 | Regression loss: 0.01700 | Running loss: 0.01142\n",
            "Epoch: 180 | Iteration: 63 | Classification loss: 0.00001 | Regression loss: 0.00409 | Running loss: 0.01142\n",
            "Epoch: 180 | Iteration: 64 | Classification loss: 0.00001 | Regression loss: 0.00316 | Running loss: 0.01141\n",
            "Epoch: 180 | Iteration: 65 | Classification loss: 0.00009 | Regression loss: 0.01685 | Running loss: 0.01138\n",
            "Epoch: 180 | Iteration: 66 | Classification loss: 0.00001 | Regression loss: 0.00637 | Running loss: 0.01137\n",
            "Epoch: 180 | Iteration: 67 | Classification loss: 0.00002 | Regression loss: 0.01413 | Running loss: 0.01137\n",
            "Epoch: 180 | Iteration: 68 | Classification loss: 0.00008 | Regression loss: 0.01363 | Running loss: 0.01139\n",
            "Epoch: 180 | Iteration: 69 | Classification loss: 0.00001 | Regression loss: 0.00645 | Running loss: 0.01140\n",
            "Epoch: 180 | Iteration: 70 | Classification loss: 0.00002 | Regression loss: 0.00434 | Running loss: 0.01138\n",
            "Epoch: 180 | Iteration: 71 | Classification loss: 0.00002 | Regression loss: 0.01187 | Running loss: 0.01136\n",
            "Epoch: 180 | Iteration: 72 | Classification loss: 0.00001 | Regression loss: 0.01056 | Running loss: 0.01137\n",
            "Epoch: 180 | Iteration: 73 | Classification loss: 0.00002 | Regression loss: 0.01512 | Running loss: 0.01136\n",
            "Epoch: 180 | Iteration: 74 | Classification loss: 0.00001 | Regression loss: 0.00511 | Running loss: 0.01137\n",
            "Epoch: 180 | Iteration: 75 | Classification loss: 0.00001 | Regression loss: 0.00606 | Running loss: 0.01135\n",
            "Epoch: 180 | Iteration: 76 | Classification loss: 0.00002 | Regression loss: 0.00743 | Running loss: 0.01136\n",
            "Epoch: 180 | Iteration: 77 | Classification loss: 0.00006 | Regression loss: 0.00997 | Running loss: 0.01137\n",
            "Epoch: 180 | Iteration: 78 | Classification loss: 0.00001 | Regression loss: 0.00457 | Running loss: 0.01137\n",
            "Epoch: 180 | Iteration: 79 | Classification loss: 0.00011 | Regression loss: 0.00657 | Running loss: 0.01135\n",
            "Epoch: 180 | Iteration: 80 | Classification loss: 0.00002 | Regression loss: 0.01131 | Running loss: 0.01135\n",
            "Epoch: 180 | Iteration: 81 | Classification loss: 0.00001 | Regression loss: 0.00831 | Running loss: 0.01134\n",
            "Epoch: 180 | Iteration: 82 | Classification loss: 0.00001 | Regression loss: 0.00368 | Running loss: 0.01133\n",
            "Epoch: 180 | Iteration: 83 | Classification loss: 0.00001 | Regression loss: 0.00404 | Running loss: 0.01131\n",
            "Evaluating dataset\n",
            "\n",
            "mAP:\n",
            "person: 0.731154087710585\n",
            "Precision:  0.5433070866141733\n",
            "Recall:  0.8181818181818182\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}, {0: (0.7314384456550078, 253.0)}, {0: (0.7334466242447818, 253.0)}, {0: (0.7357024222917343, 253.0)}, {0: (0.7381550871546381, 253.0)}, {0: (0.739092434551302, 253.0)}, {0: (0.72091448182607, 253.0)}, {0: (0.7389512050165785, 253.0)}, {0: (0.73050932626622, 253.0)}, {0: (0.7263226120133723, 253.0)}, {0: (0.7283463725823673, 253.0)}, {0: (0.7254913772539314, 253.0)}, {0: (0.7281179290861395, 253.0)}, {0: (0.7260020498565122, 253.0)}, {0: (0.7217236108381175, 253.0)}, {0: (0.7267514829312511, 253.0)}, {0: (0.7227483652714889, 253.0)}, {0: (0.7241298168882813, 253.0)}, {0: (0.7261051666397325, 253.0)}, {0: (0.7277246704086413, 253.0)}, {0: (0.7259496886680885, 253.0)}, {0: (0.7290251963867072, 253.0)}, {0: (0.7242894157206492, 253.0)}, {0: (0.7329303843522665, 253.0)}, {0: (0.733154886135177, 253.0)}, {0: (0.7291842105419786, 253.0)}, {0: (0.7251405627702188, 253.0)}, {0: (0.731154087710585, 253.0)}]\n",
            "Epoch: 181 | Iteration: 0 | Classification loss: 0.00001 | Regression loss: 0.02808 | Running loss: 0.01132\n",
            "Epoch: 181 | Iteration: 1 | Classification loss: 0.00012 | Regression loss: 0.00718 | Running loss: 0.01132\n",
            "Epoch: 181 | Iteration: 2 | Classification loss: 0.00001 | Regression loss: 0.00622 | Running loss: 0.01132\n",
            "Epoch: 181 | Iteration: 3 | Classification loss: 0.00001 | Regression loss: 0.00368 | Running loss: 0.01131\n",
            "Epoch: 181 | Iteration: 4 | Classification loss: 0.00001 | Regression loss: 0.01462 | Running loss: 0.01133\n",
            "Epoch: 181 | Iteration: 5 | Classification loss: 0.00001 | Regression loss: 0.01028 | Running loss: 0.01126\n",
            "Epoch: 181 | Iteration: 6 | Classification loss: 0.00003 | Regression loss: 0.00875 | Running loss: 0.01122\n",
            "Epoch: 181 | Iteration: 7 | Classification loss: 0.00001 | Regression loss: 0.01087 | Running loss: 0.01122\n",
            "Epoch: 181 | Iteration: 8 | Classification loss: 0.00003 | Regression loss: 0.01509 | Running loss: 0.01124\n",
            "Epoch: 181 | Iteration: 9 | Classification loss: 0.00001 | Regression loss: 0.00672 | Running loss: 0.01123\n",
            "Epoch: 181 | Iteration: 10 | Classification loss: 0.00002 | Regression loss: 0.01057 | Running loss: 0.01121\n",
            "Epoch: 181 | Iteration: 11 | Classification loss: 0.00003 | Regression loss: 0.01450 | Running loss: 0.01122\n",
            "Epoch: 181 | Iteration: 12 | Classification loss: 0.00003 | Regression loss: 0.01310 | Running loss: 0.01122\n",
            "Epoch: 181 | Iteration: 13 | Classification loss: 0.00001 | Regression loss: 0.01106 | Running loss: 0.01121\n",
            "Epoch: 181 | Iteration: 14 | Classification loss: 0.00001 | Regression loss: 0.00687 | Running loss: 0.01121\n",
            "Epoch: 181 | Iteration: 15 | Classification loss: 0.00003 | Regression loss: 0.01591 | Running loss: 0.01122\n",
            "Epoch: 181 | Iteration: 16 | Classification loss: 0.00003 | Regression loss: 0.01137 | Running loss: 0.01122\n",
            "Epoch: 181 | Iteration: 17 | Classification loss: 0.00002 | Regression loss: 0.01394 | Running loss: 0.01123\n",
            "Epoch: 181 | Iteration: 18 | Classification loss: 0.00002 | Regression loss: 0.01238 | Running loss: 0.01125\n",
            "Epoch: 181 | Iteration: 19 | Classification loss: 0.00004 | Regression loss: 0.01632 | Running loss: 0.01126\n",
            "Epoch: 181 | Iteration: 20 | Classification loss: 0.00001 | Regression loss: 0.01105 | Running loss: 0.01126\n",
            "Epoch: 181 | Iteration: 21 | Classification loss: 0.00004 | Regression loss: 0.01360 | Running loss: 0.01128\n",
            "Epoch: 181 | Iteration: 22 | Classification loss: 0.00001 | Regression loss: 0.01375 | Running loss: 0.01128\n",
            "Epoch: 181 | Iteration: 23 | Classification loss: 0.00002 | Regression loss: 0.00639 | Running loss: 0.01129\n",
            "Epoch: 181 | Iteration: 24 | Classification loss: 0.00002 | Regression loss: 0.01650 | Running loss: 0.01131\n",
            "Epoch: 181 | Iteration: 25 | Classification loss: 0.00001 | Regression loss: 0.00528 | Running loss: 0.01130\n",
            "Epoch: 181 | Iteration: 26 | Classification loss: 0.00004 | Regression loss: 0.02088 | Running loss: 0.01132\n",
            "Epoch: 181 | Iteration: 27 | Classification loss: 0.00003 | Regression loss: 0.01216 | Running loss: 0.01131\n",
            "Epoch: 181 | Iteration: 28 | Classification loss: 0.00005 | Regression loss: 0.01190 | Running loss: 0.01131\n",
            "Epoch: 181 | Iteration: 29 | Classification loss: 0.00001 | Regression loss: 0.01041 | Running loss: 0.01131\n",
            "Epoch: 181 | Iteration: 30 | Classification loss: 0.00003 | Regression loss: 0.01365 | Running loss: 0.01132\n",
            "Epoch: 181 | Iteration: 31 | Classification loss: 0.00001 | Regression loss: 0.00375 | Running loss: 0.01130\n",
            "Epoch: 181 | Iteration: 32 | Classification loss: 0.00003 | Regression loss: 0.01138 | Running loss: 0.01130\n",
            "Epoch: 181 | Iteration: 33 | Classification loss: 0.00001 | Regression loss: 0.00644 | Running loss: 0.01128\n",
            "Epoch: 181 | Iteration: 34 | Classification loss: 0.00001 | Regression loss: 0.00920 | Running loss: 0.01129\n",
            "Epoch: 181 | Iteration: 35 | Classification loss: 0.00002 | Regression loss: 0.00650 | Running loss: 0.01128\n",
            "Epoch: 181 | Iteration: 36 | Classification loss: 0.00002 | Regression loss: 0.00985 | Running loss: 0.01128\n",
            "Epoch: 181 | Iteration: 37 | Classification loss: 0.00000 | Regression loss: 0.00819 | Running loss: 0.01127\n",
            "Epoch: 181 | Iteration: 38 | Classification loss: 0.00001 | Regression loss: 0.01109 | Running loss: 0.01125\n",
            "Epoch: 181 | Iteration: 39 | Classification loss: 0.00001 | Regression loss: 0.00604 | Running loss: 0.01123\n",
            "Epoch: 181 | Iteration: 40 | Classification loss: 0.00002 | Regression loss: 0.01395 | Running loss: 0.01125\n",
            "Epoch: 181 | Iteration: 41 | Classification loss: 0.00002 | Regression loss: 0.00264 | Running loss: 0.01125\n",
            "Epoch: 181 | Iteration: 42 | Classification loss: 0.00001 | Regression loss: 0.00394 | Running loss: 0.01125\n",
            "Epoch: 181 | Iteration: 43 | Classification loss: 0.00001 | Regression loss: 0.02749 | Running loss: 0.01128\n",
            "Epoch: 181 | Iteration: 44 | Classification loss: 0.00002 | Regression loss: 0.01315 | Running loss: 0.01128\n",
            "Epoch: 181 | Iteration: 45 | Classification loss: 0.00003 | Regression loss: 0.01368 | Running loss: 0.01130\n",
            "Epoch: 181 | Iteration: 46 | Classification loss: 0.00002 | Regression loss: 0.00684 | Running loss: 0.01129\n",
            "Epoch: 181 | Iteration: 47 | Classification loss: 0.00001 | Regression loss: 0.01211 | Running loss: 0.01129\n",
            "Epoch: 181 | Iteration: 48 | Classification loss: 0.00002 | Regression loss: 0.00729 | Running loss: 0.01129\n",
            "Epoch: 181 | Iteration: 49 | Classification loss: 0.00002 | Regression loss: 0.01083 | Running loss: 0.01128\n",
            "Epoch: 181 | Iteration: 50 | Classification loss: 0.00005 | Regression loss: 0.01009 | Running loss: 0.01128\n",
            "Epoch: 181 | Iteration: 51 | Classification loss: 0.00001 | Regression loss: 0.00466 | Running loss: 0.01128\n",
            "Epoch: 181 | Iteration: 52 | Classification loss: 0.00001 | Regression loss: 0.00948 | Running loss: 0.01129\n",
            "Epoch: 181 | Iteration: 53 | Classification loss: 0.00001 | Regression loss: 0.00596 | Running loss: 0.01126\n",
            "Epoch: 181 | Iteration: 54 | Classification loss: 0.00007 | Regression loss: 0.01737 | Running loss: 0.01128\n",
            "Epoch: 181 | Iteration: 55 | Classification loss: 0.00001 | Regression loss: 0.00494 | Running loss: 0.01124\n",
            "Epoch: 181 | Iteration: 56 | Classification loss: 0.00001 | Regression loss: 0.00729 | Running loss: 0.01122\n",
            "Epoch: 181 | Iteration: 57 | Classification loss: 0.00001 | Regression loss: 0.00314 | Running loss: 0.01121\n",
            "Epoch: 181 | Iteration: 58 | Classification loss: 0.00001 | Regression loss: 0.00353 | Running loss: 0.01119\n",
            "Epoch: 181 | Iteration: 59 | Classification loss: 0.00002 | Regression loss: 0.02336 | Running loss: 0.01121\n",
            "Epoch: 181 | Iteration: 60 | Classification loss: 0.00002 | Regression loss: 0.01884 | Running loss: 0.01124\n",
            "Epoch: 181 | Iteration: 61 | Classification loss: 0.00001 | Regression loss: 0.01630 | Running loss: 0.01124\n",
            "Epoch: 181 | Iteration: 62 | Classification loss: 0.00001 | Regression loss: 0.00322 | Running loss: 0.01124\n",
            "Epoch: 181 | Iteration: 63 | Classification loss: 0.00001 | Regression loss: 0.00980 | Running loss: 0.01124\n",
            "Epoch: 181 | Iteration: 64 | Classification loss: 0.00002 | Regression loss: 0.01138 | Running loss: 0.01126\n",
            "Epoch: 181 | Iteration: 65 | Classification loss: 0.00001 | Regression loss: 0.00409 | Running loss: 0.01125\n",
            "Epoch: 181 | Iteration: 66 | Classification loss: 0.00004 | Regression loss: 0.00356 | Running loss: 0.01123\n",
            "Epoch: 181 | Iteration: 67 | Classification loss: 0.00002 | Regression loss: 0.01333 | Running loss: 0.01123\n",
            "Epoch: 181 | Iteration: 68 | Classification loss: 0.00003 | Regression loss: 0.00897 | Running loss: 0.01124\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 190.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 181 | Iteration: 70 | Classification loss: 0.00005 | Regression loss: 0.01310 | Running loss: 0.01123\n",
            "Epoch: 181 | Iteration: 71 | Classification loss: 0.00002 | Regression loss: 0.01283 | Running loss: 0.01124\n",
            "Epoch: 181 | Iteration: 72 | Classification loss: 0.00002 | Regression loss: 0.01083 | Running loss: 0.01123\n",
            "Epoch: 181 | Iteration: 73 | Classification loss: 0.00009 | Regression loss: 0.01331 | Running loss: 0.01121\n",
            "Epoch: 181 | Iteration: 74 | Classification loss: 0.00002 | Regression loss: 0.00565 | Running loss: 0.01121\n",
            "Epoch: 181 | Iteration: 75 | Classification loss: 0.00003 | Regression loss: 0.01482 | Running loss: 0.01121\n",
            "Epoch: 181 | Iteration: 76 | Classification loss: 0.00002 | Regression loss: 0.00414 | Running loss: 0.01117\n",
            "Epoch: 181 | Iteration: 77 | Classification loss: 0.00001 | Regression loss: 0.01765 | Running loss: 0.01119\n",
            "Epoch: 181 | Iteration: 78 | Classification loss: 0.00004 | Regression loss: 0.00925 | Running loss: 0.01118\n",
            "Epoch: 181 | Iteration: 79 | Classification loss: 0.00009 | Regression loss: 0.04190 | Running loss: 0.01125\n",
            "Epoch: 181 | Iteration: 80 | Classification loss: 0.00001 | Regression loss: 0.00503 | Running loss: 0.01123\n",
            "Epoch: 181 | Iteration: 81 | Classification loss: 0.00002 | Regression loss: 0.01008 | Running loss: 0.01124\n",
            "Epoch: 181 | Iteration: 82 | Classification loss: 0.00006 | Regression loss: 0.01136 | Running loss: 0.01124\n",
            "Epoch: 181 | Iteration: 83 | Classification loss: 0.00001 | Regression loss: 0.00335 | Running loss: 0.01123\n",
            "Evaluating dataset\n",
            "\n",
            "mAP:\n",
            "person: 0.7317124239374072\n",
            "Precision:  0.550531914893617\n",
            "Recall:  0.8181818181818182\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}, {0: (0.7314384456550078, 253.0)}, {0: (0.7334466242447818, 253.0)}, {0: (0.7357024222917343, 253.0)}, {0: (0.7381550871546381, 253.0)}, {0: (0.739092434551302, 253.0)}, {0: (0.72091448182607, 253.0)}, {0: (0.7389512050165785, 253.0)}, {0: (0.73050932626622, 253.0)}, {0: (0.7263226120133723, 253.0)}, {0: (0.7283463725823673, 253.0)}, {0: (0.7254913772539314, 253.0)}, {0: (0.7281179290861395, 253.0)}, {0: (0.7260020498565122, 253.0)}, {0: (0.7217236108381175, 253.0)}, {0: (0.7267514829312511, 253.0)}, {0: (0.7227483652714889, 253.0)}, {0: (0.7241298168882813, 253.0)}, {0: (0.7261051666397325, 253.0)}, {0: (0.7277246704086413, 253.0)}, {0: (0.7259496886680885, 253.0)}, {0: (0.7290251963867072, 253.0)}, {0: (0.7242894157206492, 253.0)}, {0: (0.7329303843522665, 253.0)}, {0: (0.733154886135177, 253.0)}, {0: (0.7291842105419786, 253.0)}, {0: (0.7251405627702188, 253.0)}, {0: (0.731154087710585, 253.0)}, {0: (0.7317124239374072, 253.0)}]\n",
            "Epoch: 182 | Iteration: 0 | Classification loss: 0.00002 | Regression loss: 0.01186 | Running loss: 0.01121\n",
            "Epoch: 182 | Iteration: 1 | Classification loss: 0.00003 | Regression loss: 0.01468 | Running loss: 0.01122\n",
            "Epoch: 182 | Iteration: 2 | Classification loss: 0.00001 | Regression loss: 0.00348 | Running loss: 0.01119\n",
            "Epoch: 182 | Iteration: 3 | Classification loss: 0.00002 | Regression loss: 0.00700 | Running loss: 0.01119\n",
            "Epoch: 182 | Iteration: 4 | Classification loss: 0.00001 | Regression loss: 0.02718 | Running loss: 0.01119\n",
            "Epoch: 182 | Iteration: 5 | Classification loss: 0.00002 | Regression loss: 0.00261 | Running loss: 0.01117\n",
            "Epoch: 182 | Iteration: 6 | Classification loss: 0.00001 | Regression loss: 0.00443 | Running loss: 0.01115\n",
            "Epoch: 182 | Iteration: 7 | Classification loss: 0.00004 | Regression loss: 0.00933 | Running loss: 0.01116\n",
            "Epoch: 182 | Iteration: 8 | Classification loss: 0.00002 | Regression loss: 0.01253 | Running loss: 0.01117\n",
            "Epoch: 182 | Iteration: 9 | Classification loss: 0.00003 | Regression loss: 0.00442 | Running loss: 0.01116\n",
            "Epoch: 182 | Iteration: 10 | Classification loss: 0.00002 | Regression loss: 0.01152 | Running loss: 0.01116\n",
            "Epoch: 182 | Iteration: 11 | Classification loss: 0.00001 | Regression loss: 0.00358 | Running loss: 0.01113\n",
            "Epoch: 182 | Iteration: 12 | Classification loss: 0.00002 | Regression loss: 0.01147 | Running loss: 0.01115\n",
            "Epoch: 182 | Iteration: 13 | Classification loss: 0.00002 | Regression loss: 0.00619 | Running loss: 0.01113\n",
            "Epoch: 182 | Iteration: 14 | Classification loss: 0.00002 | Regression loss: 0.01302 | Running loss: 0.01114\n",
            "Epoch: 182 | Iteration: 15 | Classification loss: 0.00002 | Regression loss: 0.02670 | Running loss: 0.01113\n",
            "Epoch: 182 | Iteration: 16 | Classification loss: 0.00001 | Regression loss: 0.01050 | Running loss: 0.01112\n",
            "Epoch: 182 | Iteration: 17 | Classification loss: 0.00001 | Regression loss: 0.00889 | Running loss: 0.01114\n",
            "Epoch: 182 | Iteration: 18 | Classification loss: 0.00004 | Regression loss: 0.01986 | Running loss: 0.01115\n",
            "Epoch: 182 | Iteration: 19 | Classification loss: 0.00002 | Regression loss: 0.01205 | Running loss: 0.01115\n",
            "Epoch: 182 | Iteration: 20 | Classification loss: 0.00003 | Regression loss: 0.00907 | Running loss: 0.01115\n",
            "Epoch: 182 | Iteration: 21 | Classification loss: 0.00007 | Regression loss: 0.01657 | Running loss: 0.01117\n",
            "Epoch: 182 | Iteration: 22 | Classification loss: 0.00002 | Regression loss: 0.01128 | Running loss: 0.01116\n",
            "Epoch: 182 | Iteration: 23 | Classification loss: 0.00001 | Regression loss: 0.00395 | Running loss: 0.01114\n",
            "Epoch: 182 | Iteration: 24 | Classification loss: 0.00002 | Regression loss: 0.00666 | Running loss: 0.01112\n",
            "Epoch: 182 | Iteration: 25 | Classification loss: 0.00001 | Regression loss: 0.00619 | Running loss: 0.01112\n",
            "Epoch: 182 | Iteration: 26 | Classification loss: 0.00003 | Regression loss: 0.01048 | Running loss: 0.01112\n",
            "Epoch: 182 | Iteration: 27 | Classification loss: 0.00001 | Regression loss: 0.00481 | Running loss: 0.01111\n",
            "Epoch: 182 | Iteration: 28 | Classification loss: 0.00001 | Regression loss: 0.00637 | Running loss: 0.01108\n",
            "Epoch: 182 | Iteration: 29 | Classification loss: 0.00004 | Regression loss: 0.01345 | Running loss: 0.01110\n",
            "Epoch: 182 | Iteration: 30 | Classification loss: 0.00001 | Regression loss: 0.00520 | Running loss: 0.01103\n",
            "Epoch: 182 | Iteration: 31 | Classification loss: 0.00003 | Regression loss: 0.00916 | Running loss: 0.01103\n",
            "Epoch: 182 | Iteration: 32 | Classification loss: 0.00002 | Regression loss: 0.00743 | Running loss: 0.01102\n",
            "Epoch: 182 | Iteration: 33 | Classification loss: 0.00001 | Regression loss: 0.00704 | Running loss: 0.01103\n",
            "Epoch: 182 | Iteration: 34 | Classification loss: 0.00001 | Regression loss: 0.01372 | Running loss: 0.01103\n",
            "Epoch: 182 | Iteration: 35 | Classification loss: 0.00001 | Regression loss: 0.00316 | Running loss: 0.01101\n",
            "Epoch: 182 | Iteration: 36 | Classification loss: 0.00002 | Regression loss: 0.01468 | Running loss: 0.01103\n",
            "Epoch: 182 | Iteration: 37 | Classification loss: 0.00001 | Regression loss: 0.01339 | Running loss: 0.01104\n",
            "Epoch: 182 | Iteration: 38 | Classification loss: 0.00001 | Regression loss: 0.01172 | Running loss: 0.01105\n",
            "Epoch: 182 | Iteration: 39 | Classification loss: 0.00003 | Regression loss: 0.01753 | Running loss: 0.01107\n",
            "Epoch: 182 | Iteration: 40 | Classification loss: 0.00001 | Regression loss: 0.00636 | Running loss: 0.01105\n",
            "Epoch: 182 | Iteration: 41 | Classification loss: 0.00001 | Regression loss: 0.01020 | Running loss: 0.01103\n",
            "Epoch: 182 | Iteration: 42 | Classification loss: 0.00001 | Regression loss: 0.00311 | Running loss: 0.01101\n",
            "Epoch: 182 | Iteration: 43 | Classification loss: 0.00001 | Regression loss: 0.00455 | Running loss: 0.01099\n",
            "Epoch: 182 | Iteration: 44 | Classification loss: 0.00001 | Regression loss: 0.01472 | Running loss: 0.01100\n",
            "Epoch: 182 | Iteration: 45 | Classification loss: 0.00002 | Regression loss: 0.00597 | Running loss: 0.01100\n",
            "Epoch: 182 | Iteration: 46 | Classification loss: 0.00006 | Regression loss: 0.01355 | Running loss: 0.01100\n",
            "Epoch: 182 | Iteration: 47 | Classification loss: 0.00003 | Regression loss: 0.01827 | Running loss: 0.01103\n",
            "Epoch: 182 | Iteration: 48 | Classification loss: 0.00002 | Regression loss: 0.01107 | Running loss: 0.01103\n",
            "Epoch: 182 | Iteration: 49 | Classification loss: 0.00001 | Regression loss: 0.01232 | Running loss: 0.01100\n",
            "Epoch: 182 | Iteration: 50 | Classification loss: 0.00000 | Regression loss: 0.00846 | Running loss: 0.01101\n",
            "Epoch: 182 | Iteration: 51 | Classification loss: 0.00001 | Regression loss: 0.00706 | Running loss: 0.01100\n",
            "Epoch: 182 | Iteration: 52 | Classification loss: 0.00001 | Regression loss: 0.01167 | Running loss: 0.01099\n",
            "Epoch: 182 | Iteration: 53 | Classification loss: 0.00002 | Regression loss: 0.00525 | Running loss: 0.01098\n",
            "Epoch: 182 | Iteration: 54 | Classification loss: 0.00008 | Regression loss: 0.01350 | Running loss: 0.01098\n",
            "Epoch: 182 | Iteration: 55 | Classification loss: 0.00002 | Regression loss: 0.02117 | Running loss: 0.01100\n",
            "Epoch: 182 | Iteration: 56 | Classification loss: 0.00002 | Regression loss: 0.01065 | Running loss: 0.01102\n",
            "Epoch: 182 | Iteration: 57 | Classification loss: 0.00003 | Regression loss: 0.01284 | Running loss: 0.01103\n",
            "Epoch: 182 | Iteration: 58 | Classification loss: 0.00002 | Regression loss: 0.01769 | Running loss: 0.01105\n",
            "Epoch: 182 | Iteration: 59 | Classification loss: 0.00003 | Regression loss: 0.00572 | Running loss: 0.01105\n",
            "Epoch: 182 | Iteration: 60 | Classification loss: 0.00002 | Regression loss: 0.01442 | Running loss: 0.01106\n",
            "Epoch: 182 | Iteration: 61 | Classification loss: 0.00006 | Regression loss: 0.01183 | Running loss: 0.01105\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 188.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 182 | Iteration: 63 | Classification loss: 0.00002 | Regression loss: 0.01002 | Running loss: 0.01105\n",
            "Epoch: 182 | Iteration: 64 | Classification loss: 0.00003 | Regression loss: 0.01659 | Running loss: 0.01106\n",
            "Epoch: 182 | Iteration: 65 | Classification loss: 0.00001 | Regression loss: 0.00415 | Running loss: 0.01104\n",
            "Epoch: 182 | Iteration: 66 | Classification loss: 0.00005 | Regression loss: 0.01164 | Running loss: 0.01106\n",
            "Epoch: 182 | Iteration: 67 | Classification loss: 0.00004 | Regression loss: 0.01635 | Running loss: 0.01108\n",
            "Epoch: 182 | Iteration: 68 | Classification loss: 0.00001 | Regression loss: 0.01691 | Running loss: 0.01110\n",
            "Epoch: 182 | Iteration: 69 | Classification loss: 0.00003 | Regression loss: 0.01508 | Running loss: 0.01109\n",
            "Epoch: 182 | Iteration: 70 | Classification loss: 0.00001 | Regression loss: 0.01111 | Running loss: 0.01110\n",
            "Epoch: 182 | Iteration: 71 | Classification loss: 0.00001 | Regression loss: 0.01734 | Running loss: 0.01110\n",
            "Epoch: 182 | Iteration: 72 | Classification loss: 0.00001 | Regression loss: 0.01123 | Running loss: 0.01111\n",
            "Epoch: 182 | Iteration: 73 | Classification loss: 0.00011 | Regression loss: 0.00627 | Running loss: 0.01110\n",
            "Epoch: 182 | Iteration: 74 | Classification loss: 0.00002 | Regression loss: 0.01363 | Running loss: 0.01110\n",
            "Epoch: 182 | Iteration: 75 | Classification loss: 0.00001 | Regression loss: 0.00553 | Running loss: 0.01111\n",
            "Epoch: 182 | Iteration: 76 | Classification loss: 0.00001 | Regression loss: 0.00331 | Running loss: 0.01110\n",
            "Epoch: 182 | Iteration: 77 | Classification loss: 0.00002 | Regression loss: 0.01038 | Running loss: 0.01110\n",
            "Epoch: 182 | Iteration: 78 | Classification loss: 0.00002 | Regression loss: 0.01034 | Running loss: 0.01110\n",
            "Epoch: 182 | Iteration: 79 | Classification loss: 0.00002 | Regression loss: 0.00425 | Running loss: 0.01110\n",
            "Epoch: 182 | Iteration: 80 | Classification loss: 0.00007 | Regression loss: 0.04269 | Running loss: 0.01116\n",
            "Epoch: 182 | Iteration: 81 | Classification loss: 0.00001 | Regression loss: 0.00443 | Running loss: 0.01116\n",
            "Epoch: 182 | Iteration: 82 | Classification loss: 0.00004 | Regression loss: 0.01485 | Running loss: 0.01116\n",
            "Epoch: 182 | Iteration: 83 | Classification loss: 0.00001 | Regression loss: 0.03012 | Running loss: 0.01121\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.7263717757478444\n",
            "Precision:  0.5447368421052632\n",
            "Recall:  0.8181818181818182\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}, {0: (0.7314384456550078, 253.0)}, {0: (0.7334466242447818, 253.0)}, {0: (0.7357024222917343, 253.0)}, {0: (0.7381550871546381, 253.0)}, {0: (0.739092434551302, 253.0)}, {0: (0.72091448182607, 253.0)}, {0: (0.7389512050165785, 253.0)}, {0: (0.73050932626622, 253.0)}, {0: (0.7263226120133723, 253.0)}, {0: (0.7283463725823673, 253.0)}, {0: (0.7254913772539314, 253.0)}, {0: (0.7281179290861395, 253.0)}, {0: (0.7260020498565122, 253.0)}, {0: (0.7217236108381175, 253.0)}, {0: (0.7267514829312511, 253.0)}, {0: (0.7227483652714889, 253.0)}, {0: (0.7241298168882813, 253.0)}, {0: (0.7261051666397325, 253.0)}, {0: (0.7277246704086413, 253.0)}, {0: (0.7259496886680885, 253.0)}, {0: (0.7290251963867072, 253.0)}, {0: (0.7242894157206492, 253.0)}, {0: (0.7329303843522665, 253.0)}, {0: (0.733154886135177, 253.0)}, {0: (0.7291842105419786, 253.0)}, {0: (0.7251405627702188, 253.0)}, {0: (0.731154087710585, 253.0)}, {0: (0.7317124239374072, 253.0)}, {0: (0.7263717757478444, 253.0)}]\n",
            "Epoch: 183 | Iteration: 0 | Classification loss: 0.00002 | Regression loss: 0.01145 | Running loss: 0.01121\n",
            "Epoch: 183 | Iteration: 1 | Classification loss: 0.00001 | Regression loss: 0.00375 | Running loss: 0.01119\n",
            "Epoch: 183 | Iteration: 2 | Classification loss: 0.00006 | Regression loss: 0.00997 | Running loss: 0.01119\n",
            "Epoch: 183 | Iteration: 3 | Classification loss: 0.00002 | Regression loss: 0.00820 | Running loss: 0.01114\n",
            "Epoch: 183 | Iteration: 4 | Classification loss: 0.00002 | Regression loss: 0.00944 | Running loss: 0.01115\n",
            "Epoch: 183 | Iteration: 5 | Classification loss: 0.00001 | Regression loss: 0.00388 | Running loss: 0.01114\n",
            "Epoch: 183 | Iteration: 6 | Classification loss: 0.00001 | Regression loss: 0.00933 | Running loss: 0.01115\n",
            "Epoch: 183 | Iteration: 7 | Classification loss: 0.00001 | Regression loss: 0.00433 | Running loss: 0.01115\n",
            "Epoch: 183 | Iteration: 8 | Classification loss: 0.00000 | Regression loss: 0.00834 | Running loss: 0.01115\n",
            "Epoch: 183 | Iteration: 9 | Classification loss: 0.00002 | Regression loss: 0.01222 | Running loss: 0.01115\n",
            "Epoch: 183 | Iteration: 10 | Classification loss: 0.00001 | Regression loss: 0.01194 | Running loss: 0.01117\n",
            "Epoch: 183 | Iteration: 11 | Classification loss: 0.00000 | Regression loss: 0.00440 | Running loss: 0.01116\n",
            "Epoch: 183 | Iteration: 12 | Classification loss: 0.00002 | Regression loss: 0.00634 | Running loss: 0.01114\n",
            "Epoch: 183 | Iteration: 13 | Classification loss: 0.00001 | Regression loss: 0.00349 | Running loss: 0.01112\n",
            "Epoch: 183 | Iteration: 14 | Classification loss: 0.00005 | Regression loss: 0.01232 | Running loss: 0.01113\n",
            "Epoch: 183 | Iteration: 15 | Classification loss: 0.00002 | Regression loss: 0.01167 | Running loss: 0.01112\n",
            "Epoch: 183 | Iteration: 16 | Classification loss: 0.00004 | Regression loss: 0.00428 | Running loss: 0.01110\n",
            "Epoch: 183 | Iteration: 17 | Classification loss: 0.00010 | Regression loss: 0.00676 | Running loss: 0.01109\n",
            "Epoch: 183 | Iteration: 18 | Classification loss: 0.00001 | Regression loss: 0.00431 | Running loss: 0.01109\n",
            "Epoch: 183 | Iteration: 19 | Classification loss: 0.00002 | Regression loss: 0.00476 | Running loss: 0.01109\n",
            "Epoch: 183 | Iteration: 20 | Classification loss: 0.00003 | Regression loss: 0.00998 | Running loss: 0.01109\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 190.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 183 | Iteration: 22 | Classification loss: 0.00001 | Regression loss: 0.00393 | Running loss: 0.01107\n",
            "Epoch: 183 | Iteration: 23 | Classification loss: 0.00002 | Regression loss: 0.01481 | Running loss: 0.01107\n",
            "Epoch: 183 | Iteration: 24 | Classification loss: 0.00002 | Regression loss: 0.01076 | Running loss: 0.01108\n",
            "Epoch: 183 | Iteration: 25 | Classification loss: 0.00001 | Regression loss: 0.01090 | Running loss: 0.01109\n",
            "Epoch: 183 | Iteration: 26 | Classification loss: 0.00003 | Regression loss: 0.01594 | Running loss: 0.01109\n",
            "Epoch: 183 | Iteration: 27 | Classification loss: 0.00003 | Regression loss: 0.01235 | Running loss: 0.01109\n",
            "Epoch: 183 | Iteration: 28 | Classification loss: 0.00004 | Regression loss: 0.01597 | Running loss: 0.01109\n",
            "Epoch: 183 | Iteration: 29 | Classification loss: 0.00004 | Regression loss: 0.01495 | Running loss: 0.01110\n",
            "Epoch: 183 | Iteration: 30 | Classification loss: 0.00007 | Regression loss: 0.01259 | Running loss: 0.01110\n",
            "Epoch: 183 | Iteration: 31 | Classification loss: 0.00002 | Regression loss: 0.01520 | Running loss: 0.01111\n",
            "Epoch: 183 | Iteration: 32 | Classification loss: 0.00001 | Regression loss: 0.01062 | Running loss: 0.01111\n",
            "Epoch: 183 | Iteration: 33 | Classification loss: 0.00001 | Regression loss: 0.02115 | Running loss: 0.01112\n",
            "Epoch: 183 | Iteration: 34 | Classification loss: 0.00002 | Regression loss: 0.00422 | Running loss: 0.01109\n",
            "Epoch: 183 | Iteration: 35 | Classification loss: 0.00002 | Regression loss: 0.01268 | Running loss: 0.01110\n",
            "Epoch: 183 | Iteration: 36 | Classification loss: 0.00003 | Regression loss: 0.01950 | Running loss: 0.01114\n",
            "Epoch: 183 | Iteration: 37 | Classification loss: 0.00003 | Regression loss: 0.01562 | Running loss: 0.01113\n",
            "Epoch: 183 | Iteration: 38 | Classification loss: 0.00001 | Regression loss: 0.00579 | Running loss: 0.01111\n",
            "Epoch: 183 | Iteration: 39 | Classification loss: 0.00003 | Regression loss: 0.01208 | Running loss: 0.01110\n",
            "Epoch: 183 | Iteration: 40 | Classification loss: 0.00001 | Regression loss: 0.00299 | Running loss: 0.01110\n",
            "Epoch: 183 | Iteration: 41 | Classification loss: 0.00001 | Regression loss: 0.01011 | Running loss: 0.01110\n",
            "Epoch: 183 | Iteration: 42 | Classification loss: 0.00002 | Regression loss: 0.01598 | Running loss: 0.01111\n",
            "Epoch: 183 | Iteration: 43 | Classification loss: 0.00002 | Regression loss: 0.00280 | Running loss: 0.01110\n",
            "Epoch: 183 | Iteration: 44 | Classification loss: 0.00002 | Regression loss: 0.01013 | Running loss: 0.01110\n",
            "Epoch: 183 | Iteration: 45 | Classification loss: 0.00002 | Regression loss: 0.01483 | Running loss: 0.01111\n",
            "Epoch: 183 | Iteration: 46 | Classification loss: 0.00001 | Regression loss: 0.01567 | Running loss: 0.01113\n",
            "Epoch: 183 | Iteration: 47 | Classification loss: 0.00001 | Regression loss: 0.02743 | Running loss: 0.01114\n",
            "Epoch: 183 | Iteration: 48 | Classification loss: 0.00003 | Regression loss: 0.01671 | Running loss: 0.01115\n",
            "Epoch: 183 | Iteration: 49 | Classification loss: 0.00003 | Regression loss: 0.01649 | Running loss: 0.01117\n",
            "Epoch: 183 | Iteration: 50 | Classification loss: 0.00001 | Regression loss: 0.01082 | Running loss: 0.01116\n",
            "Epoch: 183 | Iteration: 51 | Classification loss: 0.00000 | Regression loss: 0.00335 | Running loss: 0.01115\n",
            "Epoch: 183 | Iteration: 52 | Classification loss: 0.00001 | Regression loss: 0.00597 | Running loss: 0.01115\n",
            "Epoch: 183 | Iteration: 53 | Classification loss: 0.00001 | Regression loss: 0.02797 | Running loss: 0.01118\n",
            "Epoch: 183 | Iteration: 54 | Classification loss: 0.00002 | Regression loss: 0.01377 | Running loss: 0.01119\n",
            "Epoch: 183 | Iteration: 55 | Classification loss: 0.00001 | Regression loss: 0.00685 | Running loss: 0.01112\n",
            "Epoch: 183 | Iteration: 56 | Classification loss: 0.00005 | Regression loss: 0.01375 | Running loss: 0.01112\n",
            "Epoch: 183 | Iteration: 57 | Classification loss: 0.00007 | Regression loss: 0.04113 | Running loss: 0.01118\n",
            "Epoch: 183 | Iteration: 58 | Classification loss: 0.00001 | Regression loss: 0.00736 | Running loss: 0.01117\n",
            "Epoch: 183 | Iteration: 59 | Classification loss: 0.00002 | Regression loss: 0.00615 | Running loss: 0.01116\n",
            "Epoch: 183 | Iteration: 60 | Classification loss: 0.00002 | Regression loss: 0.00415 | Running loss: 0.01111\n",
            "Epoch: 183 | Iteration: 61 | Classification loss: 0.00001 | Regression loss: 0.01101 | Running loss: 0.01111\n",
            "Epoch: 183 | Iteration: 62 | Classification loss: 0.00001 | Regression loss: 0.01734 | Running loss: 0.01113\n",
            "Epoch: 183 | Iteration: 63 | Classification loss: 0.00003 | Regression loss: 0.02197 | Running loss: 0.01116\n",
            "Epoch: 183 | Iteration: 64 | Classification loss: 0.00003 | Regression loss: 0.01214 | Running loss: 0.01116\n",
            "Epoch: 183 | Iteration: 65 | Classification loss: 0.00004 | Regression loss: 0.00931 | Running loss: 0.01117\n",
            "Epoch: 183 | Iteration: 66 | Classification loss: 0.00001 | Regression loss: 0.00598 | Running loss: 0.01117\n",
            "Epoch: 183 | Iteration: 67 | Classification loss: 0.00001 | Regression loss: 0.01303 | Running loss: 0.01118\n",
            "Epoch: 183 | Iteration: 68 | Classification loss: 0.00002 | Regression loss: 0.01710 | Running loss: 0.01119\n",
            "Epoch: 183 | Iteration: 69 | Classification loss: 0.00003 | Regression loss: 0.00867 | Running loss: 0.01117\n",
            "Epoch: 183 | Iteration: 70 | Classification loss: 0.00001 | Regression loss: 0.00970 | Running loss: 0.01116\n",
            "Epoch: 183 | Iteration: 71 | Classification loss: 0.00001 | Regression loss: 0.01195 | Running loss: 0.01118\n",
            "Epoch: 183 | Iteration: 72 | Classification loss: 0.00006 | Regression loss: 0.01618 | Running loss: 0.01120\n",
            "Epoch: 183 | Iteration: 73 | Classification loss: 0.00001 | Regression loss: 0.01131 | Running loss: 0.01121\n",
            "Epoch: 183 | Iteration: 74 | Classification loss: 0.00001 | Regression loss: 0.00979 | Running loss: 0.01122\n",
            "Epoch: 183 | Iteration: 75 | Classification loss: 0.00001 | Regression loss: 0.00540 | Running loss: 0.01120\n",
            "Epoch: 183 | Iteration: 76 | Classification loss: 0.00001 | Regression loss: 0.00722 | Running loss: 0.01120\n",
            "Epoch: 183 | Iteration: 77 | Classification loss: 0.00002 | Regression loss: 0.01157 | Running loss: 0.01121\n",
            "Epoch: 183 | Iteration: 78 | Classification loss: 0.00002 | Regression loss: 0.00924 | Running loss: 0.01120\n",
            "Epoch: 183 | Iteration: 79 | Classification loss: 0.00001 | Regression loss: 0.00566 | Running loss: 0.01119\n",
            "Epoch: 183 | Iteration: 80 | Classification loss: 0.00001 | Regression loss: 0.00639 | Running loss: 0.01117\n",
            "Epoch: 183 | Iteration: 81 | Classification loss: 0.00001 | Regression loss: 0.00350 | Running loss: 0.01117\n",
            "Epoch: 183 | Iteration: 82 | Classification loss: 0.00001 | Regression loss: 0.01725 | Running loss: 0.01118\n",
            "Epoch: 183 | Iteration: 83 | Classification loss: 0.00002 | Regression loss: 0.00568 | Running loss: 0.01116\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.7278912613628667\n",
            "Precision:  0.5461741424802111\n",
            "Recall:  0.8181818181818182\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}, {0: (0.7314384456550078, 253.0)}, {0: (0.7334466242447818, 253.0)}, {0: (0.7357024222917343, 253.0)}, {0: (0.7381550871546381, 253.0)}, {0: (0.739092434551302, 253.0)}, {0: (0.72091448182607, 253.0)}, {0: (0.7389512050165785, 253.0)}, {0: (0.73050932626622, 253.0)}, {0: (0.7263226120133723, 253.0)}, {0: (0.7283463725823673, 253.0)}, {0: (0.7254913772539314, 253.0)}, {0: (0.7281179290861395, 253.0)}, {0: (0.7260020498565122, 253.0)}, {0: (0.7217236108381175, 253.0)}, {0: (0.7267514829312511, 253.0)}, {0: (0.7227483652714889, 253.0)}, {0: (0.7241298168882813, 253.0)}, {0: (0.7261051666397325, 253.0)}, {0: (0.7277246704086413, 253.0)}, {0: (0.7259496886680885, 253.0)}, {0: (0.7290251963867072, 253.0)}, {0: (0.7242894157206492, 253.0)}, {0: (0.7329303843522665, 253.0)}, {0: (0.733154886135177, 253.0)}, {0: (0.7291842105419786, 253.0)}, {0: (0.7251405627702188, 253.0)}, {0: (0.731154087710585, 253.0)}, {0: (0.7317124239374072, 253.0)}, {0: (0.7263717757478444, 253.0)}, {0: (0.7278912613628667, 253.0)}]\n",
            "Epoch: 184 | Iteration: 0 | Classification loss: 0.00001 | Regression loss: 0.00644 | Running loss: 0.01116\n",
            "Epoch: 184 | Iteration: 1 | Classification loss: 0.00002 | Regression loss: 0.00253 | Running loss: 0.01114\n",
            "Epoch: 184 | Iteration: 2 | Classification loss: 0.00002 | Regression loss: 0.01167 | Running loss: 0.01114\n",
            "Epoch: 184 | Iteration: 3 | Classification loss: 0.00002 | Regression loss: 0.00554 | Running loss: 0.01113\n",
            "Epoch: 184 | Iteration: 4 | Classification loss: 0.00000 | Regression loss: 0.00841 | Running loss: 0.01112\n",
            "Epoch: 184 | Iteration: 5 | Classification loss: 0.00005 | Regression loss: 0.01322 | Running loss: 0.01112\n",
            "Epoch: 184 | Iteration: 6 | Classification loss: 0.00001 | Regression loss: 0.00501 | Running loss: 0.01112\n",
            "Epoch: 184 | Iteration: 7 | Classification loss: 0.00001 | Regression loss: 0.01027 | Running loss: 0.01111\n",
            "Epoch: 184 | Iteration: 8 | Classification loss: 0.00002 | Regression loss: 0.01455 | Running loss: 0.01113\n",
            "Epoch: 184 | Iteration: 9 | Classification loss: 0.00001 | Regression loss: 0.00448 | Running loss: 0.01112\n",
            "Epoch: 184 | Iteration: 10 | Classification loss: 0.00010 | Regression loss: 0.00689 | Running loss: 0.01112\n",
            "Epoch: 184 | Iteration: 11 | Classification loss: 0.00001 | Regression loss: 0.00612 | Running loss: 0.01113\n",
            "Epoch: 184 | Iteration: 12 | Classification loss: 0.00001 | Regression loss: 0.02991 | Running loss: 0.01115\n",
            "Epoch: 184 | Iteration: 13 | Classification loss: 0.00001 | Regression loss: 0.02707 | Running loss: 0.01117\n",
            "Epoch: 184 | Iteration: 14 | Classification loss: 0.00001 | Regression loss: 0.00886 | Running loss: 0.01117\n",
            "Epoch: 184 | Iteration: 15 | Classification loss: 0.00001 | Regression loss: 0.00413 | Running loss: 0.01115\n",
            "Epoch: 184 | Iteration: 16 | Classification loss: 0.00003 | Regression loss: 0.01323 | Running loss: 0.01115\n",
            "Epoch: 184 | Iteration: 17 | Classification loss: 0.00002 | Regression loss: 0.00446 | Running loss: 0.01113\n",
            "Epoch: 184 | Iteration: 18 | Classification loss: 0.00002 | Regression loss: 0.00993 | Running loss: 0.01114\n",
            "Epoch: 184 | Iteration: 19 | Classification loss: 0.00002 | Regression loss: 0.01136 | Running loss: 0.01115\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 188.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 184 | Iteration: 21 | Classification loss: 0.00002 | Regression loss: 0.02398 | Running loss: 0.01119\n",
            "Epoch: 184 | Iteration: 22 | Classification loss: 0.00001 | Regression loss: 0.00336 | Running loss: 0.01119\n",
            "Epoch: 184 | Iteration: 23 | Classification loss: 0.00001 | Regression loss: 0.01575 | Running loss: 0.01121\n",
            "Epoch: 184 | Iteration: 24 | Classification loss: 0.00001 | Regression loss: 0.00474 | Running loss: 0.01121\n",
            "Epoch: 184 | Iteration: 25 | Classification loss: 0.00004 | Regression loss: 0.01492 | Running loss: 0.01122\n",
            "Epoch: 184 | Iteration: 26 | Classification loss: 0.00002 | Regression loss: 0.00988 | Running loss: 0.01120\n",
            "Epoch: 184 | Iteration: 27 | Classification loss: 0.00001 | Regression loss: 0.00934 | Running loss: 0.01119\n",
            "Epoch: 184 | Iteration: 28 | Classification loss: 0.00002 | Regression loss: 0.01487 | Running loss: 0.01120\n",
            "Epoch: 184 | Iteration: 29 | Classification loss: 0.00001 | Regression loss: 0.00609 | Running loss: 0.01118\n",
            "Epoch: 184 | Iteration: 30 | Classification loss: 0.00002 | Regression loss: 0.01044 | Running loss: 0.01118\n",
            "Epoch: 184 | Iteration: 31 | Classification loss: 0.00002 | Regression loss: 0.01841 | Running loss: 0.01121\n",
            "Epoch: 184 | Iteration: 32 | Classification loss: 0.00002 | Regression loss: 0.00934 | Running loss: 0.01121\n",
            "Epoch: 184 | Iteration: 33 | Classification loss: 0.00001 | Regression loss: 0.00417 | Running loss: 0.01117\n",
            "Epoch: 184 | Iteration: 34 | Classification loss: 0.00003 | Regression loss: 0.00854 | Running loss: 0.01117\n",
            "Epoch: 184 | Iteration: 35 | Classification loss: 0.00001 | Regression loss: 0.01194 | Running loss: 0.01116\n",
            "Epoch: 184 | Iteration: 36 | Classification loss: 0.00001 | Regression loss: 0.01756 | Running loss: 0.01118\n",
            "Epoch: 184 | Iteration: 37 | Classification loss: 0.00001 | Regression loss: 0.01517 | Running loss: 0.01120\n",
            "Epoch: 184 | Iteration: 38 | Classification loss: 0.00002 | Regression loss: 0.01146 | Running loss: 0.01119\n",
            "Epoch: 184 | Iteration: 39 | Classification loss: 0.00004 | Regression loss: 0.00903 | Running loss: 0.01120\n",
            "Epoch: 184 | Iteration: 40 | Classification loss: 0.00001 | Regression loss: 0.00307 | Running loss: 0.01118\n",
            "Epoch: 184 | Iteration: 41 | Classification loss: 0.00001 | Regression loss: 0.01156 | Running loss: 0.01118\n",
            "Epoch: 184 | Iteration: 42 | Classification loss: 0.00003 | Regression loss: 0.01357 | Running loss: 0.01118\n",
            "Epoch: 184 | Iteration: 43 | Classification loss: 0.00004 | Regression loss: 0.00827 | Running loss: 0.01119\n",
            "Epoch: 184 | Iteration: 44 | Classification loss: 0.00001 | Regression loss: 0.00331 | Running loss: 0.01117\n",
            "Epoch: 184 | Iteration: 45 | Classification loss: 0.00002 | Regression loss: 0.00637 | Running loss: 0.01117\n",
            "Epoch: 184 | Iteration: 46 | Classification loss: 0.00002 | Regression loss: 0.00941 | Running loss: 0.01117\n",
            "Epoch: 184 | Iteration: 47 | Classification loss: 0.00003 | Regression loss: 0.01349 | Running loss: 0.01118\n",
            "Epoch: 184 | Iteration: 48 | Classification loss: 0.00001 | Regression loss: 0.00348 | Running loss: 0.01117\n",
            "Epoch: 184 | Iteration: 49 | Classification loss: 0.00001 | Regression loss: 0.00688 | Running loss: 0.01116\n",
            "Epoch: 184 | Iteration: 50 | Classification loss: 0.00003 | Regression loss: 0.01294 | Running loss: 0.01116\n",
            "Epoch: 184 | Iteration: 51 | Classification loss: 0.00001 | Regression loss: 0.00551 | Running loss: 0.01115\n",
            "Epoch: 184 | Iteration: 52 | Classification loss: 0.00001 | Regression loss: 0.00387 | Running loss: 0.01115\n",
            "Epoch: 184 | Iteration: 53 | Classification loss: 0.00001 | Regression loss: 0.00527 | Running loss: 0.01114\n",
            "Epoch: 184 | Iteration: 54 | Classification loss: 0.00002 | Regression loss: 0.00911 | Running loss: 0.01111\n",
            "Epoch: 184 | Iteration: 55 | Classification loss: 0.00001 | Regression loss: 0.01098 | Running loss: 0.01109\n",
            "Epoch: 184 | Iteration: 56 | Classification loss: 0.00005 | Regression loss: 0.01213 | Running loss: 0.01109\n",
            "Epoch: 184 | Iteration: 57 | Classification loss: 0.00001 | Regression loss: 0.00411 | Running loss: 0.01104\n",
            "Epoch: 184 | Iteration: 58 | Classification loss: 0.00002 | Regression loss: 0.00590 | Running loss: 0.01104\n",
            "Epoch: 184 | Iteration: 59 | Classification loss: 0.00002 | Regression loss: 0.01197 | Running loss: 0.01104\n",
            "Epoch: 184 | Iteration: 60 | Classification loss: 0.00003 | Regression loss: 0.01575 | Running loss: 0.01104\n",
            "Epoch: 184 | Iteration: 61 | Classification loss: 0.00001 | Regression loss: 0.01003 | Running loss: 0.01106\n",
            "Epoch: 184 | Iteration: 62 | Classification loss: 0.00001 | Regression loss: 0.00556 | Running loss: 0.01106\n",
            "Epoch: 184 | Iteration: 63 | Classification loss: 0.00003 | Regression loss: 0.00549 | Running loss: 0.01106\n",
            "Epoch: 184 | Iteration: 64 | Classification loss: 0.00001 | Regression loss: 0.00277 | Running loss: 0.01103\n",
            "Epoch: 184 | Iteration: 65 | Classification loss: 0.00002 | Regression loss: 0.01556 | Running loss: 0.01104\n",
            "Epoch: 184 | Iteration: 66 | Classification loss: 0.00008 | Regression loss: 0.01790 | Running loss: 0.01105\n",
            "Epoch: 184 | Iteration: 67 | Classification loss: 0.00001 | Regression loss: 0.01386 | Running loss: 0.01106\n",
            "Epoch: 184 | Iteration: 68 | Classification loss: 0.00004 | Regression loss: 0.00361 | Running loss: 0.01106\n",
            "Epoch: 184 | Iteration: 69 | Classification loss: 0.00008 | Regression loss: 0.01280 | Running loss: 0.01102\n",
            "Epoch: 184 | Iteration: 70 | Classification loss: 0.00002 | Regression loss: 0.01214 | Running loss: 0.01104\n",
            "Epoch: 184 | Iteration: 71 | Classification loss: 0.00001 | Regression loss: 0.01075 | Running loss: 0.01104\n",
            "Epoch: 184 | Iteration: 72 | Classification loss: 0.00004 | Regression loss: 0.01650 | Running loss: 0.01104\n",
            "Epoch: 184 | Iteration: 73 | Classification loss: 0.00001 | Regression loss: 0.00638 | Running loss: 0.01104\n",
            "Epoch: 184 | Iteration: 74 | Classification loss: 0.00003 | Regression loss: 0.01947 | Running loss: 0.01107\n",
            "Epoch: 184 | Iteration: 75 | Classification loss: 0.00002 | Regression loss: 0.01027 | Running loss: 0.01106\n",
            "Epoch: 184 | Iteration: 76 | Classification loss: 0.00001 | Regression loss: 0.01194 | Running loss: 0.01105\n",
            "Epoch: 184 | Iteration: 77 | Classification loss: 0.00002 | Regression loss: 0.02204 | Running loss: 0.01100\n",
            "Epoch: 184 | Iteration: 78 | Classification loss: 0.00001 | Regression loss: 0.01119 | Running loss: 0.01100\n",
            "Epoch: 184 | Iteration: 79 | Classification loss: 0.00001 | Regression loss: 0.00734 | Running loss: 0.01098\n",
            "Epoch: 184 | Iteration: 80 | Classification loss: 0.00002 | Regression loss: 0.01189 | Running loss: 0.01098\n",
            "Epoch: 184 | Iteration: 81 | Classification loss: 0.00008 | Regression loss: 0.04282 | Running loss: 0.01105\n",
            "Epoch: 184 | Iteration: 82 | Classification loss: 0.00002 | Regression loss: 0.01225 | Running loss: 0.01106\n",
            "Epoch: 184 | Iteration: 83 | Classification loss: 0.00002 | Regression loss: 0.01076 | Running loss: 0.01106\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.7330251175928468\n",
            "Precision:  0.5433070866141733\n",
            "Recall:  0.8181818181818182\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}, {0: (0.7314384456550078, 253.0)}, {0: (0.7334466242447818, 253.0)}, {0: (0.7357024222917343, 253.0)}, {0: (0.7381550871546381, 253.0)}, {0: (0.739092434551302, 253.0)}, {0: (0.72091448182607, 253.0)}, {0: (0.7389512050165785, 253.0)}, {0: (0.73050932626622, 253.0)}, {0: (0.7263226120133723, 253.0)}, {0: (0.7283463725823673, 253.0)}, {0: (0.7254913772539314, 253.0)}, {0: (0.7281179290861395, 253.0)}, {0: (0.7260020498565122, 253.0)}, {0: (0.7217236108381175, 253.0)}, {0: (0.7267514829312511, 253.0)}, {0: (0.7227483652714889, 253.0)}, {0: (0.7241298168882813, 253.0)}, {0: (0.7261051666397325, 253.0)}, {0: (0.7277246704086413, 253.0)}, {0: (0.7259496886680885, 253.0)}, {0: (0.7290251963867072, 253.0)}, {0: (0.7242894157206492, 253.0)}, {0: (0.7329303843522665, 253.0)}, {0: (0.733154886135177, 253.0)}, {0: (0.7291842105419786, 253.0)}, {0: (0.7251405627702188, 253.0)}, {0: (0.731154087710585, 253.0)}, {0: (0.7317124239374072, 253.0)}, {0: (0.7263717757478444, 253.0)}, {0: (0.7278912613628667, 253.0)}, {0: (0.7330251175928468, 253.0)}]\n",
            "Epoch: 185 | Iteration: 0 | Classification loss: 0.00003 | Regression loss: 0.00840 | Running loss: 0.01105\n",
            "Epoch: 185 | Iteration: 1 | Classification loss: 0.00002 | Regression loss: 0.00915 | Running loss: 0.01105\n",
            "Epoch: 185 | Iteration: 2 | Classification loss: 0.00000 | Regression loss: 0.00326 | Running loss: 0.01105\n",
            "Epoch: 185 | Iteration: 3 | Classification loss: 0.00002 | Regression loss: 0.00928 | Running loss: 0.01106\n",
            "Epoch: 185 | Iteration: 4 | Classification loss: 0.00002 | Regression loss: 0.01464 | Running loss: 0.01107\n",
            "Epoch: 185 | Iteration: 5 | Classification loss: 0.00003 | Regression loss: 0.01479 | Running loss: 0.01109\n",
            "Epoch: 185 | Iteration: 6 | Classification loss: 0.00001 | Regression loss: 0.00478 | Running loss: 0.01107\n",
            "Epoch: 185 | Iteration: 7 | Classification loss: 0.00002 | Regression loss: 0.01647 | Running loss: 0.01107\n",
            "Epoch: 185 | Iteration: 8 | Classification loss: 0.00002 | Regression loss: 0.00572 | Running loss: 0.01106\n",
            "Epoch: 185 | Iteration: 9 | Classification loss: 0.00001 | Regression loss: 0.00376 | Running loss: 0.01105\n",
            "Epoch: 185 | Iteration: 10 | Classification loss: 0.00007 | Regression loss: 0.04168 | Running loss: 0.01111\n",
            "Epoch: 185 | Iteration: 11 | Classification loss: 0.00001 | Regression loss: 0.01094 | Running loss: 0.01111\n",
            "Epoch: 185 | Iteration: 12 | Classification loss: 0.00004 | Regression loss: 0.01584 | Running loss: 0.01112\n",
            "Epoch: 185 | Iteration: 13 | Classification loss: 0.00001 | Regression loss: 0.01067 | Running loss: 0.01114\n",
            "Epoch: 185 | Iteration: 14 | Classification loss: 0.00002 | Regression loss: 0.01252 | Running loss: 0.01115\n",
            "Epoch: 185 | Iteration: 15 | Classification loss: 0.00003 | Regression loss: 0.01300 | Running loss: 0.01114\n",
            "Epoch: 185 | Iteration: 16 | Classification loss: 0.00001 | Regression loss: 0.01062 | Running loss: 0.01114\n",
            "Epoch: 185 | Iteration: 17 | Classification loss: 0.00001 | Regression loss: 0.01708 | Running loss: 0.01115\n",
            "Epoch: 185 | Iteration: 18 | Classification loss: 0.00001 | Regression loss: 0.01090 | Running loss: 0.01117\n",
            "Epoch: 185 | Iteration: 19 | Classification loss: 0.00002 | Regression loss: 0.00650 | Running loss: 0.01117\n",
            "Epoch: 185 | Iteration: 20 | Classification loss: 0.00002 | Regression loss: 0.00658 | Running loss: 0.01116\n",
            "Epoch: 185 | Iteration: 21 | Classification loss: 0.00003 | Regression loss: 0.01514 | Running loss: 0.01118\n",
            "Epoch: 185 | Iteration: 22 | Classification loss: 0.00001 | Regression loss: 0.01711 | Running loss: 0.01120\n",
            "Epoch: 185 | Iteration: 23 | Classification loss: 0.00001 | Regression loss: 0.02709 | Running loss: 0.01123\n",
            "Epoch: 185 | Iteration: 24 | Classification loss: 0.00004 | Regression loss: 0.01344 | Running loss: 0.01123\n",
            "Epoch: 185 | Iteration: 25 | Classification loss: 0.00004 | Regression loss: 0.00370 | Running loss: 0.01119\n",
            "Epoch: 185 | Iteration: 26 | Classification loss: 0.00008 | Regression loss: 0.01685 | Running loss: 0.01121\n",
            "Epoch: 185 | Iteration: 27 | Classification loss: 0.00002 | Regression loss: 0.02969 | Running loss: 0.01123\n",
            "Epoch: 185 | Iteration: 28 | Classification loss: 0.00001 | Regression loss: 0.01019 | Running loss: 0.01122\n",
            "Epoch: 185 | Iteration: 29 | Classification loss: 0.00002 | Regression loss: 0.01099 | Running loss: 0.01123\n",
            "Epoch: 185 | Iteration: 30 | Classification loss: 0.00001 | Regression loss: 0.00595 | Running loss: 0.01121\n",
            "Epoch: 185 | Iteration: 31 | Classification loss: 0.00002 | Regression loss: 0.01428 | Running loss: 0.01121\n",
            "Epoch: 185 | Iteration: 32 | Classification loss: 0.00001 | Regression loss: 0.00545 | Running loss: 0.01120\n",
            "Epoch: 185 | Iteration: 33 | Classification loss: 0.00004 | Regression loss: 0.01927 | Running loss: 0.01120\n",
            "Epoch: 185 | Iteration: 34 | Classification loss: 0.00003 | Regression loss: 0.01488 | Running loss: 0.01117\n",
            "Epoch: 185 | Iteration: 35 | Classification loss: 0.00005 | Regression loss: 0.01281 | Running loss: 0.01114\n",
            "Epoch: 185 | Iteration: 36 | Classification loss: 0.00003 | Regression loss: 0.01005 | Running loss: 0.01111\n",
            "Epoch: 185 | Iteration: 37 | Classification loss: 0.00001 | Regression loss: 0.00313 | Running loss: 0.01109\n",
            "Epoch: 185 | Iteration: 38 | Classification loss: 0.00001 | Regression loss: 0.01021 | Running loss: 0.01108\n",
            "Epoch: 185 | Iteration: 39 | Classification loss: 0.00001 | Regression loss: 0.00376 | Running loss: 0.01106\n",
            "Epoch: 185 | Iteration: 40 | Classification loss: 0.00001 | Regression loss: 0.00468 | Running loss: 0.01104\n",
            "Epoch: 185 | Iteration: 41 | Classification loss: 0.00011 | Regression loss: 0.00607 | Running loss: 0.01104\n",
            "Epoch: 185 | Iteration: 42 | Classification loss: 0.00002 | Regression loss: 0.01140 | Running loss: 0.01106\n",
            "Epoch: 185 | Iteration: 43 | Classification loss: 0.00003 | Regression loss: 0.00993 | Running loss: 0.01104\n",
            "Epoch: 185 | Iteration: 44 | Classification loss: 0.00003 | Regression loss: 0.01481 | Running loss: 0.01106\n",
            "Epoch: 185 | Iteration: 45 | Classification loss: 0.00001 | Regression loss: 0.00871 | Running loss: 0.01105\n",
            "Epoch: 185 | Iteration: 46 | Classification loss: 0.00007 | Regression loss: 0.01262 | Running loss: 0.01105\n",
            "Epoch: 185 | Iteration: 47 | Classification loss: 0.00005 | Regression loss: 0.01057 | Running loss: 0.01105\n",
            "Epoch: 185 | Iteration: 48 | Classification loss: 0.00002 | Regression loss: 0.01483 | Running loss: 0.01107\n",
            "Epoch: 185 | Iteration: 49 | Classification loss: 0.00001 | Regression loss: 0.00448 | Running loss: 0.01106\n",
            "Epoch: 185 | Iteration: 50 | Classification loss: 0.00002 | Regression loss: 0.02060 | Running loss: 0.01107\n",
            "Epoch: 185 | Iteration: 51 | Classification loss: 0.00002 | Regression loss: 0.00930 | Running loss: 0.01108\n",
            "Epoch: 185 | Iteration: 52 | Classification loss: 0.00001 | Regression loss: 0.00363 | Running loss: 0.01108\n",
            "Epoch: 185 | Iteration: 53 | Classification loss: 0.00001 | Regression loss: 0.00665 | Running loss: 0.01101\n",
            "Epoch: 185 | Iteration: 54 | Classification loss: 0.00003 | Regression loss: 0.01160 | Running loss: 0.01101\n",
            "Epoch: 185 | Iteration: 55 | Classification loss: 0.00003 | Regression loss: 0.01337 | Running loss: 0.01101\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 190.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 185 | Iteration: 57 | Classification loss: 0.00001 | Regression loss: 0.00688 | Running loss: 0.01100\n",
            "Epoch: 185 | Iteration: 58 | Classification loss: 0.00001 | Regression loss: 0.00603 | Running loss: 0.01098\n",
            "Epoch: 185 | Iteration: 59 | Classification loss: 0.00001 | Regression loss: 0.01060 | Running loss: 0.01097\n",
            "Epoch: 185 | Iteration: 60 | Classification loss: 0.00001 | Regression loss: 0.00490 | Running loss: 0.01095\n",
            "Epoch: 185 | Iteration: 61 | Classification loss: 0.00002 | Regression loss: 0.01203 | Running loss: 0.01097\n",
            "Epoch: 185 | Iteration: 62 | Classification loss: 0.00003 | Regression loss: 0.02596 | Running loss: 0.01101\n",
            "Epoch: 185 | Iteration: 63 | Classification loss: 0.00001 | Regression loss: 0.00741 | Running loss: 0.01101\n",
            "Epoch: 185 | Iteration: 64 | Classification loss: 0.00001 | Regression loss: 0.01337 | Running loss: 0.01103\n",
            "Epoch: 185 | Iteration: 65 | Classification loss: 0.00001 | Regression loss: 0.00589 | Running loss: 0.01103\n",
            "Epoch: 185 | Iteration: 66 | Classification loss: 0.00001 | Regression loss: 0.00360 | Running loss: 0.01102\n",
            "Epoch: 185 | Iteration: 67 | Classification loss: 0.00001 | Regression loss: 0.01036 | Running loss: 0.01102\n",
            "Epoch: 185 | Iteration: 68 | Classification loss: 0.00002 | Regression loss: 0.00853 | Running loss: 0.01103\n",
            "Epoch: 185 | Iteration: 69 | Classification loss: 0.00002 | Regression loss: 0.00944 | Running loss: 0.01102\n",
            "Epoch: 185 | Iteration: 70 | Classification loss: 0.00001 | Regression loss: 0.00744 | Running loss: 0.01102\n",
            "Epoch: 185 | Iteration: 71 | Classification loss: 0.00001 | Regression loss: 0.00435 | Running loss: 0.01101\n",
            "Epoch: 185 | Iteration: 72 | Classification loss: 0.00001 | Regression loss: 0.00575 | Running loss: 0.01101\n",
            "Epoch: 185 | Iteration: 73 | Classification loss: 0.00001 | Regression loss: 0.01494 | Running loss: 0.01102\n",
            "Epoch: 185 | Iteration: 74 | Classification loss: 0.00000 | Regression loss: 0.00801 | Running loss: 0.01102\n",
            "Epoch: 185 | Iteration: 75 | Classification loss: 0.00007 | Regression loss: 0.01424 | Running loss: 0.01104\n",
            "Epoch: 185 | Iteration: 76 | Classification loss: 0.00001 | Regression loss: 0.01176 | Running loss: 0.01103\n",
            "Epoch: 185 | Iteration: 77 | Classification loss: 0.00001 | Regression loss: 0.00339 | Running loss: 0.01102\n",
            "Epoch: 185 | Iteration: 78 | Classification loss: 0.00001 | Regression loss: 0.01070 | Running loss: 0.01101\n",
            "Epoch: 185 | Iteration: 79 | Classification loss: 0.00002 | Regression loss: 0.01120 | Running loss: 0.01101\n",
            "Epoch: 185 | Iteration: 80 | Classification loss: 0.00002 | Regression loss: 0.00429 | Running loss: 0.01101\n",
            "Epoch: 185 | Iteration: 81 | Classification loss: 0.00001 | Regression loss: 0.00251 | Running loss: 0.01099\n",
            "Epoch: 185 | Iteration: 82 | Classification loss: 0.00002 | Regression loss: 0.01213 | Running loss: 0.01100\n",
            "Epoch: 185 | Iteration: 83 | Classification loss: 0.00001 | Regression loss: 0.00686 | Running loss: 0.01100\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.7278399291817672\n",
            "Precision:  0.5433070866141733\n",
            "Recall:  0.8181818181818182\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}, {0: (0.7314384456550078, 253.0)}, {0: (0.7334466242447818, 253.0)}, {0: (0.7357024222917343, 253.0)}, {0: (0.7381550871546381, 253.0)}, {0: (0.739092434551302, 253.0)}, {0: (0.72091448182607, 253.0)}, {0: (0.7389512050165785, 253.0)}, {0: (0.73050932626622, 253.0)}, {0: (0.7263226120133723, 253.0)}, {0: (0.7283463725823673, 253.0)}, {0: (0.7254913772539314, 253.0)}, {0: (0.7281179290861395, 253.0)}, {0: (0.7260020498565122, 253.0)}, {0: (0.7217236108381175, 253.0)}, {0: (0.7267514829312511, 253.0)}, {0: (0.7227483652714889, 253.0)}, {0: (0.7241298168882813, 253.0)}, {0: (0.7261051666397325, 253.0)}, {0: (0.7277246704086413, 253.0)}, {0: (0.7259496886680885, 253.0)}, {0: (0.7290251963867072, 253.0)}, {0: (0.7242894157206492, 253.0)}, {0: (0.7329303843522665, 253.0)}, {0: (0.733154886135177, 253.0)}, {0: (0.7291842105419786, 253.0)}, {0: (0.7251405627702188, 253.0)}, {0: (0.731154087710585, 253.0)}, {0: (0.7317124239374072, 253.0)}, {0: (0.7263717757478444, 253.0)}, {0: (0.7278912613628667, 253.0)}, {0: (0.7330251175928468, 253.0)}, {0: (0.7278399291817672, 253.0)}]\n",
            "Epoch: 186 | Iteration: 0 | Classification loss: 0.00002 | Regression loss: 0.01153 | Running loss: 0.01099\n",
            "Epoch: 186 | Iteration: 1 | Classification loss: 0.00002 | Regression loss: 0.00365 | Running loss: 0.01097\n",
            "Epoch: 186 | Iteration: 2 | Classification loss: 0.00004 | Regression loss: 0.01572 | Running loss: 0.01094\n",
            "Epoch: 186 | Iteration: 3 | Classification loss: 0.00002 | Regression loss: 0.01305 | Running loss: 0.01096\n",
            "Epoch: 186 | Iteration: 4 | Classification loss: 0.00002 | Regression loss: 0.00976 | Running loss: 0.01096\n",
            "Epoch: 186 | Iteration: 5 | Classification loss: 0.00001 | Regression loss: 0.01057 | Running loss: 0.01096\n",
            "Epoch: 186 | Iteration: 6 | Classification loss: 0.00001 | Regression loss: 0.01091 | Running loss: 0.01097\n",
            "Epoch: 186 | Iteration: 7 | Classification loss: 0.00002 | Regression loss: 0.01372 | Running loss: 0.01091\n",
            "Epoch: 186 | Iteration: 8 | Classification loss: 0.00003 | Regression loss: 0.01509 | Running loss: 0.01091\n",
            "Epoch: 186 | Iteration: 9 | Classification loss: 0.00001 | Regression loss: 0.00263 | Running loss: 0.01090\n",
            "Epoch: 186 | Iteration: 10 | Classification loss: 0.00005 | Regression loss: 0.01365 | Running loss: 0.01090\n",
            "Epoch: 186 | Iteration: 11 | Classification loss: 0.00003 | Regression loss: 0.00947 | Running loss: 0.01089\n",
            "Epoch: 186 | Iteration: 12 | Classification loss: 0.00001 | Regression loss: 0.00348 | Running loss: 0.01087\n",
            "Epoch: 186 | Iteration: 13 | Classification loss: 0.00002 | Regression loss: 0.00947 | Running loss: 0.01087\n",
            "Epoch: 186 | Iteration: 14 | Classification loss: 0.00001 | Regression loss: 0.00685 | Running loss: 0.01086\n",
            "Epoch: 186 | Iteration: 15 | Classification loss: 0.00003 | Regression loss: 0.00938 | Running loss: 0.01084\n",
            "Epoch: 186 | Iteration: 16 | Classification loss: 0.00001 | Regression loss: 0.00991 | Running loss: 0.01085\n",
            "Epoch: 186 | Iteration: 17 | Classification loss: 0.00001 | Regression loss: 0.01457 | Running loss: 0.01087\n",
            "Epoch: 186 | Iteration: 18 | Classification loss: 0.00001 | Regression loss: 0.00349 | Running loss: 0.01082\n",
            "Epoch: 186 | Iteration: 19 | Classification loss: 0.00002 | Regression loss: 0.00958 | Running loss: 0.01082\n",
            "Epoch: 186 | Iteration: 20 | Classification loss: 0.00001 | Regression loss: 0.00549 | Running loss: 0.01081\n",
            "Epoch: 186 | Iteration: 21 | Classification loss: 0.00002 | Regression loss: 0.00841 | Running loss: 0.01080\n",
            "Epoch: 186 | Iteration: 22 | Classification loss: 0.00003 | Regression loss: 0.01967 | Running loss: 0.01082\n",
            "Epoch: 186 | Iteration: 23 | Classification loss: 0.00000 | Regression loss: 0.00800 | Running loss: 0.01083\n",
            "Epoch: 186 | Iteration: 24 | Classification loss: 0.00001 | Regression loss: 0.00717 | Running loss: 0.01083\n",
            "Epoch: 186 | Iteration: 25 | Classification loss: 0.00002 | Regression loss: 0.01164 | Running loss: 0.01084\n",
            "Epoch: 186 | Iteration: 26 | Classification loss: 0.00006 | Regression loss: 0.01103 | Running loss: 0.01084\n",
            "Epoch: 186 | Iteration: 27 | Classification loss: 0.00001 | Regression loss: 0.00940 | Running loss: 0.01083\n",
            "Epoch: 186 | Iteration: 28 | Classification loss: 0.00002 | Regression loss: 0.00574 | Running loss: 0.01082\n",
            "Epoch: 186 | Iteration: 29 | Classification loss: 0.00002 | Regression loss: 0.01118 | Running loss: 0.01082\n",
            "Epoch: 186 | Iteration: 30 | Classification loss: 0.00007 | Regression loss: 0.04106 | Running loss: 0.01088\n",
            "Epoch: 186 | Iteration: 31 | Classification loss: 0.00001 | Regression loss: 0.00367 | Running loss: 0.01086\n",
            "Epoch: 186 | Iteration: 32 | Classification loss: 0.00006 | Regression loss: 0.01598 | Running loss: 0.01087\n",
            "Epoch: 186 | Iteration: 33 | Classification loss: 0.00001 | Regression loss: 0.00582 | Running loss: 0.01085\n",
            "Epoch: 186 | Iteration: 34 | Classification loss: 0.00002 | Regression loss: 0.01176 | Running loss: 0.01084\n",
            "Epoch: 186 | Iteration: 35 | Classification loss: 0.00001 | Regression loss: 0.00915 | Running loss: 0.01084\n",
            "Epoch: 186 | Iteration: 36 | Classification loss: 0.00003 | Regression loss: 0.02397 | Running loss: 0.01088\n",
            "Epoch: 186 | Iteration: 37 | Classification loss: 0.00005 | Regression loss: 0.01294 | Running loss: 0.01088\n",
            "Epoch: 186 | Iteration: 38 | Classification loss: 0.00002 | Regression loss: 0.01352 | Running loss: 0.01088\n",
            "Epoch: 186 | Iteration: 39 | Classification loss: 0.00001 | Regression loss: 0.01687 | Running loss: 0.01089\n",
            "Epoch: 186 | Iteration: 40 | Classification loss: 0.00001 | Regression loss: 0.02915 | Running loss: 0.01094\n",
            "Epoch: 186 | Iteration: 41 | Classification loss: 0.00007 | Regression loss: 0.01229 | Running loss: 0.01091\n",
            "Epoch: 186 | Iteration: 42 | Classification loss: 0.00010 | Regression loss: 0.00696 | Running loss: 0.01090\n",
            "Epoch: 186 | Iteration: 43 | Classification loss: 0.00002 | Regression loss: 0.01658 | Running loss: 0.01092\n",
            "Epoch: 186 | Iteration: 44 | Classification loss: 0.00001 | Regression loss: 0.00976 | Running loss: 0.01091\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 190.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 186 | Iteration: 46 | Classification loss: 0.00001 | Regression loss: 0.00933 | Running loss: 0.01091\n",
            "Epoch: 186 | Iteration: 47 | Classification loss: 0.00001 | Regression loss: 0.00457 | Running loss: 0.01090\n",
            "Epoch: 186 | Iteration: 48 | Classification loss: 0.00001 | Regression loss: 0.00989 | Running loss: 0.01090\n",
            "Epoch: 186 | Iteration: 49 | Classification loss: 0.00002 | Regression loss: 0.01637 | Running loss: 0.01092\n",
            "Epoch: 186 | Iteration: 50 | Classification loss: 0.00002 | Regression loss: 0.00673 | Running loss: 0.01091\n",
            "Epoch: 186 | Iteration: 51 | Classification loss: 0.00002 | Regression loss: 0.00991 | Running loss: 0.01087\n",
            "Epoch: 186 | Iteration: 52 | Classification loss: 0.00001 | Regression loss: 0.00515 | Running loss: 0.01086\n",
            "Epoch: 186 | Iteration: 53 | Classification loss: 0.00001 | Regression loss: 0.01390 | Running loss: 0.01087\n",
            "Epoch: 186 | Iteration: 54 | Classification loss: 0.00001 | Regression loss: 0.00538 | Running loss: 0.01086\n",
            "Epoch: 186 | Iteration: 55 | Classification loss: 0.00001 | Regression loss: 0.01401 | Running loss: 0.01087\n",
            "Epoch: 186 | Iteration: 56 | Classification loss: 0.00002 | Regression loss: 0.01229 | Running loss: 0.01086\n",
            "Epoch: 186 | Iteration: 57 | Classification loss: 0.00001 | Regression loss: 0.01066 | Running loss: 0.01087\n",
            "Epoch: 186 | Iteration: 58 | Classification loss: 0.00001 | Regression loss: 0.00514 | Running loss: 0.01085\n",
            "Epoch: 186 | Iteration: 59 | Classification loss: 0.00004 | Regression loss: 0.01328 | Running loss: 0.01086\n",
            "Epoch: 186 | Iteration: 60 | Classification loss: 0.00001 | Regression loss: 0.02682 | Running loss: 0.01089\n",
            "Epoch: 186 | Iteration: 61 | Classification loss: 0.00003 | Regression loss: 0.01545 | Running loss: 0.01091\n",
            "Epoch: 186 | Iteration: 62 | Classification loss: 0.00001 | Regression loss: 0.00406 | Running loss: 0.01091\n",
            "Epoch: 186 | Iteration: 63 | Classification loss: 0.00001 | Regression loss: 0.00484 | Running loss: 0.01091\n",
            "Epoch: 186 | Iteration: 64 | Classification loss: 0.00001 | Regression loss: 0.00393 | Running loss: 0.01088\n",
            "Epoch: 186 | Iteration: 65 | Classification loss: 0.00001 | Regression loss: 0.00594 | Running loss: 0.01089\n",
            "Epoch: 186 | Iteration: 66 | Classification loss: 0.00003 | Regression loss: 0.01193 | Running loss: 0.01090\n",
            "Epoch: 186 | Iteration: 67 | Classification loss: 0.00002 | Regression loss: 0.00799 | Running loss: 0.01089\n",
            "Epoch: 186 | Iteration: 68 | Classification loss: 0.00002 | Regression loss: 0.00628 | Running loss: 0.01089\n",
            "Epoch: 186 | Iteration: 69 | Classification loss: 0.00002 | Regression loss: 0.02390 | Running loss: 0.01091\n",
            "Epoch: 186 | Iteration: 70 | Classification loss: 0.00001 | Regression loss: 0.01669 | Running loss: 0.01091\n",
            "Epoch: 186 | Iteration: 71 | Classification loss: 0.00001 | Regression loss: 0.00445 | Running loss: 0.01091\n",
            "Epoch: 186 | Iteration: 72 | Classification loss: 0.00004 | Regression loss: 0.00884 | Running loss: 0.01092\n",
            "Epoch: 186 | Iteration: 73 | Classification loss: 0.00001 | Regression loss: 0.00434 | Running loss: 0.01090\n",
            "Epoch: 186 | Iteration: 74 | Classification loss: 0.00001 | Regression loss: 0.00647 | Running loss: 0.01089\n",
            "Epoch: 186 | Iteration: 75 | Classification loss: 0.00002 | Regression loss: 0.00270 | Running loss: 0.01087\n",
            "Epoch: 186 | Iteration: 76 | Classification loss: 0.00001 | Regression loss: 0.00583 | Running loss: 0.01087\n",
            "Epoch: 186 | Iteration: 77 | Classification loss: 0.00002 | Regression loss: 0.01480 | Running loss: 0.01089\n",
            "Epoch: 186 | Iteration: 78 | Classification loss: 0.00003 | Regression loss: 0.00389 | Running loss: 0.01088\n",
            "Epoch: 186 | Iteration: 79 | Classification loss: 0.00001 | Regression loss: 0.00346 | Running loss: 0.01087\n",
            "Epoch: 186 | Iteration: 80 | Classification loss: 0.00002 | Regression loss: 0.01224 | Running loss: 0.01088\n",
            "Epoch: 186 | Iteration: 81 | Classification loss: 0.00001 | Regression loss: 0.00502 | Running loss: 0.01088\n",
            "Epoch: 186 | Iteration: 82 | Classification loss: 0.00002 | Regression loss: 0.01693 | Running loss: 0.01089\n",
            "Epoch: 186 | Iteration: 83 | Classification loss: 0.00001 | Regression loss: 0.00929 | Running loss: 0.01089\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.728608785848319\n",
            "Precision:  0.5418848167539267\n",
            "Recall:  0.8181818181818182\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}, {0: (0.7314384456550078, 253.0)}, {0: (0.7334466242447818, 253.0)}, {0: (0.7357024222917343, 253.0)}, {0: (0.7381550871546381, 253.0)}, {0: (0.739092434551302, 253.0)}, {0: (0.72091448182607, 253.0)}, {0: (0.7389512050165785, 253.0)}, {0: (0.73050932626622, 253.0)}, {0: (0.7263226120133723, 253.0)}, {0: (0.7283463725823673, 253.0)}, {0: (0.7254913772539314, 253.0)}, {0: (0.7281179290861395, 253.0)}, {0: (0.7260020498565122, 253.0)}, {0: (0.7217236108381175, 253.0)}, {0: (0.7267514829312511, 253.0)}, {0: (0.7227483652714889, 253.0)}, {0: (0.7241298168882813, 253.0)}, {0: (0.7261051666397325, 253.0)}, {0: (0.7277246704086413, 253.0)}, {0: (0.7259496886680885, 253.0)}, {0: (0.7290251963867072, 253.0)}, {0: (0.7242894157206492, 253.0)}, {0: (0.7329303843522665, 253.0)}, {0: (0.733154886135177, 253.0)}, {0: (0.7291842105419786, 253.0)}, {0: (0.7251405627702188, 253.0)}, {0: (0.731154087710585, 253.0)}, {0: (0.7317124239374072, 253.0)}, {0: (0.7263717757478444, 253.0)}, {0: (0.7278912613628667, 253.0)}, {0: (0.7330251175928468, 253.0)}, {0: (0.7278399291817672, 253.0)}, {0: (0.728608785848319, 253.0)}]\n",
            "Epoch: 187 | Iteration: 0 | Classification loss: 0.00002 | Regression loss: 0.00960 | Running loss: 0.01090\n",
            "Epoch: 187 | Iteration: 1 | Classification loss: 0.00001 | Regression loss: 0.01127 | Running loss: 0.01092\n",
            "Epoch: 187 | Iteration: 2 | Classification loss: 0.00001 | Regression loss: 0.00764 | Running loss: 0.01088\n",
            "Epoch: 187 | Iteration: 3 | Classification loss: 0.00002 | Regression loss: 0.01197 | Running loss: 0.01089\n",
            "Epoch: 187 | Iteration: 4 | Classification loss: 0.00001 | Regression loss: 0.00998 | Running loss: 0.01089\n",
            "Epoch: 187 | Iteration: 5 | Classification loss: 0.00001 | Regression loss: 0.01444 | Running loss: 0.01092\n",
            "Epoch: 187 | Iteration: 6 | Classification loss: 0.00001 | Regression loss: 0.02634 | Running loss: 0.01094\n",
            "Epoch: 187 | Iteration: 7 | Classification loss: 0.00006 | Regression loss: 0.01183 | Running loss: 0.01094\n",
            "Epoch: 187 | Iteration: 8 | Classification loss: 0.00001 | Regression loss: 0.00687 | Running loss: 0.01094\n",
            "Epoch: 187 | Iteration: 9 | Classification loss: 0.00002 | Regression loss: 0.00292 | Running loss: 0.01092\n",
            "Epoch: 187 | Iteration: 10 | Classification loss: 0.00001 | Regression loss: 0.00909 | Running loss: 0.01091\n",
            "Epoch: 187 | Iteration: 11 | Classification loss: 0.00003 | Regression loss: 0.00344 | Running loss: 0.01090\n",
            "Epoch: 187 | Iteration: 12 | Classification loss: 0.00001 | Regression loss: 0.01671 | Running loss: 0.01092\n",
            "Epoch: 187 | Iteration: 13 | Classification loss: 0.00001 | Regression loss: 0.00514 | Running loss: 0.01090\n",
            "Epoch: 187 | Iteration: 14 | Classification loss: 0.00006 | Regression loss: 0.01237 | Running loss: 0.01090\n",
            "Epoch: 187 | Iteration: 15 | Classification loss: 0.00001 | Regression loss: 0.01064 | Running loss: 0.01090\n",
            "Epoch: 187 | Iteration: 16 | Classification loss: 0.00010 | Regression loss: 0.00683 | Running loss: 0.01090\n",
            "Epoch: 187 | Iteration: 17 | Classification loss: 0.00001 | Regression loss: 0.02149 | Running loss: 0.01091\n",
            "Epoch: 187 | Iteration: 18 | Classification loss: 0.00000 | Regression loss: 0.00771 | Running loss: 0.01090\n",
            "Epoch: 187 | Iteration: 19 | Classification loss: 0.00000 | Regression loss: 0.00294 | Running loss: 0.01088\n",
            "Epoch: 187 | Iteration: 20 | Classification loss: 0.00002 | Regression loss: 0.00943 | Running loss: 0.01087\n",
            "Epoch: 187 | Iteration: 21 | Classification loss: 0.00001 | Regression loss: 0.01446 | Running loss: 0.01087\n",
            "Epoch: 187 | Iteration: 22 | Classification loss: 0.00005 | Regression loss: 0.01319 | Running loss: 0.01087\n",
            "Epoch: 187 | Iteration: 23 | Classification loss: 0.00007 | Regression loss: 0.01627 | Running loss: 0.01088\n",
            "Epoch: 187 | Iteration: 24 | Classification loss: 0.00002 | Regression loss: 0.00649 | Running loss: 0.01086\n",
            "Epoch: 187 | Iteration: 25 | Classification loss: 0.00004 | Regression loss: 0.01522 | Running loss: 0.01088\n",
            "Epoch: 187 | Iteration: 26 | Classification loss: 0.00001 | Regression loss: 0.00579 | Running loss: 0.01086\n",
            "Epoch: 187 | Iteration: 27 | Classification loss: 0.00007 | Regression loss: 0.04158 | Running loss: 0.01093\n",
            "Epoch: 187 | Iteration: 28 | Classification loss: 0.00001 | Regression loss: 0.00949 | Running loss: 0.01091\n",
            "Epoch: 187 | Iteration: 29 | Classification loss: 0.00001 | Regression loss: 0.01018 | Running loss: 0.01091\n",
            "Epoch: 187 | Iteration: 30 | Classification loss: 0.00002 | Regression loss: 0.01176 | Running loss: 0.01091\n",
            "Epoch: 187 | Iteration: 31 | Classification loss: 0.00002 | Regression loss: 0.01573 | Running loss: 0.01092\n",
            "Epoch: 187 | Iteration: 32 | Classification loss: 0.00001 | Regression loss: 0.00426 | Running loss: 0.01090\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 190.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 187 | Iteration: 34 | Classification loss: 0.00001 | Regression loss: 0.00651 | Running loss: 0.01090\n",
            "Epoch: 187 | Iteration: 35 | Classification loss: 0.00001 | Regression loss: 0.00357 | Running loss: 0.01089\n",
            "Epoch: 187 | Iteration: 36 | Classification loss: 0.00001 | Regression loss: 0.01079 | Running loss: 0.01090\n",
            "Epoch: 187 | Iteration: 37 | Classification loss: 0.00002 | Regression loss: 0.01072 | Running loss: 0.01090\n",
            "Epoch: 187 | Iteration: 38 | Classification loss: 0.00001 | Regression loss: 0.01603 | Running loss: 0.01092\n",
            "Epoch: 187 | Iteration: 39 | Classification loss: 0.00002 | Regression loss: 0.01104 | Running loss: 0.01092\n",
            "Epoch: 187 | Iteration: 40 | Classification loss: 0.00000 | Regression loss: 0.00408 | Running loss: 0.01091\n",
            "Epoch: 187 | Iteration: 41 | Classification loss: 0.00001 | Regression loss: 0.00757 | Running loss: 0.01090\n",
            "Epoch: 187 | Iteration: 42 | Classification loss: 0.00002 | Regression loss: 0.01109 | Running loss: 0.01092\n",
            "Epoch: 187 | Iteration: 43 | Classification loss: 0.00001 | Regression loss: 0.01125 | Running loss: 0.01091\n",
            "Epoch: 187 | Iteration: 44 | Classification loss: 0.00001 | Regression loss: 0.00328 | Running loss: 0.01091\n",
            "Epoch: 187 | Iteration: 45 | Classification loss: 0.00001 | Regression loss: 0.00584 | Running loss: 0.01091\n",
            "Epoch: 187 | Iteration: 46 | Classification loss: 0.00001 | Regression loss: 0.00492 | Running loss: 0.01087\n",
            "Epoch: 187 | Iteration: 47 | Classification loss: 0.00001 | Regression loss: 0.01116 | Running loss: 0.01087\n",
            "Epoch: 187 | Iteration: 48 | Classification loss: 0.00002 | Regression loss: 0.02552 | Running loss: 0.01089\n",
            "Epoch: 187 | Iteration: 49 | Classification loss: 0.00002 | Regression loss: 0.00419 | Running loss: 0.01088\n",
            "Epoch: 187 | Iteration: 50 | Classification loss: 0.00002 | Regression loss: 0.01547 | Running loss: 0.01089\n",
            "Epoch: 187 | Iteration: 51 | Classification loss: 0.00001 | Regression loss: 0.00395 | Running loss: 0.01088\n",
            "Epoch: 187 | Iteration: 52 | Classification loss: 0.00001 | Regression loss: 0.01100 | Running loss: 0.01088\n",
            "Epoch: 187 | Iteration: 53 | Classification loss: 0.00002 | Regression loss: 0.00611 | Running loss: 0.01088\n",
            "Epoch: 187 | Iteration: 54 | Classification loss: 0.00001 | Regression loss: 0.01467 | Running loss: 0.01090\n",
            "Epoch: 187 | Iteration: 55 | Classification loss: 0.00003 | Regression loss: 0.00939 | Running loss: 0.01090\n",
            "Epoch: 187 | Iteration: 56 | Classification loss: 0.00001 | Regression loss: 0.00458 | Running loss: 0.01089\n",
            "Epoch: 187 | Iteration: 57 | Classification loss: 0.00002 | Regression loss: 0.00899 | Running loss: 0.01088\n",
            "Epoch: 187 | Iteration: 58 | Classification loss: 0.00001 | Regression loss: 0.00374 | Running loss: 0.01087\n",
            "Epoch: 187 | Iteration: 59 | Classification loss: 0.00002 | Regression loss: 0.00857 | Running loss: 0.01088\n",
            "Epoch: 187 | Iteration: 60 | Classification loss: 0.00002 | Regression loss: 0.01128 | Running loss: 0.01089\n",
            "Epoch: 187 | Iteration: 61 | Classification loss: 0.00001 | Regression loss: 0.00369 | Running loss: 0.01089\n",
            "Epoch: 187 | Iteration: 62 | Classification loss: 0.00001 | Regression loss: 0.01135 | Running loss: 0.01087\n",
            "Epoch: 187 | Iteration: 63 | Classification loss: 0.00003 | Regression loss: 0.01816 | Running loss: 0.01087\n",
            "Epoch: 187 | Iteration: 64 | Classification loss: 0.00002 | Regression loss: 0.00623 | Running loss: 0.01085\n",
            "Epoch: 187 | Iteration: 65 | Classification loss: 0.00001 | Regression loss: 0.00360 | Running loss: 0.01085\n",
            "Epoch: 187 | Iteration: 66 | Classification loss: 0.00001 | Regression loss: 0.01035 | Running loss: 0.01085\n",
            "Epoch: 187 | Iteration: 67 | Classification loss: 0.00001 | Regression loss: 0.01365 | Running loss: 0.01085\n",
            "Epoch: 187 | Iteration: 68 | Classification loss: 0.00002 | Regression loss: 0.01543 | Running loss: 0.01088\n",
            "Epoch: 187 | Iteration: 69 | Classification loss: 0.00003 | Regression loss: 0.00977 | Running loss: 0.01089\n",
            "Epoch: 187 | Iteration: 70 | Classification loss: 0.00003 | Regression loss: 0.01524 | Running loss: 0.01089\n",
            "Epoch: 187 | Iteration: 71 | Classification loss: 0.00003 | Regression loss: 0.01550 | Running loss: 0.01091\n",
            "Epoch: 187 | Iteration: 72 | Classification loss: 0.00005 | Regression loss: 0.01019 | Running loss: 0.01090\n",
            "Epoch: 187 | Iteration: 73 | Classification loss: 0.00002 | Regression loss: 0.01818 | Running loss: 0.01091\n",
            "Epoch: 187 | Iteration: 74 | Classification loss: 0.00001 | Regression loss: 0.02791 | Running loss: 0.01095\n",
            "Epoch: 187 | Iteration: 75 | Classification loss: 0.00001 | Regression loss: 0.01185 | Running loss: 0.01094\n",
            "Epoch: 187 | Iteration: 76 | Classification loss: 0.00001 | Regression loss: 0.00672 | Running loss: 0.01094\n",
            "Epoch: 187 | Iteration: 77 | Classification loss: 0.00001 | Regression loss: 0.00514 | Running loss: 0.01092\n",
            "Epoch: 187 | Iteration: 78 | Classification loss: 0.00002 | Regression loss: 0.01255 | Running loss: 0.01094\n",
            "Epoch: 187 | Iteration: 79 | Classification loss: 0.00001 | Regression loss: 0.01012 | Running loss: 0.01093\n",
            "Epoch: 187 | Iteration: 80 | Classification loss: 0.00003 | Regression loss: 0.01299 | Running loss: 0.01093\n",
            "Epoch: 187 | Iteration: 81 | Classification loss: 0.00001 | Regression loss: 0.00513 | Running loss: 0.01086\n",
            "Epoch: 187 | Iteration: 82 | Classification loss: 0.00002 | Regression loss: 0.01643 | Running loss: 0.01088\n",
            "Epoch: 187 | Iteration: 83 | Classification loss: 0.00001 | Regression loss: 0.00697 | Running loss: 0.01088\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.7294166027957658\n",
            "Precision:  0.552\n",
            "Recall:  0.8181818181818182\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}, {0: (0.7314384456550078, 253.0)}, {0: (0.7334466242447818, 253.0)}, {0: (0.7357024222917343, 253.0)}, {0: (0.7381550871546381, 253.0)}, {0: (0.739092434551302, 253.0)}, {0: (0.72091448182607, 253.0)}, {0: (0.7389512050165785, 253.0)}, {0: (0.73050932626622, 253.0)}, {0: (0.7263226120133723, 253.0)}, {0: (0.7283463725823673, 253.0)}, {0: (0.7254913772539314, 253.0)}, {0: (0.7281179290861395, 253.0)}, {0: (0.7260020498565122, 253.0)}, {0: (0.7217236108381175, 253.0)}, {0: (0.7267514829312511, 253.0)}, {0: (0.7227483652714889, 253.0)}, {0: (0.7241298168882813, 253.0)}, {0: (0.7261051666397325, 253.0)}, {0: (0.7277246704086413, 253.0)}, {0: (0.7259496886680885, 253.0)}, {0: (0.7290251963867072, 253.0)}, {0: (0.7242894157206492, 253.0)}, {0: (0.7329303843522665, 253.0)}, {0: (0.733154886135177, 253.0)}, {0: (0.7291842105419786, 253.0)}, {0: (0.7251405627702188, 253.0)}, {0: (0.731154087710585, 253.0)}, {0: (0.7317124239374072, 253.0)}, {0: (0.7263717757478444, 253.0)}, {0: (0.7278912613628667, 253.0)}, {0: (0.7330251175928468, 253.0)}, {0: (0.7278399291817672, 253.0)}, {0: (0.728608785848319, 253.0)}, {0: (0.7294166027957658, 253.0)}]\n",
            "Epoch: 188 | Iteration: 0 | Classification loss: 0.00001 | Regression loss: 0.01684 | Running loss: 0.01089\n",
            "Epoch: 188 | Iteration: 1 | Classification loss: 0.00002 | Regression loss: 0.01204 | Running loss: 0.01091\n",
            "Epoch: 188 | Iteration: 2 | Classification loss: 0.00001 | Regression loss: 0.00983 | Running loss: 0.01090\n",
            "Epoch: 188 | Iteration: 3 | Classification loss: 0.00001 | Regression loss: 0.02674 | Running loss: 0.01093\n",
            "Epoch: 188 | Iteration: 4 | Classification loss: 0.00002 | Regression loss: 0.00986 | Running loss: 0.01094\n",
            "Epoch: 188 | Iteration: 5 | Classification loss: 0.00000 | Regression loss: 0.00384 | Running loss: 0.01093\n",
            "Epoch: 188 | Iteration: 6 | Classification loss: 0.00002 | Regression loss: 0.01095 | Running loss: 0.01090\n",
            "Epoch: 188 | Iteration: 7 | Classification loss: 0.00002 | Regression loss: 0.00906 | Running loss: 0.01091\n",
            "Epoch: 188 | Iteration: 8 | Classification loss: 0.00008 | Regression loss: 0.01870 | Running loss: 0.01094\n",
            "Epoch: 188 | Iteration: 9 | Classification loss: 0.00009 | Regression loss: 0.00702 | Running loss: 0.01094\n",
            "Epoch: 188 | Iteration: 10 | Classification loss: 0.00004 | Regression loss: 0.01550 | Running loss: 0.01094\n",
            "Epoch: 188 | Iteration: 11 | Classification loss: 0.00001 | Regression loss: 0.00633 | Running loss: 0.01095\n",
            "Epoch: 188 | Iteration: 12 | Classification loss: 0.00001 | Regression loss: 0.00329 | Running loss: 0.01093\n",
            "Epoch: 188 | Iteration: 13 | Classification loss: 0.00003 | Regression loss: 0.00441 | Running loss: 0.01093\n",
            "Epoch: 188 | Iteration: 14 | Classification loss: 0.00003 | Regression loss: 0.01574 | Running loss: 0.01094\n",
            "Epoch: 188 | Iteration: 15 | Classification loss: 0.00001 | Regression loss: 0.01065 | Running loss: 0.01095\n",
            "Epoch: 188 | Iteration: 16 | Classification loss: 0.00002 | Regression loss: 0.00596 | Running loss: 0.01093\n",
            "Epoch: 188 | Iteration: 17 | Classification loss: 0.00003 | Regression loss: 0.01901 | Running loss: 0.01092\n",
            "Epoch: 188 | Iteration: 18 | Classification loss: 0.00001 | Regression loss: 0.00336 | Running loss: 0.01091\n",
            "Epoch: 188 | Iteration: 19 | Classification loss: 0.00001 | Regression loss: 0.01389 | Running loss: 0.01092\n",
            "Epoch: 188 | Iteration: 20 | Classification loss: 0.00001 | Regression loss: 0.01068 | Running loss: 0.01090\n",
            "Epoch: 188 | Iteration: 21 | Classification loss: 0.00002 | Regression loss: 0.01303 | Running loss: 0.01090\n",
            "Epoch: 188 | Iteration: 22 | Classification loss: 0.00001 | Regression loss: 0.01089 | Running loss: 0.01090\n",
            "Epoch: 188 | Iteration: 23 | Classification loss: 0.00005 | Regression loss: 0.01070 | Running loss: 0.01089\n",
            "Epoch: 188 | Iteration: 24 | Classification loss: 0.00002 | Regression loss: 0.01124 | Running loss: 0.01089\n",
            "Epoch: 188 | Iteration: 25 | Classification loss: 0.00002 | Regression loss: 0.00454 | Running loss: 0.01089\n",
            "Epoch: 188 | Iteration: 26 | Classification loss: 0.00001 | Regression loss: 0.01195 | Running loss: 0.01090\n",
            "Epoch: 188 | Iteration: 27 | Classification loss: 0.00001 | Regression loss: 0.00665 | Running loss: 0.01090\n",
            "Epoch: 188 | Iteration: 28 | Classification loss: 0.00002 | Regression loss: 0.01426 | Running loss: 0.01091\n",
            "Epoch: 188 | Iteration: 29 | Classification loss: 0.00001 | Regression loss: 0.00602 | Running loss: 0.01091\n",
            "Epoch: 188 | Iteration: 30 | Classification loss: 0.00002 | Regression loss: 0.01755 | Running loss: 0.01094\n",
            "Epoch: 188 | Iteration: 31 | Classification loss: 0.00001 | Regression loss: 0.00979 | Running loss: 0.01093\n",
            "Epoch: 188 | Iteration: 32 | Classification loss: 0.00003 | Regression loss: 0.01579 | Running loss: 0.01095\n",
            "Epoch: 188 | Iteration: 33 | Classification loss: 0.00002 | Regression loss: 0.00985 | Running loss: 0.01095\n",
            "Epoch: 188 | Iteration: 34 | Classification loss: 0.00001 | Regression loss: 0.01069 | Running loss: 0.01096\n",
            "Epoch: 188 | Iteration: 35 | Classification loss: 0.00002 | Regression loss: 0.00366 | Running loss: 0.01095\n",
            "Epoch: 188 | Iteration: 36 | Classification loss: 0.00001 | Regression loss: 0.01231 | Running loss: 0.01095\n",
            "Epoch: 188 | Iteration: 37 | Classification loss: 0.00001 | Regression loss: 0.00926 | Running loss: 0.01096\n",
            "Epoch: 188 | Iteration: 38 | Classification loss: 0.00000 | Regression loss: 0.00802 | Running loss: 0.01095\n",
            "Epoch: 188 | Iteration: 39 | Classification loss: 0.00001 | Regression loss: 0.00693 | Running loss: 0.01093\n",
            "Epoch: 188 | Iteration: 40 | Classification loss: 0.00002 | Regression loss: 0.01492 | Running loss: 0.01094\n",
            "Epoch: 188 | Iteration: 41 | Classification loss: 0.00001 | Regression loss: 0.00681 | Running loss: 0.01092\n",
            "Epoch: 188 | Iteration: 42 | Classification loss: 0.00001 | Regression loss: 0.01084 | Running loss: 0.01093\n",
            "Epoch: 188 | Iteration: 43 | Classification loss: 0.00001 | Regression loss: 0.01584 | Running loss: 0.01094\n",
            "Epoch: 188 | Iteration: 44 | Classification loss: 0.00002 | Regression loss: 0.01008 | Running loss: 0.01095\n",
            "Epoch: 188 | Iteration: 45 | Classification loss: 0.00003 | Regression loss: 0.00976 | Running loss: 0.01096\n",
            "Epoch: 188 | Iteration: 46 | Classification loss: 0.00002 | Regression loss: 0.01721 | Running loss: 0.01097\n",
            "Epoch: 188 | Iteration: 47 | Classification loss: 0.00001 | Regression loss: 0.00560 | Running loss: 0.01097\n",
            "Epoch: 188 | Iteration: 48 | Classification loss: 0.00002 | Regression loss: 0.01758 | Running loss: 0.01098\n",
            "Epoch: 188 | Iteration: 49 | Classification loss: 0.00006 | Regression loss: 0.01216 | Running loss: 0.01096\n",
            "Epoch: 188 | Iteration: 50 | Classification loss: 0.00001 | Regression loss: 0.00327 | Running loss: 0.01095\n",
            "Epoch: 188 | Iteration: 51 | Classification loss: 0.00005 | Regression loss: 0.01300 | Running loss: 0.01095\n",
            "Epoch: 188 | Iteration: 52 | Classification loss: 0.00001 | Regression loss: 0.00381 | Running loss: 0.01094\n",
            "Epoch: 188 | Iteration: 53 | Classification loss: 0.00001 | Regression loss: 0.01969 | Running loss: 0.01097\n",
            "Epoch: 188 | Iteration: 54 | Classification loss: 0.00002 | Regression loss: 0.01445 | Running loss: 0.01097\n",
            "Epoch: 188 | Iteration: 55 | Classification loss: 0.00001 | Regression loss: 0.00415 | Running loss: 0.01097\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 188.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 188 | Iteration: 57 | Classification loss: 0.00002 | Regression loss: 0.00616 | Running loss: 0.01095\n",
            "Epoch: 188 | Iteration: 58 | Classification loss: 0.00001 | Regression loss: 0.01245 | Running loss: 0.01094\n",
            "Epoch: 188 | Iteration: 59 | Classification loss: 0.00001 | Regression loss: 0.00549 | Running loss: 0.01093\n",
            "Epoch: 188 | Iteration: 60 | Classification loss: 0.00000 | Regression loss: 0.01175 | Running loss: 0.01092\n",
            "Epoch: 188 | Iteration: 61 | Classification loss: 0.00001 | Regression loss: 0.00318 | Running loss: 0.01089\n",
            "Epoch: 188 | Iteration: 62 | Classification loss: 0.00001 | Regression loss: 0.00518 | Running loss: 0.01089\n",
            "Epoch: 188 | Iteration: 63 | Classification loss: 0.00001 | Regression loss: 0.00930 | Running loss: 0.01088\n",
            "Epoch: 188 | Iteration: 64 | Classification loss: 0.00001 | Regression loss: 0.01679 | Running loss: 0.01089\n",
            "Epoch: 188 | Iteration: 65 | Classification loss: 0.00001 | Regression loss: 0.00338 | Running loss: 0.01088\n",
            "Epoch: 188 | Iteration: 66 | Classification loss: 0.00002 | Regression loss: 0.01227 | Running loss: 0.01087\n",
            "Epoch: 188 | Iteration: 67 | Classification loss: 0.00001 | Regression loss: 0.00589 | Running loss: 0.01087\n",
            "Epoch: 188 | Iteration: 68 | Classification loss: 0.00007 | Regression loss: 0.04124 | Running loss: 0.01093\n",
            "Epoch: 188 | Iteration: 69 | Classification loss: 0.00002 | Regression loss: 0.02554 | Running loss: 0.01095\n",
            "Epoch: 188 | Iteration: 70 | Classification loss: 0.00001 | Regression loss: 0.00878 | Running loss: 0.01094\n",
            "Epoch: 188 | Iteration: 71 | Classification loss: 0.00001 | Regression loss: 0.02894 | Running loss: 0.01096\n",
            "Epoch: 188 | Iteration: 72 | Classification loss: 0.00001 | Regression loss: 0.01049 | Running loss: 0.01096\n",
            "Epoch: 188 | Iteration: 73 | Classification loss: 0.00002 | Regression loss: 0.01069 | Running loss: 0.01095\n",
            "Epoch: 188 | Iteration: 74 | Classification loss: 0.00002 | Regression loss: 0.00932 | Running loss: 0.01095\n",
            "Epoch: 188 | Iteration: 75 | Classification loss: 0.00002 | Regression loss: 0.00575 | Running loss: 0.01094\n",
            "Epoch: 188 | Iteration: 76 | Classification loss: 0.00001 | Regression loss: 0.00494 | Running loss: 0.01093\n",
            "Epoch: 188 | Iteration: 77 | Classification loss: 0.00001 | Regression loss: 0.00535 | Running loss: 0.01093\n",
            "Epoch: 188 | Iteration: 78 | Classification loss: 0.00005 | Regression loss: 0.01145 | Running loss: 0.01094\n",
            "Epoch: 188 | Iteration: 79 | Classification loss: 0.00002 | Regression loss: 0.00826 | Running loss: 0.01094\n",
            "Epoch: 188 | Iteration: 80 | Classification loss: 0.00002 | Regression loss: 0.01271 | Running loss: 0.01094\n",
            "Epoch: 188 | Iteration: 81 | Classification loss: 0.00001 | Regression loss: 0.00492 | Running loss: 0.01094\n",
            "Epoch: 188 | Iteration: 82 | Classification loss: 0.00002 | Regression loss: 0.01284 | Running loss: 0.01088\n",
            "Epoch: 188 | Iteration: 83 | Classification loss: 0.00000 | Regression loss: 0.00432 | Running loss: 0.01088\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.7158140975435932\n",
            "Precision:  0.5464190981432361\n",
            "Recall:  0.8142292490118577\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}, {0: (0.7314384456550078, 253.0)}, {0: (0.7334466242447818, 253.0)}, {0: (0.7357024222917343, 253.0)}, {0: (0.7381550871546381, 253.0)}, {0: (0.739092434551302, 253.0)}, {0: (0.72091448182607, 253.0)}, {0: (0.7389512050165785, 253.0)}, {0: (0.73050932626622, 253.0)}, {0: (0.7263226120133723, 253.0)}, {0: (0.7283463725823673, 253.0)}, {0: (0.7254913772539314, 253.0)}, {0: (0.7281179290861395, 253.0)}, {0: (0.7260020498565122, 253.0)}, {0: (0.7217236108381175, 253.0)}, {0: (0.7267514829312511, 253.0)}, {0: (0.7227483652714889, 253.0)}, {0: (0.7241298168882813, 253.0)}, {0: (0.7261051666397325, 253.0)}, {0: (0.7277246704086413, 253.0)}, {0: (0.7259496886680885, 253.0)}, {0: (0.7290251963867072, 253.0)}, {0: (0.7242894157206492, 253.0)}, {0: (0.7329303843522665, 253.0)}, {0: (0.733154886135177, 253.0)}, {0: (0.7291842105419786, 253.0)}, {0: (0.7251405627702188, 253.0)}, {0: (0.731154087710585, 253.0)}, {0: (0.7317124239374072, 253.0)}, {0: (0.7263717757478444, 253.0)}, {0: (0.7278912613628667, 253.0)}, {0: (0.7330251175928468, 253.0)}, {0: (0.7278399291817672, 253.0)}, {0: (0.728608785848319, 253.0)}, {0: (0.7294166027957658, 253.0)}, {0: (0.7158140975435932, 253.0)}]\n",
            "Epoch: 189 | Iteration: 0 | Classification loss: 0.00003 | Regression loss: 0.01454 | Running loss: 0.01088\n",
            "Epoch: 189 | Iteration: 1 | Classification loss: 0.00002 | Regression loss: 0.01235 | Running loss: 0.01085\n",
            "Epoch: 189 | Iteration: 2 | Classification loss: 0.00001 | Regression loss: 0.00585 | Running loss: 0.01084\n",
            "Epoch: 189 | Iteration: 3 | Classification loss: 0.00000 | Regression loss: 0.00277 | Running loss: 0.01084\n",
            "Epoch: 189 | Iteration: 4 | Classification loss: 0.00001 | Regression loss: 0.01122 | Running loss: 0.01084\n",
            "Epoch: 189 | Iteration: 5 | Classification loss: 0.00001 | Regression loss: 0.00967 | Running loss: 0.01084\n",
            "Epoch: 189 | Iteration: 6 | Classification loss: 0.00002 | Regression loss: 0.01123 | Running loss: 0.01084\n",
            "Epoch: 189 | Iteration: 7 | Classification loss: 0.00001 | Regression loss: 0.01062 | Running loss: 0.01086\n",
            "Epoch: 189 | Iteration: 8 | Classification loss: 0.00001 | Regression loss: 0.00365 | Running loss: 0.01085\n",
            "Epoch: 189 | Iteration: 9 | Classification loss: 0.00002 | Regression loss: 0.00810 | Running loss: 0.01085\n",
            "Epoch: 189 | Iteration: 10 | Classification loss: 0.00002 | Regression loss: 0.00788 | Running loss: 0.01085\n",
            "Epoch: 189 | Iteration: 11 | Classification loss: 0.00001 | Regression loss: 0.00892 | Running loss: 0.01085\n",
            "Epoch: 189 | Iteration: 12 | Classification loss: 0.00001 | Regression loss: 0.00607 | Running loss: 0.01083\n",
            "Epoch: 189 | Iteration: 13 | Classification loss: 0.00001 | Regression loss: 0.01381 | Running loss: 0.01085\n",
            "Epoch: 189 | Iteration: 14 | Classification loss: 0.00002 | Regression loss: 0.00882 | Running loss: 0.01086\n",
            "Epoch: 189 | Iteration: 15 | Classification loss: 0.00002 | Regression loss: 0.01433 | Running loss: 0.01088\n",
            "Epoch: 189 | Iteration: 16 | Classification loss: 0.00002 | Regression loss: 0.01173 | Running loss: 0.01088\n",
            "Epoch: 189 | Iteration: 17 | Classification loss: 0.00001 | Regression loss: 0.00543 | Running loss: 0.01087\n",
            "Epoch: 189 | Iteration: 18 | Classification loss: 0.00001 | Regression loss: 0.00638 | Running loss: 0.01087\n",
            "Epoch: 189 | Iteration: 19 | Classification loss: 0.00001 | Regression loss: 0.00383 | Running loss: 0.01086\n",
            "Epoch: 189 | Iteration: 20 | Classification loss: 0.00001 | Regression loss: 0.00329 | Running loss: 0.01086\n",
            "Epoch: 189 | Iteration: 21 | Classification loss: 0.00004 | Regression loss: 0.00817 | Running loss: 0.01087\n",
            "Epoch: 189 | Iteration: 22 | Classification loss: 0.00004 | Regression loss: 0.01472 | Running loss: 0.01088\n",
            "Epoch: 189 | Iteration: 23 | Classification loss: 0.00004 | Regression loss: 0.01279 | Running loss: 0.01090\n",
            "Epoch: 189 | Iteration: 24 | Classification loss: 0.00000 | Regression loss: 0.00312 | Running loss: 0.01087\n",
            "Epoch: 189 | Iteration: 25 | Classification loss: 0.00004 | Regression loss: 0.01374 | Running loss: 0.01088\n",
            "Epoch: 189 | Iteration: 26 | Classification loss: 0.00001 | Regression loss: 0.00873 | Running loss: 0.01087\n",
            "Epoch: 189 | Iteration: 27 | Classification loss: 0.00002 | Regression loss: 0.01362 | Running loss: 0.01087\n",
            "Epoch: 189 | Iteration: 28 | Classification loss: 0.00002 | Regression loss: 0.01206 | Running loss: 0.01087\n",
            "Epoch: 189 | Iteration: 29 | Classification loss: 0.00001 | Regression loss: 0.00532 | Running loss: 0.01085\n",
            "Epoch: 189 | Iteration: 30 | Classification loss: 0.00003 | Regression loss: 0.00379 | Running loss: 0.01083\n",
            "Epoch: 189 | Iteration: 31 | Classification loss: 0.00002 | Regression loss: 0.01709 | Running loss: 0.01083\n",
            "Epoch: 189 | Iteration: 32 | Classification loss: 0.00002 | Regression loss: 0.01057 | Running loss: 0.01083\n",
            "Epoch: 189 | Iteration: 33 | Classification loss: 0.00001 | Regression loss: 0.00977 | Running loss: 0.01082\n",
            "Epoch: 189 | Iteration: 34 | Classification loss: 0.00002 | Regression loss: 0.01433 | Running loss: 0.01081\n",
            "Epoch: 189 | Iteration: 35 | Classification loss: 0.00001 | Regression loss: 0.01203 | Running loss: 0.01083\n",
            "Epoch: 189 | Iteration: 36 | Classification loss: 0.00002 | Regression loss: 0.02030 | Running loss: 0.01084\n",
            "Epoch: 189 | Iteration: 37 | Classification loss: 0.00001 | Regression loss: 0.00946 | Running loss: 0.01082\n",
            "Epoch: 189 | Iteration: 38 | Classification loss: 0.00001 | Regression loss: 0.00453 | Running loss: 0.01080\n",
            "Epoch: 189 | Iteration: 39 | Classification loss: 0.00001 | Regression loss: 0.00456 | Running loss: 0.01080\n",
            "Epoch: 189 | Iteration: 40 | Classification loss: 0.00001 | Regression loss: 0.01621 | Running loss: 0.01080\n",
            "Epoch: 189 | Iteration: 41 | Classification loss: 0.00001 | Regression loss: 0.00996 | Running loss: 0.01082\n",
            "Epoch: 189 | Iteration: 42 | Classification loss: 0.00001 | Regression loss: 0.00427 | Running loss: 0.01081\n",
            "Epoch: 189 | Iteration: 43 | Classification loss: 0.00004 | Regression loss: 0.01255 | Running loss: 0.01080\n",
            "Epoch: 189 | Iteration: 44 | Classification loss: 0.00002 | Regression loss: 0.01188 | Running loss: 0.01082\n",
            "Epoch: 189 | Iteration: 45 | Classification loss: 0.00001 | Regression loss: 0.01050 | Running loss: 0.01082\n",
            "Epoch: 189 | Iteration: 46 | Classification loss: 0.00002 | Regression loss: 0.00272 | Running loss: 0.01079\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 188.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 189 | Iteration: 48 | Classification loss: 0.00001 | Regression loss: 0.00358 | Running loss: 0.01077\n",
            "Epoch: 189 | Iteration: 49 | Classification loss: 0.00001 | Regression loss: 0.01085 | Running loss: 0.01074\n",
            "Epoch: 189 | Iteration: 50 | Classification loss: 0.00004 | Regression loss: 0.00871 | Running loss: 0.01072\n",
            "Epoch: 189 | Iteration: 51 | Classification loss: 0.00001 | Regression loss: 0.01631 | Running loss: 0.01072\n",
            "Epoch: 189 | Iteration: 52 | Classification loss: 0.00002 | Regression loss: 0.00413 | Running loss: 0.01071\n",
            "Epoch: 189 | Iteration: 53 | Classification loss: 0.00003 | Regression loss: 0.01302 | Running loss: 0.01073\n",
            "Epoch: 189 | Iteration: 54 | Classification loss: 0.00001 | Regression loss: 0.02589 | Running loss: 0.01077\n",
            "Epoch: 189 | Iteration: 55 | Classification loss: 0.00001 | Regression loss: 0.00653 | Running loss: 0.01072\n",
            "Epoch: 189 | Iteration: 56 | Classification loss: 0.00007 | Regression loss: 0.01700 | Running loss: 0.01073\n",
            "Epoch: 189 | Iteration: 57 | Classification loss: 0.00002 | Regression loss: 0.00532 | Running loss: 0.01073\n",
            "Epoch: 189 | Iteration: 58 | Classification loss: 0.00001 | Regression loss: 0.02473 | Running loss: 0.01075\n",
            "Epoch: 189 | Iteration: 59 | Classification loss: 0.00002 | Regression loss: 0.01215 | Running loss: 0.01069\n",
            "Epoch: 189 | Iteration: 60 | Classification loss: 0.00003 | Regression loss: 0.01224 | Running loss: 0.01070\n",
            "Epoch: 189 | Iteration: 61 | Classification loss: 0.00001 | Regression loss: 0.00883 | Running loss: 0.01071\n",
            "Epoch: 189 | Iteration: 62 | Classification loss: 0.00001 | Regression loss: 0.00459 | Running loss: 0.01071\n",
            "Epoch: 189 | Iteration: 63 | Classification loss: 0.00002 | Regression loss: 0.00873 | Running loss: 0.01070\n",
            "Epoch: 189 | Iteration: 64 | Classification loss: 0.00001 | Regression loss: 0.00975 | Running loss: 0.01069\n",
            "Epoch: 189 | Iteration: 65 | Classification loss: 0.00001 | Regression loss: 0.02798 | Running loss: 0.01070\n",
            "Epoch: 189 | Iteration: 66 | Classification loss: 0.00001 | Regression loss: 0.01798 | Running loss: 0.01071\n",
            "Epoch: 189 | Iteration: 67 | Classification loss: 0.00001 | Regression loss: 0.00375 | Running loss: 0.01070\n",
            "Epoch: 189 | Iteration: 68 | Classification loss: 0.00002 | Regression loss: 0.00871 | Running loss: 0.01071\n",
            "Epoch: 189 | Iteration: 69 | Classification loss: 0.00005 | Regression loss: 0.04008 | Running loss: 0.01076\n",
            "Epoch: 189 | Iteration: 70 | Classification loss: 0.00001 | Regression loss: 0.00662 | Running loss: 0.01074\n",
            "Epoch: 189 | Iteration: 71 | Classification loss: 0.00000 | Regression loss: 0.00802 | Running loss: 0.01074\n",
            "Epoch: 189 | Iteration: 72 | Classification loss: 0.00007 | Regression loss: 0.01226 | Running loss: 0.01074\n",
            "Epoch: 189 | Iteration: 73 | Classification loss: 0.00001 | Regression loss: 0.00582 | Running loss: 0.01073\n",
            "Epoch: 189 | Iteration: 74 | Classification loss: 0.00002 | Regression loss: 0.00602 | Running loss: 0.01071\n",
            "Epoch: 189 | Iteration: 75 | Classification loss: 0.00002 | Regression loss: 0.01219 | Running loss: 0.01071\n",
            "Epoch: 189 | Iteration: 76 | Classification loss: 0.00002 | Regression loss: 0.01278 | Running loss: 0.01072\n",
            "Epoch: 189 | Iteration: 77 | Classification loss: 0.00001 | Regression loss: 0.00950 | Running loss: 0.01073\n",
            "Epoch: 189 | Iteration: 78 | Classification loss: 0.00003 | Regression loss: 0.01824 | Running loss: 0.01075\n",
            "Epoch: 189 | Iteration: 79 | Classification loss: 0.00001 | Regression loss: 0.00659 | Running loss: 0.01074\n",
            "Epoch: 189 | Iteration: 80 | Classification loss: 0.00001 | Regression loss: 0.00588 | Running loss: 0.01073\n",
            "Epoch: 189 | Iteration: 81 | Classification loss: 0.00003 | Regression loss: 0.01666 | Running loss: 0.01075\n",
            "Epoch: 189 | Iteration: 82 | Classification loss: 0.00010 | Regression loss: 0.00715 | Running loss: 0.01075\n",
            "Epoch: 189 | Iteration: 83 | Classification loss: 0.00002 | Regression loss: 0.01040 | Running loss: 0.01077\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.7230740429361691\n",
            "Precision:  0.552\n",
            "Recall:  0.8181818181818182\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}, {0: (0.7314384456550078, 253.0)}, {0: (0.7334466242447818, 253.0)}, {0: (0.7357024222917343, 253.0)}, {0: (0.7381550871546381, 253.0)}, {0: (0.739092434551302, 253.0)}, {0: (0.72091448182607, 253.0)}, {0: (0.7389512050165785, 253.0)}, {0: (0.73050932626622, 253.0)}, {0: (0.7263226120133723, 253.0)}, {0: (0.7283463725823673, 253.0)}, {0: (0.7254913772539314, 253.0)}, {0: (0.7281179290861395, 253.0)}, {0: (0.7260020498565122, 253.0)}, {0: (0.7217236108381175, 253.0)}, {0: (0.7267514829312511, 253.0)}, {0: (0.7227483652714889, 253.0)}, {0: (0.7241298168882813, 253.0)}, {0: (0.7261051666397325, 253.0)}, {0: (0.7277246704086413, 253.0)}, {0: (0.7259496886680885, 253.0)}, {0: (0.7290251963867072, 253.0)}, {0: (0.7242894157206492, 253.0)}, {0: (0.7329303843522665, 253.0)}, {0: (0.733154886135177, 253.0)}, {0: (0.7291842105419786, 253.0)}, {0: (0.7251405627702188, 253.0)}, {0: (0.731154087710585, 253.0)}, {0: (0.7317124239374072, 253.0)}, {0: (0.7263717757478444, 253.0)}, {0: (0.7278912613628667, 253.0)}, {0: (0.7330251175928468, 253.0)}, {0: (0.7278399291817672, 253.0)}, {0: (0.728608785848319, 253.0)}, {0: (0.7294166027957658, 253.0)}, {0: (0.7158140975435932, 253.0)}, {0: (0.7230740429361691, 253.0)}]\n",
            "Epoch: 190 | Iteration: 0 | Classification loss: 0.00001 | Regression loss: 0.00448 | Running loss: 0.01074\n",
            "Epoch: 190 | Iteration: 1 | Classification loss: 0.00003 | Regression loss: 0.00924 | Running loss: 0.01075\n",
            "Epoch: 190 | Iteration: 2 | Classification loss: 0.00001 | Regression loss: 0.00862 | Running loss: 0.01075\n",
            "Epoch: 190 | Iteration: 3 | Classification loss: 0.00003 | Regression loss: 0.01240 | Running loss: 0.01077\n",
            "Epoch: 190 | Iteration: 4 | Classification loss: 0.00002 | Regression loss: 0.01334 | Running loss: 0.01078\n",
            "Epoch: 190 | Iteration: 5 | Classification loss: 0.00002 | Regression loss: 0.01266 | Running loss: 0.01079\n",
            "Epoch: 190 | Iteration: 6 | Classification loss: 0.00002 | Regression loss: 0.02466 | Running loss: 0.01082\n",
            "Epoch: 190 | Iteration: 7 | Classification loss: 0.00001 | Regression loss: 0.00416 | Running loss: 0.01081\n",
            "Epoch: 190 | Iteration: 8 | Classification loss: 0.00001 | Regression loss: 0.00385 | Running loss: 0.01080\n",
            "Epoch: 190 | Iteration: 9 | Classification loss: 0.00001 | Regression loss: 0.01574 | Running loss: 0.01081\n",
            "Epoch: 190 | Iteration: 10 | Classification loss: 0.00001 | Regression loss: 0.02827 | Running loss: 0.01084\n",
            "Epoch: 190 | Iteration: 11 | Classification loss: 0.00001 | Regression loss: 0.01450 | Running loss: 0.01086\n",
            "Epoch: 190 | Iteration: 12 | Classification loss: 0.00001 | Regression loss: 0.00590 | Running loss: 0.01086\n",
            "Epoch: 190 | Iteration: 13 | Classification loss: 0.00002 | Regression loss: 0.01396 | Running loss: 0.01088\n",
            "Epoch: 190 | Iteration: 14 | Classification loss: 0.00000 | Regression loss: 0.00274 | Running loss: 0.01082\n",
            "Epoch: 190 | Iteration: 15 | Classification loss: 0.00002 | Regression loss: 0.00550 | Running loss: 0.01078\n",
            "Epoch: 190 | Iteration: 16 | Classification loss: 0.00001 | Regression loss: 0.00928 | Running loss: 0.01078\n",
            "Epoch: 190 | Iteration: 17 | Classification loss: 0.00002 | Regression loss: 0.00954 | Running loss: 0.01079\n",
            "Epoch: 190 | Iteration: 18 | Classification loss: 0.00001 | Regression loss: 0.00557 | Running loss: 0.01077\n",
            "Epoch: 190 | Iteration: 19 | Classification loss: 0.00001 | Regression loss: 0.00919 | Running loss: 0.01078\n",
            "Epoch: 190 | Iteration: 20 | Classification loss: 0.00001 | Regression loss: 0.01035 | Running loss: 0.01078\n",
            "Epoch: 190 | Iteration: 21 | Classification loss: 0.00002 | Regression loss: 0.01592 | Running loss: 0.01079\n",
            "Epoch: 190 | Iteration: 22 | Classification loss: 0.00003 | Regression loss: 0.01183 | Running loss: 0.01077\n",
            "Epoch: 190 | Iteration: 23 | Classification loss: 0.00004 | Regression loss: 0.01509 | Running loss: 0.01079\n",
            "Epoch: 190 | Iteration: 24 | Classification loss: 0.00001 | Regression loss: 0.01175 | Running loss: 0.01079\n",
            "Epoch: 190 | Iteration: 25 | Classification loss: 0.00002 | Regression loss: 0.01129 | Running loss: 0.01080\n",
            "Epoch: 190 | Iteration: 26 | Classification loss: 0.00001 | Regression loss: 0.00627 | Running loss: 0.01078\n",
            "Epoch: 190 | Iteration: 27 | Classification loss: 0.00001 | Regression loss: 0.02599 | Running loss: 0.01081\n",
            "Epoch: 190 | Iteration: 28 | Classification loss: 0.00002 | Regression loss: 0.00803 | Running loss: 0.01081\n",
            "Epoch: 190 | Iteration: 29 | Classification loss: 0.00001 | Regression loss: 0.01003 | Running loss: 0.01080\n",
            "Epoch: 190 | Iteration: 30 | Classification loss: 0.00008 | Regression loss: 0.00574 | Running loss: 0.01080\n",
            "Epoch: 190 | Iteration: 31 | Classification loss: 0.00001 | Regression loss: 0.00328 | Running loss: 0.01079\n",
            "Epoch: 190 | Iteration: 32 | Classification loss: 0.00001 | Regression loss: 0.01010 | Running loss: 0.01077\n",
            "Epoch: 190 | Iteration: 33 | Classification loss: 0.00005 | Regression loss: 0.01239 | Running loss: 0.01078\n",
            "Epoch: 190 | Iteration: 34 | Classification loss: 0.00003 | Regression loss: 0.00394 | Running loss: 0.01078\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 190.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 190 | Iteration: 36 | Classification loss: 0.00002 | Regression loss: 0.00443 | Running loss: 0.01077\n",
            "Epoch: 190 | Iteration: 37 | Classification loss: 0.00001 | Regression loss: 0.00514 | Running loss: 0.01075\n",
            "Epoch: 190 | Iteration: 38 | Classification loss: 0.00002 | Regression loss: 0.01836 | Running loss: 0.01075\n",
            "Epoch: 190 | Iteration: 39 | Classification loss: 0.00000 | Regression loss: 0.00774 | Running loss: 0.01074\n",
            "Epoch: 190 | Iteration: 40 | Classification loss: 0.00002 | Regression loss: 0.01087 | Running loss: 0.01074\n",
            "Epoch: 190 | Iteration: 41 | Classification loss: 0.00001 | Regression loss: 0.00524 | Running loss: 0.01073\n",
            "Epoch: 190 | Iteration: 42 | Classification loss: 0.00001 | Regression loss: 0.02184 | Running loss: 0.01077\n",
            "Epoch: 190 | Iteration: 43 | Classification loss: 0.00001 | Regression loss: 0.00338 | Running loss: 0.01075\n",
            "Epoch: 190 | Iteration: 44 | Classification loss: 0.00001 | Regression loss: 0.00532 | Running loss: 0.01074\n",
            "Epoch: 190 | Iteration: 45 | Classification loss: 0.00002 | Regression loss: 0.00972 | Running loss: 0.01074\n",
            "Epoch: 190 | Iteration: 46 | Classification loss: 0.00001 | Regression loss: 0.00649 | Running loss: 0.01074\n",
            "Epoch: 190 | Iteration: 47 | Classification loss: 0.00003 | Regression loss: 0.01664 | Running loss: 0.01077\n",
            "Epoch: 190 | Iteration: 48 | Classification loss: 0.00002 | Regression loss: 0.00607 | Running loss: 0.01076\n",
            "Epoch: 190 | Iteration: 49 | Classification loss: 0.00002 | Regression loss: 0.01160 | Running loss: 0.01076\n",
            "Epoch: 190 | Iteration: 50 | Classification loss: 0.00002 | Regression loss: 0.01176 | Running loss: 0.01077\n",
            "Epoch: 190 | Iteration: 51 | Classification loss: 0.00001 | Regression loss: 0.00339 | Running loss: 0.01076\n",
            "Epoch: 190 | Iteration: 52 | Classification loss: 0.00001 | Regression loss: 0.00906 | Running loss: 0.01076\n",
            "Epoch: 190 | Iteration: 53 | Classification loss: 0.00001 | Regression loss: 0.00342 | Running loss: 0.01075\n",
            "Epoch: 190 | Iteration: 54 | Classification loss: 0.00003 | Regression loss: 0.00805 | Running loss: 0.01076\n",
            "Epoch: 190 | Iteration: 55 | Classification loss: 0.00002 | Regression loss: 0.01175 | Running loss: 0.01077\n",
            "Epoch: 190 | Iteration: 56 | Classification loss: 0.00005 | Regression loss: 0.04012 | Running loss: 0.01084\n",
            "Epoch: 190 | Iteration: 57 | Classification loss: 0.00001 | Regression loss: 0.01069 | Running loss: 0.01084\n",
            "Epoch: 190 | Iteration: 58 | Classification loss: 0.00001 | Regression loss: 0.01084 | Running loss: 0.01083\n",
            "Epoch: 190 | Iteration: 59 | Classification loss: 0.00002 | Regression loss: 0.01730 | Running loss: 0.01086\n",
            "Epoch: 190 | Iteration: 60 | Classification loss: 0.00001 | Regression loss: 0.00560 | Running loss: 0.01086\n",
            "Epoch: 190 | Iteration: 61 | Classification loss: 0.00008 | Regression loss: 0.01237 | Running loss: 0.01086\n",
            "Epoch: 190 | Iteration: 62 | Classification loss: 0.00002 | Regression loss: 0.01506 | Running loss: 0.01086\n",
            "Epoch: 190 | Iteration: 63 | Classification loss: 0.00001 | Regression loss: 0.01055 | Running loss: 0.01086\n",
            "Epoch: 190 | Iteration: 64 | Classification loss: 0.00001 | Regression loss: 0.00338 | Running loss: 0.01085\n",
            "Epoch: 190 | Iteration: 65 | Classification loss: 0.00007 | Regression loss: 0.01717 | Running loss: 0.01088\n",
            "Epoch: 190 | Iteration: 66 | Classification loss: 0.00001 | Regression loss: 0.00408 | Running loss: 0.01088\n",
            "Epoch: 190 | Iteration: 67 | Classification loss: 0.00001 | Regression loss: 0.01361 | Running loss: 0.01088\n",
            "Epoch: 190 | Iteration: 68 | Classification loss: 0.00001 | Regression loss: 0.01444 | Running loss: 0.01087\n",
            "Epoch: 190 | Iteration: 69 | Classification loss: 0.00001 | Regression loss: 0.00900 | Running loss: 0.01086\n",
            "Epoch: 190 | Iteration: 70 | Classification loss: 0.00001 | Regression loss: 0.01153 | Running loss: 0.01088\n",
            "Epoch: 190 | Iteration: 71 | Classification loss: 0.00000 | Regression loss: 0.00661 | Running loss: 0.01086\n",
            "Epoch: 190 | Iteration: 72 | Classification loss: 0.00001 | Regression loss: 0.00904 | Running loss: 0.01086\n",
            "Epoch: 190 | Iteration: 73 | Classification loss: 0.00004 | Regression loss: 0.01142 | Running loss: 0.01086\n",
            "Epoch: 190 | Iteration: 74 | Classification loss: 0.00001 | Regression loss: 0.00646 | Running loss: 0.01084\n",
            "Epoch: 190 | Iteration: 75 | Classification loss: 0.00003 | Regression loss: 0.00916 | Running loss: 0.01084\n",
            "Epoch: 190 | Iteration: 76 | Classification loss: 0.00001 | Regression loss: 0.01221 | Running loss: 0.01083\n",
            "Epoch: 190 | Iteration: 77 | Classification loss: 0.00002 | Regression loss: 0.00244 | Running loss: 0.01081\n",
            "Epoch: 190 | Iteration: 78 | Classification loss: 0.00001 | Regression loss: 0.01046 | Running loss: 0.01081\n",
            "Epoch: 190 | Iteration: 79 | Classification loss: 0.00002 | Regression loss: 0.00901 | Running loss: 0.01078\n",
            "Epoch: 190 | Iteration: 80 | Classification loss: 0.00002 | Regression loss: 0.01676 | Running loss: 0.01080\n",
            "Epoch: 190 | Iteration: 81 | Classification loss: 0.00002 | Regression loss: 0.01390 | Running loss: 0.01081\n",
            "Epoch: 190 | Iteration: 82 | Classification loss: 0.00004 | Regression loss: 0.00863 | Running loss: 0.01080\n",
            "Epoch: 190 | Iteration: 83 | Classification loss: 0.00002 | Regression loss: 0.00580 | Running loss: 0.01073\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.7241983182897731\n",
            "Precision:  0.5433070866141733\n",
            "Recall:  0.8181818181818182\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}, {0: (0.7314384456550078, 253.0)}, {0: (0.7334466242447818, 253.0)}, {0: (0.7357024222917343, 253.0)}, {0: (0.7381550871546381, 253.0)}, {0: (0.739092434551302, 253.0)}, {0: (0.72091448182607, 253.0)}, {0: (0.7389512050165785, 253.0)}, {0: (0.73050932626622, 253.0)}, {0: (0.7263226120133723, 253.0)}, {0: (0.7283463725823673, 253.0)}, {0: (0.7254913772539314, 253.0)}, {0: (0.7281179290861395, 253.0)}, {0: (0.7260020498565122, 253.0)}, {0: (0.7217236108381175, 253.0)}, {0: (0.7267514829312511, 253.0)}, {0: (0.7227483652714889, 253.0)}, {0: (0.7241298168882813, 253.0)}, {0: (0.7261051666397325, 253.0)}, {0: (0.7277246704086413, 253.0)}, {0: (0.7259496886680885, 253.0)}, {0: (0.7290251963867072, 253.0)}, {0: (0.7242894157206492, 253.0)}, {0: (0.7329303843522665, 253.0)}, {0: (0.733154886135177, 253.0)}, {0: (0.7291842105419786, 253.0)}, {0: (0.7251405627702188, 253.0)}, {0: (0.731154087710585, 253.0)}, {0: (0.7317124239374072, 253.0)}, {0: (0.7263717757478444, 253.0)}, {0: (0.7278912613628667, 253.0)}, {0: (0.7330251175928468, 253.0)}, {0: (0.7278399291817672, 253.0)}, {0: (0.728608785848319, 253.0)}, {0: (0.7294166027957658, 253.0)}, {0: (0.7158140975435932, 253.0)}, {0: (0.7230740429361691, 253.0)}, {0: (0.7241983182897731, 253.0)}]\n",
            "Epoch: 191 | Iteration: 0 | Classification loss: 0.00002 | Regression loss: 0.00808 | Running loss: 0.01072\n",
            "Epoch: 191 | Iteration: 1 | Classification loss: 0.00001 | Regression loss: 0.01482 | Running loss: 0.01073\n",
            "Epoch: 191 | Iteration: 2 | Classification loss: 0.00002 | Regression loss: 0.01863 | Running loss: 0.01075\n",
            "Epoch: 191 | Iteration: 3 | Classification loss: 0.00001 | Regression loss: 0.01416 | Running loss: 0.01076\n",
            "Epoch: 191 | Iteration: 4 | Classification loss: 0.00003 | Regression loss: 0.01446 | Running loss: 0.01078\n",
            "Epoch: 191 | Iteration: 5 | Classification loss: 0.00002 | Regression loss: 0.00940 | Running loss: 0.01078\n",
            "Epoch: 191 | Iteration: 6 | Classification loss: 0.00002 | Regression loss: 0.01108 | Running loss: 0.01077\n",
            "Epoch: 191 | Iteration: 7 | Classification loss: 0.00002 | Regression loss: 0.01186 | Running loss: 0.01077\n",
            "Epoch: 191 | Iteration: 8 | Classification loss: 0.00001 | Regression loss: 0.00518 | Running loss: 0.01077\n",
            "Epoch: 191 | Iteration: 9 | Classification loss: 0.00000 | Regression loss: 0.00308 | Running loss: 0.01074\n",
            "Epoch: 191 | Iteration: 10 | Classification loss: 0.00001 | Regression loss: 0.00890 | Running loss: 0.01075\n",
            "Epoch: 191 | Iteration: 11 | Classification loss: 0.00001 | Regression loss: 0.00512 | Running loss: 0.01075\n",
            "Epoch: 191 | Iteration: 12 | Classification loss: 0.00001 | Regression loss: 0.00888 | Running loss: 0.01069\n",
            "Epoch: 191 | Iteration: 13 | Classification loss: 0.00002 | Regression loss: 0.00423 | Running loss: 0.01067\n",
            "Epoch: 191 | Iteration: 14 | Classification loss: 0.00001 | Regression loss: 0.00714 | Running loss: 0.01065\n",
            "Epoch: 191 | Iteration: 15 | Classification loss: 0.00006 | Regression loss: 0.04009 | Running loss: 0.01071\n",
            "Epoch: 191 | Iteration: 16 | Classification loss: 0.00004 | Regression loss: 0.01148 | Running loss: 0.01071\n",
            "Epoch: 191 | Iteration: 17 | Classification loss: 0.00002 | Regression loss: 0.01413 | Running loss: 0.01071\n",
            "Epoch: 191 | Iteration: 18 | Classification loss: 0.00001 | Regression loss: 0.00398 | Running loss: 0.01070\n",
            "Epoch: 191 | Iteration: 19 | Classification loss: 0.00001 | Regression loss: 0.00387 | Running loss: 0.01067\n",
            "Epoch: 191 | Iteration: 20 | Classification loss: 0.00000 | Regression loss: 0.00639 | Running loss: 0.01067\n",
            "Epoch: 191 | Iteration: 21 | Classification loss: 0.00003 | Regression loss: 0.01907 | Running loss: 0.01069\n",
            "Epoch: 191 | Iteration: 22 | Classification loss: 0.00003 | Regression loss: 0.00343 | Running loss: 0.01068\n",
            "Epoch: 191 | Iteration: 23 | Classification loss: 0.00002 | Regression loss: 0.00559 | Running loss: 0.01067\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 188.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 191 | Iteration: 25 | Classification loss: 0.00001 | Regression loss: 0.00538 | Running loss: 0.01064\n",
            "Epoch: 191 | Iteration: 26 | Classification loss: 0.00001 | Regression loss: 0.00927 | Running loss: 0.01061\n",
            "Epoch: 191 | Iteration: 27 | Classification loss: 0.00002 | Regression loss: 0.00637 | Running loss: 0.01059\n",
            "Epoch: 191 | Iteration: 28 | Classification loss: 0.00002 | Regression loss: 0.01734 | Running loss: 0.01062\n",
            "Epoch: 191 | Iteration: 29 | Classification loss: 0.00002 | Regression loss: 0.01140 | Running loss: 0.01061\n",
            "Epoch: 191 | Iteration: 30 | Classification loss: 0.00006 | Regression loss: 0.01176 | Running loss: 0.01057\n",
            "Epoch: 191 | Iteration: 31 | Classification loss: 0.00002 | Regression loss: 0.00528 | Running loss: 0.01056\n",
            "Epoch: 191 | Iteration: 32 | Classification loss: 0.00001 | Regression loss: 0.00413 | Running loss: 0.01055\n",
            "Epoch: 191 | Iteration: 33 | Classification loss: 0.00001 | Regression loss: 0.00322 | Running loss: 0.01054\n",
            "Epoch: 191 | Iteration: 34 | Classification loss: 0.00002 | Regression loss: 0.00905 | Running loss: 0.01053\n",
            "Epoch: 191 | Iteration: 35 | Classification loss: 0.00001 | Regression loss: 0.01383 | Running loss: 0.01055\n",
            "Epoch: 191 | Iteration: 36 | Classification loss: 0.00001 | Regression loss: 0.00307 | Running loss: 0.01052\n",
            "Epoch: 191 | Iteration: 37 | Classification loss: 0.00003 | Regression loss: 0.00888 | Running loss: 0.01051\n",
            "Epoch: 191 | Iteration: 38 | Classification loss: 0.00001 | Regression loss: 0.01095 | Running loss: 0.01050\n",
            "Epoch: 191 | Iteration: 39 | Classification loss: 0.00001 | Regression loss: 0.01229 | Running loss: 0.01051\n",
            "Epoch: 191 | Iteration: 40 | Classification loss: 0.00001 | Regression loss: 0.02704 | Running loss: 0.01055\n",
            "Epoch: 191 | Iteration: 41 | Classification loss: 0.00003 | Regression loss: 0.01568 | Running loss: 0.01056\n",
            "Epoch: 191 | Iteration: 42 | Classification loss: 0.00000 | Regression loss: 0.00782 | Running loss: 0.01057\n",
            "Epoch: 191 | Iteration: 43 | Classification loss: 0.00001 | Regression loss: 0.00370 | Running loss: 0.01057\n",
            "Epoch: 191 | Iteration: 44 | Classification loss: 0.00001 | Regression loss: 0.01051 | Running loss: 0.01058\n",
            "Epoch: 191 | Iteration: 45 | Classification loss: 0.00001 | Regression loss: 0.00675 | Running loss: 0.01057\n",
            "Epoch: 191 | Iteration: 46 | Classification loss: 0.00001 | Regression loss: 0.00616 | Running loss: 0.01056\n",
            "Epoch: 191 | Iteration: 47 | Classification loss: 0.00001 | Regression loss: 0.01280 | Running loss: 0.01056\n",
            "Epoch: 191 | Iteration: 48 | Classification loss: 0.00001 | Regression loss: 0.00494 | Running loss: 0.01055\n",
            "Epoch: 191 | Iteration: 49 | Classification loss: 0.00001 | Regression loss: 0.01772 | Running loss: 0.01056\n",
            "Epoch: 191 | Iteration: 50 | Classification loss: 0.00002 | Regression loss: 0.00973 | Running loss: 0.01056\n",
            "Epoch: 191 | Iteration: 51 | Classification loss: 0.00002 | Regression loss: 0.01288 | Running loss: 0.01056\n",
            "Epoch: 191 | Iteration: 52 | Classification loss: 0.00004 | Regression loss: 0.01020 | Running loss: 0.01057\n",
            "Epoch: 191 | Iteration: 53 | Classification loss: 0.00002 | Regression loss: 0.01218 | Running loss: 0.01055\n",
            "Epoch: 191 | Iteration: 54 | Classification loss: 0.00001 | Regression loss: 0.00982 | Running loss: 0.01055\n",
            "Epoch: 191 | Iteration: 55 | Classification loss: 0.00000 | Regression loss: 0.00447 | Running loss: 0.01055\n",
            "Epoch: 191 | Iteration: 56 | Classification loss: 0.00001 | Regression loss: 0.00309 | Running loss: 0.01055\n",
            "Epoch: 191 | Iteration: 57 | Classification loss: 0.00001 | Regression loss: 0.01092 | Running loss: 0.01054\n",
            "Epoch: 191 | Iteration: 58 | Classification loss: 0.00001 | Regression loss: 0.02982 | Running loss: 0.01058\n",
            "Epoch: 191 | Iteration: 59 | Classification loss: 0.00001 | Regression loss: 0.01006 | Running loss: 0.01058\n",
            "Epoch: 191 | Iteration: 60 | Classification loss: 0.00002 | Regression loss: 0.00325 | Running loss: 0.01058\n",
            "Epoch: 191 | Iteration: 61 | Classification loss: 0.00001 | Regression loss: 0.01127 | Running loss: 0.01058\n",
            "Epoch: 191 | Iteration: 62 | Classification loss: 0.00001 | Regression loss: 0.01004 | Running loss: 0.01059\n",
            "Epoch: 191 | Iteration: 63 | Classification loss: 0.00002 | Regression loss: 0.01209 | Running loss: 0.01059\n",
            "Epoch: 191 | Iteration: 64 | Classification loss: 0.00001 | Regression loss: 0.00671 | Running loss: 0.01055\n",
            "Epoch: 191 | Iteration: 65 | Classification loss: 0.00001 | Regression loss: 0.00591 | Running loss: 0.01055\n",
            "Epoch: 191 | Iteration: 66 | Classification loss: 0.00002 | Regression loss: 0.01779 | Running loss: 0.01056\n",
            "Epoch: 191 | Iteration: 67 | Classification loss: 0.00001 | Regression loss: 0.01017 | Running loss: 0.01057\n",
            "Epoch: 191 | Iteration: 68 | Classification loss: 0.00001 | Regression loss: 0.00647 | Running loss: 0.01057\n",
            "Epoch: 191 | Iteration: 69 | Classification loss: 0.00004 | Regression loss: 0.01282 | Running loss: 0.01058\n",
            "Epoch: 191 | Iteration: 70 | Classification loss: 0.00001 | Regression loss: 0.01177 | Running loss: 0.01058\n",
            "Epoch: 191 | Iteration: 71 | Classification loss: 0.00006 | Regression loss: 0.01682 | Running loss: 0.01060\n",
            "Epoch: 191 | Iteration: 72 | Classification loss: 0.00002 | Regression loss: 0.01179 | Running loss: 0.01061\n",
            "Epoch: 191 | Iteration: 73 | Classification loss: 0.00000 | Regression loss: 0.00535 | Running loss: 0.01061\n",
            "Epoch: 191 | Iteration: 74 | Classification loss: 0.00003 | Regression loss: 0.01332 | Running loss: 0.01062\n",
            "Epoch: 191 | Iteration: 75 | Classification loss: 0.00001 | Regression loss: 0.00573 | Running loss: 0.01060\n",
            "Epoch: 191 | Iteration: 76 | Classification loss: 0.00001 | Regression loss: 0.00372 | Running loss: 0.01060\n",
            "Epoch: 191 | Iteration: 77 | Classification loss: 0.00001 | Regression loss: 0.01450 | Running loss: 0.01060\n",
            "Epoch: 191 | Iteration: 78 | Classification loss: 0.00002 | Regression loss: 0.01473 | Running loss: 0.01060\n",
            "Epoch: 191 | Iteration: 79 | Classification loss: 0.00002 | Regression loss: 0.00985 | Running loss: 0.01062\n",
            "Epoch: 191 | Iteration: 80 | Classification loss: 0.00001 | Regression loss: 0.00899 | Running loss: 0.01061\n",
            "Epoch: 191 | Iteration: 81 | Classification loss: 0.00001 | Regression loss: 0.02270 | Running loss: 0.01064\n",
            "Epoch: 191 | Iteration: 82 | Classification loss: 0.00010 | Regression loss: 0.00700 | Running loss: 0.01064\n",
            "Epoch: 191 | Iteration: 83 | Classification loss: 0.00001 | Regression loss: 0.01656 | Running loss: 0.01067\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.7227977326660247\n",
            "Precision:  0.550531914893617\n",
            "Recall:  0.8181818181818182\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}, {0: (0.7314384456550078, 253.0)}, {0: (0.7334466242447818, 253.0)}, {0: (0.7357024222917343, 253.0)}, {0: (0.7381550871546381, 253.0)}, {0: (0.739092434551302, 253.0)}, {0: (0.72091448182607, 253.0)}, {0: (0.7389512050165785, 253.0)}, {0: (0.73050932626622, 253.0)}, {0: (0.7263226120133723, 253.0)}, {0: (0.7283463725823673, 253.0)}, {0: (0.7254913772539314, 253.0)}, {0: (0.7281179290861395, 253.0)}, {0: (0.7260020498565122, 253.0)}, {0: (0.7217236108381175, 253.0)}, {0: (0.7267514829312511, 253.0)}, {0: (0.7227483652714889, 253.0)}, {0: (0.7241298168882813, 253.0)}, {0: (0.7261051666397325, 253.0)}, {0: (0.7277246704086413, 253.0)}, {0: (0.7259496886680885, 253.0)}, {0: (0.7290251963867072, 253.0)}, {0: (0.7242894157206492, 253.0)}, {0: (0.7329303843522665, 253.0)}, {0: (0.733154886135177, 253.0)}, {0: (0.7291842105419786, 253.0)}, {0: (0.7251405627702188, 253.0)}, {0: (0.731154087710585, 253.0)}, {0: (0.7317124239374072, 253.0)}, {0: (0.7263717757478444, 253.0)}, {0: (0.7278912613628667, 253.0)}, {0: (0.7330251175928468, 253.0)}, {0: (0.7278399291817672, 253.0)}, {0: (0.728608785848319, 253.0)}, {0: (0.7294166027957658, 253.0)}, {0: (0.7158140975435932, 253.0)}, {0: (0.7230740429361691, 253.0)}, {0: (0.7241983182897731, 253.0)}, {0: (0.7227977326660247, 253.0)}]\n",
            "Epoch: 192 | Iteration: 0 | Classification loss: 0.00005 | Regression loss: 0.01268 | Running loss: 0.01067\n",
            "Epoch: 192 | Iteration: 1 | Classification loss: 0.00001 | Regression loss: 0.00670 | Running loss: 0.01067\n",
            "Epoch: 192 | Iteration: 2 | Classification loss: 0.00001 | Regression loss: 0.00585 | Running loss: 0.01066\n",
            "Epoch: 192 | Iteration: 3 | Classification loss: 0.00001 | Regression loss: 0.00431 | Running loss: 0.01066\n",
            "Epoch: 192 | Iteration: 4 | Classification loss: 0.00003 | Regression loss: 0.01452 | Running loss: 0.01066\n",
            "Epoch: 192 | Iteration: 5 | Classification loss: 0.00002 | Regression loss: 0.01415 | Running loss: 0.01066\n",
            "Epoch: 192 | Iteration: 6 | Classification loss: 0.00001 | Regression loss: 0.00315 | Running loss: 0.01065\n",
            "Epoch: 192 | Iteration: 7 | Classification loss: 0.00000 | Regression loss: 0.00378 | Running loss: 0.01063\n",
            "Epoch: 192 | Iteration: 8 | Classification loss: 0.00002 | Regression loss: 0.01146 | Running loss: 0.01063\n",
            "Epoch: 192 | Iteration: 9 | Classification loss: 0.00001 | Regression loss: 0.00612 | Running loss: 0.01062\n",
            "Epoch: 192 | Iteration: 10 | Classification loss: 0.00002 | Regression loss: 0.01265 | Running loss: 0.01061\n",
            "Epoch: 192 | Iteration: 11 | Classification loss: 0.00005 | Regression loss: 0.03962 | Running loss: 0.01069\n",
            "Epoch: 192 | Iteration: 12 | Classification loss: 0.00002 | Regression loss: 0.01655 | Running loss: 0.01069\n",
            "Epoch: 192 | Iteration: 13 | Classification loss: 0.00001 | Regression loss: 0.01155 | Running loss: 0.01070\n",
            "Epoch: 192 | Iteration: 14 | Classification loss: 0.00001 | Regression loss: 0.01408 | Running loss: 0.01072\n",
            "Epoch: 192 | Iteration: 15 | Classification loss: 0.00002 | Regression loss: 0.00404 | Running loss: 0.01071\n",
            "Epoch: 192 | Iteration: 16 | Classification loss: 0.00002 | Regression loss: 0.00751 | Running loss: 0.01071\n",
            "Epoch: 192 | Iteration: 17 | Classification loss: 0.00001 | Regression loss: 0.00955 | Running loss: 0.01071\n",
            "Epoch: 192 | Iteration: 18 | Classification loss: 0.00001 | Regression loss: 0.01002 | Running loss: 0.01071\n",
            "Epoch: 192 | Iteration: 19 | Classification loss: 0.00001 | Regression loss: 0.00289 | Running loss: 0.01069\n",
            "Epoch: 192 | Iteration: 20 | Classification loss: 0.00001 | Regression loss: 0.00458 | Running loss: 0.01069\n",
            "Epoch: 192 | Iteration: 21 | Classification loss: 0.00001 | Regression loss: 0.00888 | Running loss: 0.01069\n",
            "Epoch: 192 | Iteration: 22 | Classification loss: 0.00008 | Regression loss: 0.00633 | Running loss: 0.01069\n",
            "Epoch: 192 | Iteration: 23 | Classification loss: 0.00001 | Regression loss: 0.00422 | Running loss: 0.01068\n",
            "Epoch: 192 | Iteration: 24 | Classification loss: 0.00001 | Regression loss: 0.01072 | Running loss: 0.01066\n",
            "Epoch: 192 | Iteration: 25 | Classification loss: 0.00003 | Regression loss: 0.01561 | Running loss: 0.01068\n",
            "Epoch: 192 | Iteration: 26 | Classification loss: 0.00001 | Regression loss: 0.00988 | Running loss: 0.01068\n",
            "Epoch: 192 | Iteration: 27 | Classification loss: 0.00002 | Regression loss: 0.01387 | Running loss: 0.01069\n",
            "Epoch: 192 | Iteration: 28 | Classification loss: 0.00001 | Regression loss: 0.00953 | Running loss: 0.01068\n",
            "Epoch: 192 | Iteration: 29 | Classification loss: 0.00006 | Regression loss: 0.01187 | Running loss: 0.01069\n",
            "Epoch: 192 | Iteration: 30 | Classification loss: 0.00001 | Regression loss: 0.00540 | Running loss: 0.01069\n",
            "Epoch: 192 | Iteration: 31 | Classification loss: 0.00001 | Regression loss: 0.00520 | Running loss: 0.01068\n",
            "Epoch: 192 | Iteration: 32 | Classification loss: 0.00001 | Regression loss: 0.01644 | Running loss: 0.01063\n",
            "Epoch: 192 | Iteration: 33 | Classification loss: 0.00001 | Regression loss: 0.00919 | Running loss: 0.01064\n",
            "Epoch: 192 | Iteration: 34 | Classification loss: 0.00003 | Regression loss: 0.01259 | Running loss: 0.01063\n",
            "Epoch: 192 | Iteration: 35 | Classification loss: 0.00002 | Regression loss: 0.01283 | Running loss: 0.01065\n",
            "Epoch: 192 | Iteration: 36 | Classification loss: 0.00003 | Regression loss: 0.01883 | Running loss: 0.01066\n",
            "Epoch: 192 | Iteration: 37 | Classification loss: 0.00004 | Regression loss: 0.01071 | Running loss: 0.01066\n",
            "Epoch: 192 | Iteration: 38 | Classification loss: 0.00001 | Regression loss: 0.00362 | Running loss: 0.01062\n",
            "Epoch: 192 | Iteration: 39 | Classification loss: 0.00000 | Regression loss: 0.00378 | Running loss: 0.01060\n",
            "Epoch: 192 | Iteration: 40 | Classification loss: 0.00003 | Regression loss: 0.01654 | Running loss: 0.01061\n",
            "Epoch: 192 | Iteration: 41 | Classification loss: 0.00001 | Regression loss: 0.00573 | Running loss: 0.01059\n",
            "Epoch: 192 | Iteration: 42 | Classification loss: 0.00001 | Regression loss: 0.01423 | Running loss: 0.01056\n",
            "Epoch: 192 | Iteration: 43 | Classification loss: 0.00002 | Regression loss: 0.00608 | Running loss: 0.01055\n",
            "Epoch: 192 | Iteration: 44 | Classification loss: 0.00002 | Regression loss: 0.00940 | Running loss: 0.01055\n",
            "Epoch: 192 | Iteration: 45 | Classification loss: 0.00002 | Regression loss: 0.01082 | Running loss: 0.01054\n",
            "Epoch: 192 | Iteration: 46 | Classification loss: 0.00001 | Regression loss: 0.01019 | Running loss: 0.01054\n",
            "Epoch: 192 | Iteration: 47 | Classification loss: 0.00001 | Regression loss: 0.01175 | Running loss: 0.01054\n",
            "Epoch: 192 | Iteration: 48 | Classification loss: 0.00002 | Regression loss: 0.00269 | Running loss: 0.01054\n",
            "Epoch: 192 | Iteration: 49 | Classification loss: 0.00001 | Regression loss: 0.00515 | Running loss: 0.01053\n",
            "Epoch: 192 | Iteration: 50 | Classification loss: 0.00001 | Regression loss: 0.01055 | Running loss: 0.01052\n",
            "Epoch: 192 | Iteration: 51 | Classification loss: 0.00001 | Regression loss: 0.01209 | Running loss: 0.01053\n",
            "Epoch: 192 | Iteration: 52 | Classification loss: 0.00000 | Regression loss: 0.00660 | Running loss: 0.01052\n",
            "Epoch: 192 | Iteration: 53 | Classification loss: 0.00001 | Regression loss: 0.01656 | Running loss: 0.01055\n",
            "Epoch: 192 | Iteration: 54 | Classification loss: 0.00002 | Regression loss: 0.01185 | Running loss: 0.01054\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 188.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 192 | Iteration: 56 | Classification loss: 0.00001 | Regression loss: 0.01151 | Running loss: 0.01055\n",
            "Epoch: 192 | Iteration: 57 | Classification loss: 0.00001 | Regression loss: 0.01087 | Running loss: 0.01055\n",
            "Epoch: 192 | Iteration: 58 | Classification loss: 0.00001 | Regression loss: 0.00491 | Running loss: 0.01053\n",
            "Epoch: 192 | Iteration: 59 | Classification loss: 0.00001 | Regression loss: 0.00575 | Running loss: 0.01052\n",
            "Epoch: 192 | Iteration: 60 | Classification loss: 0.00001 | Regression loss: 0.00431 | Running loss: 0.01052\n",
            "Epoch: 192 | Iteration: 61 | Classification loss: 0.00002 | Regression loss: 0.01419 | Running loss: 0.01052\n",
            "Epoch: 192 | Iteration: 62 | Classification loss: 0.00002 | Regression loss: 0.02774 | Running loss: 0.01053\n",
            "Epoch: 192 | Iteration: 63 | Classification loss: 0.00001 | Regression loss: 0.01715 | Running loss: 0.01053\n",
            "Epoch: 192 | Iteration: 64 | Classification loss: 0.00001 | Regression loss: 0.01191 | Running loss: 0.01054\n",
            "Epoch: 192 | Iteration: 65 | Classification loss: 0.00002 | Regression loss: 0.01037 | Running loss: 0.01056\n",
            "Epoch: 192 | Iteration: 66 | Classification loss: 0.00004 | Regression loss: 0.01266 | Running loss: 0.01057\n",
            "Epoch: 192 | Iteration: 67 | Classification loss: 0.00001 | Regression loss: 0.00400 | Running loss: 0.01057\n",
            "Epoch: 192 | Iteration: 68 | Classification loss: 0.00001 | Regression loss: 0.02666 | Running loss: 0.01060\n",
            "Epoch: 192 | Iteration: 69 | Classification loss: 0.00001 | Regression loss: 0.01020 | Running loss: 0.01060\n",
            "Epoch: 192 | Iteration: 70 | Classification loss: 0.00001 | Regression loss: 0.01399 | Running loss: 0.01062\n",
            "Epoch: 192 | Iteration: 71 | Classification loss: 0.00004 | Regression loss: 0.00356 | Running loss: 0.01058\n",
            "Epoch: 192 | Iteration: 72 | Classification loss: 0.00001 | Regression loss: 0.00368 | Running loss: 0.01055\n",
            "Epoch: 192 | Iteration: 73 | Classification loss: 0.00001 | Regression loss: 0.01361 | Running loss: 0.01057\n",
            "Epoch: 192 | Iteration: 74 | Classification loss: 0.00003 | Regression loss: 0.00930 | Running loss: 0.01057\n",
            "Epoch: 192 | Iteration: 75 | Classification loss: 0.00002 | Regression loss: 0.01000 | Running loss: 0.01058\n",
            "Epoch: 192 | Iteration: 76 | Classification loss: 0.00001 | Regression loss: 0.00629 | Running loss: 0.01058\n",
            "Epoch: 192 | Iteration: 77 | Classification loss: 0.00000 | Regression loss: 0.00802 | Running loss: 0.01059\n",
            "Epoch: 192 | Iteration: 78 | Classification loss: 0.00005 | Regression loss: 0.01690 | Running loss: 0.01062\n",
            "Epoch: 192 | Iteration: 79 | Classification loss: 0.00002 | Regression loss: 0.02367 | Running loss: 0.01063\n",
            "Epoch: 192 | Iteration: 80 | Classification loss: 0.00001 | Regression loss: 0.00376 | Running loss: 0.01063\n",
            "Epoch: 192 | Iteration: 81 | Classification loss: 0.00002 | Regression loss: 0.01664 | Running loss: 0.01066\n",
            "Epoch: 192 | Iteration: 82 | Classification loss: 0.00002 | Regression loss: 0.00914 | Running loss: 0.01065\n",
            "Epoch: 192 | Iteration: 83 | Classification loss: 0.00001 | Regression loss: 0.02277 | Running loss: 0.01069\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.7142649604059264\n",
            "Precision:  0.5392670157068062\n",
            "Recall:  0.8142292490118577\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}, {0: (0.7314384456550078, 253.0)}, {0: (0.7334466242447818, 253.0)}, {0: (0.7357024222917343, 253.0)}, {0: (0.7381550871546381, 253.0)}, {0: (0.739092434551302, 253.0)}, {0: (0.72091448182607, 253.0)}, {0: (0.7389512050165785, 253.0)}, {0: (0.73050932626622, 253.0)}, {0: (0.7263226120133723, 253.0)}, {0: (0.7283463725823673, 253.0)}, {0: (0.7254913772539314, 253.0)}, {0: (0.7281179290861395, 253.0)}, {0: (0.7260020498565122, 253.0)}, {0: (0.7217236108381175, 253.0)}, {0: (0.7267514829312511, 253.0)}, {0: (0.7227483652714889, 253.0)}, {0: (0.7241298168882813, 253.0)}, {0: (0.7261051666397325, 253.0)}, {0: (0.7277246704086413, 253.0)}, {0: (0.7259496886680885, 253.0)}, {0: (0.7290251963867072, 253.0)}, {0: (0.7242894157206492, 253.0)}, {0: (0.7329303843522665, 253.0)}, {0: (0.733154886135177, 253.0)}, {0: (0.7291842105419786, 253.0)}, {0: (0.7251405627702188, 253.0)}, {0: (0.731154087710585, 253.0)}, {0: (0.7317124239374072, 253.0)}, {0: (0.7263717757478444, 253.0)}, {0: (0.7278912613628667, 253.0)}, {0: (0.7330251175928468, 253.0)}, {0: (0.7278399291817672, 253.0)}, {0: (0.728608785848319, 253.0)}, {0: (0.7294166027957658, 253.0)}, {0: (0.7158140975435932, 253.0)}, {0: (0.7230740429361691, 253.0)}, {0: (0.7241983182897731, 253.0)}, {0: (0.7227977326660247, 253.0)}, {0: (0.7142649604059264, 253.0)}]\n",
            "Epoch: 193 | Iteration: 0 | Classification loss: 0.00001 | Regression loss: 0.00542 | Running loss: 0.01067\n",
            "Epoch: 193 | Iteration: 1 | Classification loss: 0.00001 | Regression loss: 0.00595 | Running loss: 0.01066\n",
            "Epoch: 193 | Iteration: 2 | Classification loss: 0.00004 | Regression loss: 0.00769 | Running loss: 0.01065\n",
            "Epoch: 193 | Iteration: 3 | Classification loss: 0.00001 | Regression loss: 0.00853 | Running loss: 0.01065\n",
            "Epoch: 193 | Iteration: 4 | Classification loss: 0.00002 | Regression loss: 0.01470 | Running loss: 0.01066\n",
            "Epoch: 193 | Iteration: 5 | Classification loss: 0.00003 | Regression loss: 0.01503 | Running loss: 0.01067\n",
            "Epoch: 193 | Iteration: 6 | Classification loss: 0.00002 | Regression loss: 0.01211 | Running loss: 0.01067\n",
            "Epoch: 193 | Iteration: 7 | Classification loss: 0.00001 | Regression loss: 0.00340 | Running loss: 0.01065\n",
            "Epoch: 193 | Iteration: 8 | Classification loss: 0.00001 | Regression loss: 0.01113 | Running loss: 0.01062\n",
            "Epoch: 193 | Iteration: 9 | Classification loss: 0.00001 | Regression loss: 0.01537 | Running loss: 0.01063\n",
            "Epoch: 193 | Iteration: 10 | Classification loss: 0.00001 | Regression loss: 0.00863 | Running loss: 0.01063\n",
            "Epoch: 193 | Iteration: 11 | Classification loss: 0.00001 | Regression loss: 0.00661 | Running loss: 0.01064\n",
            "Epoch: 193 | Iteration: 12 | Classification loss: 0.00000 | Regression loss: 0.00360 | Running loss: 0.01063\n",
            "Epoch: 193 | Iteration: 13 | Classification loss: 0.00002 | Regression loss: 0.01665 | Running loss: 0.01065\n",
            "Epoch: 193 | Iteration: 14 | Classification loss: 0.00002 | Regression loss: 0.01189 | Running loss: 0.01064\n",
            "Epoch: 193 | Iteration: 15 | Classification loss: 0.00001 | Regression loss: 0.01406 | Running loss: 0.01066\n",
            "Epoch: 193 | Iteration: 16 | Classification loss: 0.00002 | Regression loss: 0.00622 | Running loss: 0.01065\n",
            "Epoch: 193 | Iteration: 17 | Classification loss: 0.00001 | Regression loss: 0.00415 | Running loss: 0.01064\n",
            "Epoch: 193 | Iteration: 18 | Classification loss: 0.00001 | Regression loss: 0.01591 | Running loss: 0.01066\n",
            "Epoch: 193 | Iteration: 19 | Classification loss: 0.00002 | Regression loss: 0.00415 | Running loss: 0.01062\n",
            "Epoch: 193 | Iteration: 20 | Classification loss: 0.00000 | Regression loss: 0.00670 | Running loss: 0.01062\n",
            "Epoch: 193 | Iteration: 21 | Classification loss: 0.00001 | Regression loss: 0.00333 | Running loss: 0.01062\n",
            "Epoch: 193 | Iteration: 22 | Classification loss: 0.00004 | Regression loss: 0.01253 | Running loss: 0.01063\n",
            "Epoch: 193 | Iteration: 23 | Classification loss: 0.00003 | Regression loss: 0.00318 | Running loss: 0.01060\n",
            "Epoch: 193 | Iteration: 24 | Classification loss: 0.00001 | Regression loss: 0.00460 | Running loss: 0.01059\n",
            "Epoch: 193 | Iteration: 25 | Classification loss: 0.00006 | Regression loss: 0.01223 | Running loss: 0.01058\n",
            "Epoch: 193 | Iteration: 26 | Classification loss: 0.00002 | Regression loss: 0.00863 | Running loss: 0.01058\n",
            "Epoch: 193 | Iteration: 27 | Classification loss: 0.00001 | Regression loss: 0.01311 | Running loss: 0.01058\n",
            "Epoch: 193 | Iteration: 28 | Classification loss: 0.00000 | Regression loss: 0.00768 | Running loss: 0.01058\n",
            "Epoch: 193 | Iteration: 29 | Classification loss: 0.00001 | Regression loss: 0.00960 | Running loss: 0.01052\n",
            "Epoch: 193 | Iteration: 30 | Classification loss: 0.00001 | Regression loss: 0.01017 | Running loss: 0.01052\n",
            "Epoch: 193 | Iteration: 31 | Classification loss: 0.00000 | Regression loss: 0.00285 | Running loss: 0.01050\n",
            "Epoch: 193 | Iteration: 32 | Classification loss: 0.00001 | Regression loss: 0.00987 | Running loss: 0.01050\n",
            "Epoch: 193 | Iteration: 33 | Classification loss: 0.00001 | Regression loss: 0.00868 | Running loss: 0.01049\n",
            "Epoch: 193 | Iteration: 34 | Classification loss: 0.00003 | Regression loss: 0.01615 | Running loss: 0.01051\n",
            "Epoch: 193 | Iteration: 35 | Classification loss: 0.00002 | Regression loss: 0.01156 | Running loss: 0.01052\n",
            "Epoch: 193 | Iteration: 36 | Classification loss: 0.00002 | Regression loss: 0.01376 | Running loss: 0.01054\n",
            "Epoch: 193 | Iteration: 37 | Classification loss: 0.00001 | Regression loss: 0.02688 | Running loss: 0.01057\n",
            "Epoch: 193 | Iteration: 38 | Classification loss: 0.00005 | Regression loss: 0.01553 | Running loss: 0.01058\n",
            "Epoch: 193 | Iteration: 39 | Classification loss: 0.00002 | Regression loss: 0.00806 | Running loss: 0.01057\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 190.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 193 | Iteration: 41 | Classification loss: 0.00001 | Regression loss: 0.00938 | Running loss: 0.01056\n",
            "Epoch: 193 | Iteration: 42 | Classification loss: 0.00002 | Regression loss: 0.00633 | Running loss: 0.01057\n",
            "Epoch: 193 | Iteration: 43 | Classification loss: 0.00001 | Regression loss: 0.02886 | Running loss: 0.01061\n",
            "Epoch: 193 | Iteration: 44 | Classification loss: 0.00001 | Regression loss: 0.00528 | Running loss: 0.01060\n",
            "Epoch: 193 | Iteration: 45 | Classification loss: 0.00002 | Regression loss: 0.00775 | Running loss: 0.01059\n",
            "Epoch: 193 | Iteration: 46 | Classification loss: 0.00001 | Regression loss: 0.00760 | Running loss: 0.01060\n",
            "Epoch: 193 | Iteration: 47 | Classification loss: 0.00003 | Regression loss: 0.00920 | Running loss: 0.01061\n",
            "Epoch: 193 | Iteration: 48 | Classification loss: 0.00001 | Regression loss: 0.01121 | Running loss: 0.01062\n",
            "Epoch: 193 | Iteration: 49 | Classification loss: 0.00001 | Regression loss: 0.01418 | Running loss: 0.01063\n",
            "Epoch: 193 | Iteration: 50 | Classification loss: 0.00009 | Regression loss: 0.00883 | Running loss: 0.01059\n",
            "Epoch: 193 | Iteration: 51 | Classification loss: 0.00001 | Regression loss: 0.00579 | Running loss: 0.01060\n",
            "Epoch: 193 | Iteration: 52 | Classification loss: 0.00003 | Regression loss: 0.01844 | Running loss: 0.01060\n",
            "Epoch: 193 | Iteration: 53 | Classification loss: 0.00002 | Regression loss: 0.01132 | Running loss: 0.01062\n",
            "Epoch: 193 | Iteration: 54 | Classification loss: 0.00000 | Regression loss: 0.00261 | Running loss: 0.01060\n",
            "Epoch: 193 | Iteration: 55 | Classification loss: 0.00001 | Regression loss: 0.00413 | Running loss: 0.01060\n",
            "Epoch: 193 | Iteration: 56 | Classification loss: 0.00002 | Regression loss: 0.01149 | Running loss: 0.01059\n",
            "Epoch: 193 | Iteration: 57 | Classification loss: 0.00001 | Regression loss: 0.01002 | Running loss: 0.01059\n",
            "Epoch: 193 | Iteration: 58 | Classification loss: 0.00001 | Regression loss: 0.00397 | Running loss: 0.01059\n",
            "Epoch: 193 | Iteration: 59 | Classification loss: 0.00001 | Regression loss: 0.00577 | Running loss: 0.01058\n",
            "Epoch: 193 | Iteration: 60 | Classification loss: 0.00001 | Regression loss: 0.01525 | Running loss: 0.01061\n",
            "Epoch: 193 | Iteration: 61 | Classification loss: 0.00001 | Regression loss: 0.00982 | Running loss: 0.01061\n",
            "Epoch: 193 | Iteration: 62 | Classification loss: 0.00001 | Regression loss: 0.02151 | Running loss: 0.01063\n",
            "Epoch: 193 | Iteration: 63 | Classification loss: 0.00003 | Regression loss: 0.01168 | Running loss: 0.01064\n",
            "Epoch: 193 | Iteration: 64 | Classification loss: 0.00001 | Regression loss: 0.01145 | Running loss: 0.01064\n",
            "Epoch: 193 | Iteration: 65 | Classification loss: 0.00001 | Regression loss: 0.00313 | Running loss: 0.01061\n",
            "Epoch: 193 | Iteration: 66 | Classification loss: 0.00002 | Regression loss: 0.01167 | Running loss: 0.01063\n",
            "Epoch: 193 | Iteration: 67 | Classification loss: 0.00002 | Regression loss: 0.00903 | Running loss: 0.01064\n",
            "Epoch: 193 | Iteration: 68 | Classification loss: 0.00002 | Regression loss: 0.00267 | Running loss: 0.01062\n",
            "Epoch: 193 | Iteration: 69 | Classification loss: 0.00001 | Regression loss: 0.00932 | Running loss: 0.01061\n",
            "Epoch: 193 | Iteration: 70 | Classification loss: 0.00000 | Regression loss: 0.00675 | Running loss: 0.01060\n",
            "Epoch: 193 | Iteration: 71 | Classification loss: 0.00001 | Regression loss: 0.00518 | Running loss: 0.01059\n",
            "Epoch: 193 | Iteration: 72 | Classification loss: 0.00002 | Regression loss: 0.01503 | Running loss: 0.01059\n",
            "Epoch: 193 | Iteration: 73 | Classification loss: 0.00001 | Regression loss: 0.00355 | Running loss: 0.01056\n",
            "Epoch: 193 | Iteration: 74 | Classification loss: 0.00001 | Regression loss: 0.00460 | Running loss: 0.01055\n",
            "Epoch: 193 | Iteration: 75 | Classification loss: 0.00001 | Regression loss: 0.00505 | Running loss: 0.01052\n",
            "Epoch: 193 | Iteration: 76 | Classification loss: 0.00003 | Regression loss: 0.01100 | Running loss: 0.01049\n",
            "Epoch: 193 | Iteration: 77 | Classification loss: 0.00003 | Regression loss: 0.01333 | Running loss: 0.01049\n",
            "Epoch: 193 | Iteration: 78 | Classification loss: 0.00001 | Regression loss: 0.00933 | Running loss: 0.01050\n",
            "Epoch: 193 | Iteration: 79 | Classification loss: 0.00001 | Regression loss: 0.01062 | Running loss: 0.01051\n",
            "Epoch: 193 | Iteration: 80 | Classification loss: 0.00004 | Regression loss: 0.01157 | Running loss: 0.01051\n",
            "Epoch: 193 | Iteration: 81 | Classification loss: 0.00002 | Regression loss: 0.02214 | Running loss: 0.01053\n",
            "Epoch: 193 | Iteration: 82 | Classification loss: 0.00006 | Regression loss: 0.04082 | Running loss: 0.01059\n",
            "Epoch: 193 | Iteration: 83 | Classification loss: 0.00001 | Regression loss: 0.00961 | Running loss: 0.01060\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.721965827885861\n",
            "Precision:  0.550531914893617\n",
            "Recall:  0.8181818181818182\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}, {0: (0.7314384456550078, 253.0)}, {0: (0.7334466242447818, 253.0)}, {0: (0.7357024222917343, 253.0)}, {0: (0.7381550871546381, 253.0)}, {0: (0.739092434551302, 253.0)}, {0: (0.72091448182607, 253.0)}, {0: (0.7389512050165785, 253.0)}, {0: (0.73050932626622, 253.0)}, {0: (0.7263226120133723, 253.0)}, {0: (0.7283463725823673, 253.0)}, {0: (0.7254913772539314, 253.0)}, {0: (0.7281179290861395, 253.0)}, {0: (0.7260020498565122, 253.0)}, {0: (0.7217236108381175, 253.0)}, {0: (0.7267514829312511, 253.0)}, {0: (0.7227483652714889, 253.0)}, {0: (0.7241298168882813, 253.0)}, {0: (0.7261051666397325, 253.0)}, {0: (0.7277246704086413, 253.0)}, {0: (0.7259496886680885, 253.0)}, {0: (0.7290251963867072, 253.0)}, {0: (0.7242894157206492, 253.0)}, {0: (0.7329303843522665, 253.0)}, {0: (0.733154886135177, 253.0)}, {0: (0.7291842105419786, 253.0)}, {0: (0.7251405627702188, 253.0)}, {0: (0.731154087710585, 253.0)}, {0: (0.7317124239374072, 253.0)}, {0: (0.7263717757478444, 253.0)}, {0: (0.7278912613628667, 253.0)}, {0: (0.7330251175928468, 253.0)}, {0: (0.7278399291817672, 253.0)}, {0: (0.728608785848319, 253.0)}, {0: (0.7294166027957658, 253.0)}, {0: (0.7158140975435932, 253.0)}, {0: (0.7230740429361691, 253.0)}, {0: (0.7241983182897731, 253.0)}, {0: (0.7227977326660247, 253.0)}, {0: (0.7142649604059264, 253.0)}, {0: (0.721965827885861, 253.0)}]\n",
            "Epoch: 194 | Iteration: 0 | Classification loss: 0.00001 | Regression loss: 0.00866 | Running loss: 0.01058\n",
            "Epoch: 194 | Iteration: 1 | Classification loss: 0.00001 | Regression loss: 0.00376 | Running loss: 0.01057\n",
            "Epoch: 194 | Iteration: 2 | Classification loss: 0.00002 | Regression loss: 0.01139 | Running loss: 0.01056\n",
            "Epoch: 194 | Iteration: 3 | Classification loss: 0.00002 | Regression loss: 0.01071 | Running loss: 0.01056\n",
            "Epoch: 194 | Iteration: 4 | Classification loss: 0.00001 | Regression loss: 0.00392 | Running loss: 0.01055\n",
            "Epoch: 194 | Iteration: 5 | Classification loss: 0.00001 | Regression loss: 0.00867 | Running loss: 0.01051\n",
            "Epoch: 194 | Iteration: 6 | Classification loss: 0.00003 | Regression loss: 0.00905 | Running loss: 0.01051\n",
            "Epoch: 194 | Iteration: 7 | Classification loss: 0.00002 | Regression loss: 0.01179 | Running loss: 0.01053\n",
            "Epoch: 194 | Iteration: 8 | Classification loss: 0.00001 | Regression loss: 0.00582 | Running loss: 0.01052\n",
            "Epoch: 194 | Iteration: 9 | Classification loss: 0.00001 | Regression loss: 0.00446 | Running loss: 0.01051\n",
            "Epoch: 194 | Iteration: 10 | Classification loss: 0.00002 | Regression loss: 0.01352 | Running loss: 0.01050\n",
            "Epoch: 194 | Iteration: 11 | Classification loss: 0.00001 | Regression loss: 0.00523 | Running loss: 0.01049\n",
            "Epoch: 194 | Iteration: 12 | Classification loss: 0.00002 | Regression loss: 0.01588 | Running loss: 0.01049\n",
            "Epoch: 194 | Iteration: 13 | Classification loss: 0.00001 | Regression loss: 0.00512 | Running loss: 0.01049\n",
            "Epoch: 194 | Iteration: 14 | Classification loss: 0.00005 | Regression loss: 0.01072 | Running loss: 0.01051\n",
            "Epoch: 194 | Iteration: 15 | Classification loss: 0.00000 | Regression loss: 0.00275 | Running loss: 0.01050\n",
            "Epoch: 194 | Iteration: 16 | Classification loss: 0.00001 | Regression loss: 0.00429 | Running loss: 0.01048\n",
            "Epoch: 194 | Iteration: 17 | Classification loss: 0.00001 | Regression loss: 0.01139 | Running loss: 0.01048\n",
            "Epoch: 194 | Iteration: 18 | Classification loss: 0.00003 | Regression loss: 0.01395 | Running loss: 0.01050\n",
            "Epoch: 194 | Iteration: 19 | Classification loss: 0.00002 | Regression loss: 0.01471 | Running loss: 0.01049\n",
            "Epoch: 194 | Iteration: 20 | Classification loss: 0.00001 | Regression loss: 0.01047 | Running loss: 0.01050\n",
            "Epoch: 194 | Iteration: 21 | Classification loss: 0.00003 | Regression loss: 0.01287 | Running loss: 0.01050\n",
            "Epoch: 194 | Iteration: 22 | Classification loss: 0.00002 | Regression loss: 0.00938 | Running loss: 0.01050\n",
            "Epoch: 194 | Iteration: 23 | Classification loss: 0.00001 | Regression loss: 0.01090 | Running loss: 0.01049\n",
            "Epoch: 194 | Iteration: 24 | Classification loss: 0.00007 | Regression loss: 0.00639 | Running loss: 0.01049\n",
            "Epoch: 194 | Iteration: 25 | Classification loss: 0.00002 | Regression loss: 0.00280 | Running loss: 0.01047\n",
            "Epoch: 194 | Iteration: 26 | Classification loss: 0.00006 | Regression loss: 0.01214 | Running loss: 0.01047\n",
            "Epoch: 194 | Iteration: 27 | Classification loss: 0.00001 | Regression loss: 0.02457 | Running loss: 0.01051\n",
            "Epoch: 194 | Iteration: 28 | Classification loss: 0.00000 | Regression loss: 0.00751 | Running loss: 0.01050\n",
            "Epoch: 194 | Iteration: 29 | Classification loss: 0.00002 | Regression loss: 0.00796 | Running loss: 0.01051\n",
            "Epoch: 194 | Iteration: 30 | Classification loss: 0.00001 | Regression loss: 0.00362 | Running loss: 0.01048\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 188.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 194 | Iteration: 32 | Classification loss: 0.00002 | Regression loss: 0.00671 | Running loss: 0.01049\n",
            "Epoch: 194 | Iteration: 33 | Classification loss: 0.00001 | Regression loss: 0.00343 | Running loss: 0.01046\n",
            "Epoch: 194 | Iteration: 34 | Classification loss: 0.00006 | Regression loss: 0.03975 | Running loss: 0.01052\n",
            "Epoch: 194 | Iteration: 35 | Classification loss: 0.00001 | Regression loss: 0.00984 | Running loss: 0.01051\n",
            "Epoch: 194 | Iteration: 36 | Classification loss: 0.00001 | Regression loss: 0.00919 | Running loss: 0.01050\n",
            "Epoch: 194 | Iteration: 37 | Classification loss: 0.00001 | Regression loss: 0.01034 | Running loss: 0.01050\n",
            "Epoch: 194 | Iteration: 38 | Classification loss: 0.00001 | Regression loss: 0.01025 | Running loss: 0.01052\n",
            "Epoch: 194 | Iteration: 39 | Classification loss: 0.00001 | Regression loss: 0.00621 | Running loss: 0.01050\n",
            "Epoch: 194 | Iteration: 40 | Classification loss: 0.00001 | Regression loss: 0.00968 | Running loss: 0.01051\n",
            "Epoch: 194 | Iteration: 41 | Classification loss: 0.00002 | Regression loss: 0.01046 | Running loss: 0.01051\n",
            "Epoch: 194 | Iteration: 42 | Classification loss: 0.00004 | Regression loss: 0.01231 | Running loss: 0.01052\n",
            "Epoch: 194 | Iteration: 43 | Classification loss: 0.00001 | Regression loss: 0.00616 | Running loss: 0.01050\n",
            "Epoch: 194 | Iteration: 44 | Classification loss: 0.00001 | Regression loss: 0.00516 | Running loss: 0.01050\n",
            "Epoch: 194 | Iteration: 45 | Classification loss: 0.00001 | Regression loss: 0.01029 | Running loss: 0.01050\n",
            "Epoch: 194 | Iteration: 46 | Classification loss: 0.00001 | Regression loss: 0.00892 | Running loss: 0.01048\n",
            "Epoch: 194 | Iteration: 47 | Classification loss: 0.00002 | Regression loss: 0.01546 | Running loss: 0.01050\n",
            "Epoch: 194 | Iteration: 48 | Classification loss: 0.00001 | Regression loss: 0.02781 | Running loss: 0.01053\n",
            "Epoch: 194 | Iteration: 49 | Classification loss: 0.00001 | Regression loss: 0.01743 | Running loss: 0.01053\n",
            "Epoch: 194 | Iteration: 50 | Classification loss: 0.00006 | Regression loss: 0.01665 | Running loss: 0.01055\n",
            "Epoch: 194 | Iteration: 51 | Classification loss: 0.00002 | Regression loss: 0.01434 | Running loss: 0.01055\n",
            "Epoch: 194 | Iteration: 52 | Classification loss: 0.00002 | Regression loss: 0.01138 | Running loss: 0.01055\n",
            "Epoch: 194 | Iteration: 53 | Classification loss: 0.00001 | Regression loss: 0.00872 | Running loss: 0.01056\n",
            "Epoch: 194 | Iteration: 54 | Classification loss: 0.00000 | Regression loss: 0.00439 | Running loss: 0.01054\n",
            "Epoch: 194 | Iteration: 55 | Classification loss: 0.00001 | Regression loss: 0.00584 | Running loss: 0.01054\n",
            "Epoch: 194 | Iteration: 56 | Classification loss: 0.00001 | Regression loss: 0.01348 | Running loss: 0.01053\n",
            "Epoch: 194 | Iteration: 57 | Classification loss: 0.00001 | Regression loss: 0.00448 | Running loss: 0.01051\n",
            "Epoch: 194 | Iteration: 58 | Classification loss: 0.00001 | Regression loss: 0.00356 | Running loss: 0.01051\n",
            "Epoch: 194 | Iteration: 59 | Classification loss: 0.00001 | Regression loss: 0.00302 | Running loss: 0.01050\n",
            "Epoch: 194 | Iteration: 60 | Classification loss: 0.00003 | Regression loss: 0.00299 | Running loss: 0.01049\n",
            "Epoch: 194 | Iteration: 61 | Classification loss: 0.00002 | Regression loss: 0.01777 | Running loss: 0.01051\n",
            "Epoch: 194 | Iteration: 62 | Classification loss: 0.00003 | Regression loss: 0.01431 | Running loss: 0.01051\n",
            "Epoch: 194 | Iteration: 63 | Classification loss: 0.00002 | Regression loss: 0.00964 | Running loss: 0.01053\n",
            "Epoch: 194 | Iteration: 64 | Classification loss: 0.00000 | Regression loss: 0.00656 | Running loss: 0.01053\n",
            "Epoch: 194 | Iteration: 65 | Classification loss: 0.00002 | Regression loss: 0.00831 | Running loss: 0.01053\n",
            "Epoch: 194 | Iteration: 66 | Classification loss: 0.00001 | Regression loss: 0.01500 | Running loss: 0.01052\n",
            "Epoch: 194 | Iteration: 67 | Classification loss: 0.00001 | Regression loss: 0.00378 | Running loss: 0.01053\n",
            "Epoch: 194 | Iteration: 68 | Classification loss: 0.00004 | Regression loss: 0.01046 | Running loss: 0.01052\n",
            "Epoch: 194 | Iteration: 69 | Classification loss: 0.00002 | Regression loss: 0.00896 | Running loss: 0.01053\n",
            "Epoch: 194 | Iteration: 70 | Classification loss: 0.00001 | Regression loss: 0.01704 | Running loss: 0.01048\n",
            "Epoch: 194 | Iteration: 71 | Classification loss: 0.00001 | Regression loss: 0.01104 | Running loss: 0.01045\n",
            "Epoch: 194 | Iteration: 72 | Classification loss: 0.00001 | Regression loss: 0.00573 | Running loss: 0.01044\n",
            "Epoch: 194 | Iteration: 73 | Classification loss: 0.00002 | Regression loss: 0.01245 | Running loss: 0.01041\n",
            "Epoch: 194 | Iteration: 74 | Classification loss: 0.00001 | Regression loss: 0.01610 | Running loss: 0.01042\n",
            "Epoch: 194 | Iteration: 75 | Classification loss: 0.00000 | Regression loss: 0.00281 | Running loss: 0.01041\n",
            "Epoch: 194 | Iteration: 76 | Classification loss: 0.00001 | Regression loss: 0.01325 | Running loss: 0.01042\n",
            "Epoch: 194 | Iteration: 77 | Classification loss: 0.00001 | Regression loss: 0.00573 | Running loss: 0.01042\n",
            "Epoch: 194 | Iteration: 78 | Classification loss: 0.00001 | Regression loss: 0.01649 | Running loss: 0.01044\n",
            "Epoch: 194 | Iteration: 79 | Classification loss: 0.00001 | Regression loss: 0.00947 | Running loss: 0.01045\n",
            "Epoch: 194 | Iteration: 80 | Classification loss: 0.00002 | Regression loss: 0.02062 | Running loss: 0.01046\n",
            "Epoch: 194 | Iteration: 81 | Classification loss: 0.00001 | Regression loss: 0.00932 | Running loss: 0.01047\n",
            "Epoch: 194 | Iteration: 82 | Classification loss: 0.00001 | Regression loss: 0.02672 | Running loss: 0.01049\n",
            "Epoch: 194 | Iteration: 83 | Classification loss: 0.00002 | Regression loss: 0.00879 | Running loss: 0.01050\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.7150808407318003\n",
            "Precision:  0.5392670157068062\n",
            "Recall:  0.8142292490118577\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}, {0: (0.7314384456550078, 253.0)}, {0: (0.7334466242447818, 253.0)}, {0: (0.7357024222917343, 253.0)}, {0: (0.7381550871546381, 253.0)}, {0: (0.739092434551302, 253.0)}, {0: (0.72091448182607, 253.0)}, {0: (0.7389512050165785, 253.0)}, {0: (0.73050932626622, 253.0)}, {0: (0.7263226120133723, 253.0)}, {0: (0.7283463725823673, 253.0)}, {0: (0.7254913772539314, 253.0)}, {0: (0.7281179290861395, 253.0)}, {0: (0.7260020498565122, 253.0)}, {0: (0.7217236108381175, 253.0)}, {0: (0.7267514829312511, 253.0)}, {0: (0.7227483652714889, 253.0)}, {0: (0.7241298168882813, 253.0)}, {0: (0.7261051666397325, 253.0)}, {0: (0.7277246704086413, 253.0)}, {0: (0.7259496886680885, 253.0)}, {0: (0.7290251963867072, 253.0)}, {0: (0.7242894157206492, 253.0)}, {0: (0.7329303843522665, 253.0)}, {0: (0.733154886135177, 253.0)}, {0: (0.7291842105419786, 253.0)}, {0: (0.7251405627702188, 253.0)}, {0: (0.731154087710585, 253.0)}, {0: (0.7317124239374072, 253.0)}, {0: (0.7263717757478444, 253.0)}, {0: (0.7278912613628667, 253.0)}, {0: (0.7330251175928468, 253.0)}, {0: (0.7278399291817672, 253.0)}, {0: (0.728608785848319, 253.0)}, {0: (0.7294166027957658, 253.0)}, {0: (0.7158140975435932, 253.0)}, {0: (0.7230740429361691, 253.0)}, {0: (0.7241983182897731, 253.0)}, {0: (0.7227977326660247, 253.0)}, {0: (0.7142649604059264, 253.0)}, {0: (0.721965827885861, 253.0)}, {0: (0.7150808407318003, 253.0)}]\n",
            "Epoch: 195 | Iteration: 0 | Classification loss: 0.00002 | Regression loss: 0.00529 | Running loss: 0.01049\n",
            "Epoch: 195 | Iteration: 1 | Classification loss: 0.00000 | Regression loss: 0.00289 | Running loss: 0.01048\n",
            "Epoch: 195 | Iteration: 2 | Classification loss: 0.00002 | Regression loss: 0.01495 | Running loss: 0.01049\n",
            "Epoch: 195 | Iteration: 3 | Classification loss: 0.00003 | Regression loss: 0.00341 | Running loss: 0.01047\n",
            "Epoch: 195 | Iteration: 4 | Classification loss: 0.00001 | Regression loss: 0.00382 | Running loss: 0.01046\n",
            "Epoch: 195 | Iteration: 5 | Classification loss: 0.00002 | Regression loss: 0.01439 | Running loss: 0.01049\n",
            "Epoch: 195 | Iteration: 6 | Classification loss: 0.00001 | Regression loss: 0.00325 | Running loss: 0.01047\n",
            "Epoch: 195 | Iteration: 7 | Classification loss: 0.00001 | Regression loss: 0.00987 | Running loss: 0.01047\n",
            "Epoch: 195 | Iteration: 8 | Classification loss: 0.00001 | Regression loss: 0.00538 | Running loss: 0.01046\n",
            "Epoch: 195 | Iteration: 9 | Classification loss: 0.00002 | Regression loss: 0.01787 | Running loss: 0.01047\n",
            "Epoch: 195 | Iteration: 10 | Classification loss: 0.00008 | Regression loss: 0.00582 | Running loss: 0.01048\n",
            "Epoch: 195 | Iteration: 11 | Classification loss: 0.00002 | Regression loss: 0.01229 | Running loss: 0.01049\n",
            "Epoch: 195 | Iteration: 12 | Classification loss: 0.00002 | Regression loss: 0.00891 | Running loss: 0.01049\n",
            "Epoch: 195 | Iteration: 13 | Classification loss: 0.00001 | Regression loss: 0.00344 | Running loss: 0.01048\n",
            "Epoch: 195 | Iteration: 14 | Classification loss: 0.00001 | Regression loss: 0.01637 | Running loss: 0.01050\n",
            "Epoch: 195 | Iteration: 15 | Classification loss: 0.00001 | Regression loss: 0.00950 | Running loss: 0.01049\n",
            "Epoch: 195 | Iteration: 16 | Classification loss: 0.00001 | Regression loss: 0.00873 | Running loss: 0.01049\n",
            "Epoch: 195 | Iteration: 17 | Classification loss: 0.00006 | Regression loss: 0.04105 | Running loss: 0.01054\n",
            "Epoch: 195 | Iteration: 18 | Classification loss: 0.00001 | Regression loss: 0.01844 | Running loss: 0.01056\n",
            "Epoch: 195 | Iteration: 19 | Classification loss: 0.00001 | Regression loss: 0.01499 | Running loss: 0.01058\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 188.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 195 | Iteration: 21 | Classification loss: 0.00001 | Regression loss: 0.01249 | Running loss: 0.01059\n",
            "Epoch: 195 | Iteration: 22 | Classification loss: 0.00001 | Regression loss: 0.01331 | Running loss: 0.01061\n",
            "Epoch: 195 | Iteration: 23 | Classification loss: 0.00001 | Regression loss: 0.02909 | Running loss: 0.01066\n",
            "Epoch: 195 | Iteration: 24 | Classification loss: 0.00000 | Regression loss: 0.00456 | Running loss: 0.01065\n",
            "Epoch: 195 | Iteration: 25 | Classification loss: 0.00001 | Regression loss: 0.00557 | Running loss: 0.01063\n",
            "Epoch: 195 | Iteration: 26 | Classification loss: 0.00002 | Regression loss: 0.01220 | Running loss: 0.01063\n",
            "Epoch: 195 | Iteration: 27 | Classification loss: 0.00002 | Regression loss: 0.01372 | Running loss: 0.01065\n",
            "Epoch: 195 | Iteration: 28 | Classification loss: 0.00001 | Regression loss: 0.01069 | Running loss: 0.01065\n",
            "Epoch: 195 | Iteration: 29 | Classification loss: 0.00001 | Regression loss: 0.01107 | Running loss: 0.01065\n",
            "Epoch: 195 | Iteration: 30 | Classification loss: 0.00001 | Regression loss: 0.00863 | Running loss: 0.01064\n",
            "Epoch: 195 | Iteration: 31 | Classification loss: 0.00005 | Regression loss: 0.01700 | Running loss: 0.01065\n",
            "Epoch: 195 | Iteration: 32 | Classification loss: 0.00001 | Regression loss: 0.00852 | Running loss: 0.01066\n",
            "Epoch: 195 | Iteration: 33 | Classification loss: 0.00002 | Regression loss: 0.01002 | Running loss: 0.01067\n",
            "Epoch: 195 | Iteration: 34 | Classification loss: 0.00002 | Regression loss: 0.00254 | Running loss: 0.01064\n",
            "Epoch: 195 | Iteration: 35 | Classification loss: 0.00001 | Regression loss: 0.00419 | Running loss: 0.01063\n",
            "Epoch: 195 | Iteration: 36 | Classification loss: 0.00001 | Regression loss: 0.01420 | Running loss: 0.01064\n",
            "Epoch: 195 | Iteration: 37 | Classification loss: 0.00003 | Regression loss: 0.01533 | Running loss: 0.01064\n",
            "Epoch: 195 | Iteration: 38 | Classification loss: 0.00002 | Regression loss: 0.00921 | Running loss: 0.01063\n",
            "Epoch: 195 | Iteration: 39 | Classification loss: 0.00000 | Regression loss: 0.00277 | Running loss: 0.01060\n",
            "Epoch: 195 | Iteration: 40 | Classification loss: 0.00001 | Regression loss: 0.02211 | Running loss: 0.01062\n",
            "Epoch: 195 | Iteration: 41 | Classification loss: 0.00001 | Regression loss: 0.01007 | Running loss: 0.01063\n",
            "Epoch: 195 | Iteration: 42 | Classification loss: 0.00001 | Regression loss: 0.01418 | Running loss: 0.01065\n",
            "Epoch: 195 | Iteration: 43 | Classification loss: 0.00004 | Regression loss: 0.01315 | Running loss: 0.01065\n",
            "Epoch: 195 | Iteration: 44 | Classification loss: 0.00006 | Regression loss: 0.01250 | Running loss: 0.01065\n",
            "Epoch: 195 | Iteration: 45 | Classification loss: 0.00001 | Regression loss: 0.00942 | Running loss: 0.01066\n",
            "Epoch: 195 | Iteration: 46 | Classification loss: 0.00001 | Regression loss: 0.00333 | Running loss: 0.01065\n",
            "Epoch: 195 | Iteration: 47 | Classification loss: 0.00001 | Regression loss: 0.01061 | Running loss: 0.01064\n",
            "Epoch: 195 | Iteration: 48 | Classification loss: 0.00001 | Regression loss: 0.00924 | Running loss: 0.01064\n",
            "Epoch: 195 | Iteration: 49 | Classification loss: 0.00002 | Regression loss: 0.01159 | Running loss: 0.01066\n",
            "Epoch: 195 | Iteration: 50 | Classification loss: 0.00001 | Regression loss: 0.00341 | Running loss: 0.01066\n",
            "Epoch: 195 | Iteration: 51 | Classification loss: 0.00003 | Regression loss: 0.01138 | Running loss: 0.01066\n",
            "Epoch: 195 | Iteration: 52 | Classification loss: 0.00001 | Regression loss: 0.02616 | Running loss: 0.01069\n",
            "Epoch: 195 | Iteration: 53 | Classification loss: 0.00002 | Regression loss: 0.00969 | Running loss: 0.01068\n",
            "Epoch: 195 | Iteration: 54 | Classification loss: 0.00001 | Regression loss: 0.01208 | Running loss: 0.01070\n",
            "Epoch: 195 | Iteration: 55 | Classification loss: 0.00000 | Regression loss: 0.00655 | Running loss: 0.01068\n",
            "Epoch: 195 | Iteration: 56 | Classification loss: 0.00001 | Regression loss: 0.00510 | Running loss: 0.01064\n",
            "Epoch: 195 | Iteration: 57 | Classification loss: 0.00002 | Regression loss: 0.01703 | Running loss: 0.01066\n",
            "Epoch: 195 | Iteration: 58 | Classification loss: 0.00002 | Regression loss: 0.01049 | Running loss: 0.01065\n",
            "Epoch: 195 | Iteration: 59 | Classification loss: 0.00001 | Regression loss: 0.00523 | Running loss: 0.01065\n",
            "Epoch: 195 | Iteration: 60 | Classification loss: 0.00000 | Regression loss: 0.00395 | Running loss: 0.01061\n",
            "Epoch: 195 | Iteration: 61 | Classification loss: 0.00002 | Regression loss: 0.00943 | Running loss: 0.01060\n",
            "Epoch: 195 | Iteration: 62 | Classification loss: 0.00003 | Regression loss: 0.00955 | Running loss: 0.01060\n",
            "Epoch: 195 | Iteration: 63 | Classification loss: 0.00002 | Regression loss: 0.00616 | Running loss: 0.01059\n",
            "Epoch: 195 | Iteration: 64 | Classification loss: 0.00002 | Regression loss: 0.01190 | Running loss: 0.01061\n",
            "Epoch: 195 | Iteration: 65 | Classification loss: 0.00002 | Regression loss: 0.00854 | Running loss: 0.01061\n",
            "Epoch: 195 | Iteration: 66 | Classification loss: 0.00001 | Regression loss: 0.00921 | Running loss: 0.01060\n",
            "Epoch: 195 | Iteration: 67 | Classification loss: 0.00001 | Regression loss: 0.00398 | Running loss: 0.01056\n",
            "Epoch: 195 | Iteration: 68 | Classification loss: 0.00002 | Regression loss: 0.00900 | Running loss: 0.01054\n",
            "Epoch: 195 | Iteration: 69 | Classification loss: 0.00001 | Regression loss: 0.00304 | Running loss: 0.01054\n",
            "Epoch: 195 | Iteration: 70 | Classification loss: 0.00001 | Regression loss: 0.01009 | Running loss: 0.01054\n",
            "Epoch: 195 | Iteration: 71 | Classification loss: 0.00001 | Regression loss: 0.00551 | Running loss: 0.01047\n",
            "Epoch: 195 | Iteration: 72 | Classification loss: 0.00004 | Regression loss: 0.00870 | Running loss: 0.01047\n",
            "Epoch: 195 | Iteration: 73 | Classification loss: 0.00000 | Regression loss: 0.00763 | Running loss: 0.01047\n",
            "Epoch: 195 | Iteration: 74 | Classification loss: 0.00001 | Regression loss: 0.01343 | Running loss: 0.01048\n",
            "Epoch: 195 | Iteration: 75 | Classification loss: 0.00001 | Regression loss: 0.00645 | Running loss: 0.01048\n",
            "Epoch: 195 | Iteration: 76 | Classification loss: 0.00001 | Regression loss: 0.00547 | Running loss: 0.01048\n",
            "Epoch: 195 | Iteration: 77 | Classification loss: 0.00001 | Regression loss: 0.00395 | Running loss: 0.01046\n",
            "Epoch: 195 | Iteration: 78 | Classification loss: 0.00004 | Regression loss: 0.01240 | Running loss: 0.01046\n",
            "Epoch: 195 | Iteration: 79 | Classification loss: 0.00001 | Regression loss: 0.00920 | Running loss: 0.01046\n",
            "Epoch: 195 | Iteration: 80 | Classification loss: 0.00001 | Regression loss: 0.00881 | Running loss: 0.01044\n",
            "Epoch: 195 | Iteration: 81 | Classification loss: 0.00001 | Regression loss: 0.01629 | Running loss: 0.01046\n",
            "Epoch: 195 | Iteration: 82 | Classification loss: 0.00001 | Regression loss: 0.00590 | Running loss: 0.01046\n",
            "Epoch: 195 | Iteration: 83 | Classification loss: 0.00002 | Regression loss: 0.01085 | Running loss: 0.01045\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.7180015690245183\n",
            "Precision:  0.5421052631578948\n",
            "Recall:  0.8142292490118577\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}, {0: (0.7314384456550078, 253.0)}, {0: (0.7334466242447818, 253.0)}, {0: (0.7357024222917343, 253.0)}, {0: (0.7381550871546381, 253.0)}, {0: (0.739092434551302, 253.0)}, {0: (0.72091448182607, 253.0)}, {0: (0.7389512050165785, 253.0)}, {0: (0.73050932626622, 253.0)}, {0: (0.7263226120133723, 253.0)}, {0: (0.7283463725823673, 253.0)}, {0: (0.7254913772539314, 253.0)}, {0: (0.7281179290861395, 253.0)}, {0: (0.7260020498565122, 253.0)}, {0: (0.7217236108381175, 253.0)}, {0: (0.7267514829312511, 253.0)}, {0: (0.7227483652714889, 253.0)}, {0: (0.7241298168882813, 253.0)}, {0: (0.7261051666397325, 253.0)}, {0: (0.7277246704086413, 253.0)}, {0: (0.7259496886680885, 253.0)}, {0: (0.7290251963867072, 253.0)}, {0: (0.7242894157206492, 253.0)}, {0: (0.7329303843522665, 253.0)}, {0: (0.733154886135177, 253.0)}, {0: (0.7291842105419786, 253.0)}, {0: (0.7251405627702188, 253.0)}, {0: (0.731154087710585, 253.0)}, {0: (0.7317124239374072, 253.0)}, {0: (0.7263717757478444, 253.0)}, {0: (0.7278912613628667, 253.0)}, {0: (0.7330251175928468, 253.0)}, {0: (0.7278399291817672, 253.0)}, {0: (0.728608785848319, 253.0)}, {0: (0.7294166027957658, 253.0)}, {0: (0.7158140975435932, 253.0)}, {0: (0.7230740429361691, 253.0)}, {0: (0.7241983182897731, 253.0)}, {0: (0.7227977326660247, 253.0)}, {0: (0.7142649604059264, 253.0)}, {0: (0.721965827885861, 253.0)}, {0: (0.7150808407318003, 253.0)}, {0: (0.7180015690245183, 253.0)}]\n",
            "Epoch: 196 | Iteration: 0 | Classification loss: 0.00001 | Regression loss: 0.02572 | Running loss: 0.01048\n",
            "Epoch: 196 | Iteration: 1 | Classification loss: 0.00001 | Regression loss: 0.00333 | Running loss: 0.01047\n",
            "Epoch: 196 | Iteration: 2 | Classification loss: 0.00001 | Regression loss: 0.00889 | Running loss: 0.01048\n",
            "Epoch: 196 | Iteration: 3 | Classification loss: 0.00002 | Regression loss: 0.01195 | Running loss: 0.01048\n",
            "Epoch: 196 | Iteration: 4 | Classification loss: 0.00001 | Regression loss: 0.00404 | Running loss: 0.01048\n",
            "Epoch: 196 | Iteration: 5 | Classification loss: 0.00001 | Regression loss: 0.00552 | Running loss: 0.01046\n",
            "Epoch: 196 | Iteration: 6 | Classification loss: 0.00002 | Regression loss: 0.00906 | Running loss: 0.01045\n",
            "Epoch: 196 | Iteration: 7 | Classification loss: 0.00003 | Regression loss: 0.01190 | Running loss: 0.01045\n",
            "Epoch: 196 | Iteration: 8 | Classification loss: 0.00001 | Regression loss: 0.00863 | Running loss: 0.01042\n",
            "Epoch: 196 | Iteration: 9 | Classification loss: 0.00002 | Regression loss: 0.00874 | Running loss: 0.01043\n",
            "Epoch: 196 | Iteration: 10 | Classification loss: 0.00002 | Regression loss: 0.00241 | Running loss: 0.01043\n",
            "Epoch: 196 | Iteration: 11 | Classification loss: 0.00001 | Regression loss: 0.01118 | Running loss: 0.01042\n",
            "Epoch: 196 | Iteration: 12 | Classification loss: 0.00001 | Regression loss: 0.00856 | Running loss: 0.01038\n",
            "Epoch: 196 | Iteration: 13 | Classification loss: 0.00001 | Regression loss: 0.00894 | Running loss: 0.01037\n",
            "Epoch: 196 | Iteration: 14 | Classification loss: 0.00001 | Regression loss: 0.00618 | Running loss: 0.01037\n",
            "Epoch: 196 | Iteration: 15 | Classification loss: 0.00001 | Regression loss: 0.00505 | Running loss: 0.01035\n",
            "Epoch: 196 | Iteration: 16 | Classification loss: 0.00001 | Regression loss: 0.01001 | Running loss: 0.01036\n",
            "Epoch: 196 | Iteration: 17 | Classification loss: 0.00001 | Regression loss: 0.01488 | Running loss: 0.01038\n",
            "Epoch: 196 | Iteration: 18 | Classification loss: 0.00003 | Regression loss: 0.00344 | Running loss: 0.01037\n",
            "Epoch: 196 | Iteration: 19 | Classification loss: 0.00001 | Regression loss: 0.02786 | Running loss: 0.01041\n",
            "Epoch: 196 | Iteration: 20 | Classification loss: 0.00001 | Regression loss: 0.00579 | Running loss: 0.01041\n",
            "Epoch: 196 | Iteration: 21 | Classification loss: 0.00001 | Regression loss: 0.01471 | Running loss: 0.01042\n",
            "Epoch: 196 | Iteration: 22 | Classification loss: 0.00002 | Regression loss: 0.01049 | Running loss: 0.01042\n",
            "Epoch: 196 | Iteration: 23 | Classification loss: 0.00001 | Regression loss: 0.00318 | Running loss: 0.01039\n",
            "Epoch: 196 | Iteration: 24 | Classification loss: 0.00006 | Regression loss: 0.01160 | Running loss: 0.01039\n",
            "Epoch: 196 | Iteration: 25 | Classification loss: 0.00001 | Regression loss: 0.00561 | Running loss: 0.01037\n",
            "Epoch: 196 | Iteration: 26 | Classification loss: 0.00002 | Regression loss: 0.00806 | Running loss: 0.01037\n",
            "Epoch: 196 | Iteration: 27 | Classification loss: 0.00001 | Regression loss: 0.01031 | Running loss: 0.01036\n",
            "Epoch: 196 | Iteration: 28 | Classification loss: 0.00004 | Regression loss: 0.01213 | Running loss: 0.01038\n",
            "Epoch: 196 | Iteration: 29 | Classification loss: 0.00000 | Regression loss: 0.00262 | Running loss: 0.01033\n",
            "Epoch: 196 | Iteration: 30 | Classification loss: 0.00002 | Regression loss: 0.00544 | Running loss: 0.01032\n",
            "Epoch: 196 | Iteration: 31 | Classification loss: 0.00001 | Regression loss: 0.00593 | Running loss: 0.01032\n",
            "Epoch: 196 | Iteration: 32 | Classification loss: 0.00002 | Regression loss: 0.01352 | Running loss: 0.01033\n",
            "Epoch: 196 | Iteration: 33 | Classification loss: 0.00001 | Regression loss: 0.00544 | Running loss: 0.01034\n",
            "Epoch: 196 | Iteration: 34 | Classification loss: 0.00001 | Regression loss: 0.00592 | Running loss: 0.01033\n",
            "Epoch: 196 | Iteration: 35 | Classification loss: 0.00001 | Regression loss: 0.00430 | Running loss: 0.01031\n",
            "Epoch: 196 | Iteration: 36 | Classification loss: 0.00001 | Regression loss: 0.01398 | Running loss: 0.01033\n",
            "Epoch: 196 | Iteration: 37 | Classification loss: 0.00001 | Regression loss: 0.00991 | Running loss: 0.01034\n",
            "Epoch: 196 | Iteration: 38 | Classification loss: 0.00002 | Regression loss: 0.01628 | Running loss: 0.01036\n",
            "Epoch: 196 | Iteration: 39 | Classification loss: 0.00000 | Regression loss: 0.00968 | Running loss: 0.01035\n",
            "Epoch: 196 | Iteration: 40 | Classification loss: 0.00000 | Regression loss: 0.00317 | Running loss: 0.01034\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 190.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 196 | Iteration: 42 | Classification loss: 0.00001 | Regression loss: 0.00381 | Running loss: 0.01032\n",
            "Epoch: 196 | Iteration: 43 | Classification loss: 0.00000 | Regression loss: 0.00420 | Running loss: 0.01032\n",
            "Epoch: 196 | Iteration: 44 | Classification loss: 0.00001 | Regression loss: 0.00961 | Running loss: 0.01030\n",
            "Epoch: 196 | Iteration: 45 | Classification loss: 0.00002 | Regression loss: 0.01365 | Running loss: 0.01032\n",
            "Epoch: 196 | Iteration: 46 | Classification loss: 0.00002 | Regression loss: 0.02208 | Running loss: 0.01035\n",
            "Epoch: 196 | Iteration: 47 | Classification loss: 0.00001 | Regression loss: 0.01646 | Running loss: 0.01036\n",
            "Epoch: 196 | Iteration: 48 | Classification loss: 0.00009 | Regression loss: 0.00649 | Running loss: 0.01037\n",
            "Epoch: 196 | Iteration: 49 | Classification loss: 0.00001 | Regression loss: 0.01224 | Running loss: 0.01036\n",
            "Epoch: 196 | Iteration: 50 | Classification loss: 0.00004 | Regression loss: 0.00884 | Running loss: 0.01036\n",
            "Epoch: 196 | Iteration: 51 | Classification loss: 0.00001 | Regression loss: 0.00359 | Running loss: 0.01035\n",
            "Epoch: 196 | Iteration: 52 | Classification loss: 0.00001 | Regression loss: 0.00352 | Running loss: 0.01033\n",
            "Epoch: 196 | Iteration: 53 | Classification loss: 0.00001 | Regression loss: 0.01573 | Running loss: 0.01035\n",
            "Epoch: 196 | Iteration: 54 | Classification loss: 0.00001 | Regression loss: 0.01309 | Running loss: 0.01036\n",
            "Epoch: 196 | Iteration: 55 | Classification loss: 0.00001 | Regression loss: 0.00400 | Running loss: 0.01036\n",
            "Epoch: 196 | Iteration: 56 | Classification loss: 0.00003 | Regression loss: 0.01439 | Running loss: 0.01038\n",
            "Epoch: 196 | Iteration: 57 | Classification loss: 0.00006 | Regression loss: 0.01597 | Running loss: 0.01038\n",
            "Epoch: 196 | Iteration: 58 | Classification loss: 0.00002 | Regression loss: 0.01323 | Running loss: 0.01033\n",
            "Epoch: 196 | Iteration: 59 | Classification loss: 0.00001 | Regression loss: 0.00421 | Running loss: 0.01032\n",
            "Epoch: 196 | Iteration: 60 | Classification loss: 0.00001 | Regression loss: 0.00558 | Running loss: 0.01031\n",
            "Epoch: 196 | Iteration: 61 | Classification loss: 0.00002 | Regression loss: 0.01087 | Running loss: 0.01029\n",
            "Epoch: 196 | Iteration: 62 | Classification loss: 0.00002 | Regression loss: 0.00796 | Running loss: 0.01030\n",
            "Epoch: 196 | Iteration: 63 | Classification loss: 0.00000 | Regression loss: 0.00378 | Running loss: 0.01028\n",
            "Epoch: 196 | Iteration: 64 | Classification loss: 0.00002 | Regression loss: 0.01070 | Running loss: 0.01027\n",
            "Epoch: 196 | Iteration: 65 | Classification loss: 0.00001 | Regression loss: 0.00884 | Running loss: 0.01027\n",
            "Epoch: 196 | Iteration: 66 | Classification loss: 0.00001 | Regression loss: 0.01761 | Running loss: 0.01030\n",
            "Epoch: 196 | Iteration: 67 | Classification loss: 0.00002 | Regression loss: 0.01430 | Running loss: 0.01029\n",
            "Epoch: 196 | Iteration: 68 | Classification loss: 0.00001 | Regression loss: 0.01052 | Running loss: 0.01030\n",
            "Epoch: 196 | Iteration: 69 | Classification loss: 0.00005 | Regression loss: 0.03886 | Running loss: 0.01036\n",
            "Epoch: 196 | Iteration: 70 | Classification loss: 0.00001 | Regression loss: 0.02198 | Running loss: 0.01037\n",
            "Epoch: 196 | Iteration: 71 | Classification loss: 0.00001 | Regression loss: 0.00751 | Running loss: 0.01037\n",
            "Epoch: 196 | Iteration: 72 | Classification loss: 0.00000 | Regression loss: 0.00654 | Running loss: 0.01036\n",
            "Epoch: 196 | Iteration: 73 | Classification loss: 0.00001 | Regression loss: 0.01500 | Running loss: 0.01037\n",
            "Epoch: 196 | Iteration: 74 | Classification loss: 0.00002 | Regression loss: 0.01114 | Running loss: 0.01038\n",
            "Epoch: 196 | Iteration: 75 | Classification loss: 0.00000 | Regression loss: 0.00799 | Running loss: 0.01037\n",
            "Epoch: 196 | Iteration: 76 | Classification loss: 0.00003 | Regression loss: 0.00976 | Running loss: 0.01038\n",
            "Epoch: 196 | Iteration: 77 | Classification loss: 0.00001 | Regression loss: 0.01203 | Running loss: 0.01038\n",
            "Epoch: 196 | Iteration: 78 | Classification loss: 0.00004 | Regression loss: 0.01165 | Running loss: 0.01038\n",
            "Epoch: 196 | Iteration: 79 | Classification loss: 0.00002 | Regression loss: 0.01335 | Running loss: 0.01040\n",
            "Epoch: 196 | Iteration: 80 | Classification loss: 0.00001 | Regression loss: 0.00715 | Running loss: 0.01040\n",
            "Epoch: 196 | Iteration: 81 | Classification loss: 0.00003 | Regression loss: 0.01907 | Running loss: 0.01042\n",
            "Epoch: 196 | Iteration: 82 | Classification loss: 0.00001 | Regression loss: 0.01051 | Running loss: 0.01041\n",
            "Epoch: 196 | Iteration: 83 | Classification loss: 0.00002 | Regression loss: 0.01090 | Running loss: 0.01040\n",
            "Evaluating dataset\n",
            "\n",
            "mAP:\n",
            "person: 0.7117719784228915\n",
            "Precision:  0.5406824146981627\n",
            "Recall:  0.8142292490118577\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}, {0: (0.7314384456550078, 253.0)}, {0: (0.7334466242447818, 253.0)}, {0: (0.7357024222917343, 253.0)}, {0: (0.7381550871546381, 253.0)}, {0: (0.739092434551302, 253.0)}, {0: (0.72091448182607, 253.0)}, {0: (0.7389512050165785, 253.0)}, {0: (0.73050932626622, 253.0)}, {0: (0.7263226120133723, 253.0)}, {0: (0.7283463725823673, 253.0)}, {0: (0.7254913772539314, 253.0)}, {0: (0.7281179290861395, 253.0)}, {0: (0.7260020498565122, 253.0)}, {0: (0.7217236108381175, 253.0)}, {0: (0.7267514829312511, 253.0)}, {0: (0.7227483652714889, 253.0)}, {0: (0.7241298168882813, 253.0)}, {0: (0.7261051666397325, 253.0)}, {0: (0.7277246704086413, 253.0)}, {0: (0.7259496886680885, 253.0)}, {0: (0.7290251963867072, 253.0)}, {0: (0.7242894157206492, 253.0)}, {0: (0.7329303843522665, 253.0)}, {0: (0.733154886135177, 253.0)}, {0: (0.7291842105419786, 253.0)}, {0: (0.7251405627702188, 253.0)}, {0: (0.731154087710585, 253.0)}, {0: (0.7317124239374072, 253.0)}, {0: (0.7263717757478444, 253.0)}, {0: (0.7278912613628667, 253.0)}, {0: (0.7330251175928468, 253.0)}, {0: (0.7278399291817672, 253.0)}, {0: (0.728608785848319, 253.0)}, {0: (0.7294166027957658, 253.0)}, {0: (0.7158140975435932, 253.0)}, {0: (0.7230740429361691, 253.0)}, {0: (0.7241983182897731, 253.0)}, {0: (0.7227977326660247, 253.0)}, {0: (0.7142649604059264, 253.0)}, {0: (0.721965827885861, 253.0)}, {0: (0.7150808407318003, 253.0)}, {0: (0.7180015690245183, 253.0)}, {0: (0.7117719784228915, 253.0)}]\n",
            "Epoch: 197 | Iteration: 0 | Classification loss: 0.00002 | Regression loss: 0.01352 | Running loss: 0.01041\n",
            "Epoch: 197 | Iteration: 1 | Classification loss: 0.00002 | Regression loss: 0.01326 | Running loss: 0.01042\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 188.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 197 | Iteration: 3 | Classification loss: 0.00001 | Regression loss: 0.00343 | Running loss: 0.01042\n",
            "Epoch: 197 | Iteration: 4 | Classification loss: 0.00001 | Regression loss: 0.00544 | Running loss: 0.01040\n",
            "Epoch: 197 | Iteration: 5 | Classification loss: 0.00005 | Regression loss: 0.03872 | Running loss: 0.01044\n",
            "Epoch: 197 | Iteration: 6 | Classification loss: 0.00002 | Regression loss: 0.00885 | Running loss: 0.01043\n",
            "Epoch: 197 | Iteration: 7 | Classification loss: 0.00001 | Regression loss: 0.02547 | Running loss: 0.01045\n",
            "Epoch: 197 | Iteration: 8 | Classification loss: 0.00001 | Regression loss: 0.00355 | Running loss: 0.01044\n",
            "Epoch: 197 | Iteration: 9 | Classification loss: 0.00001 | Regression loss: 0.00317 | Running loss: 0.01042\n",
            "Epoch: 197 | Iteration: 10 | Classification loss: 0.00001 | Regression loss: 0.01045 | Running loss: 0.01042\n",
            "Epoch: 197 | Iteration: 11 | Classification loss: 0.00002 | Regression loss: 0.00236 | Running loss: 0.01041\n",
            "Epoch: 197 | Iteration: 12 | Classification loss: 0.00004 | Regression loss: 0.01119 | Running loss: 0.01043\n",
            "Epoch: 197 | Iteration: 13 | Classification loss: 0.00002 | Regression loss: 0.01439 | Running loss: 0.01044\n",
            "Epoch: 197 | Iteration: 14 | Classification loss: 0.00001 | Regression loss: 0.01627 | Running loss: 0.01046\n",
            "Epoch: 197 | Iteration: 15 | Classification loss: 0.00001 | Regression loss: 0.00964 | Running loss: 0.01046\n",
            "Epoch: 197 | Iteration: 16 | Classification loss: 0.00001 | Regression loss: 0.00567 | Running loss: 0.01047\n",
            "Epoch: 197 | Iteration: 17 | Classification loss: 0.00001 | Regression loss: 0.02690 | Running loss: 0.01051\n",
            "Epoch: 197 | Iteration: 18 | Classification loss: 0.00000 | Regression loss: 0.00671 | Running loss: 0.01044\n",
            "Epoch: 197 | Iteration: 19 | Classification loss: 0.00001 | Regression loss: 0.00936 | Running loss: 0.01043\n",
            "Epoch: 197 | Iteration: 20 | Classification loss: 0.00002 | Regression loss: 0.01111 | Running loss: 0.01043\n",
            "Epoch: 197 | Iteration: 21 | Classification loss: 0.00003 | Regression loss: 0.01275 | Running loss: 0.01045\n",
            "Epoch: 197 | Iteration: 22 | Classification loss: 0.00001 | Regression loss: 0.00329 | Running loss: 0.01044\n",
            "Epoch: 197 | Iteration: 23 | Classification loss: 0.00001 | Regression loss: 0.00586 | Running loss: 0.01044\n",
            "Epoch: 197 | Iteration: 24 | Classification loss: 0.00001 | Regression loss: 0.00531 | Running loss: 0.01042\n",
            "Epoch: 197 | Iteration: 25 | Classification loss: 0.00001 | Regression loss: 0.01017 | Running loss: 0.01043\n",
            "Epoch: 197 | Iteration: 26 | Classification loss: 0.00000 | Regression loss: 0.00277 | Running loss: 0.01042\n",
            "Epoch: 197 | Iteration: 27 | Classification loss: 0.00001 | Regression loss: 0.00921 | Running loss: 0.01043\n",
            "Epoch: 197 | Iteration: 28 | Classification loss: 0.00002 | Regression loss: 0.00917 | Running loss: 0.01043\n",
            "Epoch: 197 | Iteration: 29 | Classification loss: 0.00001 | Regression loss: 0.01711 | Running loss: 0.01045\n",
            "Epoch: 197 | Iteration: 30 | Classification loss: 0.00001 | Regression loss: 0.01182 | Running loss: 0.01044\n",
            "Epoch: 197 | Iteration: 31 | Classification loss: 0.00003 | Regression loss: 0.01954 | Running loss: 0.01046\n",
            "Epoch: 197 | Iteration: 32 | Classification loss: 0.00002 | Regression loss: 0.01314 | Running loss: 0.01046\n",
            "Epoch: 197 | Iteration: 33 | Classification loss: 0.00001 | Regression loss: 0.00590 | Running loss: 0.01046\n",
            "Epoch: 197 | Iteration: 34 | Classification loss: 0.00002 | Regression loss: 0.00596 | Running loss: 0.01047\n",
            "Epoch: 197 | Iteration: 35 | Classification loss: 0.00001 | Regression loss: 0.01176 | Running loss: 0.01048\n",
            "Epoch: 197 | Iteration: 36 | Classification loss: 0.00001 | Regression loss: 0.00454 | Running loss: 0.01047\n",
            "Epoch: 197 | Iteration: 37 | Classification loss: 0.00001 | Regression loss: 0.01863 | Running loss: 0.01048\n",
            "Epoch: 197 | Iteration: 38 | Classification loss: 0.00001 | Regression loss: 0.01584 | Running loss: 0.01051\n",
            "Epoch: 197 | Iteration: 39 | Classification loss: 0.00001 | Regression loss: 0.00948 | Running loss: 0.01051\n",
            "Epoch: 197 | Iteration: 40 | Classification loss: 0.00002 | Regression loss: 0.01443 | Running loss: 0.01052\n",
            "Epoch: 197 | Iteration: 41 | Classification loss: 0.00001 | Regression loss: 0.01476 | Running loss: 0.01052\n",
            "Epoch: 197 | Iteration: 42 | Classification loss: 0.00001 | Regression loss: 0.00859 | Running loss: 0.01048\n",
            "Epoch: 197 | Iteration: 43 | Classification loss: 0.00001 | Regression loss: 0.00495 | Running loss: 0.01046\n",
            "Epoch: 197 | Iteration: 44 | Classification loss: 0.00002 | Regression loss: 0.00506 | Running loss: 0.01046\n",
            "Epoch: 197 | Iteration: 45 | Classification loss: 0.00001 | Regression loss: 0.00868 | Running loss: 0.01047\n",
            "Epoch: 197 | Iteration: 46 | Classification loss: 0.00002 | Regression loss: 0.01038 | Running loss: 0.01047\n",
            "Epoch: 197 | Iteration: 47 | Classification loss: 0.00001 | Regression loss: 0.00888 | Running loss: 0.01047\n",
            "Epoch: 197 | Iteration: 48 | Classification loss: 0.00001 | Regression loss: 0.00897 | Running loss: 0.01048\n",
            "Epoch: 197 | Iteration: 49 | Classification loss: 0.00003 | Regression loss: 0.01389 | Running loss: 0.01048\n",
            "Epoch: 197 | Iteration: 50 | Classification loss: 0.00004 | Regression loss: 0.00754 | Running loss: 0.01049\n",
            "Epoch: 197 | Iteration: 51 | Classification loss: 0.00002 | Regression loss: 0.01185 | Running loss: 0.01047\n",
            "Epoch: 197 | Iteration: 52 | Classification loss: 0.00001 | Regression loss: 0.01268 | Running loss: 0.01048\n",
            "Epoch: 197 | Iteration: 53 | Classification loss: 0.00002 | Regression loss: 0.01035 | Running loss: 0.01047\n",
            "Epoch: 197 | Iteration: 54 | Classification loss: 0.00001 | Regression loss: 0.01014 | Running loss: 0.01047\n",
            "Epoch: 197 | Iteration: 55 | Classification loss: 0.00001 | Regression loss: 0.01144 | Running loss: 0.01047\n",
            "Epoch: 197 | Iteration: 56 | Classification loss: 0.00006 | Regression loss: 0.01269 | Running loss: 0.01048\n",
            "Epoch: 197 | Iteration: 57 | Classification loss: 0.00000 | Regression loss: 0.00377 | Running loss: 0.01048\n",
            "Epoch: 197 | Iteration: 58 | Classification loss: 0.00002 | Regression loss: 0.01376 | Running loss: 0.01050\n",
            "Epoch: 197 | Iteration: 59 | Classification loss: 0.00004 | Regression loss: 0.01221 | Running loss: 0.01050\n",
            "Epoch: 197 | Iteration: 60 | Classification loss: 0.00001 | Regression loss: 0.00426 | Running loss: 0.01045\n",
            "Epoch: 197 | Iteration: 61 | Classification loss: 0.00005 | Regression loss: 0.01482 | Running loss: 0.01046\n",
            "Epoch: 197 | Iteration: 62 | Classification loss: 0.00002 | Regression loss: 0.00946 | Running loss: 0.01047\n",
            "Epoch: 197 | Iteration: 63 | Classification loss: 0.00001 | Regression loss: 0.00364 | Running loss: 0.01046\n",
            "Epoch: 197 | Iteration: 64 | Classification loss: 0.00001 | Regression loss: 0.00523 | Running loss: 0.01045\n",
            "Epoch: 197 | Iteration: 65 | Classification loss: 0.00003 | Regression loss: 0.00377 | Running loss: 0.01043\n",
            "Epoch: 197 | Iteration: 66 | Classification loss: 0.00000 | Regression loss: 0.00333 | Running loss: 0.01042\n",
            "Epoch: 197 | Iteration: 67 | Classification loss: 0.00002 | Regression loss: 0.00828 | Running loss: 0.01043\n",
            "Epoch: 197 | Iteration: 68 | Classification loss: 0.00001 | Regression loss: 0.01423 | Running loss: 0.01042\n",
            "Epoch: 197 | Iteration: 69 | Classification loss: 0.00001 | Regression loss: 0.01009 | Running loss: 0.01042\n",
            "Epoch: 197 | Iteration: 70 | Classification loss: 0.00001 | Regression loss: 0.00866 | Running loss: 0.01043\n",
            "Epoch: 197 | Iteration: 71 | Classification loss: 0.00007 | Regression loss: 0.00583 | Running loss: 0.01041\n",
            "Epoch: 197 | Iteration: 72 | Classification loss: 0.00001 | Regression loss: 0.00833 | Running loss: 0.01040\n",
            "Epoch: 197 | Iteration: 73 | Classification loss: 0.00001 | Regression loss: 0.00957 | Running loss: 0.01039\n",
            "Epoch: 197 | Iteration: 74 | Classification loss: 0.00000 | Regression loss: 0.00757 | Running loss: 0.01038\n",
            "Epoch: 197 | Iteration: 75 | Classification loss: 0.00001 | Regression loss: 0.00404 | Running loss: 0.01038\n",
            "Epoch: 197 | Iteration: 76 | Classification loss: 0.00001 | Regression loss: 0.00570 | Running loss: 0.01036\n",
            "Epoch: 197 | Iteration: 77 | Classification loss: 0.00002 | Regression loss: 0.00923 | Running loss: 0.01037\n",
            "Epoch: 197 | Iteration: 78 | Classification loss: 0.00001 | Regression loss: 0.01376 | Running loss: 0.01039\n",
            "Epoch: 197 | Iteration: 79 | Classification loss: 0.00001 | Regression loss: 0.01018 | Running loss: 0.01038\n",
            "Epoch: 197 | Iteration: 80 | Classification loss: 0.00001 | Regression loss: 0.02052 | Running loss: 0.01039\n",
            "Epoch: 197 | Iteration: 81 | Classification loss: 0.00001 | Regression loss: 0.00348 | Running loss: 0.01038\n",
            "Epoch: 197 | Iteration: 82 | Classification loss: 0.00002 | Regression loss: 0.00778 | Running loss: 0.01038\n",
            "Epoch: 197 | Iteration: 83 | Classification loss: 0.00000 | Regression loss: 0.00645 | Running loss: 0.01035\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.7126551296496374\n",
            "Precision:  0.5464190981432361\n",
            "Recall:  0.8142292490118577\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}, {0: (0.7314384456550078, 253.0)}, {0: (0.7334466242447818, 253.0)}, {0: (0.7357024222917343, 253.0)}, {0: (0.7381550871546381, 253.0)}, {0: (0.739092434551302, 253.0)}, {0: (0.72091448182607, 253.0)}, {0: (0.7389512050165785, 253.0)}, {0: (0.73050932626622, 253.0)}, {0: (0.7263226120133723, 253.0)}, {0: (0.7283463725823673, 253.0)}, {0: (0.7254913772539314, 253.0)}, {0: (0.7281179290861395, 253.0)}, {0: (0.7260020498565122, 253.0)}, {0: (0.7217236108381175, 253.0)}, {0: (0.7267514829312511, 253.0)}, {0: (0.7227483652714889, 253.0)}, {0: (0.7241298168882813, 253.0)}, {0: (0.7261051666397325, 253.0)}, {0: (0.7277246704086413, 253.0)}, {0: (0.7259496886680885, 253.0)}, {0: (0.7290251963867072, 253.0)}, {0: (0.7242894157206492, 253.0)}, {0: (0.7329303843522665, 253.0)}, {0: (0.733154886135177, 253.0)}, {0: (0.7291842105419786, 253.0)}, {0: (0.7251405627702188, 253.0)}, {0: (0.731154087710585, 253.0)}, {0: (0.7317124239374072, 253.0)}, {0: (0.7263717757478444, 253.0)}, {0: (0.7278912613628667, 253.0)}, {0: (0.7330251175928468, 253.0)}, {0: (0.7278399291817672, 253.0)}, {0: (0.728608785848319, 253.0)}, {0: (0.7294166027957658, 253.0)}, {0: (0.7158140975435932, 253.0)}, {0: (0.7230740429361691, 253.0)}, {0: (0.7241983182897731, 253.0)}, {0: (0.7227977326660247, 253.0)}, {0: (0.7142649604059264, 253.0)}, {0: (0.721965827885861, 253.0)}, {0: (0.7150808407318003, 253.0)}, {0: (0.7180015690245183, 253.0)}, {0: (0.7117719784228915, 253.0)}, {0: (0.7126551296496374, 253.0)}]\n",
            "Epoch: 198 | Iteration: 0 | Classification loss: 0.00004 | Regression loss: 0.01525 | Running loss: 0.01036\n",
            "Epoch: 198 | Iteration: 1 | Classification loss: 0.00001 | Regression loss: 0.00334 | Running loss: 0.01034\n",
            "Epoch: 198 | Iteration: 2 | Classification loss: 0.00003 | Regression loss: 0.00431 | Running loss: 0.01032\n",
            "Epoch: 198 | Iteration: 3 | Classification loss: 0.00004 | Regression loss: 0.03915 | Running loss: 0.01038\n",
            "Epoch: 198 | Iteration: 4 | Classification loss: 0.00001 | Regression loss: 0.00885 | Running loss: 0.01039\n",
            "Epoch: 198 | Iteration: 5 | Classification loss: 0.00002 | Regression loss: 0.00965 | Running loss: 0.01040\n",
            "Epoch: 198 | Iteration: 6 | Classification loss: 0.00001 | Regression loss: 0.02569 | Running loss: 0.01042\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 190.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 198 | Iteration: 8 | Classification loss: 0.00001 | Regression loss: 0.01387 | Running loss: 0.01042\n",
            "Epoch: 198 | Iteration: 9 | Classification loss: 0.00001 | Regression loss: 0.00310 | Running loss: 0.01042\n",
            "Epoch: 198 | Iteration: 10 | Classification loss: 0.00000 | Regression loss: 0.00616 | Running loss: 0.01043\n",
            "Epoch: 198 | Iteration: 11 | Classification loss: 0.00001 | Regression loss: 0.00860 | Running loss: 0.01042\n",
            "Epoch: 198 | Iteration: 12 | Classification loss: 0.00002 | Regression loss: 0.00554 | Running loss: 0.01042\n",
            "Epoch: 198 | Iteration: 13 | Classification loss: 0.00001 | Regression loss: 0.00406 | Running loss: 0.01040\n",
            "Epoch: 198 | Iteration: 14 | Classification loss: 0.00001 | Regression loss: 0.01608 | Running loss: 0.01036\n",
            "Epoch: 198 | Iteration: 15 | Classification loss: 0.00001 | Regression loss: 0.00541 | Running loss: 0.01033\n",
            "Epoch: 198 | Iteration: 16 | Classification loss: 0.00001 | Regression loss: 0.00516 | Running loss: 0.01032\n",
            "Epoch: 198 | Iteration: 17 | Classification loss: 0.00001 | Regression loss: 0.00222 | Running loss: 0.01030\n",
            "Epoch: 198 | Iteration: 18 | Classification loss: 0.00001 | Regression loss: 0.00499 | Running loss: 0.01030\n",
            "Epoch: 198 | Iteration: 19 | Classification loss: 0.00001 | Regression loss: 0.02863 | Running loss: 0.01034\n",
            "Epoch: 198 | Iteration: 20 | Classification loss: 0.00000 | Regression loss: 0.00385 | Running loss: 0.01033\n",
            "Epoch: 198 | Iteration: 21 | Classification loss: 0.00000 | Regression loss: 0.00426 | Running loss: 0.01032\n",
            "Epoch: 198 | Iteration: 22 | Classification loss: 0.00001 | Regression loss: 0.00539 | Running loss: 0.01032\n",
            "Epoch: 198 | Iteration: 23 | Classification loss: 0.00001 | Regression loss: 0.00486 | Running loss: 0.01032\n",
            "Epoch: 198 | Iteration: 24 | Classification loss: 0.00002 | Regression loss: 0.01260 | Running loss: 0.01033\n",
            "Epoch: 198 | Iteration: 25 | Classification loss: 0.00001 | Regression loss: 0.01668 | Running loss: 0.01035\n",
            "Epoch: 198 | Iteration: 26 | Classification loss: 0.00001 | Regression loss: 0.01315 | Running loss: 0.01037\n",
            "Epoch: 198 | Iteration: 27 | Classification loss: 0.00001 | Regression loss: 0.01510 | Running loss: 0.01038\n",
            "Epoch: 198 | Iteration: 28 | Classification loss: 0.00000 | Regression loss: 0.00752 | Running loss: 0.01036\n",
            "Epoch: 198 | Iteration: 29 | Classification loss: 0.00001 | Regression loss: 0.01005 | Running loss: 0.01036\n",
            "Epoch: 198 | Iteration: 30 | Classification loss: 0.00001 | Regression loss: 0.00518 | Running loss: 0.01035\n",
            "Epoch: 198 | Iteration: 31 | Classification loss: 0.00001 | Regression loss: 0.01196 | Running loss: 0.01035\n",
            "Epoch: 198 | Iteration: 32 | Classification loss: 0.00002 | Regression loss: 0.01184 | Running loss: 0.01035\n",
            "Epoch: 198 | Iteration: 33 | Classification loss: 0.00002 | Regression loss: 0.01158 | Running loss: 0.01036\n",
            "Epoch: 198 | Iteration: 34 | Classification loss: 0.00001 | Regression loss: 0.02345 | Running loss: 0.01040\n",
            "Epoch: 198 | Iteration: 35 | Classification loss: 0.00001 | Regression loss: 0.00828 | Running loss: 0.01038\n",
            "Epoch: 198 | Iteration: 36 | Classification loss: 0.00002 | Regression loss: 0.00690 | Running loss: 0.01038\n",
            "Epoch: 198 | Iteration: 37 | Classification loss: 0.00002 | Regression loss: 0.01179 | Running loss: 0.01038\n",
            "Epoch: 198 | Iteration: 38 | Classification loss: 0.00001 | Regression loss: 0.01539 | Running loss: 0.01038\n",
            "Epoch: 198 | Iteration: 39 | Classification loss: 0.00002 | Regression loss: 0.01310 | Running loss: 0.01037\n",
            "Epoch: 198 | Iteration: 40 | Classification loss: 0.00003 | Regression loss: 0.01186 | Running loss: 0.01037\n",
            "Epoch: 198 | Iteration: 41 | Classification loss: 0.00003 | Regression loss: 0.01452 | Running loss: 0.01039\n",
            "Epoch: 198 | Iteration: 42 | Classification loss: 0.00001 | Regression loss: 0.01233 | Running loss: 0.01041\n",
            "Epoch: 198 | Iteration: 43 | Classification loss: 0.00002 | Regression loss: 0.00889 | Running loss: 0.01040\n",
            "Epoch: 198 | Iteration: 44 | Classification loss: 0.00003 | Regression loss: 0.01005 | Running loss: 0.01040\n",
            "Epoch: 198 | Iteration: 45 | Classification loss: 0.00001 | Regression loss: 0.00334 | Running loss: 0.01038\n",
            "Epoch: 198 | Iteration: 46 | Classification loss: 0.00001 | Regression loss: 0.00580 | Running loss: 0.01038\n",
            "Epoch: 198 | Iteration: 47 | Classification loss: 0.00001 | Regression loss: 0.00545 | Running loss: 0.01037\n",
            "Epoch: 198 | Iteration: 48 | Classification loss: 0.00007 | Regression loss: 0.00556 | Running loss: 0.01036\n",
            "Epoch: 198 | Iteration: 49 | Classification loss: 0.00001 | Regression loss: 0.00555 | Running loss: 0.01035\n",
            "Epoch: 198 | Iteration: 50 | Classification loss: 0.00002 | Regression loss: 0.02275 | Running loss: 0.01038\n",
            "Epoch: 198 | Iteration: 51 | Classification loss: 0.00001 | Regression loss: 0.00972 | Running loss: 0.01039\n",
            "Epoch: 198 | Iteration: 52 | Classification loss: 0.00004 | Regression loss: 0.00979 | Running loss: 0.01040\n",
            "Epoch: 198 | Iteration: 53 | Classification loss: 0.00001 | Regression loss: 0.01427 | Running loss: 0.01041\n",
            "Epoch: 198 | Iteration: 54 | Classification loss: 0.00001 | Regression loss: 0.01141 | Running loss: 0.01041\n",
            "Epoch: 198 | Iteration: 55 | Classification loss: 0.00001 | Regression loss: 0.00873 | Running loss: 0.01041\n",
            "Epoch: 198 | Iteration: 56 | Classification loss: 0.00003 | Regression loss: 0.01903 | Running loss: 0.01042\n",
            "Epoch: 198 | Iteration: 57 | Classification loss: 0.00006 | Regression loss: 0.01285 | Running loss: 0.01042\n",
            "Epoch: 198 | Iteration: 58 | Classification loss: 0.00001 | Regression loss: 0.00885 | Running loss: 0.01041\n",
            "Epoch: 198 | Iteration: 59 | Classification loss: 0.00000 | Regression loss: 0.00288 | Running loss: 0.01040\n",
            "Epoch: 198 | Iteration: 60 | Classification loss: 0.00001 | Regression loss: 0.00930 | Running loss: 0.01040\n",
            "Epoch: 198 | Iteration: 61 | Classification loss: 0.00001 | Regression loss: 0.00941 | Running loss: 0.01041\n",
            "Epoch: 198 | Iteration: 62 | Classification loss: 0.00000 | Regression loss: 0.00433 | Running loss: 0.01041\n",
            "Epoch: 198 | Iteration: 63 | Classification loss: 0.00001 | Regression loss: 0.01158 | Running loss: 0.01041\n",
            "Epoch: 198 | Iteration: 64 | Classification loss: 0.00001 | Regression loss: 0.01436 | Running loss: 0.01038\n",
            "Epoch: 198 | Iteration: 65 | Classification loss: 0.00001 | Regression loss: 0.00906 | Running loss: 0.01036\n",
            "Epoch: 198 | Iteration: 66 | Classification loss: 0.00002 | Regression loss: 0.00612 | Running loss: 0.01035\n",
            "Epoch: 198 | Iteration: 67 | Classification loss: 0.00001 | Regression loss: 0.00965 | Running loss: 0.01035\n",
            "Epoch: 198 | Iteration: 68 | Classification loss: 0.00000 | Regression loss: 0.00266 | Running loss: 0.01033\n",
            "Epoch: 198 | Iteration: 69 | Classification loss: 0.00002 | Regression loss: 0.00782 | Running loss: 0.01034\n",
            "Epoch: 198 | Iteration: 70 | Classification loss: 0.00003 | Regression loss: 0.01403 | Running loss: 0.01031\n",
            "Epoch: 198 | Iteration: 71 | Classification loss: 0.00002 | Regression loss: 0.01094 | Running loss: 0.01031\n",
            "Epoch: 198 | Iteration: 72 | Classification loss: 0.00001 | Regression loss: 0.01035 | Running loss: 0.01031\n",
            "Epoch: 198 | Iteration: 73 | Classification loss: 0.00001 | Regression loss: 0.00842 | Running loss: 0.01032\n",
            "Epoch: 198 | Iteration: 74 | Classification loss: 0.00001 | Regression loss: 0.00824 | Running loss: 0.01033\n",
            "Epoch: 198 | Iteration: 75 | Classification loss: 0.00001 | Regression loss: 0.00308 | Running loss: 0.01031\n",
            "Epoch: 198 | Iteration: 76 | Classification loss: 0.00001 | Regression loss: 0.00904 | Running loss: 0.01030\n",
            "Epoch: 198 | Iteration: 77 | Classification loss: 0.00000 | Regression loss: 0.00507 | Running loss: 0.01029\n",
            "Epoch: 198 | Iteration: 78 | Classification loss: 0.00002 | Regression loss: 0.01549 | Running loss: 0.01031\n",
            "Epoch: 198 | Iteration: 79 | Classification loss: 0.00001 | Regression loss: 0.00989 | Running loss: 0.01032\n",
            "Epoch: 198 | Iteration: 80 | Classification loss: 0.00001 | Regression loss: 0.00367 | Running loss: 0.01029\n",
            "Epoch: 198 | Iteration: 81 | Classification loss: 0.00001 | Regression loss: 0.00333 | Running loss: 0.01025\n",
            "Epoch: 198 | Iteration: 82 | Classification loss: 0.00002 | Regression loss: 0.00851 | Running loss: 0.01026\n",
            "Epoch: 198 | Iteration: 83 | Classification loss: 0.00002 | Regression loss: 0.01110 | Running loss: 0.01025\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.7172773730393486\n",
            "Precision:  0.5421052631578948\n",
            "Recall:  0.8142292490118577\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}, {0: (0.7314384456550078, 253.0)}, {0: (0.7334466242447818, 253.0)}, {0: (0.7357024222917343, 253.0)}, {0: (0.7381550871546381, 253.0)}, {0: (0.739092434551302, 253.0)}, {0: (0.72091448182607, 253.0)}, {0: (0.7389512050165785, 253.0)}, {0: (0.73050932626622, 253.0)}, {0: (0.7263226120133723, 253.0)}, {0: (0.7283463725823673, 253.0)}, {0: (0.7254913772539314, 253.0)}, {0: (0.7281179290861395, 253.0)}, {0: (0.7260020498565122, 253.0)}, {0: (0.7217236108381175, 253.0)}, {0: (0.7267514829312511, 253.0)}, {0: (0.7227483652714889, 253.0)}, {0: (0.7241298168882813, 253.0)}, {0: (0.7261051666397325, 253.0)}, {0: (0.7277246704086413, 253.0)}, {0: (0.7259496886680885, 253.0)}, {0: (0.7290251963867072, 253.0)}, {0: (0.7242894157206492, 253.0)}, {0: (0.7329303843522665, 253.0)}, {0: (0.733154886135177, 253.0)}, {0: (0.7291842105419786, 253.0)}, {0: (0.7251405627702188, 253.0)}, {0: (0.731154087710585, 253.0)}, {0: (0.7317124239374072, 253.0)}, {0: (0.7263717757478444, 253.0)}, {0: (0.7278912613628667, 253.0)}, {0: (0.7330251175928468, 253.0)}, {0: (0.7278399291817672, 253.0)}, {0: (0.728608785848319, 253.0)}, {0: (0.7294166027957658, 253.0)}, {0: (0.7158140975435932, 253.0)}, {0: (0.7230740429361691, 253.0)}, {0: (0.7241983182897731, 253.0)}, {0: (0.7227977326660247, 253.0)}, {0: (0.7142649604059264, 253.0)}, {0: (0.721965827885861, 253.0)}, {0: (0.7150808407318003, 253.0)}, {0: (0.7180015690245183, 253.0)}, {0: (0.7117719784228915, 253.0)}, {0: (0.7126551296496374, 253.0)}, {0: (0.7172773730393486, 253.0)}]\n",
            "Epoch: 199 | Iteration: 0 | Classification loss: 0.00001 | Regression loss: 0.00550 | Running loss: 0.01024\n",
            "Epoch: 199 | Iteration: 1 | Classification loss: 0.00002 | Regression loss: 0.00333 | Running loss: 0.01020\n",
            "Epoch: 199 | Iteration: 2 | Classification loss: 0.00001 | Regression loss: 0.00320 | Running loss: 0.01020\n",
            "Epoch: 199 | Iteration: 3 | Classification loss: 0.00000 | Regression loss: 0.00361 | Running loss: 0.01019\n",
            "Epoch: 199 | Iteration: 4 | Classification loss: 0.00001 | Regression loss: 0.00968 | Running loss: 0.01020\n",
            "Epoch: 199 | Iteration: 5 | Classification loss: 0.00003 | Regression loss: 0.01179 | Running loss: 0.01020\n",
            "Epoch: 199 | Iteration: 6 | Classification loss: 0.00002 | Regression loss: 0.01074 | Running loss: 0.01020\n",
            "Epoch: 199 | Iteration: 7 | Classification loss: 0.00001 | Regression loss: 0.02784 | Running loss: 0.01022\n",
            "Epoch: 199 | Iteration: 8 | Classification loss: 0.00001 | Regression loss: 0.01108 | Running loss: 0.01022\n",
            "Epoch: 199 | Iteration: 9 | Classification loss: 0.00001 | Regression loss: 0.00992 | Running loss: 0.01023\n",
            "Epoch: 199 | Iteration: 10 | Classification loss: 0.00001 | Regression loss: 0.00823 | Running loss: 0.01023\n",
            "Epoch: 199 | Iteration: 11 | Classification loss: 0.00000 | Regression loss: 0.00264 | Running loss: 0.01020\n",
            "Epoch: 199 | Iteration: 12 | Classification loss: 0.00001 | Regression loss: 0.00914 | Running loss: 0.01020\n",
            "Epoch: 199 | Iteration: 13 | Classification loss: 0.00001 | Regression loss: 0.01018 | Running loss: 0.01021\n",
            "Epoch: 199 | Iteration: 14 | Classification loss: 0.00001 | Regression loss: 0.01516 | Running loss: 0.01023\n",
            "Epoch: 199 | Iteration: 15 | Classification loss: 0.00001 | Regression loss: 0.02572 | Running loss: 0.01025\n",
            "Epoch: 199 | Iteration: 16 | Classification loss: 0.00001 | Regression loss: 0.01058 | Running loss: 0.01025\n",
            "Epoch: 199 | Iteration: 17 | Classification loss: 0.00001 | Regression loss: 0.00517 | Running loss: 0.01023\n",
            "Epoch: 199 | Iteration: 18 | Classification loss: 0.00000 | Regression loss: 0.00371 | Running loss: 0.01022\n",
            "Epoch: 199 | Iteration: 19 | Classification loss: 0.00001 | Regression loss: 0.01455 | Running loss: 0.01025\n",
            "Epoch: 199 | Iteration: 20 | Classification loss: 0.00001 | Regression loss: 0.01303 | Running loss: 0.01024\n",
            "Epoch: 199 | Iteration: 21 | Classification loss: 0.00001 | Regression loss: 0.01088 | Running loss: 0.01025\n",
            "Epoch: 199 | Iteration: 22 | Classification loss: 0.00001 | Regression loss: 0.00946 | Running loss: 0.01026\n",
            "Epoch: 199 | Iteration: 23 | Classification loss: 0.00003 | Regression loss: 0.00956 | Running loss: 0.01027\n",
            "Epoch: 199 | Iteration: 24 | Classification loss: 0.00001 | Regression loss: 0.00887 | Running loss: 0.01026\n",
            "Epoch: 199 | Iteration: 25 | Classification loss: 0.00001 | Regression loss: 0.00889 | Running loss: 0.01028\n",
            "Epoch: 199 | Iteration: 26 | Classification loss: 0.00001 | Regression loss: 0.00503 | Running loss: 0.01028\n",
            "Epoch: 199 | Iteration: 27 | Classification loss: 0.00001 | Regression loss: 0.01336 | Running loss: 0.01028\n",
            "Epoch: 199 | Iteration: 28 | Classification loss: 0.00001 | Regression loss: 0.00304 | Running loss: 0.01027\n",
            "Epoch: 199 | Iteration: 29 | Classification loss: 0.00002 | Regression loss: 0.01915 | Running loss: 0.01028\n",
            "Epoch: 199 | Iteration: 30 | Classification loss: 0.00005 | Regression loss: 0.01524 | Running loss: 0.01029\n",
            "Epoch: 199 | Iteration: 31 | Classification loss: 0.00001 | Regression loss: 0.00569 | Running loss: 0.01029\n",
            "Epoch: 199 | Iteration: 32 | Classification loss: 0.00002 | Regression loss: 0.00546 | Running loss: 0.01028\n",
            "Epoch: 199 | Iteration: 33 | Classification loss: 0.00001 | Regression loss: 0.00827 | Running loss: 0.01029\n",
            "Epoch: 199 | Iteration: 34 | Classification loss: 0.00001 | Regression loss: 0.00401 | Running loss: 0.01028\n",
            "Epoch: 199 | Iteration: 35 | Classification loss: 0.00001 | Regression loss: 0.01595 | Running loss: 0.01029\n",
            "Epoch: 199 | Iteration: 36 | Classification loss: 0.00001 | Regression loss: 0.01580 | Running loss: 0.01029\n",
            "Epoch: 199 | Iteration: 37 | Classification loss: 0.00001 | Regression loss: 0.00383 | Running loss: 0.01027\n",
            "Epoch: 199 | Iteration: 38 | Classification loss: 0.00001 | Regression loss: 0.00481 | Running loss: 0.01026\n",
            "Epoch: 199 | Iteration: 39 | Classification loss: 0.00001 | Regression loss: 0.00316 | Running loss: 0.01021\n",
            "Epoch: 199 | Iteration: 40 | Classification loss: 0.00002 | Regression loss: 0.00754 | Running loss: 0.01019\n",
            "Epoch: 199 | Iteration: 41 | Classification loss: 0.00001 | Regression loss: 0.01248 | Running loss: 0.01020\n",
            "Epoch: 199 | Iteration: 42 | Classification loss: 0.00001 | Regression loss: 0.00589 | Running loss: 0.01019\n",
            "Epoch: 199 | Iteration: 43 | Classification loss: 0.00001 | Regression loss: 0.00978 | Running loss: 0.01020\n",
            "Epoch: 199 | Iteration: 44 | Classification loss: 0.00001 | Regression loss: 0.02139 | Running loss: 0.01019\n",
            "Epoch: 199 | Iteration: 45 | Classification loss: 0.00001 | Regression loss: 0.01224 | Running loss: 0.01020\n",
            "Epoch: 199 | Iteration: 46 | Classification loss: 0.00002 | Regression loss: 0.00795 | Running loss: 0.01020\n",
            "Epoch: 199 | Iteration: 47 | Classification loss: 0.00008 | Regression loss: 0.00575 | Running loss: 0.01020\n",
            "Epoch: 199 | Iteration: 48 | Classification loss: 0.00001 | Regression loss: 0.00725 | Running loss: 0.01019\n",
            "Epoch: 199 | Iteration: 49 | Classification loss: 0.00001 | Regression loss: 0.01150 | Running loss: 0.01019\n",
            "Epoch: 199 | Iteration: 50 | Classification loss: 0.00001 | Regression loss: 0.00348 | Running loss: 0.01017\n",
            "Epoch: 199 | Iteration: 51 | Classification loss: 0.00000 | Regression loss: 0.00755 | Running loss: 0.01017\n",
            "Epoch: 199 | Iteration: 52 | Classification loss: 0.00003 | Regression loss: 0.00870 | Running loss: 0.01018\n",
            "Epoch: 199 | Iteration: 53 | Classification loss: 0.00002 | Regression loss: 0.01153 | Running loss: 0.01016\n",
            "CUDA out of memory. Tried to allocate 274.00 MiB (GPU 0; 15.77 GiB total capacity; 12.78 GiB already allocated; 188.12 MiB free; 14.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Epoch: 199 | Iteration: 55 | Classification loss: 0.00001 | Regression loss: 0.00468 | Running loss: 0.01015\n",
            "Epoch: 199 | Iteration: 56 | Classification loss: 0.00001 | Regression loss: 0.00573 | Running loss: 0.01016\n",
            "Epoch: 199 | Iteration: 57 | Classification loss: 0.00002 | Regression loss: 0.00878 | Running loss: 0.01016\n",
            "Epoch: 199 | Iteration: 58 | Classification loss: 0.00001 | Regression loss: 0.01901 | Running loss: 0.01018\n",
            "Epoch: 199 | Iteration: 59 | Classification loss: 0.00001 | Regression loss: 0.00220 | Running loss: 0.01016\n",
            "Epoch: 199 | Iteration: 60 | Classification loss: 0.00001 | Regression loss: 0.00940 | Running loss: 0.01017\n",
            "Epoch: 199 | Iteration: 61 | Classification loss: 0.00002 | Regression loss: 0.00826 | Running loss: 0.01018\n",
            "Epoch: 199 | Iteration: 62 | Classification loss: 0.00001 | Regression loss: 0.00542 | Running loss: 0.01016\n",
            "Epoch: 199 | Iteration: 63 | Classification loss: 0.00000 | Regression loss: 0.00395 | Running loss: 0.01015\n",
            "Epoch: 199 | Iteration: 64 | Classification loss: 0.00001 | Regression loss: 0.01029 | Running loss: 0.01013\n",
            "Epoch: 199 | Iteration: 65 | Classification loss: 0.00001 | Regression loss: 0.00502 | Running loss: 0.01011\n",
            "Epoch: 199 | Iteration: 66 | Classification loss: 0.00003 | Regression loss: 0.01063 | Running loss: 0.01011\n",
            "Epoch: 199 | Iteration: 67 | Classification loss: 0.00002 | Regression loss: 0.01197 | Running loss: 0.01013\n",
            "Epoch: 199 | Iteration: 68 | Classification loss: 0.00002 | Regression loss: 0.01072 | Running loss: 0.01013\n",
            "Epoch: 199 | Iteration: 69 | Classification loss: 0.00005 | Regression loss: 0.03801 | Running loss: 0.01018\n",
            "Epoch: 199 | Iteration: 70 | Classification loss: 0.00002 | Regression loss: 0.01127 | Running loss: 0.01020\n",
            "Epoch: 199 | Iteration: 71 | Classification loss: 0.00001 | Regression loss: 0.00950 | Running loss: 0.01020\n",
            "Epoch: 199 | Iteration: 72 | Classification loss: 0.00005 | Regression loss: 0.01139 | Running loss: 0.01021\n",
            "Epoch: 199 | Iteration: 73 | Classification loss: 0.00002 | Regression loss: 0.01325 | Running loss: 0.01023\n",
            "Epoch: 199 | Iteration: 74 | Classification loss: 0.00001 | Regression loss: 0.00977 | Running loss: 0.01022\n",
            "Epoch: 199 | Iteration: 75 | Classification loss: 0.00000 | Regression loss: 0.00287 | Running loss: 0.01022\n",
            "Epoch: 199 | Iteration: 76 | Classification loss: 0.00002 | Regression loss: 0.00974 | Running loss: 0.01023\n",
            "Epoch: 199 | Iteration: 77 | Classification loss: 0.00002 | Regression loss: 0.01409 | Running loss: 0.01024\n",
            "Epoch: 199 | Iteration: 78 | Classification loss: 0.00000 | Regression loss: 0.00436 | Running loss: 0.01023\n",
            "Epoch: 199 | Iteration: 79 | Classification loss: 0.00000 | Regression loss: 0.00643 | Running loss: 0.01022\n",
            "Epoch: 199 | Iteration: 80 | Classification loss: 0.00002 | Regression loss: 0.01436 | Running loss: 0.01023\n",
            "Epoch: 199 | Iteration: 81 | Classification loss: 0.00002 | Regression loss: 0.00795 | Running loss: 0.01022\n",
            "Epoch: 199 | Iteration: 82 | Classification loss: 0.00001 | Regression loss: 0.00964 | Running loss: 0.01022\n",
            "Epoch: 199 | Iteration: 83 | Classification loss: 0.00001 | Regression loss: 0.01581 | Running loss: 0.01021\n",
            "Evaluating dataset\n",
            "126/126\n",
            "mAP:\n",
            "person: 0.7159214919004002\n",
            "Precision:  0.544973544973545\n",
            "Recall:  0.8142292490118577\n",
            "mAps: [{0: (0.32946288931223194, 253.0)}, {0: (0.49363345903476397, 253.0)}, {0: (0.5714747329766365, 253.0)}, {0: (0.6133063608029532, 253.0)}, {0: (0.6514572250606802, 253.0)}, {0: (0.6493901380120624, 253.0)}, {0: (0.6544162422764787, 253.0)}, {0: (0.691230401765245, 253.0)}, {0: (0.6731952170278043, 253.0)}, {0: (0.6940797231598989, 253.0)}, {0: (0.686643146262702, 253.0)}, {0: (0.6963635827909593, 253.0)}, {0: (0.697120050905158, 253.0)}, {0: (0.6962259132860424, 253.0)}, {0: (0.7037310599066398, 253.0)}, {0: (0.7159296675216338, 253.0)}, {0: (0.7304426066725824, 253.0)}, {0: (0.7093822418520426, 253.0)}, {0: (0.699131850242804, 253.0)}, {0: (0.7175262189452377, 253.0)}, {0: (0.7095247018763102, 253.0)}, {0: (0.7215295575625227, 253.0)}, {0: (0.7120003797952823, 253.0)}, {0: (0.7189707063846775, 253.0)}, {0: (0.7020855442350151, 253.0)}, {0: (0.7351958095148037, 253.0)}, {0: (0.7235725854644546, 253.0)}, {0: (0.7399312008523232, 253.0)}, {0: (0.7301704265801439, 253.0)}, {0: (0.7296727668008861, 253.0)}, {0: (0.7210065734187271, 253.0)}, {0: (0.7075585714679445, 253.0)}, {0: (0.7375232897806944, 253.0)}, {0: (0.7299911919853038, 253.0)}, {0: (0.7317444112345507, 253.0)}, {0: (0.7331367506092994, 253.0)}, {0: (0.7238733126390802, 253.0)}, {0: (0.7134796544732205, 253.0)}, {0: (0.7336735457542756, 253.0)}, {0: (0.744306591553658, 253.0)}, {0: (0.7282091502220764, 253.0)}, {0: (0.7353390680805811, 253.0)}, {0: (0.7382253386870249, 253.0)}, {0: (0.7425037964203292, 253.0)}, {0: (0.7452258362472721, 253.0)}, {0: (0.7348848054276402, 253.0)}, {0: (0.728014303423373, 253.0)}, {0: (0.7528701635415589, 253.0)}, {0: (0.7448244983350581, 253.0)}, {0: (0.748344224795793, 253.0)}, {0: (0.747680573864466, 253.0)}, {0: (0.736956821380469, 253.0)}, {0: (0.7359117304221444, 253.0)}, {0: (0.7398808172919619, 253.0)}, {0: (0.7343542670856048, 253.0)}, {0: (0.758098710447562, 253.0)}, {0: (0.732914120683658, 253.0)}, {0: (0.7437106374100707, 253.0)}, {0: (0.7401776683317483, 253.0)}, {0: (0.7393807086148567, 253.0)}, {0: (0.7465487079334714, 253.0)}, {0: (0.7462122394197749, 253.0)}, {0: (0.7530293627601756, 253.0)}, {0: (0.7475502266674776, 253.0)}, {0: (0.7417616876034356, 253.0)}, {0: (0.7444302722357428, 253.0)}, {0: (0.7465665798821952, 253.0)}, {0: (0.7462854092323405, 253.0)}, {0: (0.7335529553836413, 253.0)}, {0: (0.7425353280489213, 253.0)}, {0: (0.7349326779260774, 253.0)}, {0: (0.7432855354093734, 253.0)}, {0: (0.7418792777631802, 253.0)}, {0: (0.7311009905400847, 253.0)}, {0: (0.7533663142754672, 253.0)}, {0: (0.750228541714743, 253.0)}, {0: (0.7344922936158023, 253.0)}, {0: (0.7430278747999977, 253.0)}, {0: (0.7286732032275327, 253.0)}, {0: (0.7603154024648144, 253.0)}, {0: (0.7446171420008312, 253.0)}, {0: (0.7404168720548546, 253.0)}, {0: (0.7523522667825502, 253.0)}, {0: (0.7498678094135993, 253.0)}, {0: (0.746972532321618, 253.0)}, {0: (0.7533825272467438, 253.0)}, {0: (0.7438379459708082, 253.0)}, {0: (0.7523779876513548, 253.0)}, {0: (0.7516321389864954, 253.0)}, {0: (0.7506974820043963, 253.0)}, {0: (0.755360688044558, 253.0)}, {0: (0.7480040576264128, 253.0)}, {0: (0.7598479149522945, 253.0)}, {0: (0.7448286192867686, 253.0)}, {0: (0.7277447496034266, 253.0)}, {0: (0.7495945217644377, 253.0)}, {0: (0.7469172035652272, 253.0)}, {0: (0.7662578583443979, 253.0)}, {0: (0.7567821815263825, 253.0)}, {0: (0.7523368164633716, 253.0)}, {0: (0.7527896866108441, 253.0)}, {0: (0.7449496444403327, 253.0)}, {0: (0.750889251901165, 253.0)}, {0: (0.7480714205192154, 253.0)}, {0: (0.7565839508630683, 253.0)}, {0: (0.7460715952807464, 253.0)}, {0: (0.7409973149105539, 253.0)}, {0: (0.7662011806888601, 253.0)}, {0: (0.7432101421394234, 253.0)}, {0: (0.751712771990717, 253.0)}, {0: (0.7575830581195757, 253.0)}, {0: (0.7396013059551989, 253.0)}, {0: (0.760864282590092, 253.0)}, {0: (0.7527777287176562, 253.0)}, {0: (0.7628563666026849, 253.0)}, {0: (0.7618033080849929, 253.0)}, {0: (0.7622627798284044, 253.0)}, {0: (0.7610324650806986, 253.0)}, {0: (0.7561985027739486, 253.0)}, {0: (0.7576731604677205, 253.0)}, {0: (0.7588493292681946, 253.0)}, {0: (0.755437519833976, 253.0)}, {0: (0.7558176211579406, 253.0)}, {0: (0.7583579103979197, 253.0)}, {0: (0.7561473360694084, 253.0)}, {0: (0.7581361585172005, 253.0)}, {0: (0.7562216963739097, 253.0)}, {0: (0.7574923196741643, 253.0)}, {0: (0.7567935435898404, 253.0)}, {0: (0.7563318501952667, 253.0)}, {0: (0.7540759401670909, 253.0)}, {0: (0.7529846568252176, 253.0)}, {0: (0.7545568710153958, 253.0)}, {0: (0.756597201444879, 253.0)}, {0: (0.755983897446338, 253.0)}, {0: (0.7534780602084326, 253.0)}, {0: (0.7525513258999209, 253.0)}, {0: (0.7508961491330797, 253.0)}, {0: (0.7490843281821495, 253.0)}, {0: (0.7553438189097572, 253.0)}, {0: (0.7566610039137575, 253.0)}, {0: (0.7483520147591369, 253.0)}, {0: (0.7546759970398451, 253.0)}, {0: (0.7539326322941468, 253.0)}, {0: (0.7539836677313965, 253.0)}, {0: (0.7561054217186449, 253.0)}, {0: (0.7488370631827469, 253.0)}, {0: (0.7433985536668353, 253.0)}, {0: (0.7385026345668402, 253.0)}, {0: (0.7452146140123328, 253.0)}, {0: (0.735336116655414, 253.0)}, {0: (0.7383163241711468, 253.0)}, {0: (0.733713296012671, 253.0)}, {0: (0.727926315626886, 253.0)}, {0: (0.7314384456550078, 253.0)}, {0: (0.7334466242447818, 253.0)}, {0: (0.7357024222917343, 253.0)}, {0: (0.7381550871546381, 253.0)}, {0: (0.739092434551302, 253.0)}, {0: (0.72091448182607, 253.0)}, {0: (0.7389512050165785, 253.0)}, {0: (0.73050932626622, 253.0)}, {0: (0.7263226120133723, 253.0)}, {0: (0.7283463725823673, 253.0)}, {0: (0.7254913772539314, 253.0)}, {0: (0.7281179290861395, 253.0)}, {0: (0.7260020498565122, 253.0)}, {0: (0.7217236108381175, 253.0)}, {0: (0.7267514829312511, 253.0)}, {0: (0.7227483652714889, 253.0)}, {0: (0.7241298168882813, 253.0)}, {0: (0.7261051666397325, 253.0)}, {0: (0.7277246704086413, 253.0)}, {0: (0.7259496886680885, 253.0)}, {0: (0.7290251963867072, 253.0)}, {0: (0.7242894157206492, 253.0)}, {0: (0.7329303843522665, 253.0)}, {0: (0.733154886135177, 253.0)}, {0: (0.7291842105419786, 253.0)}, {0: (0.7251405627702188, 253.0)}, {0: (0.731154087710585, 253.0)}, {0: (0.7317124239374072, 253.0)}, {0: (0.7263717757478444, 253.0)}, {0: (0.7278912613628667, 253.0)}, {0: (0.7330251175928468, 253.0)}, {0: (0.7278399291817672, 253.0)}, {0: (0.728608785848319, 253.0)}, {0: (0.7294166027957658, 253.0)}, {0: (0.7158140975435932, 253.0)}, {0: (0.7230740429361691, 253.0)}, {0: (0.7241983182897731, 253.0)}, {0: (0.7227977326660247, 253.0)}, {0: (0.7142649604059264, 253.0)}, {0: (0.721965827885861, 253.0)}, {0: (0.7150808407318003, 253.0)}, {0: (0.7180015690245183, 253.0)}, {0: (0.7117719784228915, 253.0)}, {0: (0.7126551296496374, 253.0)}, {0: (0.7172773730393486, 253.0)}, {0: (0.7159214919004002, 253.0)}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python custom_visualize.py --dataset csv --csv_classes /content/RetinaNET/class_list.csv  --csv_val /content/RetinaNET/Kaggle-Person-Detection-2/valid/_annotations.csv --model /content/RetinaNET/results/model_final.pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtLy2dk3Gw6d",
        "outputId": "d4c03a19-f418-4f94-9614-3e622bc29394"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "Elapsed time: 1.5620372295379639\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.03211069107055664\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.07252907752990723\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.062367916107177734\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.028536319732666016\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.03191089630126953\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.02873516082763672\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.05903935432434082\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.06272649765014648\n",
            "person\n",
            "Elapsed time: 0.029358625411987305\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.06630825996398926\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.028096437454223633\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.027932167053222656\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.033982038497924805\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.0519559383392334\n",
            "person\n",
            "Elapsed time: 0.055490732192993164\n",
            "person\n",
            "Elapsed time: 0.028610706329345703\n",
            "person\n",
            "Elapsed time: 0.028669357299804688\n",
            "person\n",
            "Elapsed time: 0.028499126434326172\n",
            "person\n",
            "Elapsed time: 0.028711557388305664\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.05465435981750488\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.027681350708007812\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.05523967742919922\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.03149127960205078\n",
            "person\n",
            "Elapsed time: 0.027724504470825195\n",
            "person\n",
            "Elapsed time: 0.029984235763549805\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.028547048568725586\n",
            "person\n",
            "Elapsed time: 0.027501821517944336\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.05387759208679199\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.028533220291137695\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.028191089630126953\n",
            "person\n",
            "Elapsed time: 0.05702686309814453\n",
            "person\n",
            "Elapsed time: 0.029021739959716797\n",
            "person\n",
            "Elapsed time: 0.06690573692321777\n",
            "person\n",
            "Elapsed time: 0.02791762351989746\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.030812501907348633\n",
            "person\n",
            "Elapsed time: 0.028599262237548828\n",
            "person\n",
            "Elapsed time: 0.02969503402709961\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.03073906898498535\n",
            "person\n",
            "Elapsed time: 0.02800583839416504\n",
            "person\n",
            "Elapsed time: 0.029567480087280273\n",
            "person\n",
            "Elapsed time: 0.027986526489257812\n",
            "person\n",
            "Elapsed time: 0.027628660202026367\n",
            "Elapsed time: 0.027413368225097656\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.0287477970123291\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.031726837158203125\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.027773618698120117\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.052994728088378906\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.027761220932006836\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.028713226318359375\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.027784109115600586\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.023925304412841797\n",
            "person\n",
            "Elapsed time: 0.027607440948486328\n",
            "person\n",
            "Elapsed time: 0.029202699661254883\n",
            "person\n",
            "Elapsed time: 0.027767181396484375\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.02942824363708496\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.0284116268157959\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.05689096450805664\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.02850055694580078\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.027909040451049805\n",
            "person\n",
            "Elapsed time: 0.06311607360839844\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.02927994728088379\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.03105306625366211\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.02962040901184082\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.0293276309967041\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.0312650203704834\n",
            "Elapsed time: 0.059674978256225586\n",
            "person\n",
            "Elapsed time: 0.028626203536987305\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.02972555160522461\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.030288219451904297\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.04646420478820801\n",
            "person\n",
            "Elapsed time: 0.027625560760498047\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.02936577796936035\n",
            "person\n",
            "Elapsed time: 0.027869701385498047\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.02906513214111328\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.029391050338745117\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.031768798828125\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.028105497360229492\n",
            "person\n",
            "Elapsed time: 0.028890132904052734\n",
            "person\n",
            "Elapsed time: 0.030707836151123047\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.05545973777770996\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.028787612915039062\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.029588699340820312\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.03146696090698242\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.02920985221862793\n",
            "person\n",
            "Elapsed time: 0.028850317001342773\n",
            "person\n",
            "Elapsed time: 0.02874302864074707\n",
            "person\n",
            "Elapsed time: 0.027906417846679688\n",
            "person\n",
            "Elapsed time: 0.0279390811920166\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.030039548873901367\n",
            "person\n",
            "Elapsed time: 0.025832176208496094\n",
            "person\n",
            "Elapsed time: 0.027629852294921875\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.031732797622680664\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.02865767478942871\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.028687238693237305\n",
            "person\n",
            "Elapsed time: 0.031087160110473633\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.030625343322753906\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.029303312301635742\n",
            "person\n",
            "Elapsed time: 0.02856731414794922\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.027667522430419922\n",
            "person\n",
            "Elapsed time: 0.030694246292114258\n",
            "person\n",
            "Elapsed time: 0.02878570556640625\n",
            "person\n",
            "Elapsed time: 0.02764892578125\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.0314633846282959\n",
            "person\n",
            "Elapsed time: 0.029529094696044922\n",
            "person\n",
            "Elapsed time: 0.029837608337402344\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.02879810333251953\n",
            "person\n",
            "Elapsed time: 0.02788066864013672\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.029973983764648438\n",
            "person\n",
            "Elapsed time: 0.03162193298339844\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.029464006423950195\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.028768301010131836\n",
            "person\n",
            "Elapsed time: 0.02843785285949707\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.029571056365966797\n",
            "person\n",
            "Elapsed time: 0.02965545654296875\n",
            "person\n",
            "Elapsed time: 0.025821447372436523\n",
            "person\n",
            "Elapsed time: 0.028285503387451172\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.029651880264282227\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.028469085693359375\n",
            "person\n",
            "Elapsed time: 0.02966475486755371\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.05856728553771973\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.027869224548339844\n",
            "person\n",
            "Elapsed time: 0.030711650848388672\n",
            "person\n",
            "person\n",
            "person\n",
            "Elapsed time: 0.028661727905273438\n",
            "Elapsed time: 0.030600547790527344\n",
            "person\n",
            "Elapsed time: 0.028183460235595703\n",
            "person\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XmY5X7ecG36H"
      }
    }
  ]
}